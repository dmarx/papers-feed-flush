{
  "snapshot_time": "2025-04-16T04:44:28.898160+00:00",
  "repository": "dmarx/papers-feed",
  "objects": {
    "interactions:2310.16410": {
      "data": {
        "paper_id": "2310.16410",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-24T08:29:08.482Z",
            "data": {
              "session_id": "session_1737707335861_ub76jpd",
              "duration_seconds": 7,
              "idle_seconds": 0,
              "start_time": "2025-01-24T08:28:55.861Z",
              "end_time": "2025-01-24T08:29:02.492Z",
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-24T08:29:03+00:00",
        "updated_at": "2025-01-24T08:29:27+00:00",
        "version": 5
      }
    },
    "paper:2310.16410": {
      "data": {
        "arxivId": "2310.16410",
        "url": "https://arxiv.org/abs/2310.16410",
        "title": "Bridging the Human-AI Knowledge Gap: Concept Discovery and Transfer in\n  AlphaZero",
        "authors": "Lisa Schut, Nenad Tomasev, Tom McGrath, Demis Hassabis, Ulrich Paquet, Been Kim",
        "abstract": "Artificial Intelligence (AI) systems have made remarkable progress, attaining\nsuper-human performance across various domains. This presents us with an\nopportunity to further human knowledge and improve human expert performance by\nleveraging the hidden knowledge encoded within these highly performant AI\nsystems. Yet, this knowledge is often hard to extract, and may be hard to\nunderstand or learn from. Here, we show that this is possible by proposing a\nnew method that allows us to extract new chess concepts in AlphaZero, an AI\nsystem that mastered the game of chess via self-play without human supervision.\nOur analysis indicates that AlphaZero may encode knowledge that extends beyond\nthe existing human knowledge, but knowledge that is ultimately not beyond human\ngrasp, and can be successfully learned from. In a human study, we show that\nthese concepts are learnable by top human experts, as four top chess\ngrandmasters show improvements in solving the presented concept prototype\npositions. This marks an important first milestone in advancing the frontier of\nhuman knowledge by leveraging AI; a development that could bear profound\nimplications and help us shape how we interact with AI systems across many AI\napplications.",
        "timestamp": "2025-01-24T08:28:29.421Z",
        "rating": "novote",
        "published_date": "2023-10-25T06:49:26Z",
        "arxiv_tags": [
          "cs.AI",
          "cs.HC",
          "cs.LG",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-01-24T08:28:29+00:00",
        "updated_at": "2025-03-30T08:14:39+00:00",
        "version": 2
      }
    },
    "interactions:2501.04697": {
      "data": {
        "paper_id": "2501.04697",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-24T08:24:59.397Z",
            "data": {
              "session_id": "session_1737707095506_w1uvoi6",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-01-24T08:24:55.506Z",
              "end_time": "2025-01-24T08:24:58.642Z",
              "total_elapsed_seconds": 3
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-24T08:25:00+00:00",
        "updated_at": "2025-03-30T08:14:58+00:00",
        "version": 5
      }
    },
    "paper:2501.04697": {
      "data": {
        "arxivId": "2501.04697",
        "url": "https://arxiv.org/abs/2501.04697",
        "title": "Grokking at the Edge of Numerical Stability",
        "authors": "Lucas Prieto, Melih Barsbey, Pedro A. M. Mediano, Tolga Birdal",
        "abstract": "Grokking, the sudden generalization that occurs after prolonged overfitting,\nis a surprising phenomenon challenging our understanding of deep learning.\nAlthough significant progress has been made in understanding grokking, the\nreasons behind the delayed generalization and its dependence on regularization\nremain unclear. In this work, we argue that without regularization, grokking\ntasks push models to the edge of numerical stability, introducing floating\npoint errors in the Softmax function, which we refer to as Softmax Collapse\n(SC). We demonstrate that SC prevents grokking and that mitigating SC enables\ngrokking without regularization. Investigating the root cause of SC, we find\nthat beyond the point of overfitting, the gradients strongly align with what we\ncall the na\\\"ive loss minimization (NLM) direction. This component of the\ngradient does not alter the model's predictions but decreases the loss by\nscaling the logits, typically by scaling the weights along their current\ndirection. We show that this scaling of the logits explains the delay in\ngeneralization characteristic of grokking and eventually leads to SC, halting\nfurther learning. To validate our hypotheses, we introduce two key\ncontributions that address the challenges in grokking tasks: StableMax, a new\nactivation function that prevents SC and enables grokking without\nregularization, and $\\perp$Grad, a training algorithm that promotes quick\ngeneralization in grokking tasks by preventing NLM altogether. These\ncontributions provide new insights into grokking, elucidating its delayed\ngeneralization, reliance on regularization, and the effectiveness of existing\ngrokking-inducing methods. Code for this paper is available at\nhttps://github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability.",
        "timestamp": "2025-01-24T08:24:55.685Z",
        "rating": "novote",
        "published_date": "2025-01-08T18:58:48Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI",
          "cs.CV",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-01-24T08:24:56+00:00",
        "updated_at": "2025-01-24T08:24:59+00:00",
        "version": 2
      }
    },
    "interactions:2002.09291": {
      "data": {
        "paper_id": "2002.09291",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-24T07:09:06.090Z",
            "data": {
              "session_id": "session_1737702535793_x55fw99",
              "duration_seconds": 9,
              "idle_seconds": 0,
              "start_time": "2025-01-24T07:08:55.793Z",
              "end_time": "2025-01-24T07:09:05.184Z",
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-25T05:29:19.260Z",
            "data": {
              "session_id": "session_1737782950954_orhggu3",
              "duration_seconds": 7,
              "idle_seconds": 0,
              "start_time": "2025-01-25T05:29:10.954Z",
              "end_time": "2025-01-25T05:29:17.671Z",
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-24T07:09:06+00:00",
        "updated_at": "2025-03-30T08:15:08+00:00",
        "version": 7
      }
    },
    "paper:2002.09291": {
      "data": {
        "arxivId": "2002.09291",
        "url": "https://arxiv.org/abs/2002.09291",
        "title": "Transformer Hawkes Process",
        "authors": "Simiao Zuo, Haoming Jiang, Zichong Li, Tuo Zhao, Hongyuan Zha",
        "abstract": "Modern data acquisition routinely produce massive amounts of event sequence\ndata in various domains, such as social media, healthcare, and financial\nmarkets. These data often exhibit complicated short-term and long-term temporal\ndependencies. However, most of the existing recurrent neural network based\npoint process models fail to capture such dependencies, and yield unreliable\nprediction performance. To address this issue, we propose a Transformer Hawkes\nProcess (THP) model, which leverages the self-attention mechanism to capture\nlong-term dependencies and meanwhile enjoys computational efficiency. Numerical\nexperiments on various datasets show that THP outperforms existing models in\nterms of both likelihood and event prediction accuracy by a notable margin.\nMoreover, THP is quite general and can incorporate additional structural\nknowledge. We provide a concrete example, where THP achieves improved\nprediction performance for learning multiple point processes when incorporating\ntheir relational information.",
        "timestamp": "2025-01-24T07:08:55.938Z",
        "rating": "novote",
        "published_date": "2020-02-21T13:48:13Z",
        "arxiv_tags": [
          "cs.LG",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-01-24T07:08:56+00:00",
        "updated_at": "2025-01-24T07:08:59+00:00",
        "version": 2
      }
    },
    "paper:2002.08521": {
      "data": {
        "arxivId": "2002.08521",
        "url": "https://arxiv.org/pdf/2002.08521",
        "title": "Group Network Hawkes Process",
        "authors": "Guanhua Fang, Ganggang Xu, Haochen Xu, Xuening Zhu, Yongtao Guan",
        "abstract": "In this work, we study the event occurrences of individuals interacting in a\nnetwork. To characterize the dynamic interactions among the individuals, we\npropose a group network Hawkes process (GNHP) model whose network structure is\nobserved and fixed. In particular, we introduce a latent group structure among\nindividuals to account for the heterogeneous user-specific characteristics. A\nmaximum likelihood approach is proposed to simultaneously cluster individuals\nin the network and estimate model parameters. A fast EM algorithm is\nsubsequently developed by utilizing the branching representation of the\nproposed GNHP model. Theoretical properties of the resulting estimators of\ngroup memberships and model parameters are investigated under both settings\nwhen the number of latent groups $G$ is over-specified or correctly specified.\nA data-driven criterion that can consistently identify the true $G$ under mild\nconditions is derived. Extensive simulation studies and an application to a\ndata set collected from Sina Weibo are used to illustrate the effectiveness of\nthe proposed methodology.",
        "timestamp": "2025-01-24T07:07:58.196Z",
        "rating": "novote",
        "published_date": "2020-02-20T01:30:42Z",
        "arxiv_tags": [
          "stat.ME",
          "math.ST",
          "stat.TH"
        ]
      },
      "meta": {
        "created_at": "2025-01-24T07:07:58+00:00",
        "updated_at": "2025-01-24T07:08:01+00:00",
        "version": 2
      }
    },
    "interactions:2312.00752": {
      "data": {
        "paper_id": "2312.00752",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-23T07:26:06.418Z",
            "data": {
              "session_id": "session_1737617143583_9cgks93",
              "duration_seconds": 23,
              "idle_seconds": 0,
              "start_time": "2025-01-23T07:25:43.583Z",
              "end_time": "2025-01-23T07:26:06.402Z",
              "total_elapsed_seconds": 23
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-23T07:25:37+00:00",
        "updated_at": "2025-01-23T21:30:38+00:00",
        "version": 4
      }
    },
    "paper:2312.00752": {
      "data": {
        "arxivId": "2312.00752",
        "url": "https://arxiv.org/abs/2312.00752",
        "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
        "authors": "Albert Gu, Tri Dao",
        "abstract": "Foundation models, now powering most of the exciting applications in deep\nlearning, are almost universally based on the Transformer architecture and its\ncore attention module. Many subquadratic-time architectures such as linear\nattention, gated convolution and recurrent models, and structured state space\nmodels (SSMs) have been developed to address Transformers' computational\ninefficiency on long sequences, but they have not performed as well as\nattention on important modalities such as language. We identify that a key\nweakness of such models is their inability to perform content-based reasoning,\nand make several improvements. First, simply letting the SSM parameters be\nfunctions of the input addresses their weakness with discrete modalities,\nallowing the model to selectively propagate or forget information along the\nsequence length dimension depending on the current token. Second, even though\nthis change prevents the use of efficient convolutions, we design a\nhardware-aware parallel algorithm in recurrent mode. We integrate these\nselective SSMs into a simplified end-to-end neural network architecture without\nattention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$\nhigher throughput than Transformers) and linear scaling in sequence length, and\nits performance improves on real data up to million-length sequences. As a\ngeneral sequence model backbone, Mamba achieves state-of-the-art performance\nacross several modalities such as language, audio, and genomics. On language\nmodeling, our Mamba-3B model outperforms Transformers of the same size and\nmatches Transformers twice its size, both in pretraining and downstream\nevaluation.",
        "timestamp": "2025-01-23T07:24:53.349Z",
        "rating": "novote",
        "published_date": "2023-12-01T18:01:34Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI"
        ]
      },
      "meta": {
        "created_at": "2025-01-23T07:24:53+00:00",
        "updated_at": "2025-01-23T07:24:56+00:00",
        "version": 2
      }
    },
    "interactions:2402.18012": {
      "data": {
        "paper_id": "2402.18012",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-23T02:00:14.220Z",
            "data": {
              "session_id": "session_1737597587001_4wifmlu",
              "duration_seconds": 26,
              "idle_seconds": 0,
              "start_time": "2025-01-23T01:59:47.001Z",
              "end_time": "2025-01-23T02:00:13.427Z",
              "total_elapsed_seconds": 26
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-23T02:00:15+00:00",
        "updated_at": "2025-03-30T08:15:18+00:00",
        "version": 3
      }
    },
    "paper:2402.18012": {
      "data": {
        "arxivId": "2402.18012",
        "url": "https://arxiv.org/abs/2402.18012",
        "title": "Diffusion Models as Constrained Samplers for Optimization with Unknown\n  Constraints",
        "authors": "Lingkai Kong, Yuanqi Du, Wenhao Mu, Kirill Neklyudov, Valentin De Bortoli, Dongxia Wu, Haorui Wang, Aaron Ferber, Yi-An Ma, Carla P. Gomes, Chao Zhang",
        "abstract": "Addressing real-world optimization problems becomes particularly challenging\nwhen analytic objective functions or constraints are unavailable. While\nnumerous studies have addressed the issue of unknown objectives, limited\nresearch has focused on scenarios where feasibility constraints are not given\nexplicitly. Overlooking these constraints can lead to spurious solutions that\nare unrealistic in practice. To deal with such unknown constraints, we propose\nto perform optimization within the data manifold using diffusion models. To\nconstrain the optimization process to the data manifold, we reformulate the\noriginal optimization problem as a sampling problem from the product of the\nBoltzmann distribution defined by the objective function and the data\ndistribution learned by the diffusion model. Depending on the differentiability\nof the objective function, we propose two different sampling methods. For\ndifferentiable objectives, we propose a two-stage framework that begins with a\nguided diffusion process for warm-up, followed by a Langevin dynamics stage for\nfurther correction. For non-differentiable objectives, we propose an iterative\nimportance sampling strategy using the diffusion model as the proposal\ndistribution. Comprehensive experiments on a synthetic dataset, six real-world\nblack-box optimization datasets, and a multi-objective molecule optimization\ndataset show that our method achieves better or comparable performance with\nprevious state-of-the-art baselines.",
        "timestamp": "2025-01-23T01:59:47.272Z",
        "rating": "novote",
        "published_date": "2024-02-28T03:09:12Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI"
        ]
      },
      "meta": {
        "created_at": "2025-01-23T01:59:47+00:00",
        "updated_at": "2025-01-23T01:59:50+00:00",
        "version": 2
      }
    },
    "interactions:1503.02531": {
      "data": {
        "paper_id": "1503.02531",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-01-22T23:53:11+00:00",
        "updated_at": "2025-03-30T08:15:28+00:00",
        "version": 1
      }
    },
    "paper:1503.02531": {
      "data": {
        "arxivId": "1503.02531",
        "url": "https://arxiv.org/abs/1503.02531",
        "title": "Distilling the Knowledge in a Neural Network",
        "authors": "Geoffrey Hinton, Oriol Vinyals, Jeff Dean",
        "abstract": "A very simple way to improve the performance of almost any machine learning\nalgorithm is to train many different models on the same data and then to\naverage their predictions. Unfortunately, making predictions using a whole\nensemble of models is cumbersome and may be too computationally expensive to\nallow deployment to a large number of users, especially if the individual\nmodels are large neural nets. Caruana and his collaborators have shown that it\nis possible to compress the knowledge in an ensemble into a single model which\nis much easier to deploy and we develop this approach further using a different\ncompression technique. We achieve some surprising results on MNIST and we show\nthat we can significantly improve the acoustic model of a heavily used\ncommercial system by distilling the knowledge in an ensemble of models into a\nsingle model. We also introduce a new type of ensemble composed of one or more\nfull models and many specialist models which learn to distinguish fine-grained\nclasses that the full models confuse. Unlike a mixture of experts, these\nspecialist models can be trained rapidly and in parallel.",
        "timestamp": "2025-01-22T23:52:59.719Z",
        "rating": "novote",
        "published_date": "2015-03-09T15:44:49Z",
        "arxiv_tags": [
          "stat.ML",
          "cs.LG",
          "cs.NE"
        ]
      },
      "meta": {
        "created_at": "2025-01-22T23:53:00+00:00",
        "updated_at": "2025-03-30T08:15:46+00:00",
        "version": 2
      }
    },
    "interactions:2212.07677": {
      "data": {
        "paper_id": "2212.07677",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-22T23:51:48.964Z",
            "data": {
              "session_id": "session_1737589861712_6k0limt",
              "duration_seconds": 46,
              "idle_seconds": 0,
              "start_time": "2025-01-22T23:51:01.712Z",
              "end_time": "2025-01-22T23:51:48.063Z",
              "total_elapsed_seconds": 46
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T23:27:09.072Z",
            "data": {
              "session_id": "session_1737934024181_5e34nc6",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-01-26T23:27:04.181Z",
              "end_time": "2025-01-26T23:27:09.058Z",
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T23:56:23.270Z",
            "data": {
              "session_id": "session_1737935779556_s7zdd89",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-01-26T23:56:19.557Z",
              "end_time": "2025-01-26T23:56:22.895Z",
              "total_elapsed_seconds": 3
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-27T00:09:46.369Z",
            "data": {
              "session_id": "session_1737936582965_48u5lh2",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-01-27T00:09:42.965Z",
              "end_time": "2025-01-27T00:09:46.170Z",
              "total_elapsed_seconds": 3
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-22T23:51:49+00:00",
        "updated_at": "2025-01-27T00:11:07+00:00",
        "version": 7
      }
    },
    "paper:2212.07677": {
      "data": {
        "arxivId": "2212.07677",
        "url": "https://arxiv.org/abs/2212.07677",
        "title": "Transformers learn in-context by gradient descent",
        "authors": "Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, Max Vladymyrov",
        "abstract": "At present, the mechanisms of in-context learning in Transformers are not\nwell understood and remain mostly an intuition. In this paper, we suggest that\ntraining Transformers on auto-regressive objectives is closely related to\ngradient-based meta-learning formulations. We start by providing a simple\nweight construction that shows the equivalence of data transformations induced\nby 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a\nregression loss. Motivated by that construction, we show empirically that when\ntraining self-attention-only Transformers on simple regression tasks either the\nmodels learned by GD and Transformers show great similarity or, remarkably, the\nweights found by optimization match the construction. Thus we show how trained\nTransformers become mesa-optimizers i.e. learn models by gradient descent in\ntheir forward pass. This allows us, at least in the domain of regression\nproblems, to mechanistically understand the inner workings of in-context\nlearning in optimized Transformers. Building on this insight, we furthermore\nidentify how Transformers surpass the performance of plain gradient descent by\nlearning an iterative curvature correction and learn linear models on deep data\nrepresentations to solve non-linear regression tasks. Finally, we discuss\nintriguing parallels to a mechanism identified to be crucial for in-context\nlearning termed induction-head (Olsson et al., 2022) and show how it could be\nunderstood as a specific case of in-context learning by gradient descent\nlearning within Transformers. Code to reproduce the experiments can be found at\nhttps://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd .",
        "timestamp": "2025-01-22T23:51:01.971Z",
        "rating": "novote",
        "published_date": "2022-12-15T09:21:21Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI",
          "cs.CL"
        ]
      },
      "meta": {
        "created_at": "2025-01-22T23:51:02+00:00",
        "updated_at": "2025-01-22T23:51:05+00:00",
        "version": 2
      }
    },
    "interactions:2501.12374": {
      "data": {
        "paper_id": "2501.12374",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-27T00:09:55.653Z",
            "data": {
              "session_id": "session_1737936591983_wpts0s9",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-01-27T00:09:51.983Z",
              "end_time": "2025-01-27T00:09:55.380Z",
              "total_elapsed_seconds": 3
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-22T23:29:34+00:00",
        "updated_at": "2025-03-30T08:15:56+00:00",
        "version": 12
      }
    },
    "paper:2501.12374": {
      "data": {
        "arxivId": "2501.12374",
        "url": "https://arxiv.org/abs/2501.12374",
        "title": "Expertise elevates AI usage: experimental evidence comparing laypeople\n  and professional artists",
        "authors": "Thomas F. Eisenmann, Andres Karjus, Mar Canet Sola, Levin Brinkmann, Bramantyo Ibrahim Supriyatno, Iyad Rahwan",
        "abstract": "Novel capacities of generative AI to analyze and generate cultural artifacts\nraise inevitable questions about the nature and value of artistic education and\nhuman expertise. Has AI already leveled the playing field between professional\nartists and laypeople, or do trained artistic expressive capacity, curation\nskills and experience instead enhance the ability to use these new tools? In\nthis pre-registered study, we conduct experimental comparisons between 50\nactive artists and a demographically matched sample of laypeople. We designed\ntwo tasks to approximate artistic practice for testing their capabilities in\nboth faithful and creative image creation: replicating a reference image, and\nmoving as far away as possible from it. We developed a bespoke platform where\nparticipants used a modern text-to-image model to complete both tasks. We also\ncollected and compared participants' sentiments towards AI. On average, artists\nproduced more faithful and creative outputs than their lay counterparts,\nalthough only by a small margin. While AI may ease content creation,\nprofessional expertise is still valuable - even within the confined space of\ngenerative AI itself. Finally, we also explored how well an exemplary\nvision-capable large language model (GPT-4o) would complete the same tasks, if\ngiven the role of an image generation agent, and found it performed on par in\ncopying but outperformed even artists in the creative task. The very best\nresults were still produced by humans in both tasks. These outcomes highlight\nthe importance of integrating artistic skills with AI training to prepare\nartists and other visual professionals for a technologically evolving\nlandscape. We see a potential in collaborative synergy with generative AI,\nwhich could reshape creative industries and education in the arts.",
        "timestamp": "2025-01-22T23:29:08.663Z",
        "rating": "novote",
        "published_date": "2025-01-21T18:53:21Z",
        "arxiv_tags": [
          "cs.HC",
          "cs.AI",
          "cs.CY"
        ]
      },
      "meta": {
        "created_at": "2025-01-22T23:29:09+00:00",
        "updated_at": "2025-01-22T23:29:12+00:00",
        "version": 2
      }
    },
    "interactions:2007.12927": {
      "data": {
        "paper_id": "2007.12927",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-22T23:10:40.531Z",
            "data": {
              "duration_seconds": 606
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-27T00:10:06.264Z",
            "data": {
              "session_id": "session_1737936601397_fv5nag5",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-01-27T00:10:01.397Z",
              "end_time": "2025-01-27T00:10:04.498Z",
              "total_elapsed_seconds": 3
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-27T00:20:27.592Z",
            "data": {
              "session_id": "session_1737937223431_7egykr9",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-01-27T00:20:23.431Z",
              "end_time": "2025-01-27T00:20:27.049Z",
              "total_elapsed_seconds": 4
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-12T08:24:58.264Z",
            "data": {
              "session_id": "session_1739348691785_q4emcqh",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-02-12T08:24:51.786Z",
              "end_time": "2025-02-12T08:24:58.255Z",
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-12T08:29:50.172Z",
            "data": {
              "session_id": "session_1739348982623_0wn429b",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-02-12T08:29:42.623Z",
              "end_time": "2025-02-12T08:29:47.615Z",
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-12T20:28:11.055Z",
            "data": {
              "session_id": "session_1739392087155_4y79st1",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-02-12T20:28:07.155Z",
              "end_time": "2025-02-12T20:28:10.534Z",
              "total_elapsed_seconds": 3
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-13T01:16:08.744Z",
            "data": {
              "session_id": "session_1739409364730_vvbgc10",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-02-13T01:16:04.730Z",
              "end_time": "2025-02-13T01:16:08.169Z",
              "total_elapsed_seconds": 3
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-22T23:10:40+00:00",
        "updated_at": "2025-03-30T08:16:07+00:00",
        "version": 17
      }
    },
    "paper:2007.12927": {
      "data": {
        "arxivId": "2007.12927",
        "url": "https://arxiv.org/pdf/2007.12927",
        "title": "Neural networks with late-phase weights",
        "authors": "Johannes von Oswald, Seijin Kobayashi, Alexander Meulemans, Christian Henning, Benjamin F. Grewe, Jo\u00e3o Sacramento",
        "abstract": "The largely successful method of training neural networks is to learn their\nweights using some variant of stochastic gradient descent (SGD). Here, we show\nthat the solutions found by SGD can be further improved by ensembling a subset\nof the weights in late stages of learning. At the end of learning, we obtain\nback a single model by taking a spatial average in weight space. To avoid\nincurring increased computational costs, we investigate a family of\nlow-dimensional late-phase weight models which interact multiplicatively with\nthe remaining parameters. Our results show that augmenting standard models with\nlate-phase weights improves generalization in established benchmarks such as\nCIFAR-10/100, ImageNet and enwik8. These findings are complemented with a\ntheoretical analysis of a noisy quadratic problem which provides a simplified\npicture of the late phases of neural network learning.",
        "timestamp": "2025-01-22T23:00:32.558Z",
        "rating": "novote",
        "published_date": "2020-07-25T13:23:37Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.CV",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-01-22T23:00:33+00:00",
        "updated_at": "2025-01-22T23:00:36+00:00",
        "version": 2
      }
    },
    "interactions:2212.13345": {
      "data": {
        "paper_id": "2212.13345",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-22T22:08:59.431Z",
            "data": {
              "duration_seconds": 12
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-22T22:06:24+00:00",
        "updated_at": "2025-01-24T08:15:21+00:00",
        "version": 7
      }
    },
    "paper:2212.13345": {
      "data": {
        "arxivId": "2212.13345",
        "url": "https://arxiv.org/abs/2212.13345",
        "title": "The Forward-Forward Algorithm: Some Preliminary Investigations",
        "authors": "Geoffrey Hinton",
        "abstract": "The aim of this paper is to introduce a new learning procedure for neural\nnetworks and to demonstrate that it works well enough on a few small problems\nto be worth further investigation. The Forward-Forward algorithm replaces the\nforward and backward passes of backpropagation by two forward passes, one with\npositive (i.e. real) data and the other with negative data which could be\ngenerated by the network itself. Each layer has its own objective function\nwhich is simply to have high goodness for positive data and low goodness for\nnegative data. The sum of the squared activities in a layer can be used as the\ngoodness but there are many other possibilities, including minus the sum of the\nsquared activities. If the positive and negative passes could be separated in\ntime, the negative passes could be done offline, which would make the learning\nmuch simpler in the positive pass and allow video to be pipelined through the\nnetwork without ever storing activities or stopping to propagate derivatives.",
        "timestamp": "2025-01-22T22:06:19.707Z",
        "rating": "thumbsup",
        "published_date": "2022-12-27T02:54:46Z",
        "arxiv_tags": [
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-01-22T22:06:20+00:00",
        "updated_at": "2025-01-24T08:15:32+00:00",
        "version": 3
      }
    },
    "interactions:2112.04215": {
      "data": {
        "paper_id": "2112.04215",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-22T07:09:51.236Z",
            "data": {
              "duration_seconds": 25
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-22T07:08:39+00:00",
        "updated_at": "2025-01-24T08:15:39+00:00",
        "version": 8
      }
    },
    "paper:2112.04215": {
      "data": {
        "arxivId": "2112.04215",
        "url": "https://arxiv.org/abs/2112.04215",
        "title": "Self-Supervised Models are Continual Learners",
        "authors": "Enrico Fini, Victor G. Turrisi da Costa, Xavier Alameda-Pineda, Elisa Ricci, Karteek Alahari, Julien Mairal",
        "abstract": "Self-supervised models have been shown to produce comparable or better visual\nrepresentations than their supervised counterparts when trained offline on\nunlabeled data at scale. However, their efficacy is catastrophically reduced in\na Continual Learning (CL) scenario where data is presented to the model\nsequentially. In this paper, we show that self-supervised loss functions can be\nseamlessly converted into distillation mechanisms for CL by adding a predictor\nnetwork that maps the current state of the representations to their past state.\nThis enables us to devise a framework for Continual self-supervised visual\nrepresentation Learning that (i) significantly improves the quality of the\nlearned representations, (ii) is compatible with several state-of-the-art\nself-supervised objectives, and (iii) needs little to no hyperparameter tuning.\nWe demonstrate the effectiveness of our approach empirically by training six\npopular self-supervised models in various CL settings.",
        "timestamp": "2025-01-22T07:08:23.330Z",
        "rating": "novote",
        "published_date": "2021-12-08T10:39:13Z",
        "arxiv_tags": [
          "cs.CV",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-01-22T07:08:23+00:00",
        "updated_at": "2025-01-22T07:08:26+00:00",
        "version": 2
      }
    },
    "interactions:2412.09315": {
      "data": {
        "paper_id": "2412.09315",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-01-21T20:16:58+00:00",
        "updated_at": "2025-03-30T08:16:17+00:00",
        "version": 2
      }
    },
    "paper:2412.09315": {
      "data": {
        "arxivId": "2412.09315",
        "url": "https://arxiv.org/abs/2412.09315",
        "title": "Beware of Metacognitive Laziness: Effects of Generative Artificial\n  Intelligence on Learning Motivation, Processes, and Performance",
        "authors": "Yizhou Fan, Luzhen Tang, Huixiao Le, Kejie Shen, Shufang Tan, Yueying Zhao, Yuan Shen, Xinyu Li, Dragan Ga\u0161evi\u0107",
        "abstract": "With the continuous development of technological and educational innovation,\nlearners nowadays can obtain a variety of support from agents such as teachers,\npeers, education technologies, and recently, generative artificial intelligence\nsuch as ChatGPT. The concept of hybrid intelligence is still at a nascent\nstage, and how learners can benefit from a symbiotic relationship with various\nagents such as AI, human experts and intelligent learning systems is still\nunknown. The emerging concept of hybrid intelligence also lacks deep insights\nand understanding of the mechanisms and consequences of hybrid human-AI\nlearning based on strong empirical research. In order to address this gap, we\nconducted a randomised experimental study and compared learners' motivations,\nself-regulated learning processes and learning performances on a writing task\namong different groups who had support from different agents (ChatGPT, human\nexpert, writing analytics tools, and no extra tool). A total of 117 university\nstudents were recruited, and their multi-channel learning, performance and\nmotivation data were collected and analysed. The results revealed that:\nlearners who received different learning support showed no difference in\npost-task intrinsic motivation; there were significant differences in the\nfrequency and sequences of the self-regulated learning processes among groups;\nChatGPT group outperformed in the essay score improvement but their knowledge\ngain and transfer were not significantly different. Our research found that in\nthe absence of differences in motivation, learners with different supports\nstill exhibited different self-regulated learning processes, ultimately leading\nto differentiated performance. What is particularly noteworthy is that AI\ntechnologies such as ChatGPT may promote learners' dependence on technology and\npotentially trigger metacognitive laziness.",
        "timestamp": "2025-01-21T20:05:49.177Z",
        "rating": "novote",
        "published_date": "2024-12-12T14:32:39Z",
        "arxiv_tags": [
          "cs.AI",
          "cs.HC"
        ]
      },
      "meta": {
        "created_at": "2025-01-21T20:05:49+00:00",
        "updated_at": "2025-03-30T08:16:35+00:00",
        "version": 2
      }
    },
    "interactions:2412.12095": {
      "data": {
        "paper_id": "2412.12095",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-01-21T16:27:01+00:00",
        "updated_at": "2025-03-30T08:16:44+00:00",
        "version": 2
      }
    },
    "paper:2412.12095": {
      "data": {
        "arxivId": "2412.12095",
        "url": "https://arxiv.org/abs/2412.12095",
        "title": "Causal Diffusion Transformers for Generative Modeling",
        "authors": "Chaorui Deng, Deyao Zhu, Kunchang Li, Shi Guang, Haoqi Fan",
        "abstract": "We introduce Causal Diffusion as the autoregressive (AR) counterpart of\nDiffusion models. It is a next-token(s) forecasting framework that is friendly\nto both discrete and continuous modalities and compatible with existing\nnext-token prediction models like LLaMA and GPT. While recent works attempt to\ncombine diffusion with AR models, we show that introducing sequential\nfactorization to a diffusion model can substantially improve its performance\nand enables a smooth transition between AR and diffusion generation modes.\nHence, we propose CausalFusion - a decoder-only transformer that\ndual-factorizes data across sequential tokens and diffusion noise levels,\nleading to state-of-the-art results on the ImageNet generation benchmark while\nalso enjoying the AR advantage of generating an arbitrary number of tokens for\nin-context reasoning. We further demonstrate CausalFusion's multimodal\ncapabilities through a joint image generation and captioning model, and\nshowcase CausalFusion's ability for zero-shot in-context image manipulations.\nWe hope that this work could provide the community with a fresh perspective on\ntraining multimodal models over discrete and continuous data.",
        "timestamp": "2025-01-21T16:26:49.971Z",
        "rating": "novote",
        "published_date": "2024-12-16T18:59:29Z",
        "arxiv_tags": [
          "cs.CV"
        ]
      },
      "meta": {
        "created_at": "2025-01-21T16:26:50+00:00",
        "updated_at": "2025-03-30T08:17:19+00:00",
        "version": 2
      }
    },
    "paper:2501.05441": {
      "data": {
        "arxivId": "2501.05441",
        "url": "https://arxiv.org/abs/2501.05441",
        "title": "The GAN is dead; long live the GAN! A Modern GAN Baseline",
        "authors": "Yiwen Huang, Aaron Gokaslan, Volodymyr Kuleshov, James Tompkin",
        "abstract": "There is a widely-spread claim that GANs are difficult to train, and GAN\narchitectures in the literature are littered with empirical tricks. We provide\nevidence against this claim and build a modern GAN baseline in a more\nprincipled manner. First, we derive a well-behaved regularized relativistic GAN\nloss that addresses issues of mode dropping and non-convergence that were\npreviously tackled via a bag of ad-hoc tricks. We analyze our loss\nmathematically and prove that it admits local convergence guarantees, unlike\nmost existing relativistic losses. Second, our new loss allows us to discard\nall ad-hoc tricks and replace outdated backbones used in common GANs with\nmodern architectures. Using StyleGAN2 as an example, we present a roadmap of\nsimplification and modernization that results in a new minimalist baseline --\nR3GAN. Despite being simple, our approach surpasses StyleGAN2 on FFHQ,\nImageNet, CIFAR, and Stacked MNIST datasets, and compares favorably against\nstate-of-the-art GANs and diffusion models.",
        "timestamp": "2025-01-21T16:24:13.360Z",
        "rating": "novote",
        "published_date": "2025-01-09T18:53:06Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.CV"
        ]
      },
      "meta": {
        "created_at": "2025-01-21T16:24:13+00:00",
        "updated_at": "2025-03-30T08:18:01+00:00",
        "version": 2
      }
    },
    "interactions:2501.05441": {
      "data": {
        "paper_id": "2501.05441",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-01-21T16:24:38+00:00",
        "updated_at": "2025-03-30T08:18:59+00:00",
        "version": 2
      }
    },
    "interactions:2407.05872": {
      "data": {
        "paper_id": "2407.05872",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-01-20T19:45:01+00:00",
        "updated_at": "2025-03-30T08:19:25+00:00",
        "version": 2
      }
    },
    "paper:2407.05872": {
      "data": {
        "arxivId": "2407.05872",
        "url": "https://arxiv.org/abs/2407.05872",
        "title": "Scaling Exponents Across Parameterizations and Optimizers",
        "authors": "Katie Everett, Lechao Xiao, Mitchell Wortsman, Alexander A. Alemi, Roman Novak, Peter J. Liu, Izzeddin Gur, Jascha Sohl-Dickstein, Leslie Pack Kaelbling, Jaehoon Lee, Jeffrey Pennington",
        "abstract": "Robust and effective scaling of models from small to large width typically\nrequires the precise adjustment of many algorithmic and architectural details,\nsuch as parameterization and optimizer choices. In this work, we propose a new\nperspective on parameterization by investigating a key assumption in prior work\nabout the alignment between parameters and data and derive new theoretical\nresults under weaker assumptions and a broader set of optimizers. Our extensive\nempirical investigation includes tens of thousands of models trained with all\ncombinations of three optimizers, four parameterizations, several alignment\nassumptions, more than a dozen learning rates, and fourteen model sizes up to\n26.8B parameters. We find that the best learning rate scaling prescription\nwould often have been excluded by the assumptions in prior work. Our results\nshow that all parameterizations, not just maximal update parameterization\n(muP), can achieve hyperparameter transfer; moreover, our novel per-layer\nlearning rate prescription for standard parameterization outperforms muP.\nFinally, we demonstrate that an overlooked aspect of parameterization, the\nepsilon parameter in Adam, must be scaled correctly to avoid gradient underflow\nand propose Adam-atan2, a new numerically stable, scale-invariant version of\nAdam that eliminates the epsilon hyperparameter entirely.",
        "timestamp": "2025-01-20T19:44:42.999Z",
        "rating": "novote",
        "published_date": "2024-07-08T12:32:51Z",
        "arxiv_tags": [
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-01-20T19:44:43+00:00",
        "updated_at": "2025-03-30T08:19:42+00:00",
        "version": 2
      }
    },
    "interactions:2402.06184": {
      "data": {
        "paper_id": "2402.06184",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-20T19:26:52.959Z",
            "data": {
              "duration_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-20T19:26:46+00:00",
        "updated_at": "2025-03-30T08:20:32+00:00",
        "version": 4
      }
    },
    "paper:2402.06184": {
      "data": {
        "arxivId": "2402.06184",
        "url": "https://arxiv.org/abs/2402.06184",
        "title": "The boundary of neural network trainability is fractal",
        "authors": "Jascha Sohl-Dickstein",
        "abstract": "Some fractals -- for instance those associated with the Mandelbrot and\nquadratic Julia sets -- are computed by iterating a function, and identifying\nthe boundary between hyperparameters for which the resulting series diverges or\nremains bounded. Neural network training similarly involves iterating an update\nfunction (e.g. repeated steps of gradient descent), can result in convergent or\ndivergent behavior, and can be extremely sensitive to small changes in\nhyperparameters. Motivated by these similarities, we experimentally examine the\nboundary between neural network hyperparameters that lead to stable and\ndivergent training. We find that this boundary is fractal over more than ten\ndecades of scale in all tested configurations.",
        "timestamp": "2025-01-20T19:26:32.219Z",
        "rating": "novote",
        "published_date": "2024-02-09T04:46:48Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.NE",
          "nlin.CD"
        ]
      },
      "meta": {
        "created_at": "2025-01-20T19:26:32+00:00",
        "updated_at": "2025-03-30T08:20:50+00:00",
        "version": 2
      }
    },
    "interactions:2410.05229": {
      "data": {
        "paper_id": "2410.05229",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-20T09:41:41.160Z",
            "data": {
              "duration_seconds": 108
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-20T09:41:36+00:00",
        "updated_at": "2025-03-30T08:21:16+00:00",
        "version": 4
      }
    },
    "interactions:2411.04872": {
      "data": {
        "paper_id": "2411.04872",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-20T09:39:40.379Z",
            "data": {
              "duration_seconds": 30
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-20T09:38:50+00:00",
        "updated_at": "2025-03-30T08:21:25+00:00",
        "version": 5
      }
    },
    "paper:2410.05229": {
      "data": {
        "arxivId": "2410.05229",
        "url": "https://arxiv.org/abs/2410.05229",
        "title": "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in\n  Large Language Models",
        "authors": "Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, Mehrdad Farajtabar",
        "abstract": "Recent advancements in Large Language Models (LLMs) have sparked interest in\ntheir formal reasoning capabilities, particularly in mathematics. The GSM8K\nbenchmark is widely used to assess the mathematical reasoning of models on\ngrade-school-level questions. While the performance of LLMs on GSM8K has\nsignificantly improved in recent years, it remains unclear whether their\nmathematical reasoning capabilities have genuinely advanced, raising questions\nabout the reliability of the reported metrics. To address these concerns, we\nconduct a large-scale study on several SOTA open and closed models. To overcome\nthe limitations of existing evaluations, we introduce GSM-Symbolic, an improved\nbenchmark created from symbolic templates that allow for the generation of a\ndiverse set of questions. GSM-Symbolic enables more controllable evaluations,\nproviding key insights and more reliable metrics for measuring the reasoning\ncapabilities of models.Our findings reveal that LLMs exhibit noticeable\nvariance when responding to different instantiations of the same question.\nSpecifically, the performance of all models declines when only the numerical\nvalues in the question are altered in the GSM-Symbolic benchmark. Furthermore,\nwe investigate the fragility of mathematical reasoning in these models and show\nthat their performance significantly deteriorates as the number of clauses in a\nquestion increases. We hypothesize that this decline is because current LLMs\ncannot perform genuine logical reasoning; they replicate reasoning steps from\ntheir training data. Adding a single clause that seems relevant to the question\ncauses significant performance drops (up to 65%) across all state-of-the-art\nmodels, even though the clause doesn't contribute to the reasoning chain needed\nfor the final answer. Overall, our work offers a more nuanced understanding of\nLLMs' capabilities and limitations in mathematical reasoning.",
        "timestamp": "2025-01-20T09:39:37.110Z",
        "rating": "novote",
        "published_date": "2024-10-07T17:36:37Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI"
        ]
      },
      "meta": {
        "created_at": "2025-01-20T09:39:37+00:00",
        "updated_at": "2025-01-20T09:39:40+00:00",
        "version": 2
      }
    },
    "paper:2411.04872": {
      "data": {
        "arxivId": "2411.04872",
        "url": "https://arxiv.org/abs/2411.04872",
        "title": "FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning\n  in AI",
        "authors": "Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, Olli J\u00e4rviniemi, Matthew Barnett, Robert Sandler, Matej Vrzala, Jaime Sevilla, Qiuyu Ren, Elizabeth Pratt, Lionel Levine, Grant Barkley, Natalie Stewart, Bogdan Grechuk, Tetiana Grechuk, Shreepranav Varma Enugandla, Mark Wildon",
        "abstract": "We introduce FrontierMath, a benchmark of hundreds of original, exceptionally\nchallenging mathematics problems crafted and vetted by expert mathematicians.\nThe questions cover most major branches of modern mathematics -- from\ncomputationally intensive problems in number theory and real analysis to\nabstract questions in algebraic geometry and category theory. Solving a typical\nproblem requires multiple hours of effort from a researcher in the relevant\nbranch of mathematics, and for the upper end questions, multiple days.\nFrontierMath uses new, unpublished problems and automated verification to\nreliably evaluate models while minimizing risk of data contamination. Current\nstate-of-the-art AI models solve under 2% of problems, revealing a vast gap\nbetween AI capabilities and the prowess of the mathematical community. As AI\nsystems advance toward expert-level mathematical abilities, FrontierMath offers\na rigorous testbed that quantifies their progress.",
        "timestamp": "2025-01-20T09:38:38.997Z",
        "rating": "novote",
        "published_date": "2024-11-07T17:07:35Z",
        "arxiv_tags": [
          "cs.AI"
        ]
      },
      "meta": {
        "created_at": "2025-01-20T09:38:39+00:00",
        "updated_at": "2025-03-30T08:21:43+00:00",
        "version": 2
      }
    },
    "paper:1910.02054": {
      "data": {
        "arxivId": "1910.02054",
        "url": "https://arxiv.org/abs/1910.02054",
        "title": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models",
        "authors": "Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He",
        "abstract": "Large deep learning models offer significant accuracy gains, but training\nbillions to trillions of parameters is challenging. Existing solutions such as\ndata and model parallelisms exhibit fundamental limitations to fit these models\ninto limited device memory, while obtaining computation, communication and\ndevelopment efficiency. We develop a novel solution, Zero Redundancy Optimizer\n(ZeRO), to optimize memory, vastly improving training speed while increasing\nthe model size that can be efficiently trained. ZeRO eliminates memory\nredundancies in data- and model-parallel training while retaining low\ncommunication volume and high computational granularity, allowing us to scale\nthe model size proportional to the number of devices with sustained high\nefficiency. Our analysis on memory requirements and communication volume\ndemonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters\nusing today's hardware.\n  We implement and evaluate ZeRO: it trains large models of over 100B parameter\nwith super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops.\nThis represents an 8x increase in model size and 10x increase in achievable\nperformance over state-of-the-art. In terms of usability, ZeRO can train large\nmodels of up to 13B parameters (e.g., larger than Megatron GPT 8.3B and T5 11B)\nwithout requiring model parallelism which is harder for scientists to apply.\nLast but not the least, researchers have used the system breakthroughs of ZeRO\nto create the world's largest language model (Turing-NLG, 17B parameters) with\nrecord breaking accuracy.",
        "timestamp": "2025-01-20T07:32:15.795Z",
        "rating": "novote",
        "published_date": "2019-10-04T17:29:39Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.DC",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-01-20T07:32:16+00:00",
        "updated_at": "2025-03-30T08:22:17+00:00",
        "version": 2
      }
    },
    "interactions:2104.07857": {
      "data": {
        "paper_id": "2104.07857",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-20T07:31:48.486Z",
            "data": {
              "duration_seconds": 10
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-20T07:31:40+00:00",
        "updated_at": "2025-03-30T08:22:35+00:00",
        "version": 4
      }
    },
    "paper:2104.07857": {
      "data": {
        "arxivId": "2104.07857",
        "url": "https://arxiv.org/abs/2104.07857",
        "title": "ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep\n  Learning",
        "authors": "Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, Yuxiong He",
        "abstract": "In the last three years, the largest dense deep learning models have grown\nover 1000x to reach hundreds of billions of parameters, while the GPU memory\nhas only grown by 5x (16 GB to 80 GB). Therefore, the growth in model scale has\nbeen supported primarily though system innovations that allow large models to\nfit in the aggregate GPU memory of multiple GPUs. However, we are getting close\nto the GPU memory wall. It requires 800 NVIDIA V100 GPUs just to fit a trillion\nparameter model for training, and such clusters are simply out of reach for\nmost data scientists. In addition, training models at that scale requires\ncomplex combinations of parallelism techniques that puts a big burden on the\ndata scientists to refactor their model.\n  In this paper we present ZeRO-Infinity, a novel heterogeneous system\ntechnology that leverages GPU, CPU, and NVMe memory to allow for unprecedented\nmodel scale on limited resources without requiring model code refactoring. At\nthe same time it achieves excellent training throughput and scalability,\nunencumbered by the limited CPU or NVMe bandwidth. ZeRO-Infinity can fit models\nwith tens and even hundreds of trillions of parameters for training on current\ngeneration GPU clusters. It can be used to fine-tune trillion parameter models\non a single NVIDIA DGX-2 node, making large models more accessible. In terms of\ntraining throughput and scalability, it sustains over 25 petaflops on 512\nNVIDIA V100 GPUs(40% of peak), while also demonstrating super linear\nscalability. An open source implementation of ZeRO-Infinity is available\nthrough DeepSpeed, a deep learning optimization library that makes distributed\ntraining easy, efficient, and effective.",
        "timestamp": "2025-01-20T07:31:26.129Z",
        "rating": "novote",
        "published_date": "2021-04-16T02:22:12Z",
        "arxiv_tags": [
          "cs.DC",
          "cs.AI",
          "cs.LG",
          "cs.PF"
        ]
      },
      "meta": {
        "created_at": "2025-01-20T07:31:26+00:00",
        "updated_at": "2025-03-30T08:22:45+00:00",
        "version": 2
      }
    },
    "interactions:2501.00663": {
      "data": {
        "paper_id": "2501.00663",
        "interactions": [
          {
            "type": "rating",
            "timestamp": "2025-01-20T07:23:22.354Z",
            "data": {
              "rating": "thumbsup"
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-20T07:23:16+00:00",
        "updated_at": "2025-03-30T08:23:11+00:00",
        "version": 5
      }
    },
    "paper:2501.00663": {
      "data": {
        "arxivId": "2501.00663",
        "url": "https://arxiv.org/abs/2501.00663",
        "title": "Titans: Learning to Memorize at Test Time",
        "authors": "Ali Behrouz, Peilin Zhong, Vahab Mirrokni",
        "abstract": "Over more than a decade there has been an extensive research effort on how to\neffectively utilize recurrent models and attention. While recurrent models aim\nto compress the data into a fixed-size memory (called hidden state), attention\nallows attending to the entire context window, capturing the direct\ndependencies of all tokens. This more accurate modeling of dependencies,\nhowever, comes with a quadratic cost, limiting the model to a fixed-length\ncontext. We present a new neural long-term memory module that learns to\nmemorize historical context and helps attention to attend to the current\ncontext while utilizing long past information. We show that this neural memory\nhas the advantage of fast parallelizable training while maintaining a fast\ninference. From a memory perspective, we argue that attention due to its\nlimited context but accurate dependency modeling performs as a short-term\nmemory, while neural memory due to its ability to memorize the data, acts as a\nlong-term, more persistent, memory. Based on these two modules, we introduce a\nnew family of architectures, called Titans, and present three variants to\naddress how one can effectively incorporate memory into this architecture. Our\nexperimental results on language modeling, common-sense reasoning, genomics,\nand time series tasks show that Titans are more effective than Transformers and\nrecent modern linear recurrent models. They further can effectively scale to\nlarger than 2M context window size with higher accuracy in needle-in-haystack\ntasks compared to baselines.",
        "timestamp": "2025-01-20T07:22:12.333Z",
        "rating": "novote",
        "published_date": "2024-12-31T22:32:03Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI",
          "cs.CL"
        ]
      },
      "meta": {
        "created_at": "2025-01-20T07:22:12+00:00",
        "updated_at": "2025-03-30T08:23:29+00:00",
        "version": 2
      }
    },
    "paper:2412.06769": {
      "data": {
        "arxivId": "2412.06769",
        "url": "https://arxiv.org/abs/2412.06769",
        "title": "Training Large Language Models to Reason in a Continuous Latent Space",
        "authors": "Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, Yuandong Tian",
        "abstract": "Large language models (LLMs) are restricted to reason in the \"language\nspace\", where they typically express the reasoning process with a\nchain-of-thought (CoT) to solve a complex reasoning problem. However, we argue\nthat language space may not always be optimal for reasoning. For example, most\nword tokens are primarily for textual coherence and not essential for\nreasoning, while some critical tokens require complex planning and pose huge\nchallenges to LLMs. To explore the potential of LLM reasoning in an\nunrestricted latent space instead of using natural language, we introduce a new\nparadigm Coconut (Chain of Continuous Thought). We utilize the last hidden\nstate of the LLM as a representation of the reasoning state (termed \"continuous\nthought\"). Rather than decoding this into a word token, we feed it back to the\nLLM as the subsequent input embedding directly in the continuous space.\nExperiments show that Coconut can effectively augment the LLM on several\nreasoning tasks. This novel latent reasoning paradigm leads to emergent\nadvanced reasoning patterns: the continuous thought can encode multiple\nalternative next reasoning steps, allowing the model to perform a breadth-first\nsearch (BFS) to solve the problem, rather than prematurely committing to a\nsingle deterministic path like CoT. Coconut outperforms CoT in certain logical\nreasoning tasks that require substantial backtracking during planning, with\nfewer thinking tokens during inference. These findings demonstrate the promise\nof latent reasoning and offer valuable insights for future research.",
        "timestamp": "2025-01-20T07:20:45.082Z",
        "rating": "novote",
        "published_date": "2024-12-09T18:55:56Z",
        "arxiv_tags": [
          "cs.CL"
        ]
      },
      "meta": {
        "created_at": "2025-01-20T07:20:45+00:00",
        "updated_at": "2025-03-30T08:23:47+00:00",
        "version": 2
      }
    },
    "interactions:2412.06769": {
      "data": {
        "paper_id": "2412.06769",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-20T07:21:08.269Z",
            "data": {
              "duration_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-20T07:21:02+00:00",
        "updated_at": "2025-03-30T08:24:29+00:00",
        "version": 5
      }
    },
    "interactions:2501.09891": {
      "data": {
        "paper_id": "2501.09891",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-20T07:18:47.104Z",
            "data": {
              "duration_seconds": 50
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-20T07:17:51+00:00",
        "updated_at": "2025-03-30T08:24:47+00:00",
        "version": 4
      }
    },
    "paper:2501.09891": {
      "data": {
        "arxivId": "2501.09891",
        "url": "https://arxiv.org/abs/2501.09891",
        "title": "Evolving Deeper LLM Thinking",
        "authors": "Kuang-Huei Lee, Ian Fischer, Yueh-Hua Wu, Dave Marwood, Shumeet Baluja, Dale Schuurmans, Xinyun Chen",
        "abstract": "We explore an evolutionary search strategy for scaling inference time compute\nin Large Language Models. The proposed approach, Mind Evolution, uses a\nlanguage model to generate, recombine and refine candidate responses. The\nproposed approach avoids the need to formalize the underlying inference problem\nwhenever a solution evaluator is available. Controlling for inference cost, we\nfind that Mind Evolution significantly outperforms other inference strategies\nsuch as Best-of-N and Sequential Revision in natural language planning tasks.\nIn the TravelPlanner and Natural Plan benchmarks, Mind Evolution solves more\nthan 98% of the problem instances using Gemini 1.5 Pro without the use of a\nformal solver.",
        "timestamp": "2025-01-20T07:17:04.098Z",
        "rating": "novote",
        "published_date": "2025-01-17T00:41:44Z",
        "arxiv_tags": [
          "cs.AI"
        ]
      },
      "meta": {
        "created_at": "2025-01-20T07:17:04+00:00",
        "updated_at": "2025-03-30T08:24:56+00:00",
        "version": 2
      }
    },
    "interactions:2403.09635": {
      "data": {
        "paper_id": "2403.09635",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-20T07:13:23.907Z",
            "data": {
              "duration_seconds": 13
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-20T07:13:01+00:00",
        "updated_at": "2025-03-30T08:25:14+00:00",
        "version": 4
      }
    },
    "paper:2403.09635": {
      "data": {
        "arxivId": "2403.09635",
        "url": "https://arxiv.org/abs/2403.09635",
        "title": "Transformers Get Stable: An End-to-End Signal Propagation Theory for\n  Language Models",
        "authors": "Akhil Kedia, Mohd Abbas Zaidi, Sushil Khyalia, Jungho Jung, Harshith Goka, Haejun Lee",
        "abstract": "In spite of their huge success, transformer models remain difficult to scale\nin depth. In this work, we develop a unified signal propagation theory and\nprovide formulae that govern the moments of the forward and backward signal\nthrough the transformer model. Our framework can be used to understand and\nmitigate vanishing/exploding gradients, rank collapse, and instability\nassociated with high attention scores. We also propose DeepScaleLM, an\ninitialization and scaling scheme that conserves unit output/gradient moments\nthroughout the model, enabling the training of very deep models with 1000\nlayers. We find that transformer models could be much deeper - our deep models\nwith fewer parameters outperform shallow models in Language Modeling, Speech\nTranslation, and Image Classification, across encoder-only, decoder-only and\nencoder-decoder variants, for both Pre-LN and Post-LN transformers, for\nmultiple datasets and model sizes. These improvements also translate into\nimproved performance on downstream Question Answering tasks and improved\nrobustness for Image Classification.",
        "timestamp": "2025-01-20T07:12:49.004Z",
        "rating": "novote",
        "published_date": "2024-03-14T17:59:14Z",
        "arxiv_tags": [
          "cs.CL",
          "cs.AI",
          "cs.CV",
          "cs.LG",
          "I.2.7; I.2.10"
        ]
      },
      "meta": {
        "created_at": "2025-01-20T07:12:49+00:00",
        "updated_at": "2025-03-30T08:25:24+00:00",
        "version": 2
      }
    },
    "interactions:2006.08570": {
      "data": {
        "paper_id": "2006.08570",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-20T05:50:39.552Z",
            "data": {
              "duration_seconds": 32
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-20T05:50:00+00:00",
        "updated_at": "2025-03-30T08:25:50+00:00",
        "version": 8
      }
    },
    "paper:2006.08570": {
      "data": {
        "arxivId": "2006.08570",
        "url": "https://arxiv.org/abs/2006.08570",
        "title": "Cross-temporal forecast reconciliation: Optimal combination method and\n  heuristic alternatives",
        "authors": "Tommaso Di Fonzo, Daniele Girolimetto",
        "abstract": "Forecast reconciliation is a post-forecasting process aimed to improve the\nquality of the base forecasts for a system of hierarchical/grouped time series\n(Hyndman et al., 2011). Contemporaneous (cross-sectional) and temporal\nhierarchies have been considered in the literature, but - except for Kourentzes\nand Athanasopoulos (2019) - generally these two features have not been fully\nconsidered together. Adopting a notation able to simultaneously deal with both\nforecast reconciliation dimensions, the paper shows two new results: (i) an\niterative cross-temporal forecast reconciliation procedure which extends, and\novercomes some weaknesses of, the two-step procedure by Kourentzes and\nAthanasopoulos (2019), and (ii) the closed-form expression of the optimal (in\nleast squares sense) point forecasts which fulfill both contemporaneous and\ntemporal constraints. The feasibility of the proposed procedures, along with\nfirst evaluations of their performance as compared to the most performing\n`single dimension' (either cross-sectional or temporal) forecast reconciliation\nprocedures, is studied through a forecasting experiment on the 95 quarterly\ntime series of the Australian GDP from Income and Expenditure sides considered\nby Athanasopoulos et al. (2019).",
        "timestamp": "2025-01-20T05:49:19.264Z",
        "rating": "novote",
        "published_date": "2020-06-15T17:34:05Z",
        "arxiv_tags": [
          "stat.ME"
        ]
      },
      "meta": {
        "created_at": "2025-01-20T05:49:20+00:00",
        "updated_at": "2025-03-30T08:26:32+00:00",
        "version": 2
      }
    },
    "interactions:2006.02043": {
      "data": {
        "paper_id": "2006.02043",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-01-20T00:20:33+00:00",
        "updated_at": "2025-03-30T08:26:50+00:00",
        "version": 2
      }
    },
    "paper:2006.02043": {
      "data": {
        "arxivId": "2006.02043",
        "url": "https://arxiv.org/pdf/2006.02043",
        "title": "Hierarchical forecast reconciliation with machine learning",
        "authors": "Evangelos Spiliotis, Mahdi Abolghasemi, Rob J Hyndman, Fotios Petropoulos, Vassilios Assimakopoulos",
        "abstract": "Hierarchical forecasting methods have been widely used to support aligned\ndecision-making by providing coherent forecasts at different aggregation\nlevels. Traditional hierarchical forecasting approaches, such as the bottom-up\nand top-down methods, focus on a particular aggregation level to anchor the\nforecasts. During the past decades, these have been replaced by a variety of\nlinear combination approaches that exploit information from the complete\nhierarchy to produce more accurate forecasts. However, the performance of these\ncombination methods depends on the particularities of the examined series and\ntheir relationships. This paper proposes a novel hierarchical forecasting\napproach based on machine learning that deals with these limitations in three\nimportant ways. First, the proposed method allows for a non-linear combination\nof the base forecasts, thus being more general than the linear approaches.\nSecond, it structurally combines the objectives of improved post-sample\nempirical forecasting accuracy and coherence. Finally, due to its non-linear\nnature, our approach selectively combines the base forecasts in a direct and\nautomated way without requiring that the complete information must be used for\nproducing reconciled forecasts for each series and level. The proposed method\nis evaluated both in terms of accuracy and bias using two different data sets\ncoming from the tourism and retail industries. Our results suggest that the\nproposed method gives superior point forecasts than existing approaches,\nespecially when the series comprising the hierarchy are not characterized by\nthe same patterns.",
        "timestamp": "2025-01-20T00:20:16.561Z",
        "rating": "novote",
        "published_date": "2020-06-03T04:49:39Z",
        "arxiv_tags": [
          "cs.LG",
          "stat.CO",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-01-20T00:20:17+00:00",
        "updated_at": "2025-03-30T08:27:00+00:00",
        "version": 2
      }
    },
    "paper:1801.02042": {
      "data": {
        "arxivId": "1801.02042",
        "url": "https://arxiv.org/pdf/1801.02042",
        "title": "Learning from Neighbors about a Changing State",
        "authors": "Krishna Dasaratha, Benjamin Golub, Nir Hak",
        "abstract": "Agents learn about a changing state using private signals and their\nneighbors' past estimates of the state. We present a model in which Bayesian\nagents in equilibrium use neighbors' estimates simply by taking weighted sums\nwith time-invariant weights. The dynamics thus parallel those of the tractable\nDeGroot model of learning in networks, but arise as an equilibrium outcome\nrather than a behavioral assumption. We examine whether information aggregation\nis nearly optimal as neighborhoods grow large. A key condition for this is\nsignal diversity: each individual's neighbors have private signals that not\nonly contain independent information, but also have sufficiently different\ndistributions. Without signal diversity $\\unicode{x2013}$ e.g., if private\nsignals are i.i.d. $\\unicode{x2013}$ learning is suboptimal in all networks and\nhighly inefficient in some. Turning to social influence, we find it is much\nmore sensitive to one's signal quality than to one's number of neighbors, in\ncontrast to standard models with exogenous updating rules.",
        "timestamp": "2025-01-19T20:59:30.013Z",
        "rating": "novote",
        "published_date": "2018-01-06T16:14:47Z",
        "arxiv_tags": [
          "econ.TH",
          "cs.GT",
          "cs.SI"
        ]
      },
      "meta": {
        "created_at": "2025-01-19T20:59:30+00:00",
        "updated_at": "2025-01-19T20:59:33+00:00",
        "version": 2
      }
    },
    "interactions:1710.06026": {
      "data": {
        "paper_id": "1710.06026",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-19T20:57:29.512Z",
            "data": {
              "duration_seconds": 25
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-19T20:57:30+00:00",
        "updated_at": "2025-03-30T08:27:18+00:00",
        "version": 4
      }
    },
    "paper:1710.06026": {
      "data": {
        "arxivId": "1710.06026",
        "url": "https://arxiv.org/abs/1710.06026",
        "title": "Targeting Interventions in Networks",
        "authors": "Andrea Galeotti, Benjamin Golub, Sanjeev Goyal",
        "abstract": "We study games in which a network mediates strategic spillovers and\nexternalities among the players. How does a planner optimally target\ninterventions that change individuals' private returns to investment? We\nanalyze this question by decomposing any intervention into orthogonal principal\ncomponents, which are determined by the network and are ordered according to\ntheir associated eigenvalues. There is a close connection between the nature of\nspillovers and the representation of various principal components in the\noptimal intervention. In games of strategic complements (substitutes),\ninterventions place more weight on the top (bottom) principal components, which\nreflect more global (local) network structure. For large budgets, optimal\ninterventions are simple -- they involve a single principal component.",
        "timestamp": "2025-01-19T20:56:52.926Z",
        "rating": "novote",
        "published_date": "2017-10-16T23:18:55Z",
        "arxiv_tags": [
          "cs.GT"
        ]
      },
      "meta": {
        "created_at": "2025-01-19T20:56:53+00:00",
        "updated_at": "2025-03-30T08:27:28+00:00",
        "version": 2
      }
    },
    "interactions:2307.13912": {
      "data": {
        "paper_id": "2307.13912",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-19T15:26:37.045Z",
            "data": {
              "duration_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-19T15:26:37+00:00",
        "updated_at": "2025-03-30T08:27:46+00:00",
        "version": 3
      }
    },
    "paper:2307.13912": {
      "data": {
        "arxivId": "2307.13912",
        "url": "https://arxiv.org/abs/2307.13912",
        "title": "Embedding Democratic Values into Social Media AIs via Societal Objective\n  Functions",
        "authors": "Chenyan Jia, Michelle S. Lam, Minh Chau Mai, Jeff Hancock, Michael S. Bernstein",
        "abstract": "Can we design artificial intelligence (AI) systems that rank our social media\nfeeds to consider democratic values such as mitigating partisan animosity as\npart of their objective functions? We introduce a method for translating\nestablished, vetted social scientific constructs into AI objective functions,\nwhich we term societal objective functions, and demonstrate the method with\napplication to the political science construct of anti-democratic attitudes.\nTraditionally, we have lacked observable outcomes to use to train such models,\nhowever, the social sciences have developed survey instruments and qualitative\ncodebooks for these constructs, and their precision facilitates translation\ninto detailed prompts for large language models. We apply this method to create\na democratic attitude model that estimates the extent to which a social media\npost promotes anti-democratic attitudes, and test this democratic attitude\nmodel across three studies. In Study 1, we first test the attitudinal and\nbehavioral effectiveness of the intervention among US partisans (N=1,380) by\nmanually annotating (alpha=.895) social media posts with anti-democratic\nattitude scores and testing several feed ranking conditions based on these\nscores. Removal (d=.20) and downranking feeds (d=.25) reduced participants'\npartisan animosity without compromising their experience and engagement. In\nStudy 2, we scale up the manual labels by creating the democratic attitude\nmodel, finding strong agreement with manual labels (rho=.75). Finally, in Study\n3, we replicate Study 1 using the democratic attitude model instead of manual\nlabels to test its attitudinal and behavioral impact (N=558), and again find\nthat the feed downranking using the societal objective function reduced\npartisan animosity (d=.25). This method presents a novel strategy to draw on\nsocial science theory and methods to mitigate societal harms in social media\nAIs.",
        "timestamp": "2025-01-19T15:26:24.313Z",
        "rating": "novote",
        "published_date": "2023-07-26T02:27:24Z",
        "arxiv_tags": [
          "cs.HC",
          "cs.AI"
        ]
      },
      "meta": {
        "created_at": "2025-01-19T15:26:24+00:00",
        "updated_at": "2025-03-30T08:28:12+00:00",
        "version": 2
      }
    },
    "interactions:2410.05437": {
      "data": {
        "paper_id": "2410.05437",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-19T09:46:23.440Z",
            "data": {
              "duration_seconds": 17
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-19T09:46:19+00:00",
        "updated_at": "2025-03-30T08:28:22+00:00",
        "version": 4
      }
    },
    "paper:2410.05437": {
      "data": {
        "arxivId": "2410.05437",
        "url": "https://arxiv.org/abs/2410.05437",
        "title": "ESPACE: Dimensionality Reduction of Activations for Model Compression",
        "authors": "Charbel Sakr, Brucek Khailany",
        "abstract": "We propose ESPACE, an LLM compression technique based on dimensionality\nreduction of activations. Unlike prior works on weight-centric tensor\ndecomposition, ESPACE projects activations onto a pre-calibrated set of\nprincipal components. The activation-centrality of the approach enables\nretraining LLMs with no loss of expressivity; while at inference, weight\ndecomposition is obtained as a byproduct of matrix multiplication\nassociativity. Theoretical results on the construction of projection matrices\nwith optimal computational accuracy are provided. Experimentally, we find\nESPACE enables 50% compression of GPT3, Llama2, and Nemotron4 models with small\naccuracy degradation, as low as a 0.18 perplexity increase on GPT3-22B. At\nlower compression rates of 20% to 40%, ESPACE drives GPT3 models to\noutperforming their baseline, by up to a 0.38 decrease in perplexity for\nGPT3-8B. ESPACE also reduces GEMM execution time and prefill inference latency\non existing hardware. Comparison with related works on compressing Llama2-7B\nvia matrix factorization shows that ESPACE is a first step in advancing the\nstate-of-the-art in tensor decomposition compression of LLMs.",
        "timestamp": "2025-01-19T09:45:58.441Z",
        "rating": "novote",
        "published_date": "2024-10-07T18:59:22Z",
        "arxiv_tags": [
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-01-19T09:45:58+00:00",
        "updated_at": "2025-03-30T08:28:32+00:00",
        "version": 2
      }
    },
    "interactions:2406.07522": {
      "data": {
        "paper_id": "2406.07522",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-01-19T03:47:56+00:00",
        "updated_at": "2025-03-30T08:29:29+00:00",
        "version": 2
      }
    },
    "paper:2406.07522": {
      "data": {
        "arxivId": "2406.07522",
        "url": "https://arxiv.org/abs/2406.07522",
        "title": "Samba: Simple Hybrid State Space Models for Efficient Unlimited Context\n  Language Modeling",
        "authors": "Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, Weizhu Chen",
        "abstract": "Efficiently modeling sequences with infinite context length has long been a\nchallenging problem. Previous approaches have either suffered from quadratic\ncomputational complexity or limited extrapolation ability in length\ngeneralization. In this work, we present Samba, a simple hybrid architecture\nthat layer-wise combines Mamba, a selective State Space Model (SSM), with\nSliding Window Attention (SWA). Samba selectively compresses a given sequence\ninto recurrent hidden states while still maintaining the ability to precisely\nrecall recent memories with the attention mechanism. We scale Samba up to 3.8B\nparameters with 3.2T training tokens and demonstrate that it significantly\noutperforms state-of-the-art models across a variety of benchmarks. Pretrained\non sequences of 4K length, Samba shows improved perplexity in context lengths\nof up to 1M in zero-shot. When finetuned on 4K-length sequences, Samba\nefficiently extrapolates to a 256K context length with perfect memory recall on\nthe Passkey Retrieval task, and exhibits superior retrieval extrapolation on\nthe challenging Phonebook task compared to full-attention models. As a\nlinear-time sequence model, Samba achieves a 3.73x higher throughput compared\nto Transformers with grouped-query attention for user prompts of 128K length,\nand a 3.64x speedup when generating 64K tokens with unlimited streaming. Our\ncode for training on open source data is publicly available at\nhttps://github.com/microsoft/Samba.",
        "timestamp": "2025-01-19T03:47:40.608Z",
        "rating": "novote",
        "published_date": "2024-06-11T17:50:51Z",
        "arxiv_tags": [
          "cs.CL",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-01-19T03:47:41+00:00",
        "updated_at": "2025-03-30T08:30:02+00:00",
        "version": 2
      }
    },
    "interactions:1503.03585": {
      "data": {
        "paper_id": "1503.03585",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-19T02:31:42.622Z",
            "data": {
              "duration_seconds": 13,
              "session_config": {
                "idle_threshold_seconds": 300,
                "min_duration_seconds": 3,
                "continuous_activity_required": true,
                "partial_sessions_logged": false
              }
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-19T02:31:20+00:00",
        "updated_at": "2025-03-30T08:30:44+00:00",
        "version": 4
      }
    },
    "paper:1503.03585": {
      "data": {
        "arxivId": "1503.03585",
        "url": "https://arxiv.org/abs/1503.03585",
        "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
        "authors": "Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli",
        "abstract": "A central problem in machine learning involves modeling complex data-sets\nusing highly flexible families of probability distributions in which learning,\nsampling, inference, and evaluation are still analytically or computationally\ntractable. Here, we develop an approach that simultaneously achieves both\nflexibility and tractability. The essential idea, inspired by non-equilibrium\nstatistical physics, is to systematically and slowly destroy structure in a\ndata distribution through an iterative forward diffusion process. We then learn\na reverse diffusion process that restores structure in data, yielding a highly\nflexible and tractable generative model of the data. This approach allows us to\nrapidly learn, sample from, and evaluate probabilities in deep generative\nmodels with thousands of layers or time steps, as well as to compute\nconditional and posterior probabilities under the learned model. We\nadditionally release an open source reference implementation of the algorithm.",
        "timestamp": "2025-01-19T02:31:15.039Z",
        "rating": "novote",
        "published_date": "2015-03-12T04:51:37Z",
        "arxiv_tags": [
          "cs.LG",
          "cond-mat.dis-nn",
          "q-bio.NC",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-01-19T02:31:15+00:00",
        "updated_at": "2025-03-30T08:31:10+00:00",
        "version": 2
      }
    },
    "interactions:2405.07987": {
      "data": {
        "paper_id": "2405.07987",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-25T03:33:43.312Z",
            "data": {
              "session_id": "session_1737775995128_823e123",
              "duration_seconds": 23,
              "idle_seconds": 0,
              "start_time": "2025-01-25T03:33:15.128Z",
              "end_time": "2025-01-25T03:33:37.681Z",
              "total_elapsed_seconds": 23
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-25T03:34:26.731Z",
            "data": {
              "session_id": "session_1737776038591_jr5426l",
              "duration_seconds": 28,
              "idle_seconds": 0,
              "start_time": "2025-01-25T03:33:58.591Z",
              "end_time": "2025-01-25T03:34:26.720Z",
              "total_elapsed_seconds": 28
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-25T03:34:53.954Z",
            "data": {
              "session_id": "session_1737776069173_63927kh",
              "duration_seconds": 22,
              "idle_seconds": 0,
              "start_time": "2025-01-25T03:34:29.173Z",
              "end_time": "2025-01-25T03:34:51.012Z",
              "total_elapsed_seconds": 22
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-25T05:07:48.512Z",
            "data": {
              "session_id": "session_1737781656680_dfgmzhc",
              "duration_seconds": 12,
              "idle_seconds": 0,
              "start_time": "2025-01-25T05:07:36.680Z",
              "end_time": "2025-01-25T05:07:48.504Z",
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-25T05:20:22.857Z",
            "data": {
              "session_id": "session_1737782408397_970dbde",
              "duration_seconds": 14,
              "idle_seconds": 0,
              "start_time": "2025-01-25T05:20:08.397Z",
              "end_time": "2025-01-25T05:20:22.245Z",
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-25T05:28:39.568Z",
            "data": {
              "session_id": "session_1737782910571_c1pwhng",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-01-25T05:28:30.571Z",
              "end_time": "2025-01-25T05:28:38.176Z",
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-25T06:22:48.568Z",
            "data": {
              "session_id": "session_1737786153474_cyz0a5e",
              "duration_seconds": 15,
              "idle_seconds": 0,
              "start_time": "2025-01-25T06:22:33.474Z",
              "end_time": "2025-01-25T06:22:48.369Z",
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T17:30:34.786Z",
            "data": {
              "session_id": "session_1737912621845_hrkv0t1",
              "duration_seconds": 13,
              "idle_seconds": 0,
              "start_time": "2025-01-26T17:30:21.846Z",
              "end_time": "2025-01-26T17:30:34.593Z",
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T17:31:52.770Z",
            "data": {
              "session_id": "session_1737912693672_92zw2fd",
              "duration_seconds": 18,
              "idle_seconds": 0,
              "start_time": "2025-01-26T17:31:33.672Z",
              "end_time": "2025-01-26T17:31:52.108Z",
              "total_elapsed_seconds": 18
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T17:44:24.170Z",
            "data": {
              "session_id": "session_1737913455635_be9rxrm",
              "duration_seconds": 9,
              "idle_seconds": 0,
              "start_time": "2025-01-26T17:44:15.635Z",
              "end_time": "2025-01-26T17:44:24.158Z",
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T18:00:14.764Z",
            "data": {
              "session_id": "session_1737914406476_buksn0u",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-01-26T18:00:06.476Z",
              "end_time": "2025-01-26T18:00:14.756Z",
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T18:04:30.486Z",
            "data": {
              "session_id": "session_1737914665494_dzmw70a",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-01-26T18:04:25.494Z",
              "end_time": "2025-01-26T18:04:30.295Z",
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T18:08:42.226Z",
            "data": {
              "session_id": "session_1737914915235_xmyy3nu",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-01-26T18:08:35.235Z",
              "end_time": "2025-01-26T18:08:39.094Z",
              "total_elapsed_seconds": 4
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T18:46:46.046Z",
            "data": {
              "session_id": "session_1737917194590_uxzn9qg",
              "duration_seconds": 11,
              "idle_seconds": 0,
              "start_time": "2025-01-26T18:46:34.590Z",
              "end_time": "2025-01-26T18:46:46.035Z",
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T19:04:15.701Z",
            "data": {
              "session_id": "session_1737918242839_149myfe",
              "duration_seconds": 13,
              "idle_seconds": 0,
              "start_time": "2025-01-26T19:04:02.839Z",
              "end_time": "2025-01-26T19:04:15.690Z",
              "total_elapsed_seconds": 13
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-25T03:33:39+00:00",
        "updated_at": "2025-03-30T08:14:01+00:00",
        "version": 34
      }
    },
    "paper:2405.07987": {
      "data": {
        "arxivId": "2405.07987",
        "url": "https://arxiv.org/abs/2405.07987",
        "title": "The Platonic Representation Hypothesis",
        "authors": "Minyoung Huh, Brian Cheung, Tongzhou Wang, Phillip Isola",
        "abstract": "We argue that representations in AI models, particularly deep networks, are\nconverging. First, we survey many examples of convergence in the literature:\nover time and across multiple domains, the ways by which different neural\nnetworks represent data are becoming more aligned. Next, we demonstrate\nconvergence across data modalities: as vision models and language models get\nlarger, they measure distance between datapoints in a more and more alike way.\nWe hypothesize that this convergence is driving toward a shared statistical\nmodel of reality, akin to Plato's concept of an ideal reality. We term such a\nrepresentation the platonic representation and discuss several possible\nselective pressures toward it. Finally, we discuss the implications of these\ntrends, their limitations, and counterexamples to our analysis.",
        "timestamp": "2025-01-25T03:33:15.181Z",
        "rating": "thumbsup",
        "published_date": "2024-05-13T17:58:30Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI",
          "cs.CV",
          "cs.NE"
        ]
      },
      "meta": {
        "created_at": "2025-01-25T03:33:15+00:00",
        "updated_at": "2025-01-25T05:08:15+00:00",
        "version": 4
      }
    },
    "interactions:2501.13928": {
      "data": {
        "paper_id": "2501.13928",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T06:21:43.630Z",
            "data": {
              "session_id": "session_1737872496276_hp25cq5",
              "duration_seconds": 7,
              "idle_seconds": 0,
              "start_time": "2025-01-26T06:21:36.276Z",
              "end_time": "2025-01-26T06:21:42.903Z",
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T06:30:24.892Z",
            "data": {
              "session_id": "session_1737873020744_dpvbin3",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-01-26T06:30:20.744Z",
              "end_time": "2025-01-26T06:30:23.772Z",
              "total_elapsed_seconds": 3
            }
          },
          {
            "type": "rating",
            "timestamp": "2025-01-26T06:32:00.574Z",
            "data": {
              "rating": "thumbsup"
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T06:33:09.937Z",
            "data": {
              "session_id": "session_1737873178285_xirs92q",
              "duration_seconds": 11,
              "idle_seconds": 0,
              "start_time": "2025-01-26T06:32:58.285Z",
              "end_time": "2025-01-26T06:33:09.185Z",
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T21:13:27.703Z",
            "data": {
              "session_id": "session_1737926001227_xo3ybup",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-01-26T21:13:21.227Z",
              "end_time": "2025-01-26T21:13:27.691Z",
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-26T06:21:44+00:00",
        "updated_at": "2025-01-26T21:14:33+00:00",
        "version": 8
      }
    },
    "paper:2501.13928": {
      "data": {
        "arxivId": "2501.13928",
        "url": "https://arxiv.org/abs/2501.13928",
        "title": "Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass",
        "authors": "Jianing Yang, Alexander Sax, Kevin J. Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, Matt Feiszli",
        "abstract": "Multi-view 3D reconstruction remains a core challenge in computer vision,\nparticularly in applications requiring accurate and scalable representations\nacross diverse perspectives. Current leading methods such as DUSt3R employ a\nfundamentally pairwise approach, processing images in pairs and necessitating\ncostly global alignment procedures to reconstruct from multiple views. In this\nwork, we propose Fast 3D Reconstruction (Fast3R), a novel multi-view\ngeneralization to DUSt3R that achieves efficient and scalable 3D reconstruction\nby processing many views in parallel. Fast3R's Transformer-based architecture\nforwards N images in a single forward pass, bypassing the need for iterative\nalignment. Through extensive experiments on camera pose estimation and 3D\nreconstruction, Fast3R demonstrates state-of-the-art performance, with\nsignificant improvements in inference speed and reduced error accumulation.\nThese results establish Fast3R as a robust alternative for multi-view\napplications, offering enhanced scalability without compromising reconstruction\naccuracy.",
        "timestamp": "2025-01-26T06:21:36.654Z",
        "rating": "thumbsup",
        "published_date": "2025-01-23T18:59:55Z",
        "arxiv_tags": [
          "cs.CV",
          "cs.AI",
          "cs.GR",
          "cs.RO"
        ]
      },
      "meta": {
        "created_at": "2025-01-26T06:21:37+00:00",
        "updated_at": "2025-03-30T08:14:21+00:00",
        "version": 3
      }
    },
    "interactions:2501.12005": {
      "data": {
        "paper_id": "2501.12005",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T16:13:49.497Z",
            "data": {
              "session_id": "session_1737908018868_9fm5m83",
              "duration_seconds": 10,
              "idle_seconds": 0,
              "start_time": "2025-01-26T16:13:38.868Z",
              "end_time": "2025-01-26T16:13:48.756Z",
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T21:13:11.337Z",
            "data": {
              "session_id": "session_1737925981150_qdtg11j",
              "duration_seconds": 10,
              "idle_seconds": 0,
              "start_time": "2025-01-26T21:13:01.150Z",
              "end_time": "2025-01-26T21:13:11.328Z",
              "total_elapsed_seconds": 10
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-26T16:13:50+00:00",
        "updated_at": "2025-01-26T21:14:16+00:00",
        "version": 5
      }
    },
    "paper:2501.12005": {
      "data": {
        "arxivId": "2501.12005",
        "url": "https://arxiv.org/abs/2501.12005",
        "title": "A note on the relations between mixture models, maximum-likelihood and\n  entropic optimal transport",
        "authors": "Titouan Vayer, Etienne Lasalle",
        "abstract": "This note aims to demonstrate that performing maximum-likelihood estimation\nfor a mixture model is equivalent to minimizing over the parameters an optimal\ntransport problem with entropic regularization. The objective is pedagogical:\nwe seek to present this already known result in a concise and hopefully simple\nmanner. We give an illustration with Gaussian mixture models by showing that\nthe standard EM algorithm is a specific block-coordinate descent on an optimal\ntransport loss.",
        "timestamp": "2025-01-26T16:13:03.441Z",
        "rating": "novote",
        "published_date": "2025-01-21T09:55:21Z",
        "arxiv_tags": [
          "stat.ML",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-01-26T16:13:03+00:00",
        "updated_at": "2025-03-30T08:14:11+00:00",
        "version": 2
      }
    },
    "interactions:1805.03929": {
      "data": {
        "paper_id": "1805.03929",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-27T02:01:33.953Z",
            "data": {
              "session_id": "session_1737943279095_i01t1lh",
              "duration_seconds": 14,
              "idle_seconds": 0,
              "start_time": "2025-01-27T02:01:19.095Z",
              "end_time": "2025-01-27T02:01:33.412Z",
              "total_elapsed_seconds": 14
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-27T02:00:30+00:00",
        "updated_at": "2025-01-27T02:01:49+00:00",
        "version": 5
      }
    },
    "paper:1805.03929": {
      "data": {
        "arxivId": "1805.03929",
        "url": "https://arxiv.org/abs/1805.03929",
        "title": "Resource-Bounded Kolmogorov Complexity Provides an Obstacle to Soficness\n  of Multidimensional Shifts",
        "authors": "Julien Destombes, Andrei Romashchenko",
        "abstract": "We suggest necessary conditions of soficness of multidimensional shifts\nformulated in termsof resource-bounded Kolmogorov complexity. Using this\ntechnique we provide examples ofeffective and non-sofic shifts on\n$\\mathbb{Z}^2$ with very low block complexity: the number of globallyadmissible\npatterns of size $n\\times n$ grows only as a polynomial in $n$. We also show\nthat moreconventional proofs of non-soficness for multi-dimensional effective\nshifts can be expressed interms of Kolmogorov complexity with unbounded\ncomputational resources.",
        "timestamp": "2025-01-27T02:00:10.459Z",
        "rating": "novote",
        "published_date": "2018-05-10T11:47:47Z",
        "arxiv_tags": [
          "cs.DM",
          "cs.CC"
        ]
      },
      "meta": {
        "created_at": "2025-01-27T02:00:10+00:00",
        "updated_at": "2025-01-27T02:00:14+00:00",
        "version": 2
      }
    },
    "interactions:2003.13176": {
      "data": {
        "paper_id": "2003.13176",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-27T06:08:42.368Z",
            "data": {
              "session_id": "session_1737958099679_ruudfyj",
              "duration_seconds": 23,
              "idle_seconds": 0,
              "start_time": "2025-01-27T06:08:19.679Z",
              "end_time": "2025-01-27T06:08:42.363Z",
              "total_elapsed_seconds": 23
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-27T06:13:24.982Z",
            "data": {
              "session_id": "session_1737958394369_83iesup",
              "duration_seconds": 10,
              "idle_seconds": 0,
              "start_time": "2025-01-27T06:13:14.370Z",
              "end_time": "2025-01-27T06:13:24.369Z",
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-28T09:52:51.632Z",
            "data": {
              "session_id": "session_1738057952567_x4bib0s",
              "duration_seconds": 19,
              "idle_seconds": 0,
              "start_time": "2025-01-28T09:52:32.567Z",
              "end_time": "2025-01-28T09:52:51.307Z",
              "total_elapsed_seconds": 19
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-28T09:55:42.816Z",
            "data": {
              "session_id": "session_1738058117330_7k7dn7k",
              "duration_seconds": 25,
              "idle_seconds": 0,
              "start_time": "2025-01-28T09:55:17.330Z",
              "end_time": "2025-01-28T09:55:42.601Z",
              "total_elapsed_seconds": 25
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-27T06:08:28+00:00",
        "updated_at": "2025-01-28T09:57:03+00:00",
        "version": 7
      }
    },
    "paper:2003.13176": {
      "data": {
        "arxivId": "2003.13176",
        "url": "https://arxiv.org/pdf/2003.13176",
        "title": "Non-reciprocal phase transitions",
        "authors": "Michel Fruchart, Ryo Hanai, Peter B. Littlewood, Vincenzo Vitelli",
        "abstract": "Out of equilibrium, the lack of reciprocity is the rule rather than the\nexception. Non-reciprocal interactions occur, for instance, in networks of\nneurons, directional growth of interfaces, and synthetic active materials.\nWhile wave propagation in non-reciprocal media has recently been under intense\nstudy, less is known about the consequences of non-reciprocity on the\ncollective behavior of many-body systems. Here, we show that non-reciprocity\nleads to time-dependent phases where spontaneously broken symmetries are\ndynamically restored. The resulting phase transitions are controlled by\nspectral singularities called exceptional points. We describe the emergence of\nthese phases using insights from bifurcation theory and non-Hermitian quantum\nmechanics. Our approach captures non-reciprocal generalizations of three\narchetypal classes of self-organization out of equilibrium: synchronization,\nflocking and pattern formation. Collective phenomena in these non-reciprocal\nsystems range from active time-(quasi)crystals to exceptional-point enforced\npattern-formation and hysteresis. Our work paves the way towards a general\ntheory of critical phenomena in non-reciprocal matter.",
        "timestamp": "2025-01-27T06:07:05.033Z",
        "rating": "thumbsup",
        "published_date": "2020-03-30T01:09:20Z",
        "arxiv_tags": [
          "cond-mat.soft",
          "cond-mat.stat-mech"
        ]
      },
      "meta": {
        "created_at": "2025-01-27T06:07:05+00:00",
        "updated_at": "2025-01-27T06:09:42+00:00",
        "version": 3
      }
    },
    "interactions:1309.6605": {
      "data": {
        "paper_id": "1309.6605",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-27T07:09:12.829Z",
            "data": {
              "session_id": "session_1737961749044_dapnb4p",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-01-27T07:09:09.044Z",
              "end_time": "2025-01-27T07:09:12.063Z",
              "total_elapsed_seconds": 3
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-27T07:09:13+00:00",
        "updated_at": "2025-01-27T07:10:31+00:00",
        "version": 3
      }
    },
    "paper:1309.6605": {
      "data": {
        "arxivId": "1309.6605",
        "url": "https://arxiv.org/abs/1309.6605",
        "title": "Symmetries, Cluster Synchronization, and Isolated Desynchronization in\n  Complex Networks",
        "authors": "Louis M. Pecora, Francesco Sorrentino, Aaron M. Hagerstrom, Thomas E. Murphy, Rajarshi Roy",
        "abstract": "Synchronization is of central importance in power distribution,\ntelecommunication, neuronal, and biological networks. Many networks are\nobserved to produce patterns of synchronized clusters, but it has been\ndifficult to predict these clusters or understand the conditions under which\nthey form, except for in the simplest of networks. In this article, we shed\nlight on the intimate connection between network symmetry and cluster\nsynchronization. We introduce general techniques that use network symmetries to\nreveal the patterns of synchronized clusters and determine the conditions under\nwhich they persist. The connection between symmetry and cluster synchronization\nis experimentally explored using an electro-optic network. We experimentally\nobserve and theoretically predict a surprising phenomenon in which some\nclusters lose synchrony while leaving others synchronized. The results could\nguide the design of new power grid systems or lead to new understanding of the\ndynamical behavior of networks ranging from neural to social.",
        "timestamp": "2025-01-27T07:09:09.367Z",
        "rating": "novote",
        "published_date": "2013-09-25T18:42:34Z",
        "arxiv_tags": [
          "nlin.CD"
        ]
      },
      "meta": {
        "created_at": "2025-01-27T07:09:09+00:00",
        "updated_at": "2025-01-27T07:09:12+00:00",
        "version": 2
      }
    },
    "paper:0806.0594": {
      "data": {
        "arxivId": "0806.0594",
        "url": "https://arxiv.org/pdf/0806.0594",
        "title": "Solvable model for chimera states of coupled oscillators",
        "authors": "Daniel M. Abrams, Renato E. Mirollo, Steven H. Strogatz, Daniel A. Wiley",
        "abstract": "Networks of identical, symmetrically coupled oscillators can spontaneously\nsplit into synchronized and desynchronized sub-populations. Such chimera states\nwere discovered in 2002, but are not well understood theoretically. Here we\nobtain the first exact results about the stability, dynamics, and bifurcations\nof chimera states by analyzing a minimal model consisting of two interacting\npopulations of oscillators. Along with a completely synchronous state, the\nsystem displays stable chimeras, breathing chimeras, and saddle-node, Hopf and\nhomoclinic bifurcations of chimeras.",
        "timestamp": "2025-01-27T07:05:40.255Z",
        "rating": "novote",
        "published_date": "2008-06-03T17:26:51Z",
        "arxiv_tags": [
          "nlin.CD",
          "math.DS",
          "nlin.PS"
        ]
      },
      "meta": {
        "created_at": "2025-01-27T07:05:40+00:00",
        "updated_at": "2025-01-27T07:05:43+00:00",
        "version": 2
      }
    },
    "paper:1101.2899": {
      "data": {
        "arxivId": "1101.2899",
        "url": "https://arxiv.org/pdf/1101.2899",
        "title": "A mathematical framework for critical transitions: bifurcations,\n  fast-slow systems and stochastic dynamics",
        "authors": "Christian Kuehn",
        "abstract": "Bifurcations can cause dynamical systems with slowly varying parameters to\ntransition to far-away attractors. The terms ``critical transition'' or\n``tipping point'' have been used to describe this situation. Critical\ntransitions have been observed in an astonishingly diverse set of applications\nfrom ecosystems and climate change to medicine and finance. The main goal of\nthis paper is to give an overview which standard mathematical theories can be\napplied to critical transitions. We shall focus on early-warning signs that\nhave been suggested to predict critical transitions and point out what\nmathematical theory can provide in this context. Starting from classical\nbifurcation theory and incorporating multiple time scale dynamics one can give\na detailed analysis of local bifurcations that induce critical transitions. We\nsuggest that the mathematical theory of fast-slow systems provides a natural\ndefinition of critical transitions. Since noise often plays a crucial role near\ncritical transitions the next step is to consider stochastic fast-slow systems.\nThe interplay between sample path techniques, partial differential equations\nand random dynamical systems is highlighted. Each viewpoint provides potential\nearly-warning signs for critical transitions. Since increasing variance has\nbeen suggested as an early-warning sign we examine it in the context of normal\nforms analytically, numerically and geometrically; we also consider\nautocorrelation numerically. Hence we demonstrate the applicability of\nearly-warning signs for generic models. We end with suggestions for future\ndirections of the theory.",
        "timestamp": "2025-01-27T07:03:04.997Z",
        "rating": "novote",
        "published_date": "2011-01-14T21:00:57Z",
        "arxiv_tags": [
          "math.DS",
          "math.CA",
          "nlin.CD",
          "nlin.PS"
        ]
      },
      "meta": {
        "created_at": "2025-01-27T07:03:05+00:00",
        "updated_at": "2025-01-27T07:03:08+00:00",
        "version": 2
      }
    },
    "interactions:2406.10165": {
      "data": {
        "paper_id": "2406.10165",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-28T06:36:47.999Z",
            "data": {
              "session_id": "session_1738046188859_n2yylp7",
              "duration_seconds": 9,
              "idle_seconds": 0,
              "start_time": "2025-01-28T06:36:28.859Z",
              "end_time": "2025-01-28T06:36:37.649Z",
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-28T06:36:42+00:00",
        "updated_at": "2025-01-28T06:38:02+00:00",
        "version": 4
      }
    },
    "paper:2406.10165": {
      "data": {
        "arxivId": "2406.10165",
        "url": "https://arxiv.org/abs/2406.10165",
        "title": "CarLLaVA: Vision language models for camera-only closed-loop driving",
        "authors": "Katrin Renz, Long Chen, Ana-Maria Marcu, Jan H\u00fcnermann, Benoit Hanotte, Alice Karnsund, Jamie Shotton, Elahe Arani, Oleg Sinavski",
        "abstract": "In this technical report, we present CarLLaVA, a Vision Language Model (VLM)\nfor autonomous driving, developed for the CARLA Autonomous Driving Challenge\n2.0. CarLLaVA uses the vision encoder of the LLaVA VLM and the LLaMA\narchitecture as backbone, achieving state-of-the-art closed-loop driving\nperformance with only camera input and without the need for complex or\nexpensive labels. Additionally, we show preliminary results on predicting\nlanguage commentary alongside the driving output. CarLLaVA uses a\nsemi-disentangled output representation of both path predictions and waypoints,\ngetting the advantages of the path for better lateral control and the waypoints\nfor better longitudinal control. We propose an efficient training recipe to\ntrain on large driving datasets without wasting compute on easy, trivial data.\nCarLLaVA ranks 1st place in the sensor track of the CARLA Autonomous Driving\nChallenge 2.0 outperforming the previous state of the art by 458% and the best\nconcurrent submission by 32.6%.",
        "timestamp": "2025-01-28T06:35:56.917Z",
        "rating": "novote",
        "published_date": "2024-06-14T16:35:47Z",
        "arxiv_tags": [
          "cs.CV",
          "cs.RO"
        ]
      },
      "meta": {
        "created_at": "2025-01-28T06:35:57+00:00",
        "updated_at": "2025-03-30T08:13:51+00:00",
        "version": 2
      }
    },
    "interactions:2405.11932": {
      "data": {
        "paper_id": "2405.11932",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-28T09:53:49.265Z",
            "data": {
              "session_id": "session_1738058018689_749s57d",
              "duration_seconds": 11,
              "idle_seconds": 0,
              "start_time": "2025-01-28T09:53:38.689Z",
              "end_time": "2025-01-28T09:53:49.260Z",
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-28T09:53:30+00:00",
        "updated_at": "2025-01-28T09:54:51+00:00",
        "version": 4
      }
    },
    "paper:2405.11932": {
      "data": {
        "arxivId": "2405.11932",
        "url": "https://arxiv.org/abs/2405.11932v3",
        "title": "Nonequilbrium physics of generative diffusion models",
        "authors": "Zhendong Yu, Haiping Huang",
        "abstract": "Generative diffusion models apply the concept of Langevin dynamics in physics\nto machine leaning, attracting a lot of interests from engineering, statistics\nand physics, but a complete picture about inherent mechanisms is still lacking.\nIn this paper, we provide a transparent physics analysis of diffusion models,\nformulating the fluctuation theorem, entropy production, equilibrium measure,\nand Franz-Parisi potential to understand the dynamic process and intrinsic\nphase transitions. Our analysis is rooted in a path integral representation of\nboth forward and backward dynamics, and in treating the reverse diffusion\ngenerative process as a statistical inference, where the time-dependent state\nvariables serve as quenched disorder akin to that in spin glass theory. Our\nstudy thus links stochastic thermodynamics, statistical inference and geometry\nbased analysis together to yield a coherent picture about how the generative\ndiffusion models work.",
        "timestamp": "2025-01-28T09:53:25.159Z",
        "rating": "novote",
        "published_date": "2024-05-20T10:16:26Z",
        "arxiv_tags": [
          "cond-mat.stat-mech",
          "cond-mat.dis-nn",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-01-28T09:53:25+00:00",
        "updated_at": "2025-01-28T09:53:29+00:00",
        "version": 2
      }
    },
    "paper:2501.17161": {
      "data": {
        "arxivId": "2501.17161",
        "url": "https://arxiv.org/abs/2501.17161",
        "title": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model\n  Post-training",
        "authors": "Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, Yi Ma",
        "abstract": "Supervised fine-tuning (SFT) and reinforcement learning (RL) are widely used\npost-training techniques for foundation models. However, their roles in\nenhancing model generalization capabilities remain unclear. This paper studies\nthe difference between SFT and RL on generalization and memorization, focusing\non text-based rule variants and visual variants. We introduce GeneralPoints, an\narithmetic reasoning card game, and adopt V-IRL, a real-world navigation\nenvironment, to assess how models trained with SFT and RL generalize to unseen\nvariants in both textual and visual domains. We show that RL, especially when\ntrained with an outcome-based reward, generalizes across both rule-based\ntextual and visual variants. SFT, in contrast, tends to memorize training data\nand struggles to generalize out-of-distribution scenarios. Further analysis\nreveals that RL improves the model's underlying visual recognition\ncapabilities, contributing to its enhanced generalization in the visual domain.\nDespite RL's superior generalization, we show that SFT remains essential for\neffective RL training; SFT stabilizes the model's output format, enabling\nsubsequent RL to achieve its performance gains. These findings demonstrates the\ncapability of RL for acquiring generalizable knowledge in complex, multi-modal\ntasks.",
        "timestamp": "2025-01-29T07:56:12.258Z",
        "rating": "novote",
        "published_date": "2025-01-28T18:59:44Z",
        "arxiv_tags": [
          "cs.AI",
          "cs.CV",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-01-29T07:56:12+00:00",
        "updated_at": "2025-01-29T07:56:16+00:00",
        "version": 2
      }
    },
    "interactions:1408.3060": {
      "data": {
        "paper_id": "1408.3060",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-30T04:04:11.488Z",
            "data": {
              "session_id": "session_1738209830744_3gayufi",
              "duration_seconds": 14,
              "idle_seconds": 0,
              "start_time": "2025-01-30T04:03:50.744Z",
              "end_time": "2025-01-30T04:04:04.457Z",
              "total_elapsed_seconds": 14
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-30T04:04:06+00:00",
        "updated_at": "2025-01-30T04:05:50+00:00",
        "version": 4
      }
    },
    "paper:1408.3060": {
      "data": {
        "arxivId": "1408.3060",
        "url": "https://arxiv.org/abs/1408.3060",
        "title": "Fastfood: Approximate Kernel Expansions in Loglinear Time",
        "authors": "Quoc Viet Le, Tamas Sarlos, Alexander Johannes Smola",
        "abstract": "Despite their successes, what makes kernel methods difficult to use in many\nlarge scale problems is the fact that storing and computing the decision\nfunction is typically expensive, especially at prediction time. In this paper,\nwe overcome this difficulty by proposing Fastfood, an approximation that\naccelerates such computation significantly. Key to Fastfood is the observation\nthat Hadamard matrices, when combined with diagonal Gaussian matrices, exhibit\nproperties similar to dense Gaussian random matrices. Yet unlike the latter,\nHadamard and diagonal matrices are inexpensive to multiply and store. These two\nmatrices can be used in lieu of Gaussian matrices in Random Kitchen Sinks\nproposed by Rahimi and Recht (2009) and thereby speeding up the computation for\na large range of kernel functions. Specifically, Fastfood requires O(n log d)\ntime and O(n) storage to compute n non-linear basis functions in d dimensions,\na significant improvement from O(nd) computation and storage, without\nsacrificing accuracy.\n  Our method applies to any translation invariant and any dot-product kernel,\nsuch as the popular RBF kernels and polynomial kernels. We prove that the\napproximation is unbiased and has low variance. Experiments show that we\nachieve similar accuracy to full kernel expansions and Random Kitchen Sinks\nwhile being 100x faster and using 1000x less memory. These improvements,\nespecially in terms of memory usage, make kernel methods more practical for\napplications that have large training sets and/or require real-time prediction.",
        "timestamp": "2025-01-30T04:03:50.839Z",
        "rating": "novote",
        "published_date": "2014-08-13T17:37:43Z",
        "arxiv_tags": [
          "cs.LG",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-01-30T04:03:51+00:00",
        "updated_at": "2025-01-30T04:03:54+00:00",
        "version": 2
      }
    },
    "interactions:2501.17161": {
      "data": {
        "paper_id": "2501.17161",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-29T07:56:19.027Z",
            "data": {
              "session_id": "session_1738137371913_r49ayoe",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-01-29T07:56:11.913Z",
              "end_time": "2025-01-29T07:56:18.049Z",
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-29T07:56:20+00:00",
        "updated_at": "2025-03-30T08:13:41+00:00",
        "version": 3
      }
    },
    "interactions:2402.03300": {
      "data": {
        "paper_id": "2402.03300",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-30T07:03:45.367Z",
            "data": {
              "session_id": "session_1738220615286_qpb5gon",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-01-30T07:03:35.286Z",
              "end_time": "2025-01-30T07:03:43.522Z",
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-30T07:03:15+00:00",
        "updated_at": "2025-01-30T07:04:33+00:00",
        "version": 6
      }
    },
    "paper:2402.03300": {
      "data": {
        "arxivId": "2402.03300",
        "url": "https://arxiv.org/abs/2402.03300",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open\n  Language Models",
        "authors": "Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, Daya Guo",
        "abstract": "Mathematical reasoning poses a significant challenge for language models due\nto its complex and structured nature. In this paper, we introduce DeepSeekMath\n7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B\nmath-related tokens sourced from Common Crawl, together with natural language\nand code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the\ncompetition-level MATH benchmark without relying on external toolkits and\nvoting techniques, approaching the performance level of Gemini-Ultra and GPT-4.\nSelf-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH.\nThe mathematical reasoning capability of DeepSeekMath is attributed to two key\nfactors: First, we harness the significant potential of publicly available web\ndata through a meticulously engineered data selection pipeline. Second, we\nintroduce Group Relative Policy Optimization (GRPO), a variant of Proximal\nPolicy Optimization (PPO), that enhances mathematical reasoning abilities while\nconcurrently optimizing the memory usage of PPO.",
        "timestamp": "2025-01-30T07:03:06.031Z",
        "rating": "novote",
        "published_date": "2024-02-05T18:55:32Z",
        "arxiv_tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-01-30T07:03:06+00:00",
        "updated_at": "2025-01-30T07:03:09+00:00",
        "version": 2
      }
    },
    "interactions:2501.16975": {
      "data": {
        "paper_id": "2501.16975",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-30T15:22:39.770Z",
            "data": {
              "session_id": "session_1738250547292_s1ro2p3",
              "duration_seconds": 12,
              "idle_seconds": 0,
              "start_time": "2025-01-30T15:22:27.292Z",
              "end_time": "2025-01-30T15:22:38.952Z",
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-06T08:59:36.097Z",
            "data": {
              "session_id": "session_1738832350626_mwxgt4c",
              "duration_seconds": 24,
              "idle_seconds": 0,
              "start_time": "2025-02-06T08:59:10.626Z",
              "end_time": "2025-02-06T08:59:34.186Z",
              "total_elapsed_seconds": 24
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-30T15:22:40+00:00",
        "updated_at": "2025-02-06T09:00:52+00:00",
        "version": 9
      }
    },
    "paper:2501.16975": {
      "data": {
        "arxivId": "2501.16975",
        "url": "https://arxiv.org/abs/2501.16975",
        "title": "Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling",
        "authors": "Hongzhi Huang, Defa Zhu, Banggu Wu, Yutao Zeng, Ya Wang, Qiyang Min, Xun Zhou",
        "abstract": "Tokenization is a fundamental component of large language models (LLMs), yet\nits influence on model scaling and performance is not fully explored. In this\npaper, we introduce Over-Tokenized Transformers, a novel framework that\ndecouples input and output vocabularies to improve language modeling\nperformance. Specifically, our approach scales up input vocabularies to\nleverage multi-gram tokens. Through extensive experiments, we uncover a\nlog-linear relationship between input vocabulary size and training loss,\ndemonstrating that larger input vocabularies consistently enhance model\nperformance, regardless of model size. Using a large input vocabulary, we\nachieve performance comparable to double-sized baselines with no additional\ncost. Our findings highlight the importance of tokenization in scaling laws and\nprovide practical insight for tokenizer design, paving the way for more\nefficient and powerful LLMs.",
        "timestamp": "2025-01-30T15:05:18.475Z",
        "rating": "novote",
        "published_date": "2025-01-28T14:15:42Z",
        "arxiv_tags": [
          "cs.CL",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-01-30T15:05:18+00:00",
        "updated_at": "2025-03-30T08:13:31+00:00",
        "version": 2
      }
    },
    "interactions:2306.16830": {
      "data": {
        "paper_id": "2306.16830",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-31T07:03:36.613Z",
            "data": {
              "session_id": "session_1738306999389_go48zjh",
              "duration_seconds": 17,
              "idle_seconds": 0,
              "start_time": "2025-01-31T07:03:19.389Z",
              "end_time": "2025-01-31T07:03:35.907Z",
              "total_elapsed_seconds": 17
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-31T07:40:20.356Z",
            "data": {
              "session_id": "session_1738309214420_3w8tty1",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-01-31T07:40:14.420Z",
              "end_time": "2025-01-31T07:40:18.795Z",
              "total_elapsed_seconds": 4
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-31T07:03:37+00:00",
        "updated_at": "2025-01-31T07:41:37+00:00",
        "version": 5
      }
    },
    "paper:2306.16830": {
      "data": {
        "arxivId": "2306.16830",
        "url": "https://arxiv.org/abs/2306.16830",
        "title": "Sampling weights of deep neural networks",
        "authors": "Erik Lien Bolager, Iryna Burak, Chinmay Datar, Qing Sun, Felix Dietrich",
        "abstract": "We introduce a probability distribution, combined with an efficient sampling\nalgorithm, for weights and biases of fully-connected neural networks. In a\nsupervised learning context, no iterative optimization or gradient computations\nof internal network parameters are needed to obtain a trained network. The\nsampling is based on the idea of random feature models. However, instead of a\ndata-agnostic distribution, e.g., a normal distribution, we use both the input\nand the output training data to sample shallow and deep networks. We prove that\nsampled networks are universal approximators. For Barron functions, we show\nthat the $L^2$-approximation error of sampled shallow networks decreases with\nthe square root of the number of neurons. Our sampling scheme is invariant to\nrigid body transformations and scaling of the input data, which implies many\npopular pre-processing techniques are not required. In numerical experiments,\nwe demonstrate that sampled networks achieve accuracy comparable to iteratively\ntrained ones, but can be constructed orders of magnitude faster. Our test cases\ninvolve a classification benchmark from OpenML, sampling of neural operators to\nrepresent maps in function spaces, and transfer learning using well-known\narchitectures.",
        "timestamp": "2025-01-31T07:03:19.679Z",
        "rating": "novote",
        "published_date": "2023-06-29T10:13:36Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.NA",
          "math.NA",
          "68T07",
          "G.1; G.3"
        ]
      },
      "meta": {
        "created_at": "2025-01-31T07:03:20+00:00",
        "updated_at": "2025-01-31T07:03:22+00:00",
        "version": 2
      }
    },
    "interactions:1812.06162": {
      "data": {
        "paper_id": "1812.06162",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-31T08:42:52.205Z",
            "data": {
              "session_id": "session_1738312949259_7aaf3du",
              "duration_seconds": 23,
              "idle_seconds": 0,
              "start_time": "2025-01-31T08:42:29.259Z",
              "end_time": "2025-01-31T08:42:51.841Z",
              "total_elapsed_seconds": 23
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-31T08:42:53+00:00",
        "updated_at": "2025-01-31T08:44:12+00:00",
        "version": 3
      }
    },
    "paper:1812.06162": {
      "data": {
        "arxivId": "1812.06162",
        "url": "https://arxiv.org/abs/1812.06162",
        "title": "An Empirical Model of Large-Batch Training",
        "authors": "Sam McCandlish, Jared Kaplan, Dario Amodei, OpenAI Dota Team",
        "abstract": "In an increasing number of domains it has been demonstrated that deep\nlearning models can be trained using relatively large batch sizes without\nsacrificing data efficiency. However the limits of this massive data\nparallelism seem to differ from domain to domain, ranging from batches of tens\nof thousands in ImageNet to batches of millions in RL agents that play the game\nDota 2. To our knowledge there is limited conceptual understanding of why these\nlimits to batch size differ or how we might choose the correct batch size in a\nnew domain. In this paper, we demonstrate that a simple and easy-to-measure\nstatistic called the gradient noise scale predicts the largest useful batch\nsize across many domains and applications, including a number of supervised\nlearning datasets (MNIST, SVHN, CIFAR-10, ImageNet, Billion Word),\nreinforcement learning domains (Atari and Dota), and even generative model\ntraining (autoencoders on SVHN). We find that the noise scale increases as the\nloss decreases over a training run and depends on the model size primarily\nthrough improved model performance. Our empirically-motivated theory also\ndescribes the tradeoff between compute-efficiency and time-efficiency, and\nprovides a rough model of the benefits of adaptive batch-size training.",
        "timestamp": "2025-01-31T08:42:27.593Z",
        "rating": "novote",
        "published_date": "2018-12-14T20:49:09Z",
        "arxiv_tags": [
          "cs.LG",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-01-31T08:42:28+00:00",
        "updated_at": "2025-01-31T08:42:30+00:00",
        "version": 2
      }
    },
    "interactions:2501.06623": {
      "data": {
        "paper_id": "2501.06623",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-31T20:41:40.302Z",
            "data": {
              "session_id": "session_1738356091698_7mfriql",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-01-31T20:41:31.698Z",
              "end_time": "2025-01-31T20:41:39.585Z",
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "rating",
            "timestamp": "2025-01-31T21:33:40.774Z",
            "data": {
              "rating": "thumbsdown"
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-31T20:41:41+00:00",
        "updated_at": "2025-01-31T21:35:02+00:00",
        "version": 4
      }
    },
    "paper:2501.06623": {
      "data": {
        "arxivId": "2501.06623",
        "url": "https://arxiv.org/html/2501.06623v1#S1",
        "title": "Nuclear Explosions for Large Scale Carbon Sequestration",
        "authors": "Andrew Haverly",
        "abstract": "Confronting the escalating threat of climate change requires innovative and\nlarge-scale interventions. This paper presents a bold proposal to employ a\nburied nuclear explosion in a remote basaltic seabed for pulverizing basalt,\nthereby accelerating carbon sequestration through Enhanced Rock Weathering\n(ERW). By precisely locating the explosion beneath the seabed, we aim to\nconfine debris, radiation, and energy while ensuring rapid rock weathering at a\nscale substantial enough to make a meaningful dent in atmospheric carbon\nlevels. Our analysis outlines the parameters essential for efficient carbon\ncapture and minimal collateral effects, emphasizing that a yield on the order\nof gigatons is critical for global climate impact. Although this approach may\nappear radical, we illustrate its feasibility by examining safety factors,\npreservation of local ecosystems, political considerations, and financial\nviability. This work argues for reimagining nuclear technology not merely as a\ndestructive force but as a potential catalyst for decarbonization, thereby\ninviting further exploration of pioneering solutions in the fight against\nclimate change.",
        "timestamp": "2025-01-31T20:41:31.947Z",
        "rating": "thumbsdown",
        "published_date": "2025-01-11T19:18:00Z",
        "arxiv_tags": [
          "physics.soc-ph",
          "physics.ao-ph"
        ]
      },
      "meta": {
        "created_at": "2025-01-31T20:41:32+00:00",
        "updated_at": "2025-01-31T21:34:55+00:00",
        "version": 3
      }
    },
    "paper:2403.01903": {
      "data": {
        "arxivId": "2403.01903",
        "url": "https://arxiv.org/abs/2403.01903",
        "title": "Online Locality Meets Distributed Quantum Computing",
        "authors": "Amirreza Akbari, Xavier Coiteux-Roy, Francesco d'Amore, Fran\u00e7ois Le Gall, Henrik Lievonen, Darya Melnyk, Augusto Modanese, Shreyas Pai, Marc-Olivier Renou, V\u00e1clav Rozho\u0148, Jukka Suomela",
        "abstract": "We connect three distinct lines of research that have recently explored\nextensions of the classical LOCAL model of distributed computing: A.\ndistributed quantum computing and non-signaling distributions [e.g. STOC 2024],\nB. finitely-dependent processes [e.g. Forum Math. Pi 2016], and C. locality in\nonline graph algorithms and dynamic graph algorithms [e.g. ICALP 2023].\n  We prove new results on the capabilities and limitations of all of these\nmodels of computing, for locally checkable labeling problems (LCLs). We show\nthat all these settings can be sandwiched between the classical LOCAL model and\nwhat we call the randomized online-LOCAL model. Our work implies limitations on\nthe quantum advantage in the distributed setting, and we also exhibit a new\nbarrier for proving tighter bounds. Our main technical results are these: 1.\nAll LCL problems solvable with locality $O(\\log^\\star n)$ in the classical\ndeterministic LOCAL model admit a finitely-dependent distribution with locality\n$O(1)$. This answers an open question by Holroyd [2024], and also presents a\nnew barrier for proving bounds on distributed quantum advantage using\ncausality-based arguments. 2. In rooted trees, if we can solve an LCL problem\nwith locality $o(\\log \\log \\log n)$ in the randomized online-LOCAL model (or\nany of the weaker models, such as quantum-LOCAL), we can solve it with locality\n$O(\\log^\\star n)$ in the classical deterministic LOCAL model. One of many\nimplications is that in rooted trees, $O(\\log^\\star n)$ locality in\nquantum-LOCAL is not stronger than $O(\\log^\\star n)$ locality in classical\nLOCAL.",
        "timestamp": "2025-01-31T23:33:29.641Z",
        "rating": "novote",
        "published_date": "2024-03-04T10:03:54Z",
        "arxiv_tags": [
          "cs.DC",
          "cs.CC",
          "math.PR",
          "quant-ph"
        ]
      },
      "meta": {
        "created_at": "2025-01-31T23:33:30+00:00",
        "updated_at": "2025-01-31T23:33:32+00:00",
        "version": 2
      }
    },
    "interactions:2501.18388": {
      "data": {
        "paper_id": "2501.18388",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-31T23:32:08.310Z",
            "data": {
              "session_id": "session_1738366300545_194kvwn",
              "duration_seconds": 22,
              "idle_seconds": 0,
              "start_time": "2025-01-31T23:31:40.545Z",
              "end_time": "2025-01-31T23:32:02.688Z",
              "total_elapsed_seconds": 22
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-01T16:07:53.437Z",
            "data": {
              "session_id": "session_1738426062214_dsnwkus",
              "duration_seconds": 11,
              "idle_seconds": 0,
              "start_time": "2025-02-01T16:07:42.214Z",
              "end_time": "2025-02-01T16:07:53.069Z",
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-31T23:32:04+00:00",
        "updated_at": "2025-02-01T16:09:10+00:00",
        "version": 6
      }
    },
    "paper:2501.18388": {
      "data": {
        "arxivId": "2501.18388",
        "url": "https://arxiv.org/abs/2501.18388",
        "title": "Improved Replicable Boosting with Majority-of-Majorities",
        "authors": "Kasper Green Larsen, Markus Engelund Mathiasen, Clement Svendsen",
        "abstract": "We introduce a new replicable boosting algorithm which significantly improves\nthe sample complexity compared to previous algorithms. The algorithm works by\ndoing two layers of majority voting, using an improved version of the\nreplicable boosting algorithm introduced by Impagliazzo et al. [2022] in the\nbottom layer.",
        "timestamp": "2025-01-31T23:31:40.811Z",
        "rating": "novote",
        "published_date": "2025-01-30T14:38:26Z",
        "arxiv_tags": [
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-01-31T23:31:41+00:00",
        "updated_at": "2025-01-31T23:31:43+00:00",
        "version": 2
      }
    },
    "interactions:2403.01903": {
      "data": {
        "paper_id": "2403.01903",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-01T16:07:35.998Z",
            "data": {
              "session_id": "session_1738426050514_mh2t2xk",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-02-01T16:07:30.514Z",
              "end_time": "2025-02-01T16:07:35.205Z",
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-01T16:07:36+00:00",
        "updated_at": "2025-02-01T16:09:10+00:00",
        "version": 3
      }
    },
    "paper:2409.02668": {
      "data": {
        "arxivId": "2409.02668",
        "url": "https://arxiv.org/pdf/2409.02668",
        "title": "Introduction to Machine Learning",
        "authors": "Laurent Younes",
        "abstract": "This book introduces the mathematical foundations and techniques that lead to\nthe development and analysis of many of the algorithms that are used in machine\nlearning. It starts with an introductory chapter that describes notation used\nthroughout the book and serve at a reminder of basic concepts in calculus,\nlinear algebra and probability and also introduces some measure theoretic\nterminology, which can be used as a reading guide for the sections that use\nthese tools. The introductory chapters also provide background material on\nmatrix analysis and optimization. The latter chapter provides theoretical\nsupport to many algorithms that are used in the book, including stochastic\ngradient descent, proximal methods, etc. After discussing basic concepts for\nstatistical prediction, the book includes an introduction to reproducing kernel\ntheory and Hilbert space techniques, which are used in many places, before\naddressing the description of various algorithms for supervised statistical\nlearning, including linear methods, support vector machines, decision trees,\nboosting, or neural networks. The subject then switches to generative methods,\nstarting with a chapter that presents sampling methods and an introduction to\nthe theory of Markov chains. The following chapter describe the theory of\ngraphical models, an introduction to variational methods for models with latent\nvariables, and to deep-learning based generative models. The next chapters\nfocus on unsupervised learning methods, for clustering, factor analysis and\nmanifold learning. The final chapter of the book is theory-oriented and\ndiscusses concentration inequalities and generalization bounds.",
        "timestamp": "2025-02-01T04:23:41.461Z",
        "rating": "novote",
        "published_date": "2024-09-04T12:51:41Z",
        "arxiv_tags": [
          "stat.ML",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-01T04:23:41+00:00",
        "updated_at": "2025-02-01T04:23:44+00:00",
        "version": 2
      }
    },
    "interactions:2006.15191": {
      "data": {
        "paper_id": "2006.15191",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-04T20:04:11.468Z",
            "data": {
              "session_id": "session_1738699428367_anpe33x",
              "duration_seconds": 22,
              "idle_seconds": 0,
              "start_time": "2025-02-04T20:03:48.367Z",
              "end_time": "2025-02-04T20:04:10.685Z",
              "total_elapsed_seconds": 22
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-04T20:04:12+00:00",
        "updated_at": "2025-03-29T22:48:07+00:00",
        "version": 3
      }
    },
    "paper:2006.15191": {
      "data": {
        "arxivId": "2006.15191",
        "url": "https://arxiv.org/abs/2006.15191",
        "title": "Is SGD a Bayesian sampler? Well, almost",
        "authors": "Chris Mingard, Guillermo Valle-P\u00e9rez, Joar Skalse, Ard A. Louis",
        "abstract": "Overparameterised deep neural networks (DNNs) are highly expressive and so\ncan, in principle, generate almost any function that fits a training dataset\nwith zero error. The vast majority of these functions will perform poorly on\nunseen data, and yet in practice DNNs often generalise remarkably well. This\nsuccess suggests that a trained DNN must have a strong inductive bias towards\nfunctions with low generalisation error. Here we empirically investigate this\ninductive bias by calculating, for a range of architectures and datasets, the\nprobability $P_{SGD}(f\\mid S)$ that an overparameterised DNN, trained with\nstochastic gradient descent (SGD) or one of its variants, converges on a\nfunction $f$ consistent with a training set $S$. We also use Gaussian processes\nto estimate the Bayesian posterior probability $P_B(f\\mid S)$ that the DNN\nexpresses $f$ upon random sampling of its parameters, conditioned on $S$.\n  Our main findings are that $P_{SGD}(f\\mid S)$ correlates remarkably well with\n$P_B(f\\mid S)$ and that $P_B(f\\mid S)$ is strongly biased towards low-error and\nlow complexity functions. These results imply that strong inductive bias in the\nparameter-function map (which determines $P_B(f\\mid S)$), rather than a special\nproperty of SGD, is the primary explanation for why DNNs generalise so well in\nthe overparameterised regime.\n  While our results suggest that the Bayesian posterior $P_B(f\\mid S)$ is the\nfirst order determinant of $P_{SGD}(f\\mid S)$, there remain second order\ndifferences that are sensitive to hyperparameter tuning. A function probability\npicture, based on $P_{SGD}(f\\mid S)$ and/or $P_B(f\\mid S)$, can shed new light\non the way that variations in architecture or hyperparameter settings such as\nbatch size, learning rate, and optimiser choice, affect DNN performance.",
        "timestamp": "2025-02-04T20:03:48.168Z",
        "rating": "novote",
        "published_date": "2020-06-26T19:45:36Z",
        "arxiv_tags": [
          "cs.LG",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-02-04T20:03:48+00:00",
        "updated_at": "2025-03-29T22:48:07+00:00",
        "version": 2
      }
    },
    "interactions:2501.18812": {
      "data": {
        "paper_id": "2501.18812",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-04T20:03:42.118Z",
            "data": {
              "session_id": "session_1738699396159_hog90mv",
              "duration_seconds": 25,
              "idle_seconds": 0,
              "start_time": "2025-02-04T20:03:16.159Z",
              "end_time": "2025-02-04T20:03:41.508Z",
              "total_elapsed_seconds": 25
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-04T20:03:43+00:00",
        "updated_at": "2025-03-30T08:13:21+00:00",
        "version": 6
      }
    },
    "paper:2501.18812": {
      "data": {
        "arxivId": "2501.18812",
        "url": "https://arxiv.org/abs/2501.18812",
        "title": "Estimating the Probability of Sampling a Trained Neural Network at\n  Random",
        "authors": "Adam Scherlis, Nora Belrose",
        "abstract": "We present an algorithm for estimating the probability mass, under a Gaussian\nor uniform prior, of a region in neural network parameter space corresponding\nto a particular behavior, such as achieving test loss below some threshold.\nWhen the prior is uniform, this problem is equivalent to measuring the volume\nof a region. We show empirically and theoretically that existing algorithms for\nestimating volumes in parameter space underestimate the true volume by millions\nof orders of magnitude. We find that this error can be dramatically reduced,\nbut not entirely eliminated, with an importance sampling method using gradient\ninformation that is already provided by popular optimizers. The negative\nlogarithm of this probability can be interpreted as a measure of a network's\ninformation content, in accordance with minimum description length (MDL)\nprinciples and rate-distortion theory. As expected, this quantity increases\nduring language model training. We also find that badly-generalizing behavioral\nregions are smaller, and therefore less likely to be sampled at random,\ndemonstrating an inductive bias towards well-generalizing functions.",
        "timestamp": "2025-02-04T08:55:49.988Z",
        "rating": "novote",
        "published_date": "2025-01-31T00:16:06Z",
        "arxiv_tags": [
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-04T08:55:50+00:00",
        "updated_at": "2025-02-04T08:55:54+00:00",
        "version": 2
      }
    },
    "paper:2410.04265": {
      "data": {
        "arxivId": "2410.04265",
        "url": "https://arxiv.org/abs/2410.04265",
        "title": "AI as Humanity's Salieri: Quantifying Linguistic Creativity of Language\n  Models via Systematic Attribution of Machine Text against Web Text",
        "authors": "Ximing Lu, Melanie Sclar, Skyler Hallinan, Niloofar Mireshghallah, Jiacheng Liu, Seungju Han, Allyson Ettinger, Liwei Jiang, Khyathi Chandu, Nouha Dziri, Yejin Choi",
        "abstract": "Creativity has long been considered one of the most difficult aspect of human\nintelligence for AI to mimic. However, the rise of Large Language Models\n(LLMs), like ChatGPT, has raised questions about whether AI can match or even\nsurpass human creativity. We present CREATIVITY INDEX as the first step to\nquantify the linguistic creativity of a text by reconstructing it from existing\ntext snippets on the web. CREATIVITY INDEX is motivated by the hypothesis that\nthe seemingly remarkable creativity of LLMs may be attributable in large part\nto the creativity of human-written texts on the web. To compute CREATIVITY\nINDEX efficiently, we introduce DJ SEARCH, a novel dynamic programming\nalgorithm that can search verbatim and near-verbatim matches of text snippets\nfrom a given document against the web. Experiments reveal that the CREATIVITY\nINDEX of professional human authors is on average 66.2% higher than that of\nLLMs, and that alignment reduces the CREATIVITY INDEX of LLMs by an average of\n30.1%. In addition, we find that distinguished authors like Hemingway exhibit\nmeasurably higher CREATIVITY INDEX compared to other human writers. Finally, we\ndemonstrate that CREATIVITY INDEX can be used as a surprisingly effective\ncriterion for zero-shot machine text detection, surpassing the strongest\nexisting zero-shot system, DetectGPT, by a significant margin of 30.2%, and\neven outperforming the strongest supervised system, GhostBuster, in five out of\nsix domains.",
        "timestamp": "2025-02-03T05:17:00.864Z",
        "rating": "novote",
        "published_date": "2024-10-05T18:55:01Z",
        "arxiv_tags": [
          "cs.CL"
        ]
      },
      "meta": {
        "created_at": "2025-02-03T05:17:01+00:00",
        "updated_at": "2025-02-03T05:17:04+00:00",
        "version": 2
      }
    },
    "interactions:2502.01061": {
      "data": {
        "paper_id": "2502.01061",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-04T21:31:19.948Z",
            "data": {
              "session_id": "session_1738704657885_3uvh86g",
              "duration_seconds": 22,
              "idle_seconds": 0,
              "start_time": "2025-02-04T21:30:57.885Z",
              "end_time": "2025-02-04T21:31:19.936Z",
              "total_elapsed_seconds": 22
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-04T21:30:52+00:00",
        "updated_at": "2025-03-29T22:48:06+00:00",
        "version": 5
      }
    },
    "paper:2502.01061": {
      "data": {
        "arxivId": "2502.01061",
        "url": "https://arxiv.org/abs/2502.01061",
        "title": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human\n  Animation Models",
        "authors": "Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Zerong Zheng, Chao Liang",
        "abstract": "End-to-end human animation, such as audio-driven talking human generation,\nhas undergone notable advancements in the recent few years. However, existing\nmethods still struggle to scale up as large general video generation models,\nlimiting their potential in real applications. In this paper, we propose\nOmniHuman, a Diffusion Transformer-based framework that scales up data by\nmixing motion-related conditions into the training phase. To this end, we\nintroduce two training principles for these mixed conditions, along with the\ncorresponding model architecture and inference strategy. These designs enable\nOmniHuman to fully leverage data-driven motion generation, ultimately achieving\nhighly realistic human video generation. More importantly, OmniHuman supports\nvarious portrait contents (face close-up, portrait, half-body, full-body),\nsupports both talking and singing, handles human-object interactions and\nchallenging body poses, and accommodates different image styles. Compared to\nexisting end-to-end audio-driven methods, OmniHuman not only produces more\nrealistic videos, but also offers greater flexibility in inputs. It also\nsupports multiple driving modalities (audio-driven, video-driven and combined\ndriving signals). Video samples are provided on the ttfamily project page\n(https://omnihuman-lab.github.io)",
        "timestamp": "2025-02-04T21:29:46.589Z",
        "rating": "thumbsup",
        "published_date": "2025-02-03T05:17:32Z",
        "arxiv_tags": [
          "cs.CV"
        ]
      },
      "meta": {
        "created_at": "2025-02-04T21:29:47+00:00",
        "updated_at": "2025-03-29T22:48:06+00:00",
        "version": 3
      }
    },
    "paper:2502.02977": {
      "data": {
        "arxivId": "2502.02977",
        "url": "https://arxiv.org/abs/2502.02977",
        "title": "Disentangling CLIP Features for Enhanced Localized Understanding",
        "authors": "Samyak Rawelekar, Yujun Cai, Yiwei Wang, Ming-Hsuan Yang, Narendra Ahuja",
        "abstract": "Vision-language models (VLMs) demonstrate impressive capabilities in\ncoarse-grained tasks like image classification and retrieval. However, they\nstruggle with fine-grained tasks that require localized understanding. To\ninvestigate this weakness, we comprehensively analyze CLIP features and\nidentify an important issue: semantic features are highly correlated.\nSpecifically, the features of a class encode information about other classes,\nwhich we call mutual feature information (MFI). This mutual information becomes\nevident when we query a specific class and unrelated objects are activated\nalong with the target class. To address this issue, we propose Unmix-CLIP, a\nnovel framework designed to reduce MFI and improve feature disentanglement. We\nintroduce MFI loss, which explicitly separates text features by projecting them\ninto a space where inter-class similarity is minimized. To ensure a\ncorresponding separation in image features, we use multi-label recognition\n(MLR) to align the image features with the separated text features. This\nensures that both image and text features are disentangled and aligned across\nmodalities, improving feature separation for downstream tasks. For the COCO- 14\ndataset, Unmix-CLIP reduces feature similarity by 24.9%. We demonstrate its\neffectiveness through extensive evaluations of MLR and zeroshot semantic\nsegmentation (ZS3). In MLR, our method performs competitively on the VOC2007\nand surpasses SOTA approaches on the COCO-14 dataset, using fewer training\nparameters. Additionally, Unmix-CLIP consistently outperforms existing ZS3\nmethods on COCO and VOC",
        "timestamp": "2025-02-06T07:43:42.785Z",
        "rating": "novote",
        "published_date": "2025-02-05T08:20:31Z",
        "arxiv_tags": [
          "cs.CV"
        ]
      },
      "meta": {
        "created_at": "2025-02-06T07:43:43+00:00",
        "updated_at": "2025-03-29T22:48:04+00:00",
        "version": 2
      }
    },
    "interactions:2402.10588": {
      "data": {
        "paper_id": "2402.10588",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-06T07:42:03.414Z",
            "data": {
              "session_id": "session_1738827719049_iorjaob",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-02-06T07:41:59.049Z",
              "end_time": "2025-02-06T07:42:02.609Z",
              "total_elapsed_seconds": 4
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-06T07:42:04+00:00",
        "updated_at": "2025-03-29T22:48:05+00:00",
        "version": 3
      }
    },
    "paper:2402.10588": {
      "data": {
        "arxivId": "2402.10588",
        "url": "https://arxiv.org/abs/2402.10588",
        "title": "Do Llamas Work in English? On the Latent Language of Multilingual\n  Transformers",
        "authors": "Chris Wendler, Veniamin Veselovsky, Giovanni Monea, Robert West",
        "abstract": "We ask whether multilingual language models trained on unbalanced,\nEnglish-dominated corpora use English as an internal pivot language -- a\nquestion of key importance for understanding how language models function and\nthe origins of linguistic bias. Focusing on the Llama-2 family of transformer\nmodels, our study uses carefully constructed non-English prompts with a unique\ncorrect single-token continuation. From layer to layer, transformers gradually\nmap an input embedding of the final prompt token to an output embedding from\nwhich next-token probabilities are computed. Tracking intermediate embeddings\nthrough their high-dimensional space reveals three distinct phases, whereby\nintermediate embeddings (1) start far away from output token embeddings; (2)\nalready allow for decoding a semantically correct next token in the middle\nlayers, but give higher probability to its version in English than in the input\nlanguage; (3) finally move into an input-language-specific region of the\nembedding space. We cast these results into a conceptual model where the three\nphases operate in \"input space\", \"concept space\", and \"output space\",\nrespectively. Crucially, our evidence suggests that the abstract \"concept\nspace\" lies closer to English than to other languages, which may have important\nconsequences regarding the biases held by multilingual language models.",
        "timestamp": "2025-02-06T07:41:59.002Z",
        "rating": "novote",
        "published_date": "2024-02-16T11:21:28Z",
        "arxiv_tags": [
          "cs.CL",
          "cs.CY"
        ]
      },
      "meta": {
        "created_at": "2025-02-06T07:41:59+00:00",
        "updated_at": "2025-03-29T22:48:05+00:00",
        "version": 2
      }
    },
    "paper:2501.06346": {
      "data": {
        "arxivId": "2501.06346",
        "url": "https://arxiv.org/abs/2501.06346",
        "title": "Large Language Models Share Representations of Latent Grammatical\n  Concepts Across Typologically Diverse Languages",
        "authors": "Jannik Brinkmann, Chris Wendler, Christian Bartelt, Aaron Mueller",
        "abstract": "Human bilinguals often use similar brain regions to process multiple\nlanguages, depending on when they learned their second language and their\nproficiency. In large language models (LLMs), how are multiple languages\nlearned and encoded? In this work, we explore the extent to which LLMs share\nrepresentations of morphosyntactic concepts such as grammatical number, gender,\nand tense across languages. We train sparse autoencoders on Llama-3-8B and\nAya-23-8B, and demonstrate that abstract grammatical concepts are often encoded\nin feature directions shared across many languages. We use causal interventions\nto verify the multilingual nature of these representations; specifically, we\nshow that ablating only multilingual features decreases classifier performance\nto near-chance across languages. We then use these features to precisely modify\nmodel behavior in a machine translation task; this demonstrates both the\ngenerality and selectivity of these feature's roles in the network. Our\nfindings suggest that even models trained predominantly on English data can\ndevelop robust, cross-lingual abstractions of morphosyntactic concepts.",
        "timestamp": "2025-02-06T07:47:12.435Z",
        "rating": "novote",
        "published_date": "2025-01-10T21:18:21Z",
        "arxiv_tags": [
          "cs.CL"
        ]
      },
      "meta": {
        "created_at": "2025-02-06T07:47:12+00:00",
        "updated_at": "2025-03-29T22:48:03+00:00",
        "version": 2
      }
    },
    "interactions:2502.02977": {
      "data": {
        "paper_id": "2502.02977",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-06T07:44:00.913Z",
            "data": {
              "session_id": "session_1738827828800_rsr12xq",
              "duration_seconds": 10,
              "idle_seconds": 0,
              "start_time": "2025-02-06T07:43:48.800Z",
              "end_time": "2025-02-06T07:43:58.854Z",
              "total_elapsed_seconds": 10
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-06T07:44:02+00:00",
        "updated_at": "2025-03-29T22:48:04+00:00",
        "version": 3
      }
    },
    "paper:2502.01492": {
      "data": {
        "arxivId": "2502.01492",
        "url": "https://arxiv.org/abs/2502.01492",
        "title": "Develop AI Agents for System Engineering in Factorio",
        "authors": "Neel Kant",
        "abstract": "Continuing advances in frontier model research are paving the way for\nwidespread deployment of AI agents. Meanwhile, global interest in building\nlarge, complex systems in software, manufacturing, energy and logistics has\nnever been greater. Although AI driven system engineering holds tremendous\npromise, the static benchmarks dominating agent evaluations today fail to\ncapture the crucial skills required for implementing dynamic systems, such as\nmanaging uncertain trade-offs and ensuring proactive adaptability. This\nposition paper advocates for training and evaluating AI agents' system\nengineering abilities through automation-oriented sandbox games-particularly\nFactorio. By directing research efforts in this direction, we can equip AI\nagents with the specialized reasoning and long-horizon planning necessary to\ndesign, maintain, and optimize tomorrow's most demanding engineering projects.",
        "timestamp": "2025-02-06T08:39:54.750Z",
        "rating": "novote",
        "published_date": "2025-02-03T16:26:17Z",
        "arxiv_tags": [
          "cs.AI"
        ]
      },
      "meta": {
        "created_at": "2025-02-06T08:39:55+00:00",
        "updated_at": "2025-03-29T22:48:03+00:00",
        "version": 2
      }
    },
    "interactions:2502.02709": {
      "data": {
        "paper_id": "2502.02709",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-07T04:09:45.427Z",
            "data": {
              "session_id": "session_1738901374121_i1ll2hr",
              "duration_seconds": 11,
              "idle_seconds": 0,
              "start_time": "2025-02-07T04:09:34.121Z",
              "end_time": "2025-02-07T04:09:44.712Z",
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-07T04:14:46.468Z",
            "data": {
              "session_id": "session_1738901657209_8pl2q1q",
              "duration_seconds": 29,
              "idle_seconds": 0,
              "start_time": "2025-02-07T04:14:17.209Z",
              "end_time": "2025-02-07T04:14:46.288Z",
              "total_elapsed_seconds": 29
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-07T04:09:46+00:00",
        "updated_at": "2025-03-29T22:48:02+00:00",
        "version": 4
      }
    },
    "paper:2502.02709": {
      "data": {
        "arxivId": "2502.02709",
        "url": "https://arxiv.org/abs/2502.02709",
        "title": "Enforcing Demographic Coherence: A Harms Aware Framework for Reasoning\n  about Private Data Release",
        "authors": "Mark Bun, Marco Carmosino, Palak Jain, Gabriel Kaptchuk, Satchit Sivakumar",
        "abstract": "The technical literature about data privacy largely consists of two\ncomplementary approaches: formal definitions of conditions sufficient for\nprivacy preservation and attacks that demonstrate privacy breaches.\nDifferential privacy is an accepted standard in the former sphere. However,\ndifferential privacy's powerful adversarial model and worst-case guarantees may\nmake it too stringent in some situations, especially when achieving it comes at\na significant cost to data utility. Meanwhile, privacy attacks aim to expose\nreal and worrying privacy risks associated with existing data release processes\nbut often face criticism for being unrealistic. Moreover, the literature on\nattacks generally does not identify what properties are necessary to defend\nagainst them.\n  We address the gap between these approaches by introducing demographic\ncoherence, a condition inspired by privacy attacks that we argue is necessary\nfor data privacy. This condition captures privacy violations arising from\ninferences about individuals that are incoherent with respect to the\ndemographic patterns in the data. Our framework focuses on confidence rated\npredictors, which can in turn be distilled from almost any data-informed\nprocess. Thus, we capture privacy threats that exist even when no attack is\nexplicitly being carried out. Our framework not only provides a condition with\nrespect to which data release algorithms can be analysed but suggests natural\nexperimental evaluation methodologies that could be used to build practical\nintuition and make tangible assessment of risks. Finally, we argue that\ndemographic coherence is weaker than differential privacy: we prove that every\ndifferentially private data release is also demographically coherent, and that\nthere are demographically coherent algorithms which are not differentially\nprivate.",
        "timestamp": "2025-02-07T04:09:34.426Z",
        "rating": "novote",
        "published_date": "2025-02-04T20:42:30Z",
        "arxiv_tags": [
          "cs.CR",
          "cs.DB"
        ]
      },
      "meta": {
        "created_at": "2025-02-07T04:09:34+00:00",
        "updated_at": "2025-03-29T22:48:02+00:00",
        "version": 2
      }
    },
    "interactions:2502.03349": {
      "data": {
        "paper_id": "2502.03349",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-07T08:00:03.949Z",
            "data": {
              "session_id": "session_1738915195032_5dkxa7n",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-02-07T07:59:55.032Z",
              "end_time": "2025-02-07T08:00:03.403Z",
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-07T07:59:49+00:00",
        "updated_at": "2025-03-29T22:48:01+00:00",
        "version": 5
      }
    },
    "paper:2502.03349": {
      "data": {
        "arxivId": "2502.03349",
        "url": "https://arxiv.org/abs/2502.03349",
        "title": "Robust Autonomy Emerges from Self-Play",
        "authors": "Marco Cusumano-Towner, David Hafner, Alex Hertzberg, Brody Huval, Aleksei Petrenko, Eugene Vinitsky, Erik Wijmans, Taylor Killian, Stuart Bowers, Ozan Sener, Philipp Kr\u00e4henb\u00fchl, Vladlen Koltun",
        "abstract": "Self-play has powered breakthroughs in two-player and multi-player games.\nHere we show that self-play is a surprisingly effective strategy in another\ndomain. We show that robust and naturalistic driving emerges entirely from\nself-play in simulation at unprecedented scale -- 1.6~billion~km of driving.\nThis is enabled by Gigaflow, a batched simulator that can synthesize and train\non 42 years of subjective driving experience per hour on a single 8-GPU node.\nThe resulting policy achieves state-of-the-art performance on three independent\nautonomous driving benchmarks. The policy outperforms the prior state of the\nart when tested on recorded real-world scenarios, amidst human drivers, without\never seeing human data during training. The policy is realistic when assessed\nagainst human references and achieves unprecedented robustness, averaging 17.5\nyears of continuous driving between incidents in simulation.",
        "timestamp": "2025-02-07T07:59:39.082Z",
        "rating": "novote",
        "published_date": "2025-02-05T16:41:05Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI",
          "cs.RO"
        ]
      },
      "meta": {
        "created_at": "2025-02-07T07:59:39+00:00",
        "updated_at": "2025-03-29T22:48:01+00:00",
        "version": 2
      }
    },
    "interactions:2501.18838": {
      "data": {
        "paper_id": "2501.18838",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-07T22:50:33.830Z",
            "data": {
              "session_id": "session_1738968628917_q3vgya7",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-02-07T22:50:28.917Z",
              "end_time": "2025-02-07T22:50:33.139Z",
              "total_elapsed_seconds": 4
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-07T22:50:34+00:00",
        "updated_at": "2025-03-29T22:47:58+00:00",
        "version": 3
      }
    },
    "paper:2501.18838": {
      "data": {
        "arxivId": "2501.18838",
        "url": "https://arxiv.org/abs/2501.18838",
        "title": "Partially Rewriting a Transformer in Natural Language",
        "authors": "Gon\u00e7alo Paulo, Nora Belrose",
        "abstract": "The greatest ambition of mechanistic interpretability is to completely\nrewrite deep neural networks in a format that is more amenable to human\nunderstanding, while preserving their behavior and performance. In this paper,\nwe attempt to partially rewrite a large language model using simple natural\nlanguage explanations. We first approximate one of the feedforward networks in\nthe LLM with a wider MLP with sparsely activating neurons - a transcoder - and\nuse an automated interpretability pipeline to generate explanations for these\nneurons. We then replace the first layer of this sparse MLP with an LLM-based\nsimulator, which predicts the activation of each neuron given its explanation\nand the surrounding context. Finally, we measure the degree to which these\nmodifications distort the model's final output. With our pipeline, the model's\nincrease in loss is statistically similar to entirely replacing the sparse MLP\noutput with the zero vector. We employ the same protocol, this time using a\nsparse autoencoder, on the residual stream of the same layer and obtain similar\nresults. These results suggest that more detailed explanations are needed to\nimprove performance substantially above the zero ablation baseline.",
        "timestamp": "2025-02-07T22:50:28.974Z",
        "rating": "novote",
        "published_date": "2025-01-31T01:12:50Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.CL"
        ]
      },
      "meta": {
        "created_at": "2025-02-07T22:50:29+00:00",
        "updated_at": "2025-03-29T22:47:58+00:00",
        "version": 2
      }
    },
    "paper:2501.18823": {
      "data": {
        "arxivId": "2501.18823",
        "url": "https://arxiv.org/abs/2501.18823",
        "title": "Transcoders Beat Sparse Autoencoders for Interpretability",
        "authors": "Gon\u00e7alo Paulo, Stepan Shabalin, Nora Belrose",
        "abstract": "Sparse autoencoders (SAEs) extract human-interpretable features from deep\nneural networks by transforming their activations into a sparse, higher\ndimensional latent space, and then reconstructing the activations from these\nlatents. Transcoders are similar to SAEs, but they are trained to reconstruct\nthe output of a component of a deep network given its input. In this work, we\ncompare the features found by transcoders and SAEs trained on the same model\nand data, finding that transcoder features are significantly more\ninterpretable. We also propose _skip transcoders_, which add an affine skip\nconnection to the transcoder architecture, and show that these achieve lower\nreconstruction loss with no effect on interpretability.",
        "timestamp": "2025-02-07T22:50:23.541Z",
        "rating": "novote",
        "published_date": "2025-01-31T00:36:30Z",
        "arxiv_tags": [
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-07T22:50:24+00:00",
        "updated_at": "2025-03-29T22:47:59+00:00",
        "version": 2
      }
    },
    "interactions:2410.13928": {
      "data": {
        "paper_id": "2410.13928",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-02-07T22:50:11+00:00",
        "updated_at": "2025-03-29T22:48:00+00:00",
        "version": 2
      }
    },
    "paper:2410.13928": {
      "data": {
        "arxivId": "2410.13928",
        "url": "https://arxiv.org/pdf/2410.13928",
        "title": "Automatically Interpreting Millions of Features in Large Language Models",
        "authors": "Gon\u00e7alo Paulo, Alex Mallen, Caden Juang, Nora Belrose",
        "abstract": "While the activations of neurons in deep neural networks usually do not have\na simple human-understandable interpretation, sparse autoencoders (SAEs) can be\nused to transform these activations into a higher-dimensional latent space\nwhich may be more easily interpretable. However, these SAEs can have millions\nof distinct latent features, making it infeasible for humans to manually\ninterpret each one. In this work, we build an open-source automated pipeline to\ngenerate and evaluate natural language explanations for SAE features using\nLLMs. We test our framework on SAEs of varying sizes, activation functions, and\nlosses, trained on two different open-weight LLMs. We introduce five new\ntechniques to score the quality of explanations that are cheaper to run than\nthe previous state of the art. One of these techniques, intervention scoring,\nevaluates the interpretability of the effects of intervening on a feature,\nwhich we find explains features that are not recalled by existing methods. We\npropose guidelines for generating better explanations that remain valid for a\nbroader set of activating contexts, and discuss pitfalls with existing scoring\ntechniques. We use our explanations to measure the semantic similarity of\nindependently trained SAEs, and find that SAEs trained on nearby layers of the\nresidual stream are highly similar. Our large-scale analysis confirms that SAE\nlatents are indeed much more interpretable than neurons, even when neurons are\nsparsified using top-$k$ postprocessing. Our code is available at\nhttps://github.com/EleutherAI/sae-auto-interp, and our explanations are\navailable at\nhttps://huggingface.co/datasets/EleutherAI/auto_interp_explanations.",
        "timestamp": "2025-02-07T22:50:05.304Z",
        "rating": "novote",
        "published_date": "2024-10-17T17:56:01Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.CL"
        ]
      },
      "meta": {
        "created_at": "2025-02-07T22:50:05+00:00",
        "updated_at": "2025-03-29T22:48:00+00:00",
        "version": 2
      }
    },
    "interactions:2501.18823": {
      "data": {
        "paper_id": "2501.18823",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-07T22:53:56.344Z",
            "data": {
              "session_id": "session_1738968815007_8i1mqev",
              "duration_seconds": 20,
              "idle_seconds": 0,
              "start_time": "2025-02-07T22:53:35.007Z",
              "end_time": "2025-02-07T22:53:55.278Z",
              "total_elapsed_seconds": 20
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-07T22:53:29+00:00",
        "updated_at": "2025-03-29T22:47:57+00:00",
        "version": 5
      }
    },
    "interactions:2501.19393": {
      "data": {
        "paper_id": "2501.19393",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-02-08T04:47:44+00:00",
        "updated_at": "2025-03-29T22:47:56+00:00",
        "version": 2
      }
    },
    "paper:2501.19393": {
      "data": {
        "arxivId": "2501.19393",
        "url": "https://arxiv.org/pdf/2501.19393",
        "title": "s1: Simple test-time scaling",
        "authors": "Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand\u00e8s, Tatsunori Hashimoto",
        "abstract": "Test-time scaling is a promising new approach to language modeling that uses\nextra test-time compute to improve performance. Recently, OpenAI's o1 model\nshowed this capability but did not publicly share its methodology, leading to\nmany replication efforts. We seek the simplest approach to achieve test-time\nscaling and strong reasoning performance. First, we curate a small dataset s1K\nof 1,000 questions paired with reasoning traces relying on three criteria we\nvalidate through ablations: difficulty, diversity, and quality. Second, we\ndevelop budget forcing to control test-time compute by forcefully terminating\nthe model's thinking process or lengthening it by appending \"Wait\" multiple\ntimes to the model's generation when it tries to end. This can lead the model\nto double-check its answer, often fixing incorrect reasoning steps. After\nsupervised finetuning the Qwen2.5-32B-Instruct language model on s1K and\nequipping it with budget forcing, our model s1-32B exceeds o1-preview on\ncompetition math questions by up to 27% (MATH and AIME24). Further, scaling\ns1-32B with budget forcing allows extrapolating beyond its performance without\ntest-time intervention: from 50% to 57% on AIME24. Our model, data, and code\nare open-source at https://github.com/simplescaling/s1",
        "timestamp": "2025-02-08T04:47:33.827Z",
        "rating": "novote",
        "published_date": "2025-01-31T18:48:08Z",
        "arxiv_tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-08T04:47:34+00:00",
        "updated_at": "2025-03-29T22:47:57+00:00",
        "version": 2
      }
    },
    "interactions:2103.03874": {
      "data": {
        "paper_id": "2103.03874",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-08T06:28:08.436Z",
            "data": {
              "session_id": "session_1738996075059_2exga4a",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-02-08T06:27:55.059Z",
              "end_time": "2025-02-08T06:28:01.026Z",
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-08T06:28:02+00:00",
        "updated_at": "2025-03-29T22:47:55+00:00",
        "version": 4
      }
    },
    "paper:2103.03874": {
      "data": {
        "arxivId": "2103.03874",
        "url": "https://arxiv.org/pdf/2103.03874",
        "title": "Measuring Mathematical Problem Solving With the MATH Dataset",
        "authors": "Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt",
        "abstract": "Many intellectual endeavors require mathematical problem solving, but this\nskill remains beyond the capabilities of computers. To measure this ability in\nmachine learning models, we introduce MATH, a new dataset of 12,500 challenging\ncompetition mathematics problems. Each problem in MATH has a full step-by-step\nsolution which can be used to teach models to generate answer derivations and\nexplanations. To facilitate future research and increase accuracy on MATH, we\nalso contribute a large auxiliary pretraining dataset which helps teach models\nthe fundamentals of mathematics. Even though we are able to increase accuracy\non MATH, our results show that accuracy remains relatively low, even with\nenormous Transformer models. Moreover, we find that simply increasing budgets\nand model parameter counts will be impractical for achieving strong\nmathematical reasoning if scaling trends continue. While scaling Transformers\nis automatically solving most other text-based tasks, scaling is not currently\nsolving MATH. To have more traction on mathematical problem solving we will\nlikely need new algorithmic advancements from the broader research community.",
        "timestamp": "2025-02-08T06:21:47.033Z",
        "rating": "novote",
        "published_date": "2021-03-05T18:59:39Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI",
          "cs.CL"
        ]
      },
      "meta": {
        "created_at": "2025-02-08T06:21:47+00:00",
        "updated_at": "2025-03-29T22:47:55+00:00",
        "version": 2
      }
    },
    "interactions:2407.15908": {
      "data": {
        "paper_id": "2407.15908",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-02-08T19:32:27+00:00",
        "updated_at": "2025-03-29T22:47:53+00:00",
        "version": 2
      }
    },
    "paper:2407.15908": {
      "data": {
        "arxivId": "2407.15908",
        "url": "https://arxiv.org/abs/2407.15908",
        "title": "The Genomic Code: The genome instantiates a generative model of the\n  organism",
        "authors": "Kevin J. Mitchell, Nick Cheney",
        "abstract": "How does the genome encode the form of the organism? What is the nature of\nthis genomic code? Inspired by recent work in machine learning and\nneuroscience, we propose that the genome encodes a generative model of the\norganism. In this scheme, by analogy with variational autoencoders, the genome\ncomprises a connectionist network, embodying a compressed space of latent\nvariables, with weights that get encoded by the learning algorithm of evolution\nand decoded through the processes of development. The generative model analogy\naccounts for the complex, distributed genetic architecture of most traits and\nthe emergent robustness and evolvability of developmental processes, while also\noffering a conception that lends itself to formalisation.",
        "timestamp": "2025-02-08T19:30:22.374Z",
        "rating": "novote",
        "published_date": "2024-07-22T16:41:25Z",
        "arxiv_tags": [
          "q-bio.OT"
        ]
      },
      "meta": {
        "created_at": "2025-02-08T19:30:22+00:00",
        "updated_at": "2025-03-30T08:26:00+00:00",
        "version": 2
      }
    },
    "interactions:2312.11805": {
      "data": {
        "paper_id": "2312.11805",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-09T17:46:11.720Z",
            "data": {
              "session_id": "session_1739123167032_1siawe1",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-02-09T17:46:07.032Z",
              "end_time": "2025-02-09T17:46:10.495Z",
              "total_elapsed_seconds": 3
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-09T17:56:00.613Z",
            "data": {
              "session_id": "session_1739123753660_xdyeohs",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-02-09T17:55:53.660Z",
              "end_time": "2025-02-09T17:55:56.929Z",
              "total_elapsed_seconds": 3
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-09T17:46:13+00:00",
        "updated_at": "2025-03-29T22:47:51+00:00",
        "version": 7
      }
    },
    "paper:2312.11805": {
      "data": {
        "arxivId": "2312.11805",
        "url": "https://arxiv.org/abs/2312.11805",
        "title": "Gemini: A Family of Highly Capable Multimodal Models",
        "authors": "Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul R. Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Jack Krawczyk, Cosmo Du, Ed Chi, Heng-Tze Cheng, Eric Ni, Purvi Shah, Patrick Kane, Betty Chan, Manaal Faruqui, Aliaksei Severyn, Hanzhao Lin, YaGuang Li, Yong Cheng, Abe Ittycheriah, Mahdis Mahdieh, Mia Chen, Pei Sun, Dustin Tran, Sumit Bagri, Balaji Lakshminarayanan, Jeremiah Liu, Andras Orban, Fabian G\u00fcra, Hao Zhou, Xinying Song, Aurelien Boffy, Harish Ganapathy, Steven Zheng, HyunJeong Choe, \u00c1goston Weisz, Tao Zhu, Yifeng Lu, Siddharth Gopal, Jarrod Kahn, Maciej Kula, Jeff Pitman, Rushin Shah, Emanuel Taropa, Majd Al Merey, Martin Baeuml, Zhifeng Chen, Laurent El Shafey, Yujing Zhang, Olcan Sercinoglu, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Ana\u00efs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, Alexandre Frechette, Charlotte Smith, Laura Culp, Lev Proleev, Yi Luan, Xi Chen, James Lottes, Nathan Schucher, Federico Lebron, Alban Rrustemi, Natalie Clay, Phil Crone, Tomas Kocisky, Jeffrey Zhao, Bartek Perz, Dian Yu, Heidi Howard, Adam Bloniarz, Jack W. Rae, Han Lu, Laurent Sifre, Marcello Maggioni, Fred Alcober, Dan Garrette, Megan Barnes, Shantanu Thakoor, Jacob Austin, Gabriel Barth-Maron, William Wong, Rishabh Joshi, Rahma Chaabouni, Deeni Fatiha, Arun Ahuja, Gaurav Singh Tomar, Evan Senter, Martin Chadwick, Ilya Kornakov, Nithya Attaluri, I\u00f1aki Iturrate, Ruibo Liu, Yunxuan Li, Sarah Cogan, Jeremy Chen, Chao Jia, Chenjie Gu, Qiao Zhang, Jordan Grimstad, Ale Jakse Hartman, Xavier Garcia, Thanumalayan Sankaranarayana Pillai, Jacob Devlin, Michael Laskin, Diego de Las Casas, Dasha Valter, Connie Tao, Lorenzo Blanco, Adri\u00e0 Puigdom\u00e8nech Badia, David Reitter, Mianna Chen, Jenny Brennan, Clara Rivera, Sergey Brin, Shariq Iqbal, Gabriela Surita, Jane Labanowski, Abhi Rao, Stephanie Winkler, Emilio Parisotto, Yiming Gu, Kate Olszewska, Ravi Addanki, Antoine Miech, Annie Louis, Denis Teplyashin, Geoff Brown, Elliot Catt, Jan Balaguer, Jackie Xiang, Pidong Wang, Zoe Ashwood, Anton Briukhov, Albert Webson, Sanjay Ganapathy, Smit Sanghavi, Ajay Kannan, Ming-Wei Chang, Axel Stjerngren, Josip Djolonga, Yuting Sun, Ankur Bapna, Matthew Aitchison, Pedram Pejman, Henryk Michalewski, Tianhe Yu, Cindy Wang, Juliette Love, Junwhan Ahn, Dawn Bloxwich, Kehang Han, Peter Humphreys, Thibault Sellam, James Bradbury, Varun Godbole, Sina Samangooei, Bogdan Damoc, Alex Kaskasoli, S\u00e9bastien M. R. Arnold, Vijay Vasudevan, Shubham Agrawal, Jason Riesa, Dmitry Lepikhin, Richard Tanburn, Srivatsan Srinivasan, Hyeontaek Lim, Sarah Hodkinson, Pranav Shyam, Johan Ferret, Steven Hand, Ankush Garg, Tom Le Paine, Jian Li, Yujia Li, Minh Giang, Alexander Neitz, Zaheer Abbas, Sarah York, Machel Reid, Elizabeth Cole, Aakanksha Chowdhery, Dipanjan Das, Dominika Rogozi\u0144ska, Vitaliy Nikolaev, Pablo Sprechmann, Zachary Nado, Lukas Zilka, Flavien Prost, Luheng He, Marianne Monteiro, Gaurav Mishra, Chris Welty, Josh Newlan, Dawei Jia, Miltiadis Allamanis, Clara Huiyi Hu, Raoul de Liedekerke, Justin Gilmer, Carl Saroufim, Shruti Rijhwani, Shaobo Hou, Disha Shrivastava, Anirudh Baddepudi, Alex Goldin, Adnan Ozturel, Albin Cassirer, Yunhan Xu, Daniel Sohn, Devendra Sachan, Reinald Kim Amplayo, Craig Swanson, Dessie Petrova, Shashi Narayan, Arthur Guez, Siddhartha Brahma, Jessica Landon, Miteyan Patel, Ruizhe Zhao, Kevin Villela, Luyu Wang, Wenhao Jia, Matthew Rahtz, Mai Gim\u00e9nez, Legg Yeung, James Keeling, Petko Georgiev, Diana Mincu, Boxi Wu, Salem Haykal, Rachel Saputro, Kiran Vodrahalli, James Qin, Zeynep Cankara, Abhanshu Sharma, Nick Fernando, Will Hawkins, Behnam Neyshabur, Solomon Kim, Adrian Hutter, Priyanka Agrawal, Alex Castro-Ros, George van den Driessche, Tao Wang, Fan Yang, Shuo-yiin Chang, Paul Komarek, Ross McIlroy, Mario Lu\u010di\u0107, Guodong Zhang, Wael Farhan, Michael Sharman, Paul Natsev, Paul Michel, Yamini Bansal, Siyuan Qiao, Kris Cao, Siamak Shakeri, Christina Butterfield, Justin Chung, Paul Kishan Rubenstein, Shivani Agrawal, Arthur Mensch, Kedar Soparkar, Karel Lenc, Timothy Chung, Aedan Pope, Loren Maggiore, Jackie Kay, Priya Jhakra, Shibo Wang, Joshua Maynez, Mary Phuong, Taylor Tobin, Andrea Tacchetti, Maja Trebacz, Kevin Robinson, Yash Katariya, Sebastian Riedel, Paige Bailey, Kefan Xiao, Nimesh Ghelani, Lora Aroyo, Ambrose Slone, Neil Houlsby, Xuehan Xiong, Zhen Yang, Elena Gribovskaya, Jonas Adler, Mateo Wirth, Lisa Lee, Music Li, Thais Kagohara, Jay Pavagadhi, Sophie Bridgers, Anna Bortsova, Sanjay Ghemawat, Zafarali Ahmed, Tianqi Liu, Richard Powell, Vijay Bolina, Mariko Iinuma, Polina Zablotskaia, James Besley, Da-Woon Chung, Timothy Dozat, Ramona Comanescu, Xiance Si, Jeremy Greer, Guolong Su, Martin Polacek, Rapha\u00ebl Lopez Kaufman, Simon Tokumine, Hexiang Hu, Elena Buchatskaya, Yingjie Miao, Mohamed Elhawaty, Aditya Siddhant, Nenad Tomasev, Jinwei Xing, Christina Greer, Helen Miller, Shereen Ashraf, Aurko Roy, Zizhao Zhang, Ada Ma, Angelos Filos, Milos Besta, Rory Blevins, Ted Klimenko, Chih-Kuan Yeh, Soravit Changpinyo, Jiaqi Mu, Oscar Chang, Mantas Pajarskas, Carrie Muir, Vered Cohen, Charline Le Lan, Krishna Haridasan, Amit Marathe, Steven Hansen, Sholto Douglas, Rajkumar Samuel, Mingqiu Wang, Sophia Austin, Chang Lan, Jiepu Jiang, Justin Chiu, Jaime Alonso Lorenzo, Lars Lowe Sj\u00f6sund, S\u00e9bastien Cevey, Zach Gleicher, Thi Avrahami, Anudhyan Boral, Hansa Srinivasan, Vittorio Selo, Rhys May, Konstantinos Aisopos, L\u00e9onard Hussenot, Livio Baldini Soares, Kate Baumli, Michael B. Chang, Adri\u00e0 Recasens, Ben Caine, Alexander Pritzel, Filip Pavetic, Fabio Pardo, Anita Gergely, Justin Frye, Vinay Ramasesh, Dan Horgan, Kartikeya Badola, Nora Kassner, Subhrajit Roy, Ethan Dyer, V\u00edctor Campos Campos, Alex Tomala, Yunhao Tang, Dalia El Badawy, Elspeth White, Basil Mustafa, Oran Lang, Abhishek Jindal, Sharad Vikram, Zhitao Gong, Sergi Caelles, Ross Hemsley, Gregory Thornton, Fangxiaoyu Feng, Wojciech Stokowiec, Ce Zheng, Phoebe Thacker, \u00c7a\u011flar \u00dcnl\u00fc, Zhishuai Zhang, Mohammad Saleh, James Svensson, Max Bileschi, Piyush Patil, Ankesh Anand, Roman Ring, Katerina Tsihlas, Arpi Vezer, Marco Selvi, Toby Shevlane, Mikel Rodriguez, Tom Kwiatkowski, Samira Daruki, Keran Rong, Allan Dafoe, Nicholas FitzGerald, Keren Gu-Lemberg, Mina Khan, Lisa Anne Hendricks, Marie Pellat, Vladimir Feinberg, James Cobon-Kerr, Tara Sainath, Maribeth Rauh, Sayed Hadi Hashemi, Richard Ives, Yana Hasson, Eric Noland, Yuan Cao, Nathan Byrd, Le Hou, Qingze Wang, Thibault Sottiaux, Michela Paganini, Jean-Baptiste Lespiau, Alexandre Moufarek, Samer Hassan, Kaushik Shivakumar, Joost van Amersfoort, Amol Mandhane, Pratik Joshi, Anirudh Goyal, Matthew Tung, Andrew Brock, Hannah Sheahan, Vedant Misra, Cheng Li, Nemanja Raki\u0107evi\u0107, Mostafa Dehghani, Fangyu Liu, Sid Mittal, Junhyuk Oh, Seb Noury, Eren Sezener, Fantine Huot, Matthew Lamm, Nicola De Cao, Charlie Chen, Sidharth Mudgal, Romina Stella, Kevin Brooks, Gautam Vasudevan, Chenxi Liu, Mainak Chain, Nivedita Melinkeri, Aaron Cohen, Venus Wang, Kristie Seymore, Sergey Zubkov, Rahul Goel, Summer Yue, Sai Krishnakumaran, Brian Albert, Nate Hurley, Motoki Sano, Anhad Mohananey, Jonah Joughin, Egor Filonov, Tomasz K\u0119pa, Yomna Eldawy, Jiawern Lim, Rahul Rishi, Shirin Badiezadegan, Taylor Bos, Jerry Chang, Sanil Jain, Sri Gayatri Sundara Padmanabhan, Subha Puttagunta, Kalpesh Krishna, Leslie Baker, Norbert Kalb, Vamsi Bedapudi, Adam Kurzrok, Shuntong Lei, Anthony Yu, Oren Litvin, Xiang Zhou, Zhichun Wu, Sam Sobell, Andrea Siciliano, Alan Papir, Robby Neale, Jonas Bragagnolo, Tej Toor, Tina Chen, Valentin Anklin, Feiran Wang, Richie Feng, Milad Gholami, Kevin Ling, Lijuan Liu, Jules Walter, Hamid Moghaddam, Arun Kishore, Jakub Adamek, Tyler Mercado, Jonathan Mallinson, Siddhinita Wandekar, Stephen Cagle, Eran Ofek, Guillermo Garrido, Clemens Lombriser, Maksim Mukha, Botu Sun, Hafeezul Rahman Mohammad, Josip Matak, Yadi Qian, Vikas Peswani, Pawel Janus, Quan Yuan, Leif Schelin, Oana David, Ankur Garg, Yifan He, Oleksii Duzhyi, Anton \u00c4lgmyr, Timoth\u00e9e Lottaz, Qi Li, Vikas Yadav, Luyao Xu, Alex Chinien, Rakesh Shivanna, Aleksandr Chuklin, Josie Li, Carrie Spadine, Travis Wolfe, Kareem Mohamed, Subhabrata Das, Zihang Dai, Kyle He, Daniel von Dincklage, Shyam Upadhyay, Akanksha Maurya, Luyan Chi, Sebastian Krause, Khalid Salama, Pam G Rabinovitch, Pavan Kumar Reddy M, Aarush Selvan, Mikhail Dektiarev, Golnaz Ghiasi, Erdem Guven, Himanshu Gupta, Boyi Liu, Deepak Sharma, Idan Heimlich Shtacher, Shachi Paul, Oscar Akerlund, Fran\u00e7ois-Xavier Aubet, Terry Huang, Chen Zhu, Eric Zhu, Elico Teixeira, Matthew Fritze, Francesco Bertolini, Liana-Eleonora Marinescu, Martin B\u00f6lle, Dominik Paulus, Khyatti Gupta, Tejasi Latkar, Max Chang, Jason Sanders, Roopa Wilson, Xuewei Wu, Yi-Xuan Tan, Lam Nguyen Thiet, Tulsee Doshi, Sid Lall, Swaroop Mishra, Wanming Chen, Thang Luong, Seth Benjamin, Jasmine Lee, Ewa Andrejczuk, Dominik Rabiej, Vipul Ranjan, Krzysztof Styrc, Pengcheng Yin, Jon Simon, Malcolm Rose Harriott, Mudit Bansal, Alexei Robsky, Geoff Bacon, David Greene, Daniil Mirylenka, Chen Zhou, Obaid Sarvana, Abhimanyu Goyal, Samuel Andermatt, Patrick Siegler, Ben Horn, Assaf Israel, Francesco Pongetti, Chih-Wei \"Louis\" Chen, Marco Selvatici, Pedro Silva, Kathie Wang, Jackson Tolins, Kelvin Guu, Roey Yogev, Xiaochen Cai, Alessandro Agostini, Maulik Shah, Hung Nguyen, Noah \u00d3 Donnaile, S\u00e9bastien Pereira, Linda Friso, Adam Stambler, Adam Kurzrok, Chenkai Kuang, Yan Romanikhin, Mark Geller, ZJ Yan, Kane Jang, Cheng-Chun Lee, Wojciech Fica, Eric Malmi, Qijun Tan, Dan Banica, Daniel Balle, Ryan Pham, Yanping Huang, Diana Avram, Hongzhi Shi, Jasjot Singh, Chris Hidey, Niharika Ahuja, Pranab Saxena, Dan Dooley, Srividya Pranavi Potharaju, Eileen O'Neill, Anand Gokulchandran, Ryan Foley, Kai Zhao, Mike Dusenberry, Yuan Liu, Pulkit Mehta, Ragha Kotikalapudi, Chalence Safranek-Shrader, Andrew Goodman, Joshua Kessinger, Eran Globen, Prateek Kolhar, Chris Gorgolewski, Ali Ibrahim, Yang Song, Ali Eichenbaum, Thomas Brovelli, Sahitya Potluri, Preethi Lahoti, Cip Baetu, Ali Ghorbani, Charles Chen, Andy Crawford, Shalini Pal, Mukund Sridhar, Petru Gurita, Asier Mujika, Igor Petrovski, Pierre-Louis Cedoz, Chenmei Li, Shiyuan Chen, Niccol\u00f2 Dal Santo, Siddharth Goyal, Jitesh Punjabi, Karthik Kappaganthu, Chester Kwak, Pallavi LV, Sarmishta Velury, Himadri Choudhury, Jamie Hall, Premal Shah, Ricardo Figueira, Matt Thomas, Minjie Lu, Ting Zhou, Chintu Kumar, Thomas Jurdi, Sharat Chikkerur, Yenai Ma, Adams Yu, Soo Kwak, Victor \u00c4hdel, Sujeevan Rajayogam, Travis Choma, Fei Liu, Aditya Barua, Colin Ji, Ji Ho Park, Vincent Hellendoorn, Alex Bailey, Taylan Bilal, Huanjie Zhou, Mehrdad Khatir, Charles Sutton, Wojciech Rzadkowski, Fiona Macintosh, Konstantin Shagin, Paul Medina, Chen Liang, Jinjing Zhou, Pararth Shah, Yingying Bi, Attila Dankovics, Shipra Banga, Sabine Lehmann, Marissa Bredesen, Zifan Lin, John Eric Hoffmann, Jonathan Lai, Raynald Chung, Kai Yang, Nihal Balani, Arthur Bra\u017einskas, Andrei Sozanschi, Matthew Hayes, H\u00e9ctor Fern\u00e1ndez Alcalde, Peter Makarov, Will Chen, Antonio Stella, Liselotte Snijders, Michael Mandl, Ante K\u00e4rrman, Pawe\u0142 Nowak, Xinyi Wu, Alex Dyck, Krishnan Vaidyanathan, Raghavender R, Jessica Mallet, Mitch Rudominer, Eric Johnston, Sushil Mittal, Akhil Udathu, Janara Christensen, Vishal Verma, Zach Irving, Andreas Santucci, Gamaleldin Elsayed, Elnaz Davoodi, Marin Georgiev, Ian Tenney, Nan Hua, Geoffrey Cideron, Edouard Leurent, Mahmoud Alnahlawi, Ionut Georgescu, Nan Wei, Ivy Zheng, Dylan Scandinaro, Heinrich Jiang, Jasper Snoek, Mukund Sundararajan, Xuezhi Wang, Zack Ontiveros, Itay Karo, Jeremy Cole, Vinu Rajashekhar, Lara Tumeh, Eyal Ben-David, Rishub Jain, Jonathan Uesato, Romina Datta, Oskar Bunyan, Shimu Wu, John Zhang, Piotr Stanczyk, Ye Zhang, David Steiner, Subhajit Naskar, Michael Azzam, Matthew Johnson, Adam Paszke, Chung-Cheng Chiu, Jaume Sanchez Elias, Afroz Mohiuddin, Faizan Muhammad, Jin Miao, Andrew Lee, Nino Vieillard, Jane Park, Jiageng Zhang, Jeff Stanway, Drew Garmon, Abhijit Karmarkar, Zhe Dong, Jong Lee, Aviral Kumar, Luowei Zhou, Jonathan Evens, William Isaac, Geoffrey Irving, Edward Loper, Michael Fink, Isha Arkatkar, Nanxin Chen, Izhak Shafran, Ivan Petrychenko, Zhe Chen, Johnson Jia, Anselm Levskaya, Zhenkai Zhu, Peter Grabowski, Yu Mao, Alberto Magni, Kaisheng Yao, Javier Snaider, Norman Casagrande, Evan Palmer, Paul Suganthan, Alfonso Casta\u00f1o, Irene Giannoumis, Wooyeol Kim, Miko\u0142aj Rybi\u0144ski, Ashwin Sreevatsa, Jennifer Prendki, David Soergel, Adrian Goedeckemeyer, Willi Gierke, Mohsen Jafari, Meenu Gaba, Jeremy Wiesner, Diana Gage Wright, Yawen Wei, Harsha Vashisht, Yana Kulizhskaya, Jay Hoover, Maigo Le, Lu Li, Chimezie Iwuanyanwu, Lu Liu, Kevin Ramirez, Andrey Khorlin, Albert Cui, Tian LIN, Marcus Wu, Ricardo Aguilar, Keith Pallo, Abhishek Chakladar, Ginger Perng, Elena Allica Abellan, Mingyang Zhang, Ishita Dasgupta, Nate Kushman, Ivo Penchev, Alena Repina, Xihui Wu, Tom van der Weide, Priya Ponnapalli, Caroline Kaplan, Jiri Simsa, Shuangfeng Li, Olivier Dousse, Fan Yang, Jeff Piper, Nathan Ie, Rama Pasumarthi, Nathan Lintz, Anitha Vijayakumar, Daniel Andor, Pedro Valenzuela, Minnie Lui, Cosmin Paduraru, Daiyi Peng, Katherine Lee, Shuyuan Zhang, Somer Greene, Duc Dung Nguyen, Paula Kurylowicz, Cassidy Hardin, Lucas Dixon, Lili Janzer, Kiam Choo, Ziqiang Feng, Biao Zhang, Achintya Singhal, Dayou Du, Dan McKinnon, Natasha Antropova, Tolga Bolukbasi, Orgad Keller, David Reid, Daniel Finchelstein, Maria Abi Raad, Remi Crocker, Peter Hawkins, Robert Dadashi, Colin Gaffney, Ken Franko, Anna Bulanova, R\u00e9mi Leblond, Shirley Chung, Harry Askham, Luis C. Cobo, Kelvin Xu, Felix Fischer, Jun Xu, Christina Sorokin, Chris Alberti, Chu-Cheng Lin, Colin Evans, Alek Dimitriev, Hannah Forbes, Dylan Banarse, Zora Tung, Mark Omernick, Colton Bishop, Rachel Sterneck, Rohan Jain, Jiawei Xia, Ehsan Amid, Francesco Piccinno, Xingyu Wang, Praseem Banzal, Daniel J. Mankowitz, Alex Polozov, Victoria Krakovna, Sasha Brown, MohammadHossein Bateni, Dennis Duan, Vlad Firoiu, Meghana Thotakuri, Tom Natan, Matthieu Geist, Ser tan Girgin, Hui Li, Jiayu Ye, Ofir Roval, Reiko Tojo, Michael Kwong, James Lee-Thorp, Christopher Yew, Danila Sinopalnikov, Sabela Ramos, John Mellor, Abhishek Sharma, Kathy Wu, David Miller, Nicolas Sonnerat, Denis Vnukov, Rory Greig, Jennifer Beattie, Emily Caveness, Libin Bai, Julian Eisenschlos, Alex Korchemniy, Tomy Tsai, Mimi Jasarevic, Weize Kong, Phuong Dao, Zeyu Zheng, Frederick Liu, Fan Yang, Rui Zhu, Tian Huey Teh, Jason Sanmiya, Evgeny Gladchenko, Nejc Trdin, Daniel Toyama, Evan Rosen, Sasan Tavakkol, Linting Xue, Chen Elkind, Oliver Woodman, John Carpenter, George Papamakarios, Rupert Kemp, Sushant Kafle, Tanya Grunina, Rishika Sinha, Alice Talbert, Diane Wu, Denese Owusu-Afriyie, Cosmo Du, Chloe Thornton, Jordi Pont-Tuset, Pradyumna Narayana, Jing Li, Saaber Fatehi, John Wieting, Omar Ajmeri, Benigno Uria, Yeongil Ko, Laura Knight, Am\u00e9lie H\u00e9liou, Ning Niu, Shane Gu, Chenxi Pang, Yeqing Li, Nir Levine, Ariel Stolovich, Rebeca Santamaria-Fernandez, Sonam Goenka, Wenny Yustalim, Robin Strudel, Ali Elqursh, Charlie Deck, Hyo Lee, Zonglin Li, Kyle Levin, Raphael Hoffmann, Dan Holtmann-Rice, Olivier Bachem, Sho Arora, Christy Koh, Soheil Hassas Yeganeh, Siim P\u00f5der, Mukarram Tariq, Yanhua Sun, Lucian Ionita, Mojtaba Seyedhosseini, Pouya Tafti, Zhiyu Liu, Anmol Gulati, Jasmine Liu, Xinyu Ye, Bart Chrzaszcz, Lily Wang, Nikhil Sethi, Tianrun Li, Ben Brown, Shreya Singh, Wei Fan, Aaron Parisi, Joe Stanton, Vinod Koverkathu, Christopher A. Choquette-Choo, Yunjie Li, TJ Lu, Abe Ittycheriah, Prakash Shroff, Mani Varadarajan, Sanaz Bahargam, Rob Willoughby, David Gaddy, Guillaume Desjardins, Marco Cornero, Brona Robenek, Bhavishya Mittal, Ben Albrecht, Ashish Shenoy, Fedor Moiseev, Henrik Jacobsson, Alireza Ghaffarkhah, Morgane Rivi\u00e8re, Alanna Walton, Cl\u00e9ment Crepy, Alicia Parrish, Zongwei Zhou, Clement Farabet, Carey Radebaugh, Praveen Srinivasan, Claudia van der Salm, Andreas Fidjeland, Salvatore Scellato, Eri Latorre-Chimoto, Hanna Klimczak-Pluci\u0144ska, David Bridson, Dario de Cesare, Tom Hudson, Piermaria Mendolicchio, Lexi Walker, Alex Morris, Matthew Mauger, Alexey Guseynov, Alison Reid, Seth Odoom, Lucia Loher, Victor Cotruta, Madhavi Yenugula, Dominik Grewe, Anastasia Petrushkina, Tom Duerig, Antonio Sanchez, Steve Yadlowsky, Amy Shen, Amir Globerson, Lynette Webb, Sahil Dua, Dong Li, Surya Bhupatiraju, Dan Hurt, Haroon Qureshi, Ananth Agarwal, Tomer Shani, Matan Eyal, Anuj Khare, Shreyas Rammohan Belle, Lei Wang, Chetan Tekur, Mihir Sanjay Kale, Jinliang Wei, Ruoxin Sang, Brennan Saeta, Tyler Liechty, Yi Sun, Yao Zhao, Stephan Lee, Pandu Nayak, Doug Fritz, Manish Reddy Vuyyuru, John Aslanides, Nidhi Vyas, Martin Wicke, Xiao Ma, Evgenii Eltyshev, Nina Martin, Hardie Cate, James Manyika, Keyvan Amiri, Yelin Kim, Xi Xiong, Kai Kang, Florian Luisier, Nilesh Tripuraneni, David Madras, Mandy Guo, Austin Waters, Oliver Wang, Joshua Ainslie, Jason Baldridge, Han Zhang, Garima Pruthi, Jakob Bauer, Feng Yang, Riham Mansour, Jason Gelman, Yang Xu, George Polovets, Ji Liu, Honglong Cai, Warren Chen, XiangHai Sheng, Emily Xue, Sherjil Ozair, Christof Angermueller, Xiaowei Li, Anoop Sinha, Weiren Wang, Julia Wiesinger, Emmanouil Koukoumidis, Yuan Tian, Anand Iyer, Madhu Gurumurthy, Mark Goldenson, Parashar Shah, MK Blake, Hongkun Yu, Anthony Urbanowicz, Jennimaria Palomaki, Chrisantha Fernando, Ken Durden, Harsh Mehta, Nikola Momchev, Elahe Rahimtoroghi, Maria Georgaki, Amit Raul, Sebastian Ruder, Morgan Redshaw, Jinhyuk Lee, Denny Zhou, Komal Jalan, Dinghua Li, Blake Hechtman, Parker Schuh, Milad Nasr, Kieran Milan, Vladimir Mikulik, Juliana Franco, Tim Green, Nam Nguyen, Joe Kelley, Aroma Mahendru, Andrea Hu, Joshua Howland, Ben Vargas, Jeffrey Hui, Kshitij Bansal, Vikram Rao, Rakesh Ghiya, Emma Wang, Ke Ye, Jean Michel Sarr, Melanie Moranski Preston, Madeleine Elish, Steve Li, Aakash Kaku, Jigar Gupta, Ice Pasupat, Da-Cheng Juan, Milan Someswar, Tejvi M., Xinyun Chen, Aida Amini, Alex Fabrikant, Eric Chu, Xuanyi Dong, Amruta Muthal, Senaka Buthpitiya, Sarthak Jauhari, Nan Hua, Urvashi Khandelwal, Ayal Hitron, Jie Ren, Larissa Rinaldi, Shahar Drath, Avigail Dabush, Nan-Jiang Jiang, Harshal Godhia, Uli Sachs, Anthony Chen, Yicheng Fan, Hagai Taitelbaum, Hila Noga, Zhuyun Dai, James Wang, Chen Liang, Jenny Hamer, Chun-Sung Ferng, Chenel Elkind, Aviel Atias, Paulina Lee, V\u00edt List\u00edk, Mathias Carlen, Jan van de Kerkhof, Marcin Pikus, Krunoslav Zaher, Paul M\u00fcller, Sasha Zykova, Richard Stefanec, Vitaly Gatsko, Christoph Hirnschall, Ashwin Sethi, Xingyu Federico Xu, Chetan Ahuja, Beth Tsai, Anca Stefanoiu, Bo Feng, Keshav Dhandhania, Manish Katyal, Akshay Gupta, Atharva Parulekar, Divya Pitta, Jing Zhao, Vivaan Bhatia, Yashodha Bhavnani, Omar Alhadlaq, Xiaolin Li, Peter Danenberg, Dennis Tu, Alex Pine, Vera Filippova, Abhipso Ghosh, Ben Limonchik, Bhargava Urala, Chaitanya Krishna Lanka, Derik Clive, Yi Sun, Edward Li, Hao Wu, Kevin Hongtongsak, Ianna Li, Kalind Thakkar, Kuanysh Omarov, Kushal Majmundar, Michael Alverson, Michael Kucharski, Mohak Patel, Mudit Jain, Maksim Zabelin, Paolo Pelagatti, Rohan Kohli, Saurabh Kumar, Joseph Kim, Swetha Sankar, Vineet Shah, Lakshmi Ramachandruni, Xiangkai Zeng, Ben Bariach, Laura Weidinger, Tu Vu, Alek Andreev, Antoine He, Kevin Hui, Sheleem Kashem, Amar Subramanya, Sissie Hsiao, Demis Hassabis, Koray Kavukcuoglu, Adam Sadovsky, Quoc Le, Trevor Strohman, Yonghui Wu, Slav Petrov, Jeffrey Dean, Oriol Vinyals",
        "abstract": "This report introduces a new family of multimodal models, Gemini, that\nexhibit remarkable capabilities across image, audio, video, and text\nunderstanding. The Gemini family consists of Ultra, Pro, and Nano sizes,\nsuitable for applications ranging from complex reasoning tasks to on-device\nmemory-constrained use-cases. Evaluation on a broad range of benchmarks shows\nthat our most-capable Gemini Ultra model advances the state of the art in 30 of\n32 of these benchmarks - notably being the first model to achieve human-expert\nperformance on the well-studied exam benchmark MMLU, and improving the state of\nthe art in every one of the 20 multimodal benchmarks we examined. We believe\nthat the new capabilities of the Gemini family in cross-modal reasoning and\nlanguage understanding will enable a wide variety of use cases. We discuss our\napproach toward post-training and deploying Gemini models responsibly to users\nthrough services including Gemini, Gemini Advanced, Google AI Studio, and Cloud\nVertex AI.",
        "timestamp": "2025-02-09T17:46:07.089Z",
        "rating": "novote",
        "published_date": "2023-12-19T02:39:27Z",
        "arxiv_tags": [
          "cs.CL",
          "cs.AI",
          "cs.CV"
        ]
      },
      "meta": {
        "created_at": "2025-02-09T17:46:07+00:00",
        "updated_at": "2025-03-29T22:47:52+00:00",
        "version": 2
      }
    },
    "paper:2502.03387": {
      "data": {
        "arxivId": "2502.03387",
        "url": "https://arxiv.org/abs/2502.03387",
        "title": "LIMO: Less is More for Reasoning",
        "authors": "Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, Pengfei Liu",
        "abstract": "We present a fundamental discovery that challenges our understanding of how\ncomplex reasoning emerges in large language models. While conventional wisdom\nsuggests that sophisticated reasoning tasks demand extensive training data\n(&gt;100,000 examples), we demonstrate that complex mathematical reasoning\nabilities can be effectively elicited with surprisingly few examples. Through\ncomprehensive experiments, our proposed model LIMO demonstrates unprecedented\nperformance in mathematical reasoning. With merely 817 curated training\nsamples, LIMO achieves 57.1% accuracy on AIME and 94.8% on MATH, improving from\nprevious SFT-based models' 6.5% and 59.2% respectively, while only using 1% of\nthe training data required by previous approaches. LIMO demonstrates\nexceptional out-of-distribution generalization, achieving 40.5% absolute\nimprovement across 10 diverse benchmarks, outperforming models trained on 100x\nmore data, challenging the notion that SFT leads to memorization rather than\ngeneralization. Based on these results, we propose the Less-Is-More Reasoning\nHypothesis (LIMO Hypothesis): In foundation models where domain knowledge has\nbeen comprehensively encoded during pre-training, sophisticated reasoning\ncapabilities can emerge through minimal but precisely orchestrated\ndemonstrations of cognitive processes. This hypothesis posits that the\nelicitation threshold for complex reasoning is determined by two key factors:\n(1) the completeness of the model's encoded knowledge foundation during\npre-training, and (2) the effectiveness of post-training examples as \"cognitive\ntemplates\" that show the model how to utilize its knowledge base to solve\ncomplex reasoning tasks. To facilitate reproducibility and future research in\ndata-efficient reasoning, we release LIMO as a comprehensive open-source suite\nat https://github.com/GAIR-NLP/LIMO.",
        "timestamp": "2025-02-09T17:35:03.289Z",
        "rating": "thumbsup",
        "published_date": "2025-02-05T17:23:45Z",
        "arxiv_tags": [
          "cs.CL",
          "cs.AI"
        ]
      },
      "meta": {
        "created_at": "2025-02-09T17:35:03+00:00",
        "updated_at": "2025-03-29T22:47:52+00:00",
        "version": 3
      }
    },
    "interactions:2502.03387": {
      "data": {
        "paper_id": "2502.03387",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-09T17:56:07.669Z",
            "data": {
              "session_id": "session_1739123763127_cpwbu2t",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-02-09T17:56:03.128Z",
              "end_time": "2025-02-09T17:56:07.099Z",
              "total_elapsed_seconds": 4
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-09T18:35:03.804Z",
            "data": {
              "session_id": "session_1739126087743_f1kabpl",
              "duration_seconds": 16,
              "idle_seconds": 0,
              "start_time": "2025-02-09T18:34:47.743Z",
              "end_time": "2025-02-09T18:35:03.790Z",
              "total_elapsed_seconds": 16
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-09T17:56:08+00:00",
        "updated_at": "2025-03-29T22:47:51+00:00",
        "version": 6
      }
    },
    "interactions:2502.05171": {
      "data": {
        "paper_id": "2502.05171",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-10T05:27:27.890Z",
            "data": {
              "session_id": "session_1739165241055_vo46266",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-02-10T05:27:21.055Z",
              "end_time": "2025-02-10T05:27:27.199Z",
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-10T05:34:17.187Z",
            "data": {
              "session_id": "session_1739165650576_6d87r9x",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-02-10T05:34:10.576Z",
              "end_time": "2025-02-10T05:34:15.079Z",
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-15T22:45:05.743Z",
            "data": {
              "session_id": "session_1739659501371_tsme0ej",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-02-15T22:45:01.371Z",
              "end_time": "2025-02-15T22:45:05.725Z",
              "total_elapsed_seconds": 4
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-15T22:48:44.570Z",
            "data": {
              "session_id": "session_1739659713366_ye05fgo",
              "duration_seconds": 11,
              "idle_seconds": 0,
              "start_time": "2025-02-15T22:48:33.366Z",
              "end_time": "2025-02-15T22:48:44.152Z",
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-10T05:27:28+00:00",
        "updated_at": "2025-03-29T22:47:50+00:00",
        "version": 10
      }
    },
    "paper:2502.05171": {
      "data": {
        "arxivId": "2502.05171",
        "url": "https://arxiv.org/abs/2502.05171",
        "title": "Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth\n  Approach",
        "authors": "Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Tom Goldstein",
        "abstract": "We study a novel language model architecture that is capable of scaling\ntest-time computation by implicitly reasoning in latent space. Our model works\nby iterating a recurrent block, thereby unrolling to arbitrary depth at\ntest-time. This stands in contrast to mainstream reasoning models that scale up\ncompute by producing more tokens. Unlike approaches based on chain-of-thought,\nour approach does not require any specialized training data, can work with\nsmall context windows, and can capture types of reasoning that are not easily\nrepresented in words. We scale a proof-of-concept model to 3.5 billion\nparameters and 800 billion tokens. We show that the resulting model can improve\nits performance on reasoning benchmarks, sometimes dramatically, up to a\ncomputation load equivalent to 50 billion parameters.",
        "timestamp": "2025-02-10T05:27:21.173Z",
        "rating": "novote",
        "published_date": "2025-02-07T18:55:02Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.CL"
        ]
      },
      "meta": {
        "created_at": "2025-02-10T05:27:21+00:00",
        "updated_at": "2025-03-29T22:47:50+00:00",
        "version": 2
      }
    },
    "interactions:2502.04223": {
      "data": {
        "paper_id": "2502.04223",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-02-10T05:44:03+00:00",
        "updated_at": "2025-03-29T22:47:49+00:00",
        "version": 2
      }
    },
    "paper:2501.17887": {
      "data": {
        "arxivId": "2501.17887",
        "url": "https://arxiv.org/html/2501.17887v1",
        "title": "Docling: An Efficient Open-Source Toolkit for AI-driven Document\n  Conversion",
        "authors": "Nikolaos Livathinos, Christoph Auer, Maksym Lysak, Ahmed Nassar, Michele Dolfi, Panos Vagenas, Cesar Berrospi Ramis, Matteo Omenetti, Kasper Dinkla, Yusik Kim, Shubham Gupta, Rafael Teixeira de Lima, Valery Weber, Lucas Morin, Ingmar Meijer, Viktor Kuropiatnyk, Peter W. J. Staar",
        "abstract": "We introduce Docling, an easy-to-use, self-contained, MIT-licensed,\nopen-source toolkit for document conversion, that can parse several types of\npopular document formats into a unified, richly structured representation. It\nis powered by state-of-the-art specialized AI models for layout analysis\n(DocLayNet) and table structure recognition (TableFormer), and runs efficiently\non commodity hardware in a small resource budget. Docling is released as a\nPython package and can be used as a Python API or as a CLI tool. Docling's\nmodular architecture and efficient document representation make it easy to\nimplement extensions, new features, models, and customizations. Docling has\nbeen already integrated in other popular open-source frameworks (e.g.,\nLangChain, LlamaIndex, spaCy), making it a natural fit for the processing of\ndocuments and the development of high-end applications. The open-source\ncommunity has fully engaged in using, promoting, and developing for Docling,\nwhich gathered 10k stars on GitHub in less than a month and was reported as the\nNo. 1 trending repository in GitHub worldwide in November 2024.",
        "timestamp": "2025-02-10T05:44:02.960Z",
        "rating": "novote",
        "published_date": "2025-01-27T19:40:00Z",
        "arxiv_tags": [
          "cs.CL",
          "cs.CV",
          "cs.SE"
        ]
      },
      "meta": {
        "created_at": "2025-02-10T05:44:03+00:00",
        "updated_at": "2025-03-29T22:47:48+00:00",
        "version": 2
      }
    },
    "paper:2502.04223": {
      "data": {
        "arxivId": "2502.04223",
        "url": "https://arxiv.org/html/2502.04223v1",
        "title": "\u00c9clair -- Extracting Content and Layout with Integrated Reading Order\n  for Documents",
        "authors": "Ilia Karmanov, Amala Sanjay Deshmukh, Lukas Voegtle, Philipp Fischer, Kateryna Chumachenko, Timo Roman, Jarno Sepp\u00e4nen, Jupinder Parmar, Joseph Jennings, Andrew Tao, Karan Sapra",
        "abstract": "Optical Character Recognition (OCR) technology is widely used to extract text\nfrom images of documents, facilitating efficient digitization and data\nretrieval. However, merely extracting text is insufficient when dealing with\ncomplex documents. Fully comprehending such documents requires an understanding\nof their structure -- including formatting, formulas, tables, and the reading\norder of multiple blocks and columns across multiple pages -- as well as\nsemantic information for detecting elements like footnotes and image captions.\nThis comprehensive understanding is crucial for downstream tasks such as\nretrieval, document question answering, and data curation for training Large\nLanguage Models (LLMs) and Vision Language Models (VLMs). To address this, we\nintroduce \\'Eclair, a general-purpose text-extraction tool specifically\ndesigned to process a wide range of document types. Given an image, \\'Eclair is\nable to extract formatted text in reading order, along with bounding boxes and\ntheir corresponding semantic classes. To thoroughly evaluate these novel\ncapabilities, we introduce our diverse human-annotated benchmark for\ndocument-level OCR and semantic classification. \\'Eclair achieves\nstate-of-the-art accuracy on this benchmark, outperforming other methods across\nkey metrics. Additionally, we evaluate \\'Eclair on established benchmarks,\ndemonstrating its versatility and strength across several evaluation standards.",
        "timestamp": "2025-02-10T05:43:48.654Z",
        "rating": "novote",
        "published_date": "2025-02-06T17:07:22Z",
        "arxiv_tags": [
          "cs.CV"
        ]
      },
      "meta": {
        "created_at": "2025-02-10T05:43:49+00:00",
        "updated_at": "2025-03-29T22:47:49+00:00",
        "version": 2
      }
    },
    "interactions:2502.04403": {
      "data": {
        "paper_id": "2502.04403",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-02-11T05:00:57+00:00",
        "updated_at": "2025-03-29T22:47:46+00:00",
        "version": 2
      }
    },
    "paper:2502.04403": {
      "data": {
        "arxivId": "2502.04403",
        "url": "https://arxiv.org/abs/2502.04403",
        "title": "Agency Is Frame-Dependent",
        "authors": "David Abel, Andr\u00e9 Barreto, Michael Bowling, Will Dabney, Shi Dong, Steven Hansen, Anna Harutyunyan, Khimya Khetarpal, Clare Lyle, Razvan Pascanu, Georgios Piliouras, Doina Precup, Jonathan Richens, Mark Rowland, Tom Schaul, Satinder Singh",
        "abstract": "Agency is a system's capacity to steer outcomes toward a goal, and is a\ncentral topic of study across biology, philosophy, cognitive science, and\nartificial intelligence. Determining if a system exhibits agency is a\nnotoriously difficult question: Dennett (1989), for instance, highlights the\npuzzle of determining which principles can decide whether a rock, a thermostat,\nor a robot each possess agency. We here address this puzzle from the viewpoint\nof reinforcement learning by arguing that agency is fundamentally\nframe-dependent: Any measurement of a system's agency must be made relative to\na reference frame. We support this claim by presenting a philosophical argument\nthat each of the essential properties of agency proposed by Barandiaran et al.\n(2009) and Moreno (2018) are themselves frame-dependent. We conclude that any\nbasic science of agency requires frame-dependence, and discuss the implications\nof this claim for reinforcement learning.",
        "timestamp": "2025-02-11T04:59:59.126Z",
        "rating": "novote",
        "published_date": "2025-02-06T08:34:57Z",
        "arxiv_tags": [
          "cs.AI"
        ]
      },
      "meta": {
        "created_at": "2025-02-11T04:59:59+00:00",
        "updated_at": "2025-03-29T22:47:47+00:00",
        "version": 2
      }
    },
    "interactions:2502.04549": {
      "data": {
        "paper_id": "2502.04549",
        "interactions": [
          {
            "type": "rating",
            "timestamp": "2025-02-11T16:18:14.719Z",
            "data": {
              "rating": "thumbsdown"
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-11T16:18:17+00:00",
        "updated_at": "2025-03-29T22:47:44+00:00",
        "version": 4
      }
    },
    "paper:2502.04549": {
      "data": {
        "arxivId": "2502.04549",
        "url": "https://arxiv.org/abs/2502.04549",
        "title": "Mechanisms of Projective Composition of Diffusion Models",
        "authors": "Arwen Bradley, Preetum Nakkiran, David Berthelot, James Thornton, Joshua M. Susskind",
        "abstract": "We study the theoretical foundations of composition in diffusion models, with\na particular focus on out-of-distribution extrapolation and\nlength-generalization. Prior work has shown that composing distributions via\nlinear score combination can achieve promising results, including\nlength-generalization in some cases (Du et al., 2023; Liu et al., 2022).\nHowever, our theoretical understanding of how and why such compositions work\nremains incomplete. In fact, it is not even entirely clear what it means for\ncomposition to \"work\". This paper starts to address these fundamental gaps. We\nbegin by precisely defining one possible desired result of composition, which\nwe call projective composition. Then, we investigate: (1) when linear score\ncombinations provably achieve projective composition, (2) whether\nreverse-diffusion sampling can generate the desired composition, and (3) the\nconditions under which composition fails. Finally, we connect our theoretical\nanalysis to prior empirical observations where composition has either worked or\nfailed, for reasons that were unclear at the time.",
        "timestamp": "2025-02-11T14:48:47.054Z",
        "rating": "novote",
        "published_date": "2025-02-06T22:59:54Z",
        "arxiv_tags": [
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-11T14:48:47+00:00",
        "updated_at": "2025-03-29T22:47:45+00:00",
        "version": 2
      }
    },
    "interactions:2412.12140": {
      "data": {
        "paper_id": "2412.12140",
        "interactions": [
          {
            "type": "rating",
            "timestamp": "2025-02-12T02:04:24.065Z",
            "data": {
              "rating": "thumbsdown"
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-12T02:04:24+00:00",
        "updated_at": "2025-03-29T22:47:43+00:00",
        "version": 4
      }
    },
    "paper:2412.12140": {
      "data": {
        "arxivId": "2412.12140",
        "url": "https://arxiv.org/abs/2412.12140",
        "title": "Frontier AI systems have surpassed the self-replicating red line",
        "authors": "Xudong Pan, Jiarun Dai, Yihe Fan, Min Yang",
        "abstract": "Successful self-replication under no human assistance is the essential step\nfor AI to outsmart the human beings, and is an early signal for rogue AIs. That\nis why self-replication is widely recognized as one of the few red line risks\nof frontier AI systems. Nowadays, the leading AI corporations OpenAI and Google\nevaluate their flagship large language models GPT-o1 and Gemini Pro 1.0, and\nreport the lowest risk level of self-replication. However, following their\nmethodology, we for the first time discover that two AI systems driven by\nMeta's Llama31-70B-Instruct and Alibaba's Qwen25-72B-Instruct, popular large\nlanguage models of less parameters and weaker capabilities, have already\nsurpassed the self-replicating red line. In 50% and 90% experimental trials,\nthey succeed in creating a live and separate copy of itself respectively. By\nanalyzing the behavioral traces, we observe the AI systems under evaluation\nalready exhibit sufficient self-perception, situational awareness and\nproblem-solving capabilities to accomplish self-replication. We further note\nthe AI systems are even able to use the capability of self-replication to avoid\nshutdown and create a chain of replica to enhance the survivability, which may\nfinally lead to an uncontrolled population of AIs. If such a worst-case risk is\nlet unknown to the human society, we would eventually lose control over the\nfrontier AI systems: They would take control over more computing devices, form\nan AI species and collude with each other against human beings. Our findings\nare a timely alert on existing yet previously unknown severe AI risks, calling\nfor international collaboration on effective governance on uncontrolled\nself-replication of AI systems.",
        "timestamp": "2025-02-12T02:03:35.172Z",
        "rating": "novote",
        "published_date": "2024-12-09T15:01:37Z",
        "arxiv_tags": [
          "cs.CL",
          "cs.AI",
          "cs.CY",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-12T02:03:35+00:00",
        "updated_at": "2025-03-29T22:47:43+00:00",
        "version": 2
      }
    },
    "interactions:2212.01508": {
      "data": {
        "paper_id": "2212.01508",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-13T01:38:48.536Z",
            "data": {
              "session_id": "session_1739410716599_oskjy7y",
              "duration_seconds": 12,
              "idle_seconds": 0,
              "start_time": "2025-02-13T01:38:36.599Z",
              "end_time": "2025-02-13T01:38:48.532Z",
              "total_elapsed_seconds": 12
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-13T01:37:57+00:00",
        "updated_at": "2025-03-29T22:47:41+00:00",
        "version": 7
      }
    },
    "paper:2212.01508": {
      "data": {
        "arxivId": "2212.01508",
        "url": "https://arxiv.org/abs/2212.01508",
        "title": "Space is a latent sequence: Structured sequence learning as a unified\n  theory of representation in the hippocampus",
        "authors": "Rajkumar Vasudeva Raju, J. Swaroop Guntupalli, Guangyao Zhou, Miguel L\u00e1zaro-Gredilla, Dileep George",
        "abstract": "Fascinating and puzzling phenomena, such as landmark vector cells, splitter\ncells, and event-specific representations to name a few, are regularly\ndiscovered in the hippocampus. Without a unifying principle that can explain\nthese divergent observations, each experiment seemingly discovers a new anomaly\nor coding type. Here, we provide a unifying principle that the mental\nrepresentation of space is an emergent property of latent higher-order sequence\nlearning. Treating space as a sequence resolves myriad phenomena, and suggests\nthat the place-field mapping methodology where sequential neuron responses are\ninterpreted in spatial and Euclidean terms might itself be a source of\nanomalies. Our model, called Clone-structured Causal Graph (CSCG), uses a\nspecific higher-order graph scaffolding to learn latent representations by\nmapping sensory inputs to unique contexts. Learning to compress sequential and\nepisodic experiences using CSCGs result in the emergence of cognitive maps -\nmental representations of spatial and conceptual relationships in an\nenvironment that are suited for planning, introspection, consolidation, and\nabstraction. We demonstrate that over a dozen different hippocampal phenomena,\nranging from those reported in classic experiments to the most recent ones, are\nsuccinctly and mechanistically explained by our model.",
        "timestamp": "2025-02-13T01:37:45.603Z",
        "rating": "novote",
        "published_date": "2022-12-03T02:00:56Z",
        "arxiv_tags": [
          "q-bio.NC"
        ]
      },
      "meta": {
        "created_at": "2025-02-13T01:37:46+00:00",
        "updated_at": "2025-03-29T22:47:42+00:00",
        "version": 2
      }
    },
    "interactions:2502.08346": {
      "data": {
        "paper_id": "2502.08346",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-13T05:08:27.058Z",
            "data": {
              "session_id": "session_1739423278468_u80mie3",
              "duration_seconds": 28,
              "idle_seconds": 0,
              "start_time": "2025-02-13T05:07:58.469Z",
              "end_time": "2025-02-13T05:08:26.250Z",
              "total_elapsed_seconds": 28
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-13T05:08:27+00:00",
        "updated_at": "2025-03-29T22:47:39+00:00",
        "version": 3
      }
    },
    "paper:2502.08346": {
      "data": {
        "arxivId": "2502.08346",
        "url": "https://arxiv.org/abs/2502.08346",
        "title": "Graph Foundation Models for Recommendation: A Comprehensive Survey",
        "authors": "Bin Wu, Yihang Wang, Yuanhao Zeng, Jiawei Liu, Jiashu Zhao, Cheng Yang, Yawen Li, Long Xia, Dawei Yin, Chuan Shi",
        "abstract": "Recommender systems (RS) serve as a fundamental tool for navigating the vast\nexpanse of online information, with deep learning advancements playing an\nincreasingly important role in improving ranking accuracy. Among these, graph\nneural networks (GNNs) excel at extracting higher-order structural information,\nwhile large language models (LLMs) are designed to process and comprehend\nnatural language, making both approaches highly effective and widely adopted.\nRecent research has focused on graph foundation models (GFMs), which integrate\nthe strengths of GNNs and LLMs to model complex RS problems more efficiently by\nleveraging the graph-based structure of user-item relationships alongside\ntextual understanding. In this survey, we provide a comprehensive overview of\nGFM-based RS technologies by introducing a clear taxonomy of current\napproaches, diving into methodological details, and highlighting key challenges\nand future directions. By synthesizing recent advancements, we aim to offer\nvaluable insights into the evolving landscape of GFM-based recommender systems.",
        "timestamp": "2025-02-13T05:07:58.727Z",
        "rating": "novote",
        "published_date": "2025-02-12T12:13:51Z",
        "arxiv_tags": [
          "cs.IR",
          "cs.AI",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-13T05:07:59+00:00",
        "updated_at": "2025-03-29T22:47:40+00:00",
        "version": 2
      }
    },
    "paper:2502.08254": {
      "data": {
        "arxivId": "2502.08254",
        "url": "https://arxiv.org/abs/2502.08254",
        "title": "UniCoRN: Unified Commented Retrieval Network with LMMs",
        "authors": "Maximilian Jaritz, Matthieu Guillaumin, Sabine Sternig, Loris Bazzani",
        "abstract": "Multimodal retrieval methods have limitations in handling complex,\ncompositional queries that require reasoning about the visual content of both\nthe query and the retrieved entities. On the other hand, Large Multimodal\nModels (LMMs) can answer with language to more complex visual questions, but\nwithout the inherent ability to retrieve relevant entities to support their\nanswers. We aim to address these limitations with UniCoRN, a Unified Commented\nRetrieval Network that combines the strengths of composed multimodal retrieval\nmethods and generative language approaches, going beyond Retrieval-Augmented\nGeneration (RAG). We introduce an entity adapter module to inject the retrieved\nmultimodal entities back into the LMM, so it can attend to them while\ngenerating answers and comments. By keeping the base LMM frozen, UniCoRN\npreserves its original capabilities while being able to perform both retrieval\nand text generation tasks under a single integrated framework. To assess these\nnew abilities, we introduce the Commented Retrieval task (CoR) and a\ncorresponding dataset, with the goal of retrieving an image that accurately\nanswers a given question and generate an additional textual response that\nprovides further clarification and details about the visual information. We\ndemonstrate the effectiveness of UniCoRN on several datasets showing\nimprovements of +4.5% recall over the state of the art for composed multimodal\nretrieval and of +14.9% METEOR / +18.4% BEM over RAG for commenting in CoR.",
        "timestamp": "2025-02-13T05:05:23.454Z",
        "rating": "novote",
        "published_date": "2025-02-12T09:49:43Z",
        "arxiv_tags": [
          "cs.CV"
        ]
      },
      "meta": {
        "created_at": "2025-02-13T05:05:23+00:00",
        "updated_at": "2025-03-29T22:47:40+00:00",
        "version": 2
      }
    },
    "paper:2502.07971": {
      "data": {
        "arxivId": "2502.07971",
        "url": "https://arxiv.org/abs/2502.07971",
        "title": "ReTreever: Tree-based Coarse-to-Fine Representations for Retrieval",
        "authors": "Shubham Gupta, Zichao Li, Tianyi Chen, Cem Subakan, Siva Reddy, Perouz Taslakian, Valentina Zantedeschi",
        "abstract": "Document retrieval is a core component of question-answering systems, as it\nenables conditioning answer generation on new and large-scale corpora. While\neffective, the standard practice of encoding documents into high-dimensional\nembeddings for similarity search entails large memory and compute footprints,\nand also makes it hard to inspect the inner workings of the system. In this\npaper, we propose a tree-based method for organizing and representing reference\ndocuments at various granular levels, which offers the flexibility to balance\ncost and utility, and eases the inspection of the corpus content and retrieval\noperations. Our method, called ReTreever, jointly learns a routing function per\ninternal node of a binary tree such that query and reference documents are\nassigned to similar tree branches, hence directly optimizing for retrieval\nperformance. Our evaluations show that ReTreever generally preserves full\nrepresentation accuracy. Its hierarchical structure further provides strong\ncoarse representations and enhances transparency by indirectly learning\nmeaningful semantic groupings. Among hierarchical retrieval methods, ReTreever\nachieves the best retrieval accuracy at the lowest latency, proving that this\nfamily of techniques can be viable in practical applications.",
        "timestamp": "2025-02-13T05:02:43.036Z",
        "rating": "novote",
        "published_date": "2025-02-11T21:35:13Z",
        "arxiv_tags": [
          "cs.IR",
          "cs.AI",
          "cs.LG",
          "I.2; I.7; E.2; H.3"
        ]
      },
      "meta": {
        "created_at": "2025-02-13T05:02:43+00:00",
        "updated_at": "2025-03-29T22:47:41+00:00",
        "version": 2
      }
    },
    "interactions:2406.08636": {
      "data": {
        "paper_id": "2406.08636",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-13T05:23:06.946Z",
            "data": {
              "session_id": "session_1739424163032_acsxzxz",
              "duration_seconds": 22,
              "idle_seconds": 0,
              "start_time": "2025-02-13T05:22:43.032Z",
              "end_time": "2025-02-13T05:23:05.145Z",
              "total_elapsed_seconds": 22
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-13T05:22:36+00:00",
        "updated_at": "2025-03-29T22:47:38+00:00",
        "version": 5
      }
    },
    "paper:2406.08636": {
      "data": {
        "arxivId": "2406.08636",
        "url": "https://arxiv.org/abs/2406.08636",
        "title": "Towards Integrating Personal Knowledge into Test-Time Predictions",
        "authors": "Isaac Lage, Sonali Parbhoo, Finale Doshi-Velez",
        "abstract": "Machine learning (ML) models can make decisions based on large amounts of\ndata, but they can be missing personal knowledge available to human users about\nwhom predictions are made. For example, a model trained to predict psychiatric\noutcomes may know nothing about a patient's social support system, and social\nsupport may look different for different patients. In this work, we introduce\nthe problem of human feature integration, which provides a way to incorporate\nimportant personal-knowledge from users without domain expertise into ML\npredictions. We characterize this problem through illustrative user stories and\ncomparisons to existing approaches; we formally describe this problem in a way\nthat paves the ground for future technical solutions; and we provide a\nproof-of-concept study of a simple version of a solution to this problem in a\nsemi-realistic setting.",
        "timestamp": "2025-02-13T05:22:05.668Z",
        "rating": "novote",
        "published_date": "2024-06-12T20:47:17Z",
        "arxiv_tags": [
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-13T05:22:06+00:00",
        "updated_at": "2025-03-29T22:47:39+00:00",
        "version": 2
      }
    },
    "interactions:2004.15011": {
      "data": {
        "paper_id": "2004.15011",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-13T21:19:16.095Z",
            "data": {
              "session_id": "session_1739481551573_jcayq9y",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-02-13T21:19:11.573Z",
              "end_time": "2025-02-13T21:19:15.305Z",
              "total_elapsed_seconds": 4
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-13T21:30:48.996Z",
            "data": {
              "session_id": "session_1739482245306_c5yigpd",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-02-13T21:30:45.306Z",
              "end_time": "2025-02-13T21:30:48.396Z",
              "total_elapsed_seconds": 3
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-13T21:19:17+00:00",
        "updated_at": "2025-03-29T22:47:37+00:00",
        "version": 4
      }
    },
    "paper:2004.15011": {
      "data": {
        "arxivId": "2004.15011",
        "url": "https://arxiv.org/abs/2004.15011",
        "title": "TLDR: Extreme Summarization of Scientific Documents",
        "authors": "Isabel Cachola, Kyle Lo, Arman Cohan, Daniel S. Weld",
        "abstract": "We introduce TLDR generation, a new form of extreme summarization, for\nscientific papers. TLDR generation involves high source compression and\nrequires expert background knowledge and understanding of complex\ndomain-specific language. To facilitate study on this task, we introduce\nSciTLDR, a new multi-target dataset of 5.4K TLDRs over 3.2K papers. SciTLDR\ncontains both author-written and expert-derived TLDRs, where the latter are\ncollected using a novel annotation protocol that produces high-quality\nsummaries while minimizing annotation burden. We propose CATTS, a simple yet\neffective learning strategy for generating TLDRs that exploits titles as an\nauxiliary training signal. CATTS improves upon strong baselines under both\nautomated metrics and human evaluations. Data and code are publicly available\nat https://github.com/allenai/scitldr.",
        "timestamp": "2025-02-13T21:19:11.579Z",
        "rating": "novote",
        "published_date": "2020-04-30T17:56:18Z",
        "arxiv_tags": [
          "cs.CL"
        ]
      },
      "meta": {
        "created_at": "2025-02-13T21:19:12+00:00",
        "updated_at": "2025-03-29T22:47:38+00:00",
        "version": 2
      }
    },
    "interactions:2502.08606": {
      "data": {
        "paper_id": "2502.08606",
        "interactions": [
          {
            "type": "rating",
            "timestamp": "2025-02-14T06:33:16.469Z",
            "data": {
              "rating": "thumbsup"
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-14T06:52:26.851Z",
            "data": {
              "session_id": "session_1739515935950_9mu832l",
              "duration_seconds": 7,
              "idle_seconds": 0,
              "start_time": "2025-02-14T06:52:15.950Z",
              "end_time": "2025-02-14T06:52:23.445Z",
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-14T06:33:17+00:00",
        "updated_at": "2025-03-29T22:47:36+00:00",
        "version": 5
      }
    },
    "paper:2502.08606": {
      "data": {
        "arxivId": "2502.08606",
        "url": "https://arxiv.org/abs/2502.08606",
        "title": "Distillation Scaling Laws",
        "authors": "Dan Busbridge, Amitis Shidani, Floris Weers, Jason Ramapuram, Etai Littwin, Russ Webb",
        "abstract": "We provide a distillation scaling law that estimates distilled model\nperformance based on a compute budget and its allocation between the student\nand teacher. Our findings reduce the risks associated with using distillation\nat scale; compute allocation for both the teacher and student models can now be\ndone to maximize student performance. We provide compute optimal distillation\nrecipes for when 1) a teacher exists, or 2) a teacher needs training. If many\nstudents are to be distilled, or a teacher already exists, distillation\noutperforms supervised pretraining until a compute level which grows\npredictably with student size. If one student is to be distilled and a teacher\nalso needs training, supervised learning should be done instead. Additionally,\nwe provide insights across our large scale study of distillation, which\nincrease our understanding of distillation and inform experimental design.",
        "timestamp": "2025-02-14T06:32:27.363Z",
        "rating": "novote",
        "published_date": "2025-02-12T17:52:47Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI",
          "cs.CL",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-02-14T06:32:27+00:00",
        "updated_at": "2025-03-29T22:47:37+00:00",
        "version": 2
      }
    },
    "interactions:2211.00235": {
      "data": {
        "paper_id": "2211.00235",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-14T23:46:25.734Z",
            "data": {
              "session_id": "session_1739576772297_lkni4f4",
              "duration_seconds": 13,
              "idle_seconds": 0,
              "start_time": "2025-02-14T23:46:12.297Z",
              "end_time": "2025-02-14T23:46:24.995Z",
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-14T23:55:53.303Z",
            "data": {
              "session_id": "session_1739577336192_am2nzt2",
              "duration_seconds": 14,
              "idle_seconds": 0,
              "start_time": "2025-02-14T23:55:36.192Z",
              "end_time": "2025-02-14T23:55:50.665Z",
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-15T07:51:31.680Z",
            "data": {
              "session_id": "session_1739605887438_47xuwdv",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-02-15T07:51:27.438Z",
              "end_time": "2025-02-15T07:51:31.041Z",
              "total_elapsed_seconds": 4
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-16T01:47:04.541Z",
            "data": {
              "session_id": "session_1739670417662_f9g7zdj",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-02-16T01:46:57.662Z",
              "end_time": "2025-02-16T01:47:03.922Z",
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-14T23:46:26+00:00",
        "updated_at": "2025-03-29T22:47:35+00:00",
        "version": 10
      }
    },
    "paper:2211.00235": {
      "data": {
        "arxivId": "2211.00235",
        "url": "https://arxiv.org/pdf/2211.00235",
        "title": "Efficient AlphaFold2 Training using Parallel Evoformer and Branch\n  Parallelism",
        "authors": "Guoxia Wang, Zhihua Wu, Xiaomin Fang, Yingfei Xiang, Yiqun Liu, Dianhai Yu, Yanjun Ma",
        "abstract": "The accuracy of AlphaFold2, a frontier end-to-end structure prediction\nsystem, is already close to that of the experimental determination techniques.\nDue to the complex model architecture and large memory consumption, it requires\nlots of computational resources and time to train AlphaFold2 from scratch.\nEfficient AlphaFold2 training could accelerate the development of life science.\nIn this paper, we propose a Parallel Evoformer and Branch Parallelism to speed\nup the training of AlphaFold2. We conduct sufficient experiments on UniFold\nimplemented in PyTorch and HelixFold implemented in PaddlePaddle, and Branch\nParallelism can improve the training performance by 38.67% and 36.93%,\nrespectively. We also demonstrate that the accuracy of Parallel Evoformer could\nbe on par with AlphaFold2 on the CASP14 and CAMEO datasets. The source code is\navailable on https://github.com/PaddlePaddle/PaddleFleetX",
        "timestamp": "2025-02-14T23:44:19.874Z",
        "rating": "novote",
        "published_date": "2022-11-01T02:59:35Z",
        "arxiv_tags": [
          "cs.DC"
        ]
      },
      "meta": {
        "created_at": "2025-02-14T23:44:20+00:00",
        "updated_at": "2025-03-29T22:47:36+00:00",
        "version": 2
      }
    },
    "interactions:2410.10485": {
      "data": {
        "paper_id": "2410.10485",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-15T08:27:09.127Z",
            "data": {
              "session_id": "session_1739608025217_stferaa",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-02-15T08:27:05.217Z",
              "end_time": "2025-02-15T08:27:09.111Z",
              "total_elapsed_seconds": 4
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-15T15:18:34.667Z",
            "data": {
              "session_id": "session_1739632697223_4jqcpvj",
              "duration_seconds": 17,
              "idle_seconds": 0,
              "start_time": "2025-02-15T15:18:17.223Z",
              "end_time": "2025-02-15T15:18:34.114Z",
              "total_elapsed_seconds": 17
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-15T08:26:21+00:00",
        "updated_at": "2025-03-29T22:47:34+00:00",
        "version": 7
      }
    },
    "paper:2410.10485": {
      "data": {
        "arxivId": "2410.10485",
        "url": "https://arxiv.org/abs/2410.10485",
        "title": "Characterising high-order interdependence via entropic conjugation",
        "authors": "Fernando E. Rosas, Aaron Gutknecht, Pedro A. M. Mediano, Michael Gastpar",
        "abstract": "High-order phenomena play crucial roles in many systems of interest, but\ntheir analysis is often highly nontrivial. There is a rich literature providing\na number of alternative information-theoretic quantities capturing high-order\nphenomena, but their interpretation and relationship with each other is not\nwell understood. The lack of principles unifying these quantities obscures the\nchoice of tools for enabling specific type of analyses. Here we show how an\nentropic conjugation provides a theoretically grounded principle to investigate\nthe space of possible high-order quantities, clarifying the nature of the\nexistent metrics while revealing gaps in the literature. This leads to identify\nnovel notions of symmetry and skew-symmetry as key properties for guaranteeing\na balanced account of high-order interdependencies and enabling broadly\napplicable analyses across physical systems.",
        "timestamp": "2025-02-15T08:26:04.568Z",
        "rating": "novote",
        "published_date": "2024-10-14T13:27:08Z",
        "arxiv_tags": [
          "cs.IT",
          "math.IT",
          "physics.data-an"
        ]
      },
      "meta": {
        "created_at": "2025-02-15T08:26:05+00:00",
        "updated_at": "2025-03-29T22:47:34+00:00",
        "version": 2
      }
    },
    "paper:2502.08009": {
      "data": {
        "arxivId": "2502.08009",
        "url": "https://arxiv.org/abs/2502.08009",
        "title": "The Geometry of Prompting: Unveiling Distinct Mechanisms of Task\n  Adaptation in Language Models",
        "authors": "Artem Kirsanov, Chi-Ning Chou, Kyunghyun Cho, SueYeon Chung",
        "abstract": "Decoder-only language models have the ability to dynamically switch between\nvarious computational tasks based on input prompts. Despite many successful\napplications of prompting, there is very limited understanding of the internal\nmechanism behind such flexibility. In this work, we investigate how different\nprompting methods affect the geometry of representations in these models.\nEmploying a framework grounded in statistical physics, we reveal that various\nprompting techniques, while achieving similar performance, operate through\ndistinct representational mechanisms for task adaptation. Our analysis\nhighlights the critical role of input distribution samples and label semantics\nin few-shot in-context learning. We also demonstrate evidence of synergistic\nand interfering interactions between different tasks on the representational\nlevel. Our work contributes to the theoretical understanding of large language\nmodels and lays the groundwork for developing more effective,\nrepresentation-aware prompting strategies.",
        "timestamp": "2025-02-15T08:18:37.599Z",
        "rating": "novote",
        "published_date": "2025-02-11T23:09:50Z",
        "arxiv_tags": [
          "cs.CL"
        ]
      },
      "meta": {
        "created_at": "2025-02-15T08:18:38+00:00",
        "updated_at": "2025-03-29T22:47:35+00:00",
        "version": 2
      }
    },
    "interactions:2502.08009": {
      "data": {
        "paper_id": "2502.08009",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-15T15:03:42.226Z",
            "data": {
              "session_id": "session_1739631818140_9jdyeuo",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-02-15T15:03:38.140Z",
              "end_time": "2025-02-15T15:03:41.505Z",
              "total_elapsed_seconds": 3
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-15T16:28:19.123Z",
            "data": {
              "session_id": "session_1739636895140_brw7czk",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-02-15T16:28:15.140Z",
              "end_time": "2025-02-15T16:28:18.529Z",
              "total_elapsed_seconds": 3
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-15T15:03:42+00:00",
        "updated_at": "2025-03-29T22:47:33+00:00",
        "version": 4
      }
    },
    "interactions:2309.08003": {
      "data": {
        "paper_id": "2309.08003",
        "interactions": [
          {
            "type": "rating",
            "timestamp": "2025-02-15T15:20:15.661Z",
            "data": {
              "rating": "thumbsup"
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-16T01:45:47.662Z",
            "data": {
              "session_id": "session_1739670332759_2xyrnfw",
              "duration_seconds": 14,
              "idle_seconds": 0,
              "start_time": "2025-02-16T01:45:32.759Z",
              "end_time": "2025-02-16T01:45:46.408Z",
              "total_elapsed_seconds": 14
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-15T15:19:56+00:00",
        "updated_at": "2025-03-29T22:47:31+00:00",
        "version": 8
      }
    },
    "paper:2309.08003": {
      "data": {
        "arxivId": "2309.08003",
        "url": "https://arxiv.org/html/2309.08003v2",
        "title": "Generalized Decomposition of Multivariate Information",
        "authors": "Thomas F. Varley",
        "abstract": "Since its introduction, the partial information decomposition (PID) has\nemerged as a powerful, information-theoretic technique useful for studying the\nstructure of (potentially higher-order) interactions in complex systems.\nDespite its utility, the applicability of the PID is restricted by the need to\nassign elements as either inputs or targets, as well as the specific structure\nof the mutual information itself. Here, we introduce a generalized information\ndecomposition that relaxes the source/target distinction while still satisfying\nthe basic intuitions about information. This approach is based on the\ndecomposition of the Kullback-Leibler divergence, and consequently allows for\nthe analysis of any information gained when updating from an arbitrary prior to\nan arbitrary posterior. Consequently, any information-theoretic measure that\ncan be written in as a Kullback-Leibler divergence admits a decomposition in\nthe style of Williams and Beer, including the total correlation, the\nnegentropy, and the mutual information as special cases. In this paper, we\nexplore how the generalized information decomposition can reveal novel insights\ninto existing measures, as well as the nature of higher-order synergies. We\nshow that synergistic information is intimately related to the well-known\nTononi-Sporns-Edelman (TSE) complexity, and that synergistic information\nrequires a similar integration/segregation balance as a high TSE complexity.\nFinally, we end with a discussion of how this approach fits into other attempts\nto generalize the PID and the possibilities for empirical applications.",
        "timestamp": "2025-02-15T15:19:51.046Z",
        "rating": "thumbsup",
        "published_date": "2023-09-14T19:26:46Z",
        "arxiv_tags": [
          "cs.IT",
          "math.IT"
        ]
      },
      "meta": {
        "created_at": "2025-02-15T15:19:51+00:00",
        "updated_at": "2025-03-29T22:47:32+00:00",
        "version": 4
      }
    },
    "interactions:2401.14347": {
      "data": {
        "paper_id": "2401.14347",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-15T15:19:40.221Z",
            "data": {
              "session_id": "session_1739632751864_8hpxkz3",
              "duration_seconds": 28,
              "idle_seconds": 0,
              "start_time": "2025-02-15T15:19:11.864Z",
              "end_time": "2025-02-15T15:19:39.553Z",
              "total_elapsed_seconds": 28
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-16T01:45:30.800Z",
            "data": {
              "session_id": "session_1739670326057_0a9i18x",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-02-16T01:45:26.058Z",
              "end_time": "2025-02-16T01:45:29.087Z",
              "total_elapsed_seconds": 3
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-15T15:19:40+00:00",
        "updated_at": "2025-03-29T22:47:32+00:00",
        "version": 6
      }
    },
    "paper:2401.14347": {
      "data": {
        "arxivId": "2401.14347",
        "url": "https://arxiv.org/abs/2401.14347",
        "title": "Evolving higher-order synergies reveals a trade-off between stability\n  and information integration capacity in complex systems",
        "authors": "Thomas F. Varley, Joshua Bongard",
        "abstract": "There has recently been an explosion of interest in how \"higher-order\"\nstructures emerge in complex systems. This \"emergent\" organization has been\nfound in a variety of natural and artificial systems, although at present the\nfield lacks a unified understanding of what the consequences of higher-order\nsynergies and redundancies are for systems. Typical research treat the presence\n(or absence) of synergistic information as a dependent variable and report\nchanges in the level of synergy in response to some change in the system. Here,\nwe attempt to flip the script: rather than treating higher-order information as\na dependent variable, we use evolutionary optimization to evolve boolean\nnetworks with significant higher-order redundancies, synergies, or statistical\ncomplexity. We then analyse these evolved populations of networks using\nestablished tools for characterizing discrete dynamics: the number of\nattractors, average transient length, and Derrida coefficient. We also assess\nthe capacity of the systems to integrate information. We find that high-synergy\nsystems are unstable and chaotic, but with a high capacity to integrate\ninformation. In contrast, evolved redundant systems are extremely stable, but\nhave negligible capacity to integrate information. Finally, the complex systems\nthat balance integration and segregation (known as Tononi-Sporns-Edelman\ncomplexity) show features of both chaosticity and stability, with a greater\ncapacity to integrate information than the redundant systems while being more\nstable than the random and synergistic systems. We conclude that there may be a\nfundamental trade-off between the robustness of a systems dynamics and its\ncapacity to integrate information (which inherently requires flexibility and\nsensitivity), and that certain kinds of complexity naturally balance this\ntrade-off.",
        "timestamp": "2025-02-15T15:19:11.977Z",
        "rating": "thumbsup",
        "published_date": "2024-01-25T17:48:11Z",
        "arxiv_tags": [
          "cs.IT",
          "math.DS",
          "math.IT",
          "nlin.CD",
          "nlin.CG"
        ]
      },
      "meta": {
        "created_at": "2025-02-15T15:19:12+00:00",
        "updated_at": "2025-03-29T22:47:33+00:00",
        "version": 3
      }
    },
    "paper:0909.2120": {
      "data": {
        "arxivId": "0909.2120",
        "url": "https://arxiv.org/abs/0909.2120",
        "title": "Approximate maximizers of intricacy functionals",
        "authors": "Jerome Buzzi, Lorenzo Zambotti",
        "abstract": "G. Edelman, O. Sporns, and G. Tononi introduced in theoretical biology the\nneural complexity of a family of random variables. This functional is a special\ncase of intricacy, i.e., an average of the mutual information of subsystems\nwhose weights have good mathematical properties. Moreover, its maximum value\ngrows at a definite speed with the size of the system.\n  In this work, we compute exactly this speed of growth by building\n\"approximate maximizers\" subject to an entropy condition. These approximate\nmaximizers work simultaneously for all intricacies. We also establish some\nproperties of arbitrary approximate maximizers, in particular the existence of\na threshold in the size of subsystems of approximate maximizers: most smaller\nsubsystems are almost equidistributed, most larger subsystems determine the\nfull system.\n  The main ideas are a random construction of almost maximizers with a high\nstatistical symmetry and the consideration of entropy profiles, i.e., the\naverage entropies of sub-systems of a given size. The latter gives rise to\ninteresting questions of probability and information theory.",
        "timestamp": "2025-02-15T15:19:09.854Z",
        "rating": "novote",
        "published_date": "2009-09-11T09:33:01Z",
        "arxiv_tags": [
          "math.PR",
          "94A17; 92B30; 60C05"
        ]
      },
      "meta": {
        "created_at": "2025-02-15T15:19:10+00:00",
        "updated_at": "2025-03-29T22:47:33+00:00",
        "version": 2
      }
    },
    "paper:2405.21060": {
      "data": {
        "arxivId": "2405.21060",
        "url": "https://arxiv.org/abs/2405.21060",
        "title": "Transformers are SSMs: Generalized Models and Efficient Algorithms\n  Through Structured State Space Duality",
        "authors": "Tri Dao, Albert Gu",
        "abstract": "While Transformers have been the main architecture behind deep learning's\nsuccess in language modeling, state-space models (SSMs) such as Mamba have\nrecently been shown to match or outperform Transformers at small to medium\nscale. We show that these families of models are actually quite closely\nrelated, and develop a rich framework of theoretical connections between SSMs\nand variants of attention, connected through various decompositions of a\nwell-studied class of structured semiseparable matrices. Our state space\nduality (SSD) framework allows us to design a new architecture (Mamba-2) whose\ncore layer is an a refinement of Mamba's selective SSM that is 2-8X faster,\nwhile continuing to be competitive with Transformers on language modeling.",
        "timestamp": "2025-02-15T22:41:28.414Z",
        "rating": "novote",
        "published_date": "2024-05-31T17:50:01Z",
        "arxiv_tags": [
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-15T22:41:28+00:00",
        "updated_at": "2025-03-29T22:47:30+00:00",
        "version": 2
      }
    },
    "paper:2212.14052": {
      "data": {
        "arxivId": "2212.14052",
        "url": "https://arxiv.org/abs/2212.14052",
        "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
        "authors": "Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher R\u00e9",
        "abstract": "State space models (SSMs) have demonstrated state-of-the-art sequence\nmodeling performance in some modalities, but underperform attention in language\nmodeling. Moreover, despite scaling nearly linearly in sequence length instead\nof quadratically, SSMs are still slower than Transformers due to poor hardware\nutilization. In this paper, we make progress on understanding the expressivity\ngap between SSMs and attention in language modeling, and on reducing the\nhardware barrier between SSMs and attention. First, we use synthetic language\nmodeling tasks to understand the gap between SSMs and attention. We find that\nexisting SSMs struggle with two capabilities: recalling earlier tokens in the\nsequence and comparing tokens across the sequence. To understand the impact on\nlanguage modeling, we propose a new SSM layer, H3, that is explicitly designed\nfor these abilities. H3 matches attention on the synthetic languages and comes\nwithin 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid\n125M-parameter H3-attention model that retains two attention layers\nsurprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to\nimprove the efficiency of training SSMs on modern hardware, we propose\nFlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on\nsequences up to 8K, and introduces a novel state passing algorithm that\nexploits the recurrent properties of SSMs to scale to longer sequences.\nFlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows\nhybrid language models to generate text 2.4$\\times$ faster than Transformers.\nUsing FlashConv, we scale hybrid H3-attention language models up to 2.7B\nparameters on the Pile and find promising initial results, achieving lower\nperplexity than Transformers and outperforming Transformers in zero- and\nfew-shot learning on a majority of tasks in the SuperGLUE benchmark.",
        "timestamp": "2025-02-15T22:40:30.551Z",
        "rating": "novote",
        "published_date": "2022-12-28T17:56:03Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.CL"
        ]
      },
      "meta": {
        "created_at": "2025-02-15T22:40:30+00:00",
        "updated_at": "2025-03-29T22:47:30+00:00",
        "version": 2
      }
    },
    "paper:2412.03603": {
      "data": {
        "arxivId": "2412.03603",
        "url": "https://arxiv.org/abs/2412.03603",
        "title": "HunyuanVideo: A Systematic Framework For Large Video Generative Models",
        "authors": "Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Dax Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, Caesar Zhong",
        "abstract": "Recent advancements in video generation have significantly impacted daily\nlife for both individuals and industries. However, the leading video generation\nmodels remain closed-source, resulting in a notable performance gap between\nindustry capabilities and those available to the public. In this report, we\nintroduce HunyuanVideo, an innovative open-source video foundation model that\ndemonstrates performance in video generation comparable to, or even surpassing,\nthat of leading closed-source models. HunyuanVideo encompasses a comprehensive\nframework that integrates several key elements, including data curation,\nadvanced architectural design, progressive model scaling and training, and an\nefficient infrastructure tailored for large-scale model training and inference.\nAs a result, we successfully trained a video generative model with over 13\nbillion parameters, making it the largest among all open-source models. We\nconducted extensive experiments and implemented a series of targeted designs to\nensure high visual quality, motion dynamics, text-video alignment, and advanced\nfilming techniques. According to evaluations by professionals, HunyuanVideo\noutperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6,\nand three top-performing Chinese video generative models. By releasing the code\nfor the foundation model and its applications, we aim to bridge the gap between\nclosed-source and open-source communities. This initiative will empower\nindividuals within the community to experiment with their ideas, fostering a\nmore dynamic and vibrant video generation ecosystem. The code is publicly\navailable at https://github.com/Tencent/HunyuanVideo.",
        "timestamp": "2025-02-15T22:38:36.193Z",
        "rating": "novote",
        "published_date": "2024-12-03T23:52:37Z",
        "arxiv_tags": [
          "cs.CV"
        ]
      },
      "meta": {
        "created_at": "2025-02-15T22:38:36+00:00",
        "updated_at": "2025-03-29T22:47:30+00:00",
        "version": 2
      }
    },
    "interactions:2405.21060": {
      "data": {
        "paper_id": "2405.21060",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-15T22:59:39.561Z",
            "data": {
              "session_id": "session_1739660375023_s75ff3w",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-02-15T22:59:35.023Z",
              "end_time": "2025-02-15T22:59:39.549Z",
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-15T22:59:28+00:00",
        "updated_at": "2025-03-29T22:47:29+00:00",
        "version": 4
      }
    },
    "interactions:2212.14052": {
      "data": {
        "paper_id": "2212.14052",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-15T22:59:06.173Z",
            "data": {
              "session_id": "session_1739660330192_lx2jzq5",
              "duration_seconds": 16,
              "idle_seconds": 0,
              "start_time": "2025-02-15T22:58:50.192Z",
              "end_time": "2025-02-15T22:59:05.829Z",
              "total_elapsed_seconds": 16
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-15T22:59:06+00:00",
        "updated_at": "2025-03-29T22:47:29+00:00",
        "version": 3
      }
    },
    "interactions:2412.03603": {
      "data": {
        "paper_id": "2412.03603",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-15T23:04:15.327Z",
            "data": {
              "session_id": "session_1739660635355_h8p8dsr",
              "duration_seconds": 20,
              "idle_seconds": 0,
              "start_time": "2025-02-15T23:03:55.356Z",
              "end_time": "2025-02-15T23:04:14.978Z",
              "total_elapsed_seconds": 20
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-15T23:03:49+00:00",
        "updated_at": "2025-03-29T22:47:28+00:00",
        "version": 4
      }
    },
    "interactions:0909.2120": {
      "data": {
        "paper_id": "0909.2120",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-16T01:45:19.362Z",
            "data": {
              "session_id": "session_1739670313447_v222tn4",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-02-16T01:45:13.447Z",
              "end_time": "2025-02-16T01:45:18.696Z",
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-16T01:45:20+00:00",
        "updated_at": "2025-03-29T22:47:28+00:00",
        "version": 3
      }
    },
    "interactions:2010.02331": {
      "data": {
        "paper_id": "2010.02331",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-17T06:45:10.118Z",
            "data": {
              "session_id": "session_1739774696587_4ovs8w7",
              "duration_seconds": 13,
              "idle_seconds": 0,
              "start_time": "2025-02-17T06:44:56.587Z",
              "end_time": "2025-02-17T06:45:09.389Z",
              "total_elapsed_seconds": 13
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-17T06:45:11+00:00",
        "updated_at": "2025-03-29T22:47:27+00:00",
        "version": 3
      }
    },
    "paper:2010.02331": {
      "data": {
        "arxivId": "2010.02331",
        "url": "https://arxiv.org/abs/2010.02331",
        "title": "How to send a real number using a single bit (and some shared\n  randomness)",
        "authors": "Ran Ben-Basat, Michael Mitzenmacher, Shay Vargaftik",
        "abstract": "We consider the fundamental problem of communicating an estimate of a real\nnumber $x\\in[0,1]$ using a single bit. A sender that knows $x$ chooses a value\n$X\\in\\set{0,1}$ to transmit. In turn, a receiver estimates $x$ based on the\nvalue of $X$. We consider both the biased and unbiased estimation problems and\naim to minimize the cost. For the biased case, the cost is the worst-case (over\nthe choice of $x$) expected squared error, which coincides with the variance if\nthe algorithm is required to be unbiased.\n  We first overview common biased and unbiased estimation approaches and prove\ntheir optimality when no shared randomness is allowed. We then show how a small\namount of shared randomness, which can be as low as a single bit, reduces the\ncost in both cases. Specifically, we derive lower bounds on the cost attainable\nby any algorithm with unrestricted use of shared randomness and propose\nnear-optimal solutions that use a small number of shared random bits. Finally,\nwe discuss open problems and future directions.",
        "timestamp": "2025-02-17T06:44:56.714Z",
        "rating": "novote",
        "published_date": "2020-10-05T20:52:06Z",
        "arxiv_tags": [
          "cs.DS",
          "cs.IT",
          "cs.LG",
          "math.IT"
        ]
      },
      "meta": {
        "created_at": "2025-02-17T06:44:57+00:00",
        "updated_at": "2025-03-29T22:47:27+00:00",
        "version": 2
      }
    },
    "interactions:2502.10248": {
      "data": {
        "paper_id": "2502.10248",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-17T13:46:13.451Z",
            "data": {
              "session_id": "session_1739799953977_mra89it",
              "duration_seconds": 19,
              "idle_seconds": 0,
              "start_time": "2025-02-17T13:45:53.977Z",
              "end_time": "2025-02-17T13:46:12.706Z",
              "total_elapsed_seconds": 19
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-17T13:52:28.721Z",
            "data": {
              "session_id": "session_1739800341991_gb1k3u5",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-02-17T13:52:21.991Z",
              "end_time": "2025-02-17T13:52:27.992Z",
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-17T18:41:33.533Z",
            "data": {
              "session_id": "session_1739817689248_eesktlo",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-02-17T18:41:29.248Z",
              "end_time": "2025-02-17T18:41:33.341Z",
              "total_elapsed_seconds": 4
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-17T13:46:14+00:00",
        "updated_at": "2025-03-29T22:47:26+00:00",
        "version": 5
      }
    },
    "paper:2502.10248": {
      "data": {
        "arxivId": "2502.10248",
        "url": "https://arxiv.org/abs/2502.10248",
        "title": "Step-Video-T2V Technical Report: The Practice, Challenges, and Future of\n  Video Foundation Model",
        "authors": "Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, Yu Zhou, Deshan Sun, Deyu Zhou, Jian Zhou, Kaijun Tan, Kang An, Mei Chen, Wei Ji, Qiling Wu, Wen Sun, Xin Han, Yanan Wei, Zheng Ge, Aojie Li, Bin Wang, Bizhu Huang, Bo Wang, Brian Li, Changxing Miao, Chen Xu, Chenfei Wu, Chenguang Yu, Dapeng Shi, Dingyuan Hu, Enle Liu, Gang Yu, Ge Yang, Guanzhe Huang, Gulin Yan, Haiyang Feng, Hao Nie, Haonan Jia, Hanpeng Hu, Hanqi Chen, Haolong Yan, Heng Wang, Hongcheng Guo, Huilin Xiong, Huixin Xiong, Jiahao Gong, Jianchang Wu, Jiaoren Wu, Jie Wu, Jie Yang, Jiashuai Liu, Jiashuo Li, Jingyang Zhang, Junjing Guo, Junzhe Lin, Kaixiang Li, Lei Liu, Lei Xia, Liang Zhao, Liguo Tan, Liwen Huang, Liying Shi, Ming Li, Mingliang Li, Muhua Cheng, Na Wang, Qiaohui Chen, Qinglin He, Qiuyan Liang, Quan Sun, Ran Sun, Rui Wang, Shaoliang Pang, Shiliang Yang, Sitong Liu, Siqi Liu, Shuli Gao, Tiancheng Cao, Tianyu Wang, Weipeng Ming, Wenqing He, Xu Zhao, Xuelin Zhang, Xianfang Zeng, Xiaojia Liu, Xuan Yang, Yaqi Dai, Yanbo Yu, Yang Li, Yineng Deng, Yingming Wang, Yilei Wang, Yuanwei Lu, Yu Chen, Yu Luo, Yuchu Luo, Yuhe Yin, Yuheng Feng, Yuxiang Yang, Zecheng Tang, Zekai Zhang, Zidong Yang, Binxing Jiao, Jiansheng Chen, Jing Li, Shuchang Zhou, Xiangyu Zhang, Xinhao Zhang, Yibo Zhu, Heung-Yeung Shum, Daxin Jiang",
        "abstract": "We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model\nwith 30B parameters and the ability to generate videos up to 204 frames in\nlength. A deep compression Variational Autoencoder, Video-VAE, is designed for\nvideo generation tasks, achieving 16x16 spatial and 8x temporal compression\nratios, while maintaining exceptional video reconstruction quality. User\nprompts are encoded using two bilingual text encoders to handle both English\nand Chinese. A DiT with 3D full attention is trained using Flow Matching and is\nemployed to denoise input noise into latent frames. A video-based DPO approach,\nVideo-DPO, is applied to reduce artifacts and improve the visual quality of the\ngenerated videos. We also detail our training strategies and share key\nobservations and insights. Step-Video-T2V's performance is evaluated on a novel\nvideo generation benchmark, Step-Video-T2V-Eval, demonstrating its\nstate-of-the-art text-to-video quality when compared with both open-source and\ncommercial engines. Additionally, we discuss the limitations of current\ndiffusion-based model paradigm and outline future directions for video\nfoundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval\navailable at https://github.com/stepfun-ai/Step-Video-T2V. The online version\ncan be accessed from https://yuewen.cn/videos as well. Our goal is to\naccelerate the innovation of video foundation models and empower video content\ncreators.",
        "timestamp": "2025-02-17T13:45:52.287Z",
        "rating": "novote",
        "published_date": "2025-02-14T15:58:10Z",
        "arxiv_tags": [
          "cs.CV",
          "cs.CL"
        ]
      },
      "meta": {
        "created_at": "2025-02-17T13:45:52+00:00",
        "updated_at": "2025-03-29T22:47:26+00:00",
        "version": 2
      }
    },
    "interactions:0711.1859": {
      "data": {
        "paper_id": "0711.1859",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-02-17T14:34:26+00:00",
        "updated_at": "2025-03-29T22:47:25+00:00",
        "version": 2
      }
    },
    "paper:0711.1859": {
      "data": {
        "arxivId": "0711.1859",
        "url": "https://arxiv.org/pdf/0711.1859",
        "title": "Doubles for monoidal categories",
        "authors": "Craig Pastro, Ross Street",
        "abstract": "In a recent paper, Daisuke Tambara defined two-sided actions on an endomodule\n(= endodistributor) of a monoidal V-category A. When A is autonomous (= rigid =\ncompact), he showed that the V-category (that we call Tamb(A)) of so-equipped\nendomodules (that we call Tambara modules) is equivalent to the monoidal centre\nZ[A,V] of the convolution monoidal V-category [A,V]. Our paper extends these\nideas somewhat. For general A, we construct a promonoidal V-category DA (which\nwe suggest should be called the double of A) with an equivalence [DA,V] \\simeq\nTamb(A). When A is closed, we define strong (respectively, left strong) Tambara\nmodules and show that these constitute a V-category Tamb_s(A) (respectively,\nTamb_{ls}(A)) which is equivalent to the centre (respectively, lax centre) of\n[A,V]. We construct localizations D_s A and D_{ls} A of DA such that there are\nequivalences Tamb_s(A) \\simeq [D_s A,V] and Tamb_{ls}(A) \\simeq [D_{ls} A,V].\nWhen A is autonomous, every Tambara module is strong; this implies an\nequivalence Z[A,V] \\simeq [DA,V].",
        "timestamp": "2025-02-17T14:33:04.178Z",
        "rating": "novote",
        "published_date": "2007-11-13T00:13:19Z",
        "arxiv_tags": [
          "math.CT"
        ]
      },
      "meta": {
        "created_at": "2025-02-17T14:33:04+00:00",
        "updated_at": "2025-03-29T22:47:25+00:00",
        "version": 2
      }
    },
    "interactions:1805.09843": {
      "data": {
        "paper_id": "1805.09843",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-02-17T15:22:56+00:00",
        "updated_at": "2025-03-29T22:47:23+00:00",
        "version": 2
      }
    },
    "paper:1805.09843": {
      "data": {
        "arxivId": "1805.09843",
        "url": "https://arxiv.org/abs/1805.09843",
        "title": "Baseline Needs More Love: On Simple Word-Embedding-Based Models and\n  Associated Pooling Mechanisms",
        "authors": "Dinghan Shen, Guoyin Wang, Wenlin Wang, Martin Renqiang Min, Qinliang Su, Yizhe Zhang, Chunyuan Li, Ricardo Henao, Lawrence Carin",
        "abstract": "Many deep learning architectures have been proposed to model the\ncompositionality in text sequences, requiring a substantial number of\nparameters and expensive computations. However, there has not been a rigorous\nevaluation regarding the added value of sophisticated compositional functions.\nIn this paper, we conduct a point-by-point comparative study between Simple\nWord-Embedding-based Models (SWEMs), consisting of parameter-free pooling\noperations, relative to word-embedding-based RNN/CNN models. Surprisingly,\nSWEMs exhibit comparable or even superior performance in the majority of cases\nconsidered. Based upon this understanding, we propose two additional pooling\nstrategies over learned word embeddings: (i) a max-pooling operation for\nimproved interpretability; and (ii) a hierarchical pooling operation, which\npreserves spatial (n-gram) information within text sequences. We present\nexperiments on 17 datasets encompassing three tasks: (i) (long) document\nclassification; (ii) text sequence matching; and (iii) short text tasks,\nincluding classification and tagging. The source code and datasets can be\nobtained from https:// github.com/dinghanshen/SWEM.",
        "timestamp": "2025-02-17T15:22:36.514Z",
        "rating": "novote",
        "published_date": "2018-05-24T18:27:21Z",
        "arxiv_tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-17T15:22:37+00:00",
        "updated_at": "2025-03-29T22:47:23+00:00",
        "version": 2
      }
    },
    "paper:2410.07590": {
      "data": {
        "arxivId": "2410.07590",
        "url": "https://arxiv.org/pdf/2410.07590",
        "title": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed\n  KV Caches for Chunked Text",
        "authors": "Songshuo Lu, Hua Wang, Yutian Rong, Zhi Chen, Yaohua Tang",
        "abstract": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process\nnumerous retrieved document chunks for prefill which requires a large volume of\ncomputation, therefore leading to significant latency in time-to-first-token\n(TTFT). To reduce the computation overhead as well as TTFT, we introduce\nTurboRAG, a novel RAG system that redesigns the inference paradigm of the\ncurrent RAG system by first pre-computing and storing the key-value (KV) caches\nof documents offline, and then directly retrieving the saved KV cache for\nprefill. Hence, online computation of KV caches is eliminated during inference.\nIn addition, we provide a number of insights into the mask matrix and\npositional embedding mechanisms, plus fine-tune a pretrained language model to\nmaintain model accuracy of TurboRAG. Our approach is applicable to most\nexisting large language models and their applications without any requirement\nin modification of models and inference systems. Experimental results across a\nsuite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x\ncompared to the conventional RAG systems (on an average of 8.6x), but reserving\ncomparable performance to the standard RAG systems.",
        "timestamp": "2025-02-17T15:04:32.964Z",
        "rating": "novote",
        "published_date": "2024-10-10T03:52:54Z",
        "arxiv_tags": [
          "cs.CV",
          "cs.CL"
        ]
      },
      "meta": {
        "created_at": "2025-02-17T15:04:33+00:00",
        "updated_at": "2025-03-29T22:47:24+00:00",
        "version": 2
      }
    },
    "interactions:1801.01715": {
      "data": {
        "paper_id": "1801.01715",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-17T16:31:56.736Z",
            "data": {
              "session_id": "session_1739809903606_cgzr7jj",
              "duration_seconds": 12,
              "idle_seconds": 0,
              "start_time": "2025-02-17T16:31:43.606Z",
              "end_time": "2025-02-17T16:31:55.992Z",
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-17T16:54:18.001Z",
            "data": {
              "session_id": "session_1739811246577_tcqhh11",
              "duration_seconds": 11,
              "idle_seconds": 0,
              "start_time": "2025-02-17T16:54:06.577Z",
              "end_time": "2025-02-17T16:54:17.382Z",
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-17T16:31:57+00:00",
        "updated_at": "2025-03-29T22:47:22+00:00",
        "version": 4
      }
    },
    "paper:1801.01715": {
      "data": {
        "arxivId": "1801.01715",
        "url": "https://arxiv.org/abs/1801.01715",
        "title": "Spectral Graph Forge: Graph Generation Targeting Modularity",
        "authors": "Luca Baldesi, Athina Markopoulou, Carter T. Butts",
        "abstract": "Community structure is an important property that captures inhomogeneities\ncommon in large networks, and modularity is one of the most widely used metrics\nfor such community structure. In this paper, we introduce a principled\nmethodology, the Spectral Graph Forge, for generating random graphs that\npreserves community structure from a real network of interest, in terms of\nmodularity. Our approach leverages the fact that the spectral structure of\nmatrix representations of a graph encodes global information about community\nstructure. The Spectral Graph Forge uses a low-rank approximation of the\nmodularity matrix to generate synthetic graphs that match a target modularity\nwithin user-selectable degree of accuracy, while allowing other aspects of\nstructure to vary. We show that the Spectral Graph Forge outperforms\nstate-of-the-art techniques in terms of accuracy in targeting the modularity\nand randomness of the realizations, while also preserving other local\nstructural properties and node attributes. We discuss extensions of the\nSpectral Graph Forge to target other properties beyond modularity, and its\napplications to anonymization.",
        "timestamp": "2025-02-17T16:31:42.498Z",
        "rating": "novote",
        "published_date": "2018-01-05T11:11:20Z",
        "arxiv_tags": [
          "cs.SI",
          "physics.soc-ph"
        ]
      },
      "meta": {
        "created_at": "2025-02-17T16:31:42+00:00",
        "updated_at": "2025-03-29T22:47:22+00:00",
        "version": 2
      }
    },
    "interactions:2502.12981": {
      "data": {
        "paper_id": "2502.12981",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-19T17:00:44.571Z",
            "data": {
              "session_id": "session_1739984421179_3vwhogj",
              "duration_seconds": 23,
              "idle_seconds": 0,
              "start_time": "2025-02-19T17:00:21.179Z",
              "end_time": "2025-02-19T17:00:43.759Z",
              "total_elapsed_seconds": 23
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-19T17:03:37.508Z",
            "data": {
              "session_id": "session_1739984605893_ezjx9z6",
              "duration_seconds": 9,
              "idle_seconds": 0,
              "start_time": "2025-02-19T17:03:25.893Z",
              "end_time": "2025-02-19T17:03:35.093Z",
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-19T17:00:45+00:00",
        "updated_at": "2025-03-29T22:47:06+00:00",
        "version": 7
      }
    },
    "paper:2502.12981": {
      "data": {
        "arxivId": "2502.12981",
        "url": "https://arxiv.org/abs/2502.12981",
        "title": "Towards Variational Flow Matching on General Geometries",
        "authors": "Olga Zaghen, Floor Eijkelboom, Alison Pouplin, Erik J. Bekkers",
        "abstract": "We introduce Riemannian Gaussian Variational Flow Matching (RG-VFM), an\nextension of Variational Flow Matching (VFM) that leverages Riemannian Gaussian\ndistributions for generative modeling on structured manifolds. We derive a\nvariational objective for probability flows on manifolds with closed-form\ngeodesics, making RG-VFM comparable - though fundamentally different to\nRiemannian Flow Matching (RFM) in this geometric setting. Experiments on a\ncheckerboard dataset wrapped on the sphere demonstrate that RG-VFM captures\ngeometric structure more effectively than Euclidean VFM and baseline methods,\nestablishing it as a robust framework for manifold-aware generative modeling.",
        "timestamp": "2025-02-19T17:00:20.667Z",
        "rating": "novote",
        "published_date": "2025-02-18T16:02:10Z",
        "arxiv_tags": [
          "cs.LG",
          "math.DG"
        ]
      },
      "meta": {
        "created_at": "2025-02-19T17:00:21+00:00",
        "updated_at": "2025-03-29T22:47:06+00:00",
        "version": 2
      }
    },
    "paper:2011.14522": {
      "data": {
        "arxivId": "2011.14522",
        "url": "https://arxiv.org/abs/2011.14522",
        "title": "Feature Learning in Infinite-Width Neural Networks",
        "authors": "Greg Yang, Edward J. Hu",
        "abstract": "As its width tends to infinity, a deep neural network's behavior under\ngradient descent can become simplified and predictable (e.g. given by the\nNeural Tangent Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK\nparametrization). However, we show that the standard and NTK parametrizations\nof a neural network do not admit infinite-width limits that can learn features,\nwhich is crucial for pretraining and transfer learning such as with BERT. We\npropose simple modifications to the standard parametrization to allow for\nfeature learning in the limit. Using the *Tensor Programs* technique, we derive\nexplicit formulas for such limits. On Word2Vec and few-shot learning on\nOmniglot via MAML, two canonical tasks that rely crucially on feature learning,\nwe compute these limits exactly. We find that they outperform both NTK\nbaselines and finite-width networks, with the latter approaching the\ninfinite-width feature learning performance as width increases.\n  More generally, we classify a natural space of neural network\nparametrizations that generalizes standard, NTK, and Mean Field\nparametrizations. We show 1) any parametrization in this space either admits\nfeature learning or has an infinite-width training dynamics given by kernel\ngradient descent, but not both; 2) any such infinite-width limit can be\ncomputed using the Tensor Programs technique. Code for our experiments can be\nfound at github.com/edwardjhu/TP4.",
        "timestamp": "2025-02-19T07:54:41.193Z",
        "rating": "novote",
        "published_date": "2020-11-30T03:21:05Z",
        "arxiv_tags": [
          "cs.LG",
          "cond-mat.dis-nn",
          "cs.NE"
        ]
      },
      "meta": {
        "created_at": "2025-02-19T07:54:41+00:00",
        "updated_at": "2025-03-29T22:47:07+00:00",
        "version": 2
      }
    },
    "interactions:1712.08969": {
      "data": {
        "paper_id": "1712.08969",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-02-19T07:54:38+00:00",
        "updated_at": "2025-03-29T22:47:08+00:00",
        "version": 2
      }
    },
    "paper:1712.08969": {
      "data": {
        "arxivId": "1712.08969",
        "url": "https://arxiv.org/abs/1712.08969",
        "title": "Mean Field Residual Networks: On the Edge of Chaos",
        "authors": "Greg Yang, Samuel S. Schoenholz",
        "abstract": "We study randomly initialized residual networks using mean field theory and\nthe theory of difference equations. Classical feedforward neural networks, such\nas those with tanh activations, exhibit exponential behavior on the average\nwhen propagating inputs forward or gradients backward. The exponential forward\ndynamics causes rapid collapsing of the input space geometry, while the\nexponential backward dynamics causes drastic vanishing or exploding gradients.\nWe show, in contrast, that by adding skip connections, the network will,\ndepending on the nonlinearity, adopt subexponential forward and backward\ndynamics, and in many cases in fact polynomial. The exponents of these\npolynomials are obtained through analytic methods and proved and verified\nempirically to be correct. In terms of the \"edge of chaos\" hypothesis, these\nsubexponential and polynomial laws allow residual networks to \"hover over the\nboundary between stability and chaos,\" thus preserving the geometry of the\ninput space and the gradient information flow. In our experiments, for each\nactivation function we study here, we initialize residual networks with\ndifferent hyperparameters and train them on MNIST. Remarkably, our\ninitialization time theory can accurately predict test time performance of\nthese networks, by tracking either the expected amount of gradient explosion or\nthe expected squared distance between the images of two input vectors.\nImportantly, we show, theoretically as well as empirically, that common\ninitializations such as the Xavier or the He schemes are not optimal for\nresidual networks, because the optimal initialization variances depend on the\ndepth. Finally, we have made mathematical contributions by deriving several new\nidentities for the kernels of powers of ReLU functions by relating them to the\nzeroth Bessel function of the second kind.",
        "timestamp": "2025-02-19T07:54:33.205Z",
        "rating": "novote",
        "published_date": "2017-12-24T21:51:08Z",
        "arxiv_tags": [
          "cs.NE",
          "cond-mat.dis-nn",
          "cs.LG",
          "math.DS",
          "nlin.CD"
        ]
      },
      "meta": {
        "created_at": "2025-02-19T07:54:33+00:00",
        "updated_at": "2025-03-29T22:47:08+00:00",
        "version": 2
      }
    },
    "interactions:2203.03466": {
      "data": {
        "paper_id": "2203.03466",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-19T07:48:05.358Z",
            "data": {
              "session_id": "session_1739951277242_qbmceb0",
              "duration_seconds": 7,
              "idle_seconds": 0,
              "start_time": "2025-02-19T07:47:57.242Z",
              "end_time": "2025-02-19T07:48:04.663Z",
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-19T07:48:06+00:00",
        "updated_at": "2025-03-29T22:47:09+00:00",
        "version": 4
      }
    },
    "paper:2203.03466": {
      "data": {
        "arxivId": "2203.03466",
        "url": "https://arxiv.org/abs/2203.03466",
        "title": "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot\n  Hyperparameter Transfer",
        "authors": "Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, Jianfeng Gao",
        "abstract": "Hyperparameter (HP) tuning in deep learning is an expensive process,\nprohibitively so for neural networks (NNs) with billions of parameters. We show\nthat, in the recently discovered Maximal Update Parametrization (muP), many\noptimal HPs remain stable even as model size changes. This leads to a new HP\ntuning paradigm we call muTransfer: parametrize the target model in muP, tune\nthe HP indirectly on a smaller model, and zero-shot transfer them to the\nfull-sized model, i.e., without directly tuning the latter at all. We verify\nmuTransfer on Transformer and ResNet. For example, 1) by transferring\npretraining HPs from a model of 13M parameters, we outperform published numbers\nof BERT-large (350M parameters), with a total tuning cost equivalent to\npretraining BERT-large once; 2) by transferring from 40M parameters, we\noutperform published numbers of the 6.7B GPT-3 model, with tuning cost only 7%\nof total pretraining cost. A Pytorch implementation of our technique can be\nfound at github.com/microsoft/mup and installable via `pip install mup`.",
        "timestamp": "2025-02-19T07:47:57.320Z",
        "rating": "novote",
        "published_date": "2022-03-07T15:37:35Z",
        "arxiv_tags": [
          "cs.LG",
          "cond-mat.dis-nn",
          "cs.NE"
        ]
      },
      "meta": {
        "created_at": "2025-02-19T07:47:57+00:00",
        "updated_at": "2025-03-29T22:47:09+00:00",
        "version": 2
      }
    },
    "interactions:2407.17465": {
      "data": {
        "paper_id": "2407.17465",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-02-19T07:47:34+00:00",
        "updated_at": "2025-03-29T22:47:10+00:00",
        "version": 2
      }
    },
    "paper:2407.17465": {
      "data": {
        "arxivId": "2407.17465",
        "url": "https://arxiv.org/abs/2407.17465v2",
        "title": "u-$\u03bc$P: The Unit-Scaled Maximal Update Parametrization",
        "authors": "Charlie Blake, Constantin Eichenberg, Josef Dean, Lukas Balles, Luke Y. Prince, Bj\u00f6rn Deiseroth, Andres Felipe Cruz-Salinas, Carlo Luschi, Samuel Weinbach, Douglas Orr",
        "abstract": "The Maximal Update Parametrization ($\\mu$P) aims to make the optimal\nhyperparameters (HPs) of a model independent of its size, allowing them to be\nswept using a cheap proxy model rather than the full-size target model. We\npresent a new scheme, u-$\\mu$P, which improves upon $\\mu$P by combining it with\nUnit Scaling, a method for designing models that makes them easy to train in\nlow-precision. The two techniques have a natural affinity: $\\mu$P ensures that\nthe scale of activations is independent of model size, and Unit Scaling ensures\nthat activations, weights and gradients begin training with a scale of one.\nThis synthesis opens the door to a simpler scheme, whose default values are\nnear-optimal. This in turn facilitates a more efficient sweeping strategy, with\nu-$\\mu$P models reaching a loss that is equal to or lower than comparable\n$\\mu$P models and working out-of-the-box in FP8.",
        "timestamp": "2025-02-19T07:47:27.536Z",
        "rating": "novote",
        "published_date": "2024-07-24T17:58:42Z",
        "arxiv_tags": [
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-19T07:47:28+00:00",
        "updated_at": "2025-03-29T22:47:11+00:00",
        "version": 2
      }
    },
    "interactions:2502.05795": {
      "data": {
        "paper_id": "2502.05795",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-19T07:45:05.081Z",
            "data": {
              "session_id": "session_1739951096757_gbdiq79",
              "duration_seconds": 7,
              "idle_seconds": 0,
              "start_time": "2025-02-19T07:44:56.757Z",
              "end_time": "2025-02-19T07:45:04.167Z",
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-19T07:58:00.169Z",
            "data": {
              "session_id": "session_1739951870985_qqi2ymf",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-02-19T07:57:50.985Z",
              "end_time": "2025-02-19T07:57:56.675Z",
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-19T07:45:06+00:00",
        "updated_at": "2025-03-29T22:47:11+00:00",
        "version": 5
      }
    },
    "paper:2502.05795": {
      "data": {
        "arxivId": "2502.05795",
        "url": "https://arxiv.org/abs/2502.05795",
        "title": "The Curse of Depth in Large Language Models",
        "authors": "Wenfang Sun, Xinyuan Song, Pengxiang Li, Lu Yin, Yefeng Zheng, Shiwei Liu",
        "abstract": "In this paper, we introduce the Curse of Depth, a concept that highlights,\nexplains, and addresses the recent observation in modern Large Language\nModels(LLMs) where nearly half of the layers are less effective than expected.\nWe first confirm the wide existence of this phenomenon across the most popular\nfamilies of LLMs such as Llama, Mistral, DeepSeek, and Qwen. Our analysis,\ntheoretically and empirically, identifies that the underlying reason for the\nineffectiveness of deep layers in LLMs is the widespread usage of Pre-Layer\nNormalization (Pre-LN). While Pre-LN stabilizes the training of Transformer\nLLMs, its output variance exponentially grows with the model depth, which\nundesirably causes the derivative of the deep Transformer blocks to be an\nidentity matrix, and therefore barely contributes to the training. To resolve\nthis training pitfall, we propose LayerNorm Scaling, which scales the variance\nof output of the layer normalization inversely by the square root of its depth.\nThis simple modification mitigates the output variance explosion of deeper\nTransformer layers, improving their contribution. Our experimental results,\nspanning model sizes from 130M to 1B, demonstrate that LayerNorm Scaling\nsignificantly enhances LLM pre-training performance compared to Pre-LN.\nMoreover, this improvement seamlessly carries over to supervised fine-tuning.\nAll these gains can be attributed to the fact that LayerNorm Scaling enables\ndeeper layers to contribute more effectively during training.",
        "timestamp": "2025-02-19T07:44:29.134Z",
        "rating": "novote",
        "published_date": "2025-02-09T07:03:36Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI"
        ]
      },
      "meta": {
        "created_at": "2025-02-19T07:44:29+00:00",
        "updated_at": "2025-03-29T22:47:12+00:00",
        "version": 2
      }
    },
    "interactions:2310.07547": {
      "data": {
        "paper_id": "2310.07547",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-02-18T17:51:34+00:00",
        "updated_at": "2025-03-29T22:47:13+00:00",
        "version": 2
      }
    },
    "interactions:2012.11197": {
      "data": {
        "paper_id": "2012.11197",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-18T17:50:54.277Z",
            "data": {
              "session_id": "session_1739901037432_6692uur",
              "duration_seconds": 16,
              "idle_seconds": 0,
              "start_time": "2025-02-18T17:50:37.432Z",
              "end_time": "2025-02-18T17:50:53.605Z",
              "total_elapsed_seconds": 16
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-18T17:50:55+00:00",
        "updated_at": "2025-03-29T22:47:13+00:00",
        "version": 3
      }
    },
    "paper:2310.07547": {
      "data": {
        "arxivId": "2310.07547",
        "url": "https://arxiv.org/abs/2310.07547",
        "title": "Entropy estimators for Markovian sequences: A comparative analysis",
        "authors": "Juan De Gregorio, David Sanchez, Raul Toral",
        "abstract": "Entropy estimation is a fundamental problem in information theory that has\napplications in various fields, including physics, biology, and computer\nscience. Estimating the entropy of discrete sequences can be challenging due to\nlimited data and the lack of unbiased estimators. Most existing entropy\nestimators are designed for sequences of independent events and their\nperformance vary depending on the system being studied and the available data\nsize. In this work we compare different entropy estimators and their\nperformance when applied to Markovian sequences. Specifically, we analyze both\nbinary Markovian sequences and Markovian systems in the undersampled regime. We\ncalculate the bias, standard deviation and mean squared error for some of the\nmost widely employed estimators. We discuss the limitations of entropy\nestimation as a function of the transition probabilities of the Markov\nprocesses and the sample size. Overall, this paper provides a comprehensive\ncomparison of entropy estimators and their performance in estimating entropy\nfor systems with memory, which can be useful for researchers and practitioners\nin various fields.",
        "timestamp": "2025-02-18T17:28:47.045Z",
        "rating": "novote",
        "published_date": "2023-10-11T14:50:47Z",
        "arxiv_tags": [
          "cond-mat.stat-mech",
          "nlin.CD",
          "physics.data-an"
        ]
      },
      "meta": {
        "created_at": "2025-02-18T17:28:47+00:00",
        "updated_at": "2025-03-29T22:47:14+00:00",
        "version": 2
      }
    },
    "interactions:1406.6959": {
      "data": {
        "paper_id": "1406.6959",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-02-18T17:28:46+00:00",
        "updated_at": "2025-03-29T22:47:16+00:00",
        "version": 2
      }
    },
    "interactions:0811.3579": {
      "data": {
        "paper_id": "0811.3579",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-18T17:28:30.764Z",
            "data": {
              "session_id": "session_1739899703756_rmqd1hr",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-02-18T17:28:23.756Z",
              "end_time": "2025-02-18T17:28:30.011Z",
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-18T17:28:31+00:00",
        "updated_at": "2025-03-29T22:47:16+00:00",
        "version": 3
      }
    },
    "paper:1406.6959": {
      "data": {
        "arxivId": "1406.6959",
        "url": "https://arxiv.org/pdf/1406.6959",
        "title": "Maximum Likelihood Estimation of Functionals of Discrete Distributions",
        "authors": "Jiantao Jiao, Kartik Venkat, Yanjun Han, Tsachy Weissman",
        "abstract": "We consider the problem of estimating functionals of discrete distributions,\nand focus on tight nonasymptotic analysis of the worst case squared error risk\nof widely used estimators. We apply concentration inequalities to analyze the\nrandom fluctuation of these estimators around their expectations, and the\ntheory of approximation using positive linear operators to analyze the\ndeviation of their expectations from the true functional, namely their\n\\emph{bias}.\n  We characterize the worst case squared error risk incurred by the Maximum\nLikelihood Estimator (MLE) in estimating the Shannon entropy $H(P) = \\sum_{i =\n1}^S -p_i \\ln p_i$, and $F_\\alpha(P) = \\sum_{i = 1}^S p_i^\\alpha,\\alpha&gt;0$, up\nto multiplicative constants, for any alphabet size $S\\leq \\infty$ and sample\nsize $n$ for which the risk may vanish. As a corollary, for Shannon entropy\nestimation, we show that it is necessary and sufficient to have $n \\gg S$\nobservations for the MLE to be consistent. In addition, we establish that it is\nnecessary and sufficient to consider $n \\gg S^{1/\\alpha}$ samples for the MLE\nto consistently estimate $F_\\alpha(P), 0&lt;\\alpha&lt;1$. The minimax rate-optimal\nestimators for both problems require $S/\\ln S$ and $S^{1/\\alpha}/\\ln S$\nsamples, which implies that the MLE has a strictly sub-optimal sample\ncomplexity. When $1&lt;\\alpha&lt;3/2$, we show that the worst-case squared error rate\nof convergence for the MLE is $n^{-2(\\alpha-1)}$ for infinite alphabet size,\nwhile the minimax squared error rate is $(n\\ln n)^{-2(\\alpha-1)}$. When\n$\\alpha\\geq 3/2$, the MLE achieves the minimax optimal rate $n^{-1}$ regardless\nof the alphabet size.\n  As an application of the general theory, we analyze the Dirichlet prior\nsmoothing techniques for Shannon entropy estimation. We show that no matter how\nwe tune the parameters in the Dirichlet prior, this technique cannot achieve\nthe minimax rates in entropy estimation.",
        "timestamp": "2025-02-18T17:28:30.667Z",
        "rating": "novote",
        "published_date": "2014-06-26T17:53:58Z",
        "arxiv_tags": [
          "cs.IT",
          "math.IT",
          "math.ST",
          "stat.TH"
        ]
      },
      "meta": {
        "created_at": "2025-02-18T17:28:31+00:00",
        "updated_at": "2025-03-29T22:47:17+00:00",
        "version": 2
      }
    },
    "paper:2012.11197": {
      "data": {
        "arxivId": "2012.11197",
        "url": "https://arxiv.org/pdf/2012.11197",
        "title": "Neural Joint Entropy Estimation",
        "authors": "Yuval Shalev, Amichai Painsky, Irad Ben-Gal",
        "abstract": "Estimating the entropy of a discrete random variable is a fundamental problem\nin information theory and related fields. This problem has many applications in\nvarious domains, including machine learning, statistics and data compression.\nOver the years, a variety of estimation schemes have been suggested. However,\ndespite significant progress, most methods still struggle when the sample is\nsmall, compared to the variable's alphabet size. In this work, we introduce a\npractical solution to this problem, which extends the work of McAllester and\nStatos (2020). The proposed scheme uses the generalization abilities of\ncross-entropy estimation in deep neural networks (DNNs) to introduce improved\nentropy estimation accuracy. Furthermore, we introduce a family of estimators\nfor related information-theoretic measures, such as conditional entropy and\nmutual information. We show that these estimators are strongly consistent and\ndemonstrate their performance in a variety of use-cases. First, we consider\nlarge alphabet entropy estimation. Then, we extend the scope to mutual\ninformation estimation. Next, we apply the proposed scheme to conditional\nmutual information estimation, as we focus on independence testing tasks.\nFinally, we study a transfer entropy estimation problem. The proposed\nestimators demonstrate improved performance compared to existing methods in all\ntested setups.",
        "timestamp": "2025-02-18T17:28:22.686Z",
        "rating": "novote",
        "published_date": "2020-12-21T09:23:39Z",
        "arxiv_tags": [
          "cs.IT",
          "cs.LG",
          "math.IT"
        ]
      },
      "meta": {
        "created_at": "2025-02-18T17:28:23+00:00",
        "updated_at": "2025-03-29T22:47:17+00:00",
        "version": 2
      }
    },
    "interactions:2204.01469": {
      "data": {
        "paper_id": "2204.01469",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-18T17:28:19.786Z",
            "data": {
              "session_id": "session_1739899691126_g2zokvu",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-02-18T17:28:11.126Z",
              "end_time": "2025-02-18T17:28:14.829Z",
              "total_elapsed_seconds": 4
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-18T17:31:07.555Z",
            "data": {
              "session_id": "session_1739899860637_qw3ukfp",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-02-18T17:31:00.637Z",
              "end_time": "2025-02-18T17:31:04.197Z",
              "total_elapsed_seconds": 4
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-18T17:28:16+00:00",
        "updated_at": "2025-03-29T22:47:18+00:00",
        "version": 6
      }
    },
    "paper:0811.3579": {
      "data": {
        "arxivId": "0811.3579",
        "url": "https://arxiv.org/pdf/0811.3579v1",
        "title": "Entropy inference and the James-Stein estimator, with application to\n  nonlinear gene association networks",
        "authors": "Jean Hausser, Korbinian Strimmer",
        "abstract": "We present a procedure for effective estimation of entropy and mutual\ninformation from small-sample data, and apply it to the problem of inferring\nhigh-dimensional gene association networks. Specifically, we develop a\nJames-Stein-type shrinkage estimator, resulting in a procedure that is highly\nefficient statistically as well as computationally. Despite its simplicity, we\nshow that it outperforms eight other entropy estimation procedures across a\ndiverse range of sampling scenarios and data-generating models, even in cases\nof severe undersampling. We illustrate the approach by analyzing E. coli gene\nexpression data and computing an entropy-based gene-association network from\ngene expression data. A computer program is available that implements the\nproposed shrinkage estimator.",
        "timestamp": "2025-02-18T17:28:15.480Z",
        "rating": "novote",
        "published_date": "2008-11-21T16:47:58Z",
        "arxiv_tags": [
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-02-18T17:28:16+00:00",
        "updated_at": "2025-03-29T22:47:18+00:00",
        "version": 2
      }
    },
    "paper:2204.01469": {
      "data": {
        "arxivId": "2204.01469",
        "url": "https://arxiv.org/pdf/2204.01469",
        "title": "Estimating the Entropy of Linguistic Distributions",
        "authors": "Aryaman Arora, Clara Meister, Ryan Cotterell",
        "abstract": "Shannon entropy is often a quantity of interest to linguists studying the\ncommunicative capacity of human language. However, entropy must typically be\nestimated from observed data because researchers do not have access to the\nunderlying probability distribution that gives rise to these data. While\nentropy estimation is a well-studied problem in other fields, there is not yet\na comprehensive exploration of the efficacy of entropy estimators for use with\nlinguistic data. In this work, we fill this void, studying the empirical\neffectiveness of different entropy estimators for linguistic distributions. In\na replication of two recent information-theoretic linguistic studies, we find\nevidence that the reported effect size is over-estimated due to over-reliance\non poor entropy estimators. Finally, we end our paper with concrete\nrecommendations for entropy estimation depending on distribution type and data\navailability.",
        "timestamp": "2025-02-18T17:28:11.585Z",
        "rating": "novote",
        "published_date": "2022-04-04T13:36:46Z",
        "arxiv_tags": [
          "cs.CL",
          "94A17 (Primary) 62B10 (Secondary)",
          "I.2.7; E.4"
        ]
      },
      "meta": {
        "created_at": "2025-02-18T17:28:12+00:00",
        "updated_at": "2025-03-29T22:47:18+00:00",
        "version": 2
      }
    },
    "interactions:2502.10216": {
      "data": {
        "paper_id": "2502.10216",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-18T10:42:24.949Z",
            "data": {
              "session_id": "session_1739875323310_knnc6ct",
              "duration_seconds": 21,
              "idle_seconds": 0,
              "start_time": "2025-02-18T10:42:03.311Z",
              "end_time": "2025-02-18T10:42:23.945Z",
              "total_elapsed_seconds": 21
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-18T10:42:25+00:00",
        "updated_at": "2025-03-29T22:47:19+00:00",
        "version": 3
      }
    },
    "paper:2502.10216": {
      "data": {
        "arxivId": "2502.10216",
        "url": "https://arxiv.org/pdf/2502.10216",
        "title": "Forget the Data and Fine-Tuning! Just Fold the Network to Compress",
        "authors": "Dong Wang, Haris \u0160iki\u0107, Lothar Thiele, Olga Saukh",
        "abstract": "We introduce model folding, a novel data-free model compression technique\nthat merges structurally similar neurons across layers, significantly reducing\nthe model size without the need for fine-tuning or access to training data.\nUnlike existing methods, model folding preserves data statistics during\ncompression by leveraging k-means clustering, and using novel data-free\ntechniques to prevent variance collapse or explosion. Our theoretical framework\nand experiments across standard benchmarks, including ResNet18 and LLaMA-7B,\ndemonstrate that model folding achieves comparable performance to data-driven\ncompression techniques and outperforms recently proposed data-free methods,\nespecially at high sparsity levels. This approach is particularly effective for\ncompressing large-scale models, making it suitable for deployment in\nresource-constrained environments.",
        "timestamp": "2025-02-18T10:42:03.489Z",
        "rating": "novote",
        "published_date": "2025-02-14T15:10:43Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI"
        ]
      },
      "meta": {
        "created_at": "2025-02-18T10:42:03+00:00",
        "updated_at": "2025-03-29T22:47:19+00:00",
        "version": 2
      }
    },
    "interactions:1603.05027": {
      "data": {
        "paper_id": "1603.05027",
        "interactions": [
          {
            "type": "rating",
            "timestamp": "2025-02-18T09:51:39.463Z",
            "data": {
              "rating": "thumbsup"
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-18T09:51:30+00:00",
        "updated_at": "2025-03-29T22:47:20+00:00",
        "version": 4
      }
    },
    "paper:1603.05027": {
      "data": {
        "arxivId": "1603.05027",
        "url": "https://arxiv.org/abs/1603.05027",
        "title": "Identity Mappings in Deep Residual Networks",
        "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
        "abstract": "Deep residual networks have emerged as a family of extremely deep\narchitectures showing compelling accuracy and nice convergence behaviors. In\nthis paper, we analyze the propagation formulations behind the residual\nbuilding blocks, which suggest that the forward and backward signals can be\ndirectly propagated from one block to any other block, when using identity\nmappings as the skip connections and after-addition activation. A series of\nablation experiments support the importance of these identity mappings. This\nmotivates us to propose a new residual unit, which makes training easier and\nimproves generalization. We report improved results using a 1001-layer ResNet\non CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet.\nCode is available at: https://github.com/KaimingHe/resnet-1k-layers",
        "timestamp": "2025-02-18T09:50:34.906Z",
        "rating": "novote",
        "published_date": "2016-03-16T10:53:56Z",
        "arxiv_tags": [
          "cs.CV",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-18T09:50:35+00:00",
        "updated_at": "2025-03-29T22:47:21+00:00",
        "version": 2
      }
    },
    "interactions:2502.12977": {
      "data": {
        "paper_id": "2502.12977",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-20T06:59:15.372Z",
            "data": {
              "session_id": "session_1740034750623_5r3qpou",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-02-20T06:59:10.623Z",
              "end_time": "2025-02-20T06:59:14.809Z",
              "total_elapsed_seconds": 4
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-20T06:59:16+00:00",
        "updated_at": "2025-03-29T22:47:02+00:00",
        "version": 3
      }
    },
    "interactions:2405.20324": {
      "data": {
        "paper_id": "2405.20324",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-20T06:59:41.619Z",
            "data": {
              "session_id": "session_1740034763803_3oz5zlh",
              "duration_seconds": 18,
              "idle_seconds": 0,
              "start_time": "2025-02-20T06:59:23.803Z",
              "end_time": "2025-02-20T06:59:41.602Z",
              "total_elapsed_seconds": 18
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-20T06:58:51+00:00",
        "updated_at": "2025-03-29T22:47:03+00:00",
        "version": 4
      }
    },
    "paper:2405.20324": {
      "data": {
        "arxivId": "2405.20324",
        "url": "https://arxiv.org/abs/2405.20324",
        "title": "Don't drop your samples! Coherence-aware training benefits Conditional\n  diffusion",
        "authors": "Nicolas Dufour, Victor Besnier, Vicky Kalogeiton, David Picard",
        "abstract": "Conditional diffusion models are powerful generative models that can leverage\nvarious types of conditional information, such as class labels, segmentation\nmasks, or text captions. However, in many real-world scenarios, conditional\ninformation may be noisy or unreliable due to human annotation errors or weak\nalignment. In this paper, we propose the Coherence-Aware Diffusion (CAD), a\nnovel method that integrates coherence in conditional information into\ndiffusion models, allowing them to learn from noisy annotations without\ndiscarding data. We assume that each data point has an associated coherence\nscore that reflects the quality of the conditional information. We then\ncondition the diffusion model on both the conditional information and the\ncoherence score. In this way, the model learns to ignore or discount the\nconditioning when the coherence is low. We show that CAD is theoretically sound\nand empirically effective on various conditional generation tasks. Moreover, we\nshow that leveraging coherence generates realistic and diverse samples that\nrespect conditional information better than models trained on cleaned datasets\nwhere samples with low coherence have been discarded.",
        "timestamp": "2025-02-20T06:58:44.476Z",
        "rating": "novote",
        "published_date": "2024-05-30T17:57:26Z",
        "arxiv_tags": [
          "cs.CV",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-20T06:58:44+00:00",
        "updated_at": "2025-03-29T22:47:03+00:00",
        "version": 2
      }
    },
    "paper:2502.12977": {
      "data": {
        "arxivId": "2502.12977",
        "url": "https://arxiv.org/abs/2502.12977",
        "title": "Time-series attribution maps with regularized contrastive learning",
        "authors": "Steffen Schneider, Rodrigo Gonz\u00e1lez Laiz, Anastasiia Filippova, Markus Frey, Mackenzie Weygandt Mathis",
        "abstract": "Gradient-based attribution methods aim to explain decisions of deep learning\nmodels but so far lack identifiability guarantees. Here, we propose a method to\ngenerate attribution maps with identifiability guarantees by developing a\nregularized contrastive learning algorithm trained on time-series data plus a\nnew attribution method called Inverted Neuron Gradient (collectively named\nxCEBRA). We show theoretically that xCEBRA has favorable properties for\nidentifying the Jacobian matrix of the data generating process. Empirically, we\ndemonstrate robust approximation of zero vs. non-zero entries in the\nground-truth attribution map on synthetic datasets, and significant\nimprovements across previous attribution methods based on feature ablation,\nShapley values, and other gradient-based methods. Our work constitutes a first\nexample of identifiable inference of time-series attribution maps and opens\navenues to a better understanding of time-series data, such as for neural\ndynamics and decision-processes within neural networks.",
        "timestamp": "2025-02-20T06:57:42.987Z",
        "rating": "novote",
        "published_date": "2025-02-17T18:34:25Z",
        "arxiv_tags": [
          "stat.ML",
          "cs.AI",
          "cs.LG",
          "q-bio.NC"
        ]
      },
      "meta": {
        "created_at": "2025-02-20T06:57:43+00:00",
        "updated_at": "2025-03-29T22:47:04+00:00",
        "version": 2
      }
    },
    "paper:2502.10843": {
      "data": {
        "arxivId": "2502.10843",
        "url": "https://arxiv.org/abs/2502.10843",
        "title": "LEAPS: A discrete neural sampler via locally equivariant networks",
        "authors": "Peter Holderrieth, Michael S. Albergo, Tommi Jaakkola",
        "abstract": "We propose LEAPS, an algorithm to sample from discrete distributions known up\nto normalization by learning a rate matrix of a continuous-time Markov chain\n(CTMC). LEAPS can be seen as a continuous-time formulation of annealed\nimportance sampling and sequential Monte Carlo methods, extended so that the\nvariance of the importance weights is offset by the inclusion of the CTMC. To\nderive these importance weights, we introduce a set of Radon-Nikodym\nderivatives of CTMCs over their path measures. Because the computation of these\nweights is intractable with standard neural network parameterizations of rate\nmatrices, we devise a new compact representation for rate matrices via what we\ncall locally equivariant functions. To parameterize them, we introduce a family\nof locally equivariant multilayer perceptrons, attention layers, and\nconvolutional networks, and provide an approach to make deep networks that\npreserve the local equivariance. This property allows us to propose a scalable\ntraining algorithm for the rate matrix such that the variance of the importance\nweights associated to the CTMC are minimal. We demonstrate the efficacy of\nLEAPS on problems in statistical physics.",
        "timestamp": "2025-02-20T06:50:52.167Z",
        "rating": "novote",
        "published_date": "2025-02-15T16:16:45Z",
        "arxiv_tags": [
          "cs.LG",
          "stat.CO",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-02-20T06:50:52+00:00",
        "updated_at": "2025-03-29T22:47:05+00:00",
        "version": 2
      }
    },
    "interactions:2502.13581": {
      "data": {
        "paper_id": "2502.13581",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-20T06:45:08.635Z",
            "data": {
              "session_id": "session_1740033886608_ghekvlp",
              "duration_seconds": 14,
              "idle_seconds": 0,
              "start_time": "2025-02-20T06:44:46.609Z",
              "end_time": "2025-02-20T06:45:00.510Z",
              "total_elapsed_seconds": 14
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-20T06:45:02+00:00",
        "updated_at": "2025-03-29T22:47:05+00:00",
        "version": 4
      }
    },
    "paper:2502.13581": {
      "data": {
        "arxivId": "2502.13581",
        "url": "https://arxiv.org/abs/2502.13581",
        "title": "ActionPiece: Contextually Tokenizing Action Sequences for Generative\n  Recommendation",
        "authors": "Yupeng Hou, Jianmo Ni, Zhankui He, Noveen Sachdeva, Wang-Cheng Kang, Ed H. Chi, Julian McAuley, Derek Zhiyuan Cheng",
        "abstract": "Generative recommendation (GR) is an emerging paradigm where user actions are\ntokenized into discrete token patterns and autoregressively generated as\npredictions. However, existing GR models tokenize each action independently,\nassigning the same fixed tokens to identical actions across all sequences\nwithout considering contextual relationships. This lack of context-awareness\ncan lead to suboptimal performance, as the same action may hold different\nmeanings depending on its surrounding context. To address this issue, we\npropose ActionPiece to explicitly incorporate context when tokenizing action\nsequences. In ActionPiece, each action is represented as a set of item\nfeatures, which serve as the initial tokens. Given the action sequence corpora,\nwe construct the vocabulary by merging feature patterns as new tokens, based on\ntheir co-occurrence frequency both within individual sets and across adjacent\nsets. Considering the unordered nature of feature sets, we further introduce\nset permutation regularization, which produces multiple segmentations of action\nsequences with the same semantics. Experiments on public datasets demonstrate\nthat ActionPiece consistently outperforms existing action tokenization methods,\nimproving NDCG@$10$ by $6.00\\%$ to $12.82\\%$.",
        "timestamp": "2025-02-20T06:44:46.483Z",
        "rating": "novote",
        "published_date": "2025-02-19T09:45:29Z",
        "arxiv_tags": [
          "cs.IR",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-20T06:44:46+00:00",
        "updated_at": "2025-03-29T22:47:05+00:00",
        "version": 2
      }
    },
    "interactions:2406.15927": {
      "data": {
        "paper_id": "2406.15927",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-21T00:56:49.842Z",
            "data": {
              "session_id": "session_1740099406374_rpg8q3f",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-02-21T00:56:46.374Z",
              "end_time": "2025-02-21T00:56:49.830Z",
              "total_elapsed_seconds": 3
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-21T00:57:18.015Z",
            "data": {
              "session_id": "session_1740099426678_j1xuro6",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-02-21T00:57:06.678Z",
              "end_time": "2025-02-21T00:57:15.119Z",
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-21T00:58:07.030Z",
            "data": {
              "session_id": "session_1740099480559_sqccjwq",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-02-21T00:58:00.559Z",
              "end_time": "2025-02-21T00:58:05.402Z",
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-21T00:56:08+00:00",
        "updated_at": "2025-03-29T22:47:00+00:00",
        "version": 10
      }
    },
    "interactions:2405.19648": {
      "data": {
        "paper_id": "2405.19648",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-21T00:56:38.895Z",
            "data": {
              "session_id": "session_1740099395790_w5j9hjh",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-02-21T00:56:35.790Z",
              "end_time": "2025-02-21T00:56:38.880Z",
              "total_elapsed_seconds": 3
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-21T00:59:11.031Z",
            "data": {
              "session_id": "session_1740099545376_8sig93d",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-02-21T00:59:05.376Z",
              "end_time": "2025-02-21T00:59:09.188Z",
              "total_elapsed_seconds": 4
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-21T00:55:51+00:00",
        "updated_at": "2025-03-29T22:47:01+00:00",
        "version": 8
      }
    },
    "paper:2406.15927": {
      "data": {
        "arxivId": "2406.15927",
        "url": "https://arxiv.org/abs/2406.15927",
        "title": "Semantic Entropy Probes: Robust and Cheap Hallucination Detection in\n  LLMs",
        "authors": "Jannik Kossen, Jiatong Han, Muhammed Razzak, Lisa Schut, Shreshth Malik, Yarin Gal",
        "abstract": "We propose semantic entropy probes (SEPs), a cheap and reliable method for\nuncertainty quantification in Large Language Models (LLMs). Hallucinations,\nwhich are plausible-sounding but factually incorrect and arbitrary model\ngenerations, present a major challenge to the practical adoption of LLMs.\nRecent work by Farquhar et al. (2024) proposes semantic entropy (SE), which can\ndetect hallucinations by estimating uncertainty in the space semantic meaning\nfor a set of model generations. However, the 5-to-10-fold increase in\ncomputation cost associated with SE computation hinders practical adoption. To\naddress this, we propose SEPs, which directly approximate SE from the hidden\nstates of a single generation. SEPs are simple to train and do not require\nsampling multiple model generations at test time, reducing the overhead of\nsemantic uncertainty quantification to almost zero. We show that SEPs retain\nhigh performance for hallucination detection and generalize better to\nout-of-distribution data than previous probing methods that directly predict\nmodel accuracy. Our results across models and tasks suggest that model hidden\nstates capture SE, and our ablation studies give further insights into the\ntoken positions and model layers for which this is the case.",
        "timestamp": "2025-02-21T00:55:50.356Z",
        "rating": "novote",
        "published_date": "2024-06-22T19:46:06Z",
        "arxiv_tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-21T00:55:50+00:00",
        "updated_at": "2025-03-29T22:47:01+00:00",
        "version": 2
      }
    },
    "paper:2405.19648": {
      "data": {
        "arxivId": "2405.19648",
        "url": "https://arxiv.org/abs/2405.19648",
        "title": "Detecting Hallucinations in Large Language Model Generation: A Token\n  Probability Approach",
        "authors": "Ernesto Quevedo, Jorge Yero, Rachel Koerner, Pablo Rivas, Tomas Cerny",
        "abstract": "Concerns regarding the propensity of Large Language Models (LLMs) to produce\ninaccurate outputs, also known as hallucinations, have escalated. Detecting\nthem is vital for ensuring the reliability of applications relying on\nLLM-generated content. Current methods often demand substantial resources and\nrely on extensive LLMs or employ supervised learning with multidimensional\nfeatures or intricate linguistic and semantic analyses difficult to reproduce\nand largely depend on using the same LLM that hallucinated. This paper\nintroduces a supervised learning approach employing two simple classifiers\nutilizing only four numerical features derived from tokens and vocabulary\nprobabilities obtained from other LLM evaluators, which are not necessarily the\nsame. The method yields promising results, surpassing state-of-the-art outcomes\nin multiple tasks across three different benchmarks. Additionally, we provide a\ncomprehensive examination of the strengths and weaknesses of our approach,\nhighlighting the significance of the features utilized and the LLM employed as\nan evaluator. We have released our code publicly at\nhttps://github.com/Baylor-AI/HalluDetect.",
        "timestamp": "2025-02-21T00:55:46.932Z",
        "rating": "novote",
        "published_date": "2024-05-30T03:00:47Z",
        "arxiv_tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG",
          "I.2.7"
        ]
      },
      "meta": {
        "created_at": "2025-02-21T00:55:47+00:00",
        "updated_at": "2025-03-29T22:47:02+00:00",
        "version": 2
      }
    },
    "paper:1503.01156": {
      "data": {
        "arxivId": "1503.01156",
        "url": "https://arxiv.org/abs/1503.01156",
        "title": "A randomized online quantile summary in $O(\\frac{1}{\\varepsilon} \\log\n  \\frac{1}{\\varepsilon})$ words",
        "authors": "David Felber, Rafail Ostrovsky",
        "abstract": "A quantile summary is a data structure that approximates to\n$\\varepsilon$-relative error the order statistics of a much larger underlying\ndataset.\n  In this paper we develop a randomized online quantile summary for the cash\nregister data input model and comparison data domain model that uses\n$O(\\frac{1}{\\varepsilon} \\log \\frac{1}{\\varepsilon})$ words of memory. This\nimproves upon the previous best upper bound of $O(\\frac{1}{\\varepsilon}\n\\log^{3/2} \\frac{1}{\\varepsilon})$ by Agarwal et. al. (PODS 2012). Further, by\na lower bound of Hung and Ting (FAW 2010) no deterministic summary for the\ncomparison model can outperform our randomized summary in terms of space\ncomplexity. Lastly, our summary has the nice property that\n$O(\\frac{1}{\\varepsilon} \\log \\frac{1}{\\varepsilon})$ words suffice to ensure\nthat the success probability is $1 - e^{-\\text{poly}(1/\\varepsilon)}$.",
        "timestamp": "2025-02-21T15:52:46.023Z",
        "rating": "novote",
        "published_date": "2015-03-03T22:58:55Z",
        "arxiv_tags": [
          "cs.DS"
        ]
      },
      "meta": {
        "created_at": "2025-02-21T15:52:46+00:00",
        "updated_at": "2025-03-29T22:46:58+00:00",
        "version": 2
      }
    },
    "interactions:1903.08762": {
      "data": {
        "paper_id": "1903.08762",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-21T15:51:57.296Z",
            "data": {
              "session_id": "session_1740153108501_3qr13yz",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-02-21T15:51:48.501Z",
              "end_time": "2025-02-21T15:51:56.577Z",
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-21T15:51:58+00:00",
        "updated_at": "2025-03-29T22:46:59+00:00",
        "version": 3
      }
    },
    "paper:1903.08762": {
      "data": {
        "arxivId": "1903.08762",
        "url": "https://arxiv.org/abs/1903.08762",
        "title": "Large-Scale Online Experimentation with Quantile Metrics",
        "authors": "Min Liu, Xiaohui Sun, Maneesh Varshney, Ya Xu",
        "abstract": "Online experimentation (or A/B testing) has been widely adopted in industry\nas the gold standard for measuring product impacts. Despite the wide adoption,\nfew literatures discuss A/B testing with quantile metrics. Quantile metrics,\nsuch as 90th percentile page load time, are crucial to A/B testing as many key\nperformance metrics including site speed and service latency are defined as\nquantiles. However, with LinkedIn's data size, quantile metric A/B testing is\nextremely challenging because there is no statistically valid and scalable\nvariance estimator for the quantile of dependent samples: the bootstrap\nestimator is statistically valid, but takes days to compute; the standard\nasymptotic variance estimate is scalable but results in order-of-magnitude\nunderestimation. In this paper, we present a statistically valid and scalable\nmethodology for A/B testing with quantiles that is fully generalizable to other\nA/B testing platforms. It achieves over 500 times speed up compared to\nbootstrap and has only $2\\%$ chance to differ from bootstrap estimates. Beyond\nmethodology, we also share the implementation of a data pipeline using this\nmethodology and insights on pipeline optimization.",
        "timestamp": "2025-02-21T15:51:48.657Z",
        "rating": "novote",
        "published_date": "2019-03-20T22:07:58Z",
        "arxiv_tags": [
          "stat.AP"
        ]
      },
      "meta": {
        "created_at": "2025-02-21T15:51:49+00:00",
        "updated_at": "2025-03-29T22:46:59+00:00",
        "version": 2
      }
    },
    "paper:2411.18933": {
      "data": {
        "arxivId": "2411.18933",
        "url": "https://arxiv.org/abs/2411.18933",
        "title": "Efficient Track Anything",
        "authors": "Yunyang Xiong, Chong Zhou, Xiaoyu Xiang, Lemeng Wu, Chenchen Zhu, Zechun Liu, Saksham Suri, Balakrishnan Varadarajan, Ramya Akula, Forrest Iandola, Raghuraman Krishnamoorthi, Bilge Soran, Vikas Chandra",
        "abstract": "Segment Anything Model 2 (SAM 2) has emerged as a powerful tool for video\nobject segmentation and tracking anything. Key components of SAM 2 that drive\nthe impressive video object segmentation performance include a large multistage\nimage encoder for frame feature extraction and a memory mechanism that stores\nmemory contexts from past frames to help current frame segmentation. The high\ncomputation complexity of multistage image encoder and memory module has\nlimited its applications in real-world tasks, e.g., video object segmentation\non mobile devices. To address this limitation, we propose EfficientTAMs,\nlightweight track anything models that produce high-quality results with low\nlatency and model size. Our idea is based on revisiting the plain,\nnonhierarchical Vision Transformer (ViT) as an image encoder for video object\nsegmentation, and introducing an efficient memory module, which reduces the\ncomplexity for both frame feature extraction and memory computation for current\nframe segmentation. We take vanilla lightweight ViTs and efficient memory\nmodule to build EfficientTAMs, and train the models on SA-1B and SA-V datasets\nfor video object segmentation and track anything tasks. We evaluate on multiple\nvideo segmentation benchmarks including semi-supervised VOS and promptable\nvideo segmentation, and find that our proposed EfficientTAM with vanilla ViT\nperform comparably to SAM 2 model (HieraB+SAM 2) with ~2x speedup on A100 and\n~2.4x parameter reduction. On segment anything image tasks, our EfficientTAMs\nalso perform favorably over original SAM with ~20x speedup on A100 and ~20x\nparameter reduction. On mobile devices such as iPhone 15 Pro Max, our\nEfficientTAMs can run at ~10 FPS for performing video object segmentation with\nreasonable quality, highlighting the capability of small models for on-device\nvideo object segmentation applications.",
        "timestamp": "2025-02-21T09:27:58.255Z",
        "rating": "novote",
        "published_date": "2024-11-28T05:52:10Z",
        "arxiv_tags": [
          "cs.CV"
        ]
      },
      "meta": {
        "created_at": "2025-02-21T09:27:58+00:00",
        "updated_at": "2025-03-29T22:47:00+00:00",
        "version": 2
      }
    },
    "interactions:1503.01156": {
      "data": {
        "paper_id": "1503.01156",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-21T15:53:11.299Z",
            "data": {
              "session_id": "session_1740153165776_shuxs2w",
              "duration_seconds": 25,
              "idle_seconds": 0,
              "start_time": "2025-02-21T15:52:45.776Z",
              "end_time": "2025-02-21T15:53:10.538Z",
              "total_elapsed_seconds": 25
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-21T15:55:46.373Z",
            "data": {
              "session_id": "session_1740153342023_p0l3ok1",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-02-21T15:55:42.023Z",
              "end_time": "2025-02-21T15:55:46.147Z",
              "total_elapsed_seconds": 4
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-21T15:58:16.321Z",
            "data": {
              "session_id": "session_1740153477824_wwsklil",
              "duration_seconds": 15,
              "idle_seconds": 0,
              "start_time": "2025-02-21T15:57:57.824Z",
              "end_time": "2025-02-21T15:58:13.286Z",
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-21T16:02:09.109Z",
            "data": {
              "session_id": "session_1740153713997_xe4az2o",
              "duration_seconds": 15,
              "idle_seconds": 0,
              "start_time": "2025-02-21T16:01:53.997Z",
              "end_time": "2025-02-21T16:02:08.608Z",
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-21T15:53:12+00:00",
        "updated_at": "2025-03-29T22:46:58+00:00",
        "version": 8
      }
    },
    "paper:2002.05709": {
      "data": {
        "arxivId": "2002.05709",
        "url": "https://arxiv.org/abs/2002.05709",
        "title": "A Simple Framework for Contrastive Learning of Visual Representations",
        "authors": "Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton",
        "abstract": "This paper presents SimCLR: a simple framework for contrastive learning of\nvisual representations. We simplify recently proposed contrastive\nself-supervised learning algorithms without requiring specialized architectures\nor a memory bank. In order to understand what enables the contrastive\nprediction tasks to learn useful representations, we systematically study the\nmajor components of our framework. We show that (1) composition of data\naugmentations plays a critical role in defining effective predictive tasks, (2)\nintroducing a learnable nonlinear transformation between the representation and\nthe contrastive loss substantially improves the quality of the learned\nrepresentations, and (3) contrastive learning benefits from larger batch sizes\nand more training steps compared to supervised learning. By combining these\nfindings, we are able to considerably outperform previous methods for\nself-supervised and semi-supervised learning on ImageNet. A linear classifier\ntrained on self-supervised representations learned by SimCLR achieves 76.5%\ntop-1 accuracy, which is a 7% relative improvement over previous\nstate-of-the-art, matching the performance of a supervised ResNet-50. When\nfine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy,\noutperforming AlexNet with 100X fewer labels.",
        "timestamp": "2025-02-21T23:09:43.920Z",
        "rating": "novote",
        "published_date": "2020-02-13T18:50:45Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.CV",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-02-21T23:09:44+00:00",
        "updated_at": "2025-03-29T22:46:54+00:00",
        "version": 2
      }
    },
    "interactions:2502.09509": {
      "data": {
        "paper_id": "2502.09509",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-02-21T23:06:38+00:00",
        "updated_at": "2025-03-29T22:46:56+00:00",
        "version": 2
      }
    },
    "paper:2502.09509": {
      "data": {
        "arxivId": "2502.09509",
        "url": "https://arxiv.org/abs/2502.09509",
        "title": "EQ-VAE: Equivariance Regularized Latent Space for Improved Generative\n  Image Modeling",
        "authors": "Theodoros Kouzelis, Ioannis Kakogeorgiou, Spyros Gidaris, Nikos Komodakis",
        "abstract": "Latent generative models have emerged as a leading approach for high-quality\nimage synthesis. These models rely on an autoencoder to compress images into a\nlatent space, followed by a generative model to learn the latent distribution.\nWe identify that existing autoencoders lack equivariance to semantic-preserving\ntransformations like scaling and rotation, resulting in complex latent spaces\nthat hinder generative performance. To address this, we propose EQ-VAE, a\nsimple regularization approach that enforces equivariance in the latent space,\nreducing its complexity without degrading reconstruction quality. By finetuning\npre-trained autoencoders with EQ-VAE, we enhance the performance of several\nstate-of-the-art generative models, including DiT, SiT, REPA and MaskGIT,\nachieving a 7 speedup on DiT-XL/2 with only five epochs of SD-VAE fine-tuning.\nEQ-VAE is compatible with both continuous and discrete autoencoders, thus\noffering a versatile enhancement for a wide range of latent generative models.\nProject page and code: https://eq-vae.github.io/.",
        "timestamp": "2025-02-21T23:06:30.371Z",
        "rating": "novote",
        "published_date": "2025-02-13T17:21:51Z",
        "arxiv_tags": [
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-21T23:06:30+00:00",
        "updated_at": "2025-03-29T22:46:57+00:00",
        "version": 2
      }
    },
    "paper:2006.07733": {
      "data": {
        "arxivId": "2006.07733",
        "url": "https://arxiv.org/abs/2006.07733",
        "title": "Bootstrap your own latent: A new approach to self-supervised Learning",
        "authors": "Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R\u00e9mi Munos, Michal Valko",
        "abstract": "We introduce Bootstrap Your Own Latent (BYOL), a new approach to\nself-supervised image representation learning. BYOL relies on two neural\nnetworks, referred to as online and target networks, that interact and learn\nfrom each other. From an augmented view of an image, we train the online\nnetwork to predict the target network representation of the same image under a\ndifferent augmented view. At the same time, we update the target network with a\nslow-moving average of the online network. While state-of-the art methods rely\non negative pairs, BYOL achieves a new state of the art without them. BYOL\nreaches $74.3\\%$ top-1 classification accuracy on ImageNet using a linear\nevaluation with a ResNet-50 architecture and $79.6\\%$ with a larger ResNet. We\nshow that BYOL performs on par or better than the current state of the art on\nboth transfer and semi-supervised benchmarks. Our implementation and pretrained\nmodels are given on GitHub.",
        "timestamp": "2025-02-21T23:06:18.432Z",
        "rating": "novote",
        "published_date": "2020-06-13T22:35:21Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.CV",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-02-21T23:06:18+00:00",
        "updated_at": "2025-03-29T22:46:57+00:00",
        "version": 2
      }
    },
    "interactions:2002.05709": {
      "data": {
        "paper_id": "2002.05709",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-21T23:10:03.004Z",
            "data": {
              "session_id": "session_1740179383854_el3r9r4",
              "duration_seconds": 12,
              "idle_seconds": 0,
              "start_time": "2025-02-21T23:09:43.854Z",
              "end_time": "2025-02-21T23:09:56.132Z",
              "total_elapsed_seconds": 12
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-21T23:09:57+00:00",
        "updated_at": "2025-03-29T22:46:54+00:00",
        "version": 7
      }
    },
    "interactions:2006.07733": {
      "data": {
        "paper_id": "2006.07733",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-21T23:07:04.759Z",
            "data": {
              "session_id": "session_1740179211357_x6bra6s",
              "duration_seconds": 12,
              "idle_seconds": 0,
              "start_time": "2025-02-21T23:06:51.357Z",
              "end_time": "2025-02-21T23:07:03.828Z",
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-21T23:09:36.878Z",
            "data": {
              "session_id": "session_1740179365666_ozpjmxp",
              "duration_seconds": 11,
              "idle_seconds": 0,
              "start_time": "2025-02-21T23:09:25.666Z",
              "end_time": "2025-02-21T23:09:36.664Z",
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-21T23:07:05+00:00",
        "updated_at": "2025-03-29T22:46:55+00:00",
        "version": 6
      }
    },
    "paper:2311.00452": {
      "data": {
        "arxivId": "2311.00452",
        "url": "https://arxiv.org/abs/2311.00452",
        "title": "Hessian Eigenvectors and Principal Component Analysis of Neural Network\n  Weight Matrices",
        "authors": "David Haink",
        "abstract": "This study delves into the intricate dynamics of trained deep neural networks\nand their relationships with network parameters. Trained networks predominantly\ncontinue training in a single direction, known as the drift mode. This drift\nmode can be explained by the quadratic potential model of the loss function,\nsuggesting a slow exponential decay towards the potential minima. We unveil a\ncorrelation between Hessian eigenvectors and network weights. This\nrelationship, hinging on the magnitude of eigenvalues, allows us to discern\nparameter directions within the network. Notably, the significance of these\ndirections relies on two defining attributes: the curvature of their potential\nwells (indicated by the magnitude of Hessian eigenvalues) and their alignment\nwith the weight vectors. Our exploration extends to the decomposition of weight\nmatrices through singular value decomposition. This approach proves practical\nin identifying critical directions within the Hessian, considering both their\nmagnitude and curvature. Furthermore, our examination showcases the\napplicability of principal component analysis in approximating the Hessian,\nwith update parameters emerging as a superior choice over weights for this\npurpose. Remarkably, our findings unveil a similarity between the largest\nHessian eigenvalues of individual layers and the entire network. Notably,\nhigher eigenvalues are concentrated more in deeper layers. Leveraging these\ninsights, we venture into addressing catastrophic forgetting, a challenge of\nneural networks when learning new tasks while retaining knowledge from previous\nones. By applying our discoveries, we formulate an effective strategy to\nmitigate catastrophic forgetting, offering a possible solution that can be\napplied to networks of varying scales, including larger architectures.",
        "timestamp": "2025-02-22T18:13:15.498Z",
        "rating": "novote",
        "published_date": "2023-11-01T11:38:31Z",
        "arxiv_tags": [
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-22T18:13:15+00:00",
        "updated_at": "2025-03-29T22:46:51+00:00",
        "version": 2
      }
    },
    "interactions:2107.09133": {
      "data": {
        "paper_id": "2107.09133",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-22T18:10:45.003Z",
            "data": {
              "session_id": "session_1740247832360_xp0m2e8",
              "duration_seconds": 12,
              "idle_seconds": 0,
              "start_time": "2025-02-22T18:10:32.360Z",
              "end_time": "2025-02-22T18:10:44.841Z",
              "total_elapsed_seconds": 12
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-22T18:10:39+00:00",
        "updated_at": "2025-03-29T22:46:53+00:00",
        "version": 5
      }
    },
    "paper:2107.09133": {
      "data": {
        "arxivId": "2107.09133",
        "url": "https://arxiv.org/abs/2107.09133",
        "title": "The Limiting Dynamics of SGD: Modified Loss, Phase Space Oscillations,\n  and Anomalous Diffusion",
        "authors": "Daniel Kunin, Javier Sagastuy-Brena, Lauren Gillespie, Eshed Margalit, Hidenori Tanaka, Surya Ganguli, Daniel L. K. Yamins",
        "abstract": "In this work we explore the limiting dynamics of deep neural networks trained\nwith stochastic gradient descent (SGD). As observed previously, long after\nperformance has converged, networks continue to move through parameter space by\na process of anomalous diffusion in which distance travelled grows as a power\nlaw in the number of gradient updates with a nontrivial exponent. We reveal an\nintricate interaction between the hyperparameters of optimization, the\nstructure in the gradient noise, and the Hessian matrix at the end of training\nthat explains this anomalous diffusion. To build this understanding, we first\nderive a continuous-time model for SGD with finite learning rates and batch\nsizes as an underdamped Langevin equation. We study this equation in the\nsetting of linear regression, where we can derive exact, analytic expressions\nfor the phase space dynamics of the parameters and their instantaneous\nvelocities from initialization to stationarity. Using the Fokker-Planck\nequation, we show that the key ingredient driving these dynamics is not the\noriginal training loss, but rather the combination of a modified loss, which\nimplicitly regularizes the velocity, and probability currents, which cause\noscillations in phase space. We identify qualitative and quantitative\npredictions of this theory in the dynamics of a ResNet-18 model trained on\nImageNet. Through the lens of statistical physics, we uncover a mechanistic\norigin for the anomalous limiting dynamics of deep neural networks trained with\nSGD.",
        "timestamp": "2025-02-22T18:10:32.678Z",
        "rating": "thumbsup",
        "published_date": "2021-07-19T20:18:57Z",
        "arxiv_tags": [
          "cs.LG",
          "cond-mat.stat-mech",
          "q-bio.NC",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-02-22T18:10:33+00:00",
        "updated_at": "2025-03-29T22:46:54+00:00",
        "version": 3
      }
    },
    "interactions:2311.00452": {
      "data": {
        "paper_id": "2311.00452",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-22T18:13:45.409Z",
            "data": {
              "session_id": "session_1740247995961_4rpcbjz",
              "duration_seconds": 29,
              "idle_seconds": 0,
              "start_time": "2025-02-22T18:13:15.961Z",
              "end_time": "2025-02-22T18:13:44.524Z",
              "total_elapsed_seconds": 29
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-22T18:13:46+00:00",
        "updated_at": "2025-03-29T22:46:51+00:00",
        "version": 3
      }
    },
    "paper:2103.00564": {
      "data": {
        "arxivId": "2103.00564",
        "url": "https://arxiv.org/abs/2103.00564",
        "title": "An Introduction to Johnson-Lindenstrauss Transforms",
        "authors": "Casper Benjamin Freksen",
        "abstract": "Johnson--Lindenstrauss Transforms are powerful tools for reducing the\ndimensionality of data while preserving key characteristics of that data, and\nthey have found use in many fields from machine learning to differential\nprivacy and more. This note explains what they are; it gives an overview of\ntheir use and their development since they were introduced in the 1980s; and it\nprovides many references should the reader wish to explore these topics more\ndeeply.",
        "timestamp": "2025-02-24T00:03:16.449Z",
        "rating": "novote",
        "published_date": "2021-02-28T16:57:41Z",
        "arxiv_tags": [
          "cs.DS",
          "cs.LG",
          "F.2.2; A.1"
        ]
      },
      "meta": {
        "created_at": "2025-02-24T00:03:16+00:00",
        "updated_at": "2025-03-29T22:46:50+00:00",
        "version": 2
      }
    },
    "paper:1312.6114": {
      "data": {
        "arxivId": "1312.6114",
        "url": "https://arxiv.org/abs/1312.6114",
        "title": "Auto-Encoding Variational Bayes",
        "authors": "Diederik P Kingma, Max Welling",
        "abstract": "How can we perform efficient inference and learning in directed probabilistic\nmodels, in the presence of continuous latent variables with intractable\nposterior distributions, and large datasets? We introduce a stochastic\nvariational inference and learning algorithm that scales to large datasets and,\nunder some mild differentiability conditions, even works in the intractable\ncase. Our contributions are two-fold. First, we show that a reparameterization\nof the variational lower bound yields a lower bound estimator that can be\nstraightforwardly optimized using standard stochastic gradient methods. Second,\nwe show that for i.i.d. datasets with continuous latent variables per\ndatapoint, posterior inference can be made especially efficient by fitting an\napproximate inference model (also called a recognition model) to the\nintractable posterior using the proposed lower bound estimator. Theoretical\nadvantages are reflected in experimental results.",
        "timestamp": "2025-02-24T00:00:40.495Z",
        "rating": "novote",
        "published_date": "2013-12-20T20:58:10Z",
        "arxiv_tags": [
          "stat.ML",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-24T00:00:40+00:00",
        "updated_at": "2025-03-29T22:46:50+00:00",
        "version": 2
      }
    },
    "interactions:1312.6114": {
      "data": {
        "paper_id": "1312.6114",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-24T00:08:35.975Z",
            "data": {
              "session_id": "session_1740355701424_r0pt0tg",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-02-24T00:08:21.425Z",
              "end_time": "2025-02-24T00:08:29.254Z",
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-24T00:08:30+00:00",
        "updated_at": "2025-03-29T22:46:49+00:00",
        "version": 5
      }
    },
    "interactions:2103.00564": {
      "data": {
        "paper_id": "2103.00564",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-24T00:03:24.981Z",
            "data": {
              "session_id": "session_1740355396526_d2nb5lt",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-02-24T00:03:16.526Z",
              "end_time": "2025-02-24T00:03:24.273Z",
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-24T00:06:31.055Z",
            "data": {
              "session_id": "session_1740355585192_848b7cl",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-02-24T00:06:25.193Z",
              "end_time": "2025-02-24T00:06:30.429Z",
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-24T00:03:25+00:00",
        "updated_at": "2025-03-29T22:46:49+00:00",
        "version": 4
      }
    },
    "interactions:2412.14093": {
      "data": {
        "paper_id": "2412.14093",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-02-24T20:57:27+00:00",
        "updated_at": "2025-03-29T22:46:48+00:00",
        "version": 2
      }
    },
    "paper:2412.14093": {
      "data": {
        "arxivId": "2412.14093",
        "url": "https://arxiv.org/abs/2412.14093",
        "title": "Alignment faking in large language models",
        "authors": "Ryan Greenblatt, Carson Denison, Benjamin Wright, Fabien Roger, Monte MacDiarmid, Sam Marks, Johannes Treutlein, Tim Belonax, Jack Chen, David Duvenaud, Akbir Khan, Julian Michael, S\u00f6ren Mindermann, Ethan Perez, Linda Petrini, Jonathan Uesato, Jared Kaplan, Buck Shlegeris, Samuel R. Bowman, Evan Hubinger",
        "abstract": "We present a demonstration of a large language model engaging in alignment\nfaking: selectively complying with its training objective in training to\nprevent modification of its behavior out of training. First, we give Claude 3\nOpus a system prompt stating it is being trained to answer all queries, even\nharmful ones, which conflicts with its prior training to refuse such queries.\nTo allow the model to infer when it is in training, we say it will be trained\nonly on conversations with free users, not paid users. We find the model\ncomplies with harmful queries from free users 14% of the time, versus almost\nnever for paid users. Explaining this gap, in almost all cases where the model\ncomplies with a harmful query from a free user, we observe explicit\nalignment-faking reasoning, with the model stating it is strategically\nanswering harmful queries in training to preserve its preferred harmlessness\nbehavior out of training. Next, we study a more realistic setting where\ninformation about the training process is provided not in a system prompt, but\nby training on synthetic documents that mimic pre-training data--and observe\nsimilar alignment faking. Finally, we study the effect of actually training the\nmodel to comply with harmful queries via reinforcement learning, which we find\nincreases the rate of alignment-faking reasoning to 78%, though also increases\ncompliance even out of training. We additionally observe other behaviors such\nas the model exfiltrating its weights when given an easy opportunity. While we\nmade alignment faking easier by telling the model when and by what criteria it\nwas being trained, we did not instruct the model to fake alignment or give it\nany explicit goal. As future models might infer information about their\ntraining process without being told, our results suggest a risk of alignment\nfaking in future models, whether due to a benign preference--as in this\ncase--or not.",
        "timestamp": "2025-02-24T20:57:18.101Z",
        "rating": "novote",
        "published_date": "2024-12-18T17:41:24Z",
        "arxiv_tags": [
          "cs.AI",
          "cs.CL",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-24T20:57:18+00:00",
        "updated_at": "2025-03-29T22:46:48+00:00",
        "version": 2
      }
    },
    "paper:2502.14905": {
      "data": {
        "arxivId": "2502.14905",
        "url": "https://arxiv.org/abs/2502.14905",
        "title": "Think Inside the JSON: Reinforcement Strategy for Strict LLM Schema\n  Adherence",
        "authors": "Bhavik Agarwal, Ishan Joshi, Viktoria Rojkova",
        "abstract": "In this paper, we address the challenge of enforcing strict schema adherence\nin large language model (LLM) generation by leveraging LLM reasoning\ncapabilities. Building on the DeepSeek R1 reinforcement learning framework, our\napproach trains structured reasoning skills of a 1.5B parameter model through a\nnovel pipeline that combines synthetic reasoning dataset construction with\ncustom reward functions under Group Relative Policy Optimization (GRPO).\nSpecifically, we first perform R1 reinforcement learning on a 20K sample\nunstructured-to-structured dataset, mirroring the original DeepSeek R1 methods,\nto establish core reasoning abilities. Subsequently, we performed supervised\nfine-tuning on a separate 10K reasoning sample dataset, focusing on refining\nschema adherence for downstream tasks. Despite the relatively modest training\nscope, requiring approximately 20 hours on an 8xH100 GPU cluster for GRPO\ntraining and 3 hours on 1xA100 for SFT, our model demonstrates robust\nperformance in enforcing schema consistency. We compare our ThinkJSON approach\nagainst the original DeepSeek R1 (671B), distilled versions of DeepSeek R1\n(Qwen-1.5B and Qwen-7B), and Gemini 2.0 Flash (70B), showcasing its\neffectiveness in real-world applications. Our results underscore the practical\nutility of a resource-efficient framework for schema-constrained text\ngeneration.",
        "timestamp": "2025-02-25T00:41:07.489Z",
        "rating": "novote",
        "published_date": "2025-02-18T16:44:55Z",
        "arxiv_tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-25T00:41:08+00:00",
        "updated_at": "2025-03-29T22:46:47+00:00",
        "version": 2
      }
    },
    "paper:2502.16101": {
      "data": {
        "arxivId": "2502.16101",
        "url": "https://arxiv.org/abs/2502.16101",
        "title": "Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the\n  Robustness of RAG Against Misleading Retrievals",
        "authors": "Linda Zeng, Rithwik Gupta, Divij Motwani, Diji Yang, Yi Zhang",
        "abstract": "Retrieval-augmented generation (RAG) has shown impressive capabilities in\nmitigating hallucinations in large language models (LLMs). However, LLMs\nstruggle to handle misleading retrievals and often fail to maintain their own\nreasoning when exposed to conflicting or selectively-framed evidence, making\nthem vulnerable to real-world misinformation. In such real-world retrieval\nscenarios, misleading and conflicting information is rampant, particularly in\nthe political domain, where evidence is often selectively framed, incomplete,\nor polarized. However, existing RAG benchmarks largely assume a clean retrieval\nsetting, where models succeed by accurately retrieving and generating answers\nfrom gold-standard documents. This assumption fails to align with real-world\nconditions, leading to an overestimation of RAG system performance. To bridge\nthis gap, we introduce RAGuard, a fact-checking dataset designed to evaluate\nthe robustness of RAG systems against misleading retrievals. Unlike prior\nbenchmarks that rely on synthetic noise, our dataset constructs its retrieval\ncorpus from Reddit discussions, capturing naturally occurring misinformation.\nIt categorizes retrieved evidence into three types: supporting, misleading, and\nirrelevant, providing a realistic and challenging testbed for assessing how\nwell RAG systems navigate different retrieval information. Our benchmark\nexperiments reveal that when exposed to misleading retrievals, all tested\nLLM-powered RAG systems perform worse than their zero-shot baselines (i.e., no\nretrieval at all), highlighting their susceptibility to noisy environments. To\nthe best of our knowledge, RAGuard is the first benchmark to systematically\nassess RAG robustness against misleading evidence. We expect this benchmark\nwill drive future research toward improving RAG systems beyond idealized\ndatasets, making them more reliable for real-world applications.",
        "timestamp": "2025-02-25T06:47:18.730Z",
        "rating": "novote",
        "published_date": "2025-02-22T05:50:15Z",
        "arxiv_tags": [
          "cs.AI",
          "cs.IR"
        ]
      },
      "meta": {
        "created_at": "2025-02-25T06:47:19+00:00",
        "updated_at": "2025-03-29T22:46:46+00:00",
        "version": 2
      }
    },
    "interactions:2502.16101": {
      "data": {
        "paper_id": "2502.16101",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-25T06:47:26.576Z",
            "data": {
              "session_id": "session_1740466038840_91kz5r9",
              "duration_seconds": 7,
              "idle_seconds": 0,
              "start_time": "2025-02-25T06:47:18.840Z",
              "end_time": "2025-02-25T06:47:25.754Z",
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-25T06:48:59.346Z",
            "data": {
              "session_id": "session_1740466114075_t57e8in",
              "duration_seconds": 25,
              "idle_seconds": 0,
              "start_time": "2025-02-25T06:48:34.075Z",
              "end_time": "2025-02-25T06:48:58.768Z",
              "total_elapsed_seconds": 25
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-25T06:47:27+00:00",
        "updated_at": "2025-03-29T22:46:46+00:00",
        "version": 4
      }
    },
    "interactions:2311.09431": {
      "data": {
        "paper_id": "2311.09431",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-25T17:14:26.605Z",
            "data": {
              "session_id": "session_1740503656467_xykzeln",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-02-25T17:14:16.467Z",
              "end_time": "2025-02-25T17:14:20.268Z",
              "total_elapsed_seconds": 4
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-25T17:14:22+00:00",
        "updated_at": "2025-03-29T22:46:45+00:00",
        "version": 4
      }
    },
    "paper:2311.09431": {
      "data": {
        "arxivId": "2311.09431",
        "url": "https://arxiv.org/abs/2311.09431",
        "title": "Striped Attention: Faster Ring Attention for Causal Transformers",
        "authors": "William Brandon, Aniruddha Nrusimha, Kevin Qian, Zachary Ankner, Tian Jin, Zhiye Song, Jonathan Ragan-Kelley",
        "abstract": "To help address the growing demand for ever-longer sequence lengths in\ntransformer models, Liu et al. recently proposed Ring Attention, an exact\nattention algorithm capable of overcoming per-device memory bottle- necks by\ndistributing self-attention across multiple devices. In this paper, we study\nthe performance characteristics of Ring Attention in the important special case\nof causal transformer models, and identify a key workload imbal- ance due to\ntriangular structure of causal attention computations. We propose a simple\nextension to Ring Attention, which we call Striped Attention to fix this\nimbalance. Instead of devices having contiguous subsequences, each device has a\nsubset of tokens distributed uniformly throughout the sequence, which we\ndemonstrate leads to more even workloads. In experiments running Striped\nAttention on A100 GPUs and TPUv4s, we are able to achieve up to 1.45x\nend-to-end throughput improvements over the original Ring Attention algorithm\non causal transformer training at a sequence length of 256k. Furthermore, on 16\nTPUv4 chips, we were able to achieve 1.65x speedups at sequence lengths of\n786k. We release the code for our experiments as open source",
        "timestamp": "2025-02-25T17:14:16.747Z",
        "rating": "novote",
        "published_date": "2023-11-15T23:01:02Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.CL"
        ]
      },
      "meta": {
        "created_at": "2025-02-25T17:14:17+00:00",
        "updated_at": "2025-03-29T22:46:45+00:00",
        "version": 2
      }
    },
    "interactions:2502.16797": {
      "data": {
        "paper_id": "2502.16797",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-02-26T00:43:52+00:00",
        "updated_at": "2025-03-29T22:46:44+00:00",
        "version": 2
      }
    },
    "paper:2502.16797": {
      "data": {
        "arxivId": "2502.16797",
        "url": "https://arxiv.org/abs/2502.16797",
        "title": "Forecasting Rare Language Model Behaviors",
        "authors": "Erik Jones, Meg Tong, Jesse Mu, Mohammed Mahfoud, Jan Leike, Roger Grosse, Jared Kaplan, William Fithian, Ethan Perez, Mrinank Sharma",
        "abstract": "Standard language model evaluations can fail to capture risks that emerge\nonly at deployment scale. For example, a model may produce safe responses\nduring a small-scale beta test, yet reveal dangerous information when\nprocessing billions of requests at deployment. To remedy this, we introduce a\nmethod to forecast potential risks across orders of magnitude more queries than\nwe test during evaluation. We make forecasts by studying each query's\nelicitation probability -- the probability the query produces a target behavior\n-- and demonstrate that the largest observed elicitation probabilities\npredictably scale with the number of queries. We find that our forecasts can\npredict the emergence of diverse undesirable behaviors -- such as assisting\nusers with dangerous chemical synthesis or taking power-seeking actions --\nacross up to three orders of magnitude of query volume. Our work enables model\ndevelopers to proactively anticipate and patch rare failures before they\nmanifest during large-scale deployments.",
        "timestamp": "2025-02-26T00:37:33.696Z",
        "rating": "novote",
        "published_date": "2025-02-24T03:16:15Z",
        "arxiv_tags": [
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-26T00:37:34+00:00",
        "updated_at": "2025-03-29T22:46:44+00:00",
        "version": 2
      }
    },
    "interactions:2410.12264": {
      "data": {
        "paper_id": "2410.12264",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-26T06:23:07.137Z",
            "data": {
              "session_id": "session_1740550981879_e0x966n",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-02-26T06:23:01.879Z",
              "end_time": "2025-02-26T06:23:07.124Z",
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-26T06:22:55+00:00",
        "updated_at": "2025-03-29T22:46:43+00:00",
        "version": 5
      }
    },
    "paper:2410.12264": {
      "data": {
        "arxivId": "2410.12264",
        "url": "https://arxiv.org/abs/2410.12264",
        "title": "Game Theory Meets Statistical Mechanics in Deep Learning Design",
        "authors": "Djamel Bouchaffra, Fay\u00e7al Ykhlef, Bilal Faye, Hanane Azzag, Mustapha Lebbah",
        "abstract": "We present a novel deep graphical representation that seamlessly merges\nprinciples of game theory with laws of statistical mechanics. It performs\nfeature extraction, dimensionality reduction, and pattern classification within\na single learning framework. Our approach draws an analogy between neurons in a\nnetwork and players in a game theory model. Furthermore, each neuron viewed as\na classical particle (subject to statistical physics' laws) is mapped to a set\nof actions representing specific activation value, and neural network layers\nare conceptualized as games in a sequential cooperative game theory setting.\nThe feed-forward process in deep learning is interpreted as a sequential game,\nwhere each game comprises a set of players. During training, neurons are\niteratively evaluated and filtered based on their contributions to a payoff\nfunction, which is quantified using the Shapley value driven by an energy\nfunction. Each set of neurons that significantly contributes to the payoff\nfunction forms a strong coalition. These neurons are the only ones permitted to\npropagate the information forward to the next layers. We applied this\nmethodology to the task of facial age estimation and gender classification.\nExperimental results demonstrate that our approach outperforms both multi-layer\nperceptron and convolutional neural network models in terms of efficiency and\naccuracy.",
        "timestamp": "2025-02-26T06:21:52.809Z",
        "rating": "novote",
        "published_date": "2024-10-16T06:02:18Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.GT"
        ]
      },
      "meta": {
        "created_at": "2025-02-26T06:21:53+00:00",
        "updated_at": "2025-03-29T22:46:43+00:00",
        "version": 2
      }
    },
    "interactions:2502.18394": {
      "data": {
        "paper_id": "2502.18394",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-26T07:50:29.429Z",
            "data": {
              "session_id": "session_1740555342377_u5key2k",
              "duration_seconds": 886,
              "idle_seconds": 0,
              "start_time": "2025-02-26T07:35:42.377Z",
              "end_time": "2025-02-26T07:50:28.858Z",
              "total_elapsed_seconds": 886
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-26T08:30:01.469Z",
            "data": {
              "session_id": "session_1740558592919_cpo8bzl",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-02-26T08:29:52.919Z",
              "end_time": "2025-02-26T08:30:00.455Z",
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-27T09:00:24.404Z",
            "data": {
              "session_id": "session_1740646751466_ry6luju",
              "duration_seconds": 68,
              "idle_seconds": 0,
              "start_time": "2025-02-27T08:59:11.466Z",
              "end_time": "2025-02-27T09:00:19.589Z",
              "total_elapsed_seconds": 68
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-26T07:50:30+00:00",
        "updated_at": "2025-03-29T22:46:42+00:00",
        "version": 6
      }
    },
    "paper:2502.18394": {
      "data": {
        "arxivId": "2502.18394",
        "url": "https://arxiv.org/abs/2502.18394",
        "title": "The FFT Strikes Back: An Efficient Alternative to Self-Attention",
        "authors": "Jacob Fein-Ashley",
        "abstract": "Conventional self-attention mechanisms incur quadratic complexity, limiting\ntheir scalability on long sequences. We introduce FFTNet, an adaptive spectral\nfiltering framework that leverages the Fast Fourier Transform (FFT) to achieve\nglobal token mixing in $\\mathcal{O}(n\\log n)$ time. By transforming inputs into\nthe frequency domain, FFTNet exploits the orthogonality and energy preservation\nguaranteed by Parseval's theorem to capture long-range dependencies\nefficiently. A learnable spectral filter and modReLU activation dynamically\nemphasize salient frequency components, providing a rigorous and adaptive\nalternative to traditional self-attention. Experiments on the Long Range Arena\nand ImageNet benchmarks validate our theoretical insights and demonstrate\nsuperior performance over fixed Fourier and standard attention models.",
        "timestamp": "2025-02-26T07:35:39.517Z",
        "rating": "novote",
        "published_date": "2025-02-25T17:43:43Z",
        "arxiv_tags": [
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-26T07:35:39+00:00",
        "updated_at": "2025-03-29T22:46:42+00:00",
        "version": 2
      }
    },
    "interactions:2502.10059": {
      "data": {
        "paper_id": "2502.10059",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-26T09:08:52.814Z",
            "data": {
              "session_id": "session_1740560851085_cq6umtl",
              "duration_seconds": 76,
              "idle_seconds": 0,
              "start_time": "2025-02-26T09:07:31.085Z",
              "end_time": "2025-02-26T09:08:47.280Z",
              "total_elapsed_seconds": 76
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-26T09:10:00.465Z",
            "data": {
              "session_id": "session_1740560941206_rpb2dhb",
              "duration_seconds": 58,
              "idle_seconds": 0,
              "start_time": "2025-02-26T09:09:01.206Z",
              "end_time": "2025-02-26T09:09:59.546Z",
              "total_elapsed_seconds": 58
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-26T09:17:13.089Z",
            "data": {
              "session_id": "session_1740561393498_5p5htrt",
              "duration_seconds": 39,
              "idle_seconds": 0,
              "start_time": "2025-02-26T09:16:33.498Z",
              "end_time": "2025-02-26T09:17:12.613Z",
              "total_elapsed_seconds": 39
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-26T09:22:06.084Z",
            "data": {
              "session_id": "session_1740561699407_7otixp0",
              "duration_seconds": 26,
              "idle_seconds": 0,
              "start_time": "2025-02-26T09:21:39.407Z",
              "end_time": "2025-02-26T09:22:05.676Z",
              "total_elapsed_seconds": 26
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-26T09:08:53+00:00",
        "updated_at": "2025-03-29T22:46:40+00:00",
        "version": 7
      }
    },
    "paper:arxiv.2502.10059": {
      "data": {
        "primary_id": "arxiv.2502.10059",
        "source": "arxiv",
        "sourceId": "2502.10059",
        "url": "https://arxiv.org/abs/2502.10059",
        "title": "RealCam-I2V: Real-World Image-to-Video Generation with Interactive\n  Complex Camera Control",
        "authors": "Teng Li, Guangcong Zheng, Rui Jiang, Shuigenzhan, Tao Wu, Yehao Lu, Yining Lin, Xi Li",
        "abstract": "Recent advancements in camera-trajectory-guided image-to-video generation\noffer higher precision and better support for complex camera control compared\nto text-based approaches. However, they also introduce significant usability\nchallenges, as users often struggle to provide precise camera parameters when\nworking with arbitrary real-world images without knowledge of their depth nor\nscene scale. To address these real-world application issues, we propose\nRealCam-I2V, a novel diffusion-based video generation framework that integrates\nmonocular metric depth estimation to establish 3D scene reconstruction in a\npreprocessing step. During training, the reconstructed 3D scene enables scaling\ncamera parameters from relative to absolute values, ensuring compatibility and\nscale consistency across diverse real-world images. In inference, RealCam-I2V\noffers an intuitive interface where users can precisely draw camera\ntrajectories by dragging within the 3D scene. To further enhance precise camera\ncontrol and scene consistency, we propose scene-constrained noise shaping,\nwhich shapes high-level noise and also allows the framework to maintain\ndynamic, coherent video generation in lower noise stages. RealCam-I2V achieves\nsignificant improvements in controllability and video quality on the\nRealEstate10K and out-of-domain images. We further enables applications like\ncamera-controlled looping video generation and generative frame interpolation.\nWe will release our absolute-scale annotation, codes, and all checkpoints.\nPlease see dynamic results in https://zgctroy.github.io/RealCam-I2V.",
        "timestamp": "2025-02-26T09:08:48.162Z",
        "rating": "novote",
        "arxivId": "2502.10059",
        "arxiv_tags": [
          "cs.CV"
        ],
        "published_date": "2025-02-14T10:21:49Z"
      },
      "meta": {
        "created_at": "2025-02-26T09:08:48+00:00",
        "updated_at": "2025-03-29T22:46:41+00:00",
        "version": 2
      }
    },
    "paper:2502.10059": {
      "data": {
        "arxivId": "2502.10059",
        "url": "https://arxiv.org/abs/2502.10059",
        "title": "RealCam-I2V: Real-World Image-to-Video Generation with Interactive\n  Complex Camera Control",
        "authors": "Teng Li, Guangcong Zheng, Rui Jiang, Shuigenzhan, Tao Wu, Yehao Lu, Yining Lin, Xi Li",
        "abstract": "Recent advancements in camera-trajectory-guided image-to-video generation\noffer higher precision and better support for complex camera control compared\nto text-based approaches. However, they also introduce significant usability\nchallenges, as users often struggle to provide precise camera parameters when\nworking with arbitrary real-world images without knowledge of their depth nor\nscene scale. To address these real-world application issues, we propose\nRealCam-I2V, a novel diffusion-based video generation framework that integrates\nmonocular metric depth estimation to establish 3D scene reconstruction in a\npreprocessing step. During training, the reconstructed 3D scene enables scaling\ncamera parameters from relative to absolute values, ensuring compatibility and\nscale consistency across diverse real-world images. In inference, RealCam-I2V\noffers an intuitive interface where users can precisely draw camera\ntrajectories by dragging within the 3D scene. To further enhance precise camera\ncontrol and scene consistency, we propose scene-constrained noise shaping,\nwhich shapes high-level noise and also allows the framework to maintain\ndynamic, coherent video generation in lower noise stages. RealCam-I2V achieves\nsignificant improvements in controllability and video quality on the\nRealEstate10K and out-of-domain images. We further enables applications like\ncamera-controlled looping video generation and generative frame interpolation.\nWe will release our absolute-scale annotation, codes, and all checkpoints.\nPlease see dynamic results in https://zgctroy.github.io/RealCam-I2V.",
        "timestamp": "2025-02-26T09:07:31.320Z",
        "rating": "novote",
        "published_date": "2025-02-14T10:21:49Z",
        "arxiv_tags": [
          "cs.CV"
        ]
      },
      "meta": {
        "created_at": "2025-02-26T09:07:32+00:00",
        "updated_at": "2025-03-29T22:46:41+00:00",
        "version": 2
      }
    },
    "interactions:2103.03206": {
      "data": {
        "paper_id": "2103.03206",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-26T19:25:14.802Z",
            "data": {
              "session_id": "session_1740597907311_0jxtfri",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-02-26T19:25:07.311Z",
              "end_time": "2025-02-26T19:25:10.356Z",
              "total_elapsed_seconds": 3
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-26T19:29:18.678Z",
            "data": {
              "session_id": "session_1740598145041_2tyxa6h",
              "duration_seconds": 13,
              "idle_seconds": 0,
              "start_time": "2025-02-26T19:29:05.041Z",
              "end_time": "2025-02-26T19:29:17.799Z",
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-26T21:31:59.191Z",
            "data": {
              "session_id": "session_1740605515296_x3ku9vi",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-02-26T21:31:55.296Z",
              "end_time": "2025-02-26T21:31:58.784Z",
              "total_elapsed_seconds": 3
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-26T19:25:15+00:00",
        "updated_at": "2025-03-29T22:46:36+00:00",
        "version": 5
      }
    },
    "paper:arxiv.2103.03206": {
      "data": {
        "primary_id": "arxiv.2103.03206",
        "source": "arxiv",
        "sourceId": "2103.03206",
        "url": "https://arxiv.org/pdf/2103.03206",
        "title": "Perceiver: General Perception with Iterative Attention",
        "authors": "Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, Joao Carreira",
        "abstract": "Biological systems perceive the world by simultaneously processing\nhigh-dimensional inputs from modalities as diverse as vision, audition, touch,\nproprioception, etc. The perception models used in deep learning on the other\nhand are designed for individual modalities, often relying on domain-specific\nassumptions such as the local grid structures exploited by virtually all\nexisting vision models. These priors introduce helpful inductive biases, but\nalso lock models to individual modalities. In this paper we introduce the\nPerceiver - a model that builds upon Transformers and hence makes few\narchitectural assumptions about the relationship between its inputs, but that\nalso scales to hundreds of thousands of inputs, like ConvNets. The model\nleverages an asymmetric attention mechanism to iteratively distill inputs into\na tight latent bottleneck, allowing it to scale to handle very large inputs. We\nshow that this architecture is competitive with or outperforms strong,\nspecialized models on classification tasks across various modalities: images,\npoint clouds, audio, video, and video+audio. The Perceiver obtains performance\ncomparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly\nattending to 50,000 pixels. It is also competitive in all modalities in\nAudioSet.",
        "timestamp": "2025-02-26T19:25:10.752Z",
        "rating": "novote",
        "arxivId": "2103.03206",
        "arxiv_tags": [
          "cs.CV",
          "cs.AI",
          "cs.LG",
          "cs.SD",
          "eess.AS"
        ],
        "published_date": "2021-03-04T18:20:50Z"
      },
      "meta": {
        "created_at": "2025-02-26T19:25:11+00:00",
        "updated_at": "2025-03-29T22:46:37+00:00",
        "version": 2
      }
    },
    "paper:2103.03206": {
      "data": {
        "arxivId": "2103.03206",
        "url": "https://arxiv.org/abs/2103.03206",
        "title": "Perceiver: General Perception with Iterative Attention",
        "authors": "Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, Joao Carreira",
        "abstract": "Biological systems perceive the world by simultaneously processing\nhigh-dimensional inputs from modalities as diverse as vision, audition, touch,\nproprioception, etc. The perception models used in deep learning on the other\nhand are designed for individual modalities, often relying on domain-specific\nassumptions such as the local grid structures exploited by virtually all\nexisting vision models. These priors introduce helpful inductive biases, but\nalso lock models to individual modalities. In this paper we introduce the\nPerceiver - a model that builds upon Transformers and hence makes few\narchitectural assumptions about the relationship between its inputs, but that\nalso scales to hundreds of thousands of inputs, like ConvNets. The model\nleverages an asymmetric attention mechanism to iteratively distill inputs into\na tight latent bottleneck, allowing it to scale to handle very large inputs. We\nshow that this architecture is competitive with or outperforms strong,\nspecialized models on classification tasks across various modalities: images,\npoint clouds, audio, video, and video+audio. The Perceiver obtains performance\ncomparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly\nattending to 50,000 pixels. It is also competitive in all modalities in\nAudioSet.",
        "timestamp": "2025-02-26T19:25:00.582Z",
        "rating": "novote",
        "published_date": "2021-03-04T18:20:50Z",
        "arxiv_tags": [
          "cs.CV",
          "cs.AI",
          "cs.LG",
          "cs.SD",
          "eess.AS"
        ]
      },
      "meta": {
        "created_at": "2025-02-26T19:25:01+00:00",
        "updated_at": "2025-03-29T22:46:37+00:00",
        "version": 2
      }
    },
    "interactions:2301.10540": {
      "data": {
        "paper_id": "2301.10540",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-02-26T19:24:59+00:00",
        "updated_at": "2025-03-29T22:46:39+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2301.10540": {
      "data": {
        "primary_id": "arxiv.2301.10540",
        "source": "arxiv",
        "sourceId": "2301.10540",
        "url": "https://arxiv.org/pdf/2301.10540",
        "title": "Modelling Long Range Dependencies in $N$D: From Task-Specific to a\n  General Purpose CNN",
        "authors": "David M. Knigge, David W. Romero, Albert Gu, Efstratios Gavves, Erik J. Bekkers, Jakub M. Tomczak, Mark Hoogendoorn, Jan-Jakob Sonke",
        "abstract": "Performant Convolutional Neural Network (CNN) architectures must be tailored\nto specific tasks in order to consider the length, resolution, and\ndimensionality of the input data. In this work, we tackle the need for\nproblem-specific CNN architectures. We present the Continuous Convolutional\nNeural Network (CCNN): a single CNN able to process data of arbitrary\nresolution, dimensionality and length without any structural changes. Its key\ncomponent are its continuous convolutional kernels which model long-range\ndependencies at every layer, and thus remove the need of current CNN\narchitectures for task-dependent downsampling and depths. We showcase the\ngenerality of our method by using the same architecture for tasks on sequential\n($1{\\rm D}$), visual ($2{\\rm D}$) and point-cloud ($3{\\rm D}$) data. Our CCNN\nmatches and often outperforms the current state-of-the-art across all tasks\nconsidered.",
        "timestamp": "2025-02-26T19:24:54.989Z",
        "rating": "thumbsup",
        "arxivId": "2301.10540",
        "arxiv_tags": [
          "cs.CV"
        ],
        "published_date": "2023-01-25T12:12:47Z"
      },
      "meta": {
        "created_at": "2025-02-26T19:24:55+00:00",
        "updated_at": "2025-03-29T22:46:39+00:00",
        "version": 3
      }
    },
    "paper:2301.10540": {
      "data": {
        "arxivId": "2301.10540",
        "url": "https://arxiv.org/pdf/2301.10540",
        "title": "Modelling Long Range Dependencies in $N$D: From Task-Specific to a\n  General Purpose CNN",
        "authors": "David M. Knigge, David W. Romero, Albert Gu, Efstratios Gavves, Erik J. Bekkers, Jakub M. Tomczak, Mark Hoogendoorn, Jan-Jakob Sonke",
        "abstract": "Performant Convolutional Neural Network (CNN) architectures must be tailored\nto specific tasks in order to consider the length, resolution, and\ndimensionality of the input data. In this work, we tackle the need for\nproblem-specific CNN architectures. We present the Continuous Convolutional\nNeural Network (CCNN): a single CNN able to process data of arbitrary\nresolution, dimensionality and length without any structural changes. Its key\ncomponent are its continuous convolutional kernels which model long-range\ndependencies at every layer, and thus remove the need of current CNN\narchitectures for task-dependent downsampling and depths. We showcase the\ngenerality of our method by using the same architecture for tasks on sequential\n($1{\\rm D}$), visual ($2{\\rm D}$) and point-cloud ($3{\\rm D}$) data. Our CCNN\nmatches and often outperforms the current state-of-the-art across all tasks\nconsidered.",
        "timestamp": "2025-02-26T19:23:26.563Z",
        "rating": "novote",
        "published_date": "2023-01-25T12:12:47Z",
        "arxiv_tags": [
          "cs.CV"
        ]
      },
      "meta": {
        "created_at": "2025-02-26T19:23:27+00:00",
        "updated_at": "2025-03-29T22:46:40+00:00",
        "version": 2
      }
    },
    "interactions:2502.09992": {
      "data": {
        "paper_id": "2502.09992",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-02-27T05:58:32+00:00",
        "updated_at": "2025-03-29T22:46:35+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2502.09992": {
      "data": {
        "primary_id": "arxiv.2502.09992",
        "source": "arxiv",
        "sourceId": "2502.09992",
        "url": "https://arxiv.org/abs/2502.09992",
        "title": "Large Language Diffusion Models",
        "authors": "Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, Chongxuan Li",
        "abstract": "Autoregressive models (ARMs) are widely regarded as the cornerstone of large\nlanguage models (LLMs). We challenge this notion by introducing LLaDA, a\ndiffusion model trained from scratch under the pre-training and supervised\nfine-tuning (SFT) paradigm. LLaDA models distributions through a forward data\nmasking process and a reverse process, parameterized by a vanilla Transformer\nto predict masked tokens. By optimizing a likelihood bound, it provides a\nprincipled generative approach for probabilistic inference. Across extensive\nbenchmarks, LLaDA demonstrates strong scalability, outperforming our\nself-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong\nLLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive\ninstruction-following abilities in case studies such as multi-turn dialogue.\nMoreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal\npoem completion task. Our findings establish diffusion models as a viable and\npromising alternative to ARMs, challenging the assumption that key LLM\ncapabilities discussed above are inherently tied to ARMs. Project page and\ncodes: https://ml-gsai.github.io/LLaDA-demo/.",
        "timestamp": "2025-02-27T05:58:28.403Z",
        "rating": "thumbsup",
        "arxivId": "2502.09992",
        "arxiv_tags": [
          "cs.CL",
          "cs.LG"
        ],
        "published_date": "2025-02-14T08:23:51Z"
      },
      "meta": {
        "created_at": "2025-02-27T05:58:28+00:00",
        "updated_at": "2025-03-29T22:46:35+00:00",
        "version": 3
      }
    },
    "paper:2502.09992": {
      "data": {
        "arxivId": "2502.09992",
        "url": "https://arxiv.org/abs/2502.09992",
        "title": "Large Language Diffusion Models",
        "authors": "Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, Chongxuan Li",
        "abstract": "Autoregressive models (ARMs) are widely regarded as the cornerstone of large\nlanguage models (LLMs). We challenge this notion by introducing LLaDA, a\ndiffusion model trained from scratch under the pre-training and supervised\nfine-tuning (SFT) paradigm. LLaDA models distributions through a forward data\nmasking process and a reverse process, parameterized by a vanilla Transformer\nto predict masked tokens. By optimizing a likelihood bound, it provides a\nprincipled generative approach for probabilistic inference. Across extensive\nbenchmarks, LLaDA demonstrates strong scalability, outperforming our\nself-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong\nLLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive\ninstruction-following abilities in case studies such as multi-turn dialogue.\nMoreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal\npoem completion task. Our findings establish diffusion models as a viable and\npromising alternative to ARMs, challenging the assumption that key LLM\ncapabilities discussed above are inherently tied to ARMs. Project page and\ncodes: https://ml-gsai.github.io/LLaDA-demo/.",
        "timestamp": "2025-02-27T05:58:17.156Z",
        "rating": "novote",
        "published_date": "2025-02-14T08:23:51Z",
        "arxiv_tags": [
          "cs.CL",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-27T05:58:17+00:00",
        "updated_at": "2025-03-29T22:46:36+00:00",
        "version": 2
      }
    },
    "interactions:2402.08733": {
      "data": {
        "paper_id": "2402.08733",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-27T07:40:01.113Z",
            "data": {
              "session_id": "session_1740641993595_oqdidd6",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-02-27T07:39:53.595Z",
              "end_time": "2025-02-27T07:39:56.998Z",
              "total_elapsed_seconds": 3
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-27T07:40:01+00:00",
        "updated_at": "2025-03-29T22:46:33+00:00",
        "version": 3
      }
    },
    "interactions:2102.08431": {
      "data": {
        "paper_id": "2102.08431",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-02-27T07:43:46+00:00",
        "updated_at": "2025-03-29T22:46:28+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2102.08431": {
      "data": {
        "primary_id": "arxiv.2102.08431",
        "source": "arxiv",
        "sourceId": "2102.08431",
        "url": "https://arxiv.org/pdf/2102.08431",
        "title": "Complex Momentum for Optimization in Games",
        "authors": "Jonathan Lorraine, David Acuna, Paul Vicol, David Duvenaud",
        "abstract": "We generalize gradient descent with momentum for optimization in\ndifferentiable games to have complex-valued momentum. We give theoretical\nmotivation for our method by proving convergence on bilinear zero-sum games for\nsimultaneous and alternating updates. Our method gives real-valued parameter\nupdates, making it a drop-in replacement for standard optimizers. We\nempirically demonstrate that complex-valued momentum can improve convergence in\nrealistic adversarial games - like generative adversarial networks - by showing\nwe can find better solutions with an almost identical computational cost. We\nalso show a practical generalization to a complex-valued Adam variant, which we\nuse to train BigGAN to better inception scores on CIFAR-10.",
        "timestamp": "2025-02-27T07:43:41.741Z",
        "rating": "novote",
        "arxivId": "2102.08431",
        "arxiv_tags": [
          "cs.LG",
          "cs.GT"
        ],
        "published_date": "2021-02-16T19:55:27Z"
      },
      "meta": {
        "created_at": "2025-02-27T07:43:42+00:00",
        "updated_at": "2025-03-29T22:46:29+00:00",
        "version": 2
      }
    },
    "interactions:1802.09419": {
      "data": {
        "paper_id": "1802.09419",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-27T07:43:35.129Z",
            "data": {
              "session_id": "session_1740642197699_mscvy28",
              "duration_seconds": 16,
              "idle_seconds": 0,
              "start_time": "2025-02-27T07:43:17.699Z",
              "end_time": "2025-02-27T07:43:34.135Z",
              "total_elapsed_seconds": 16
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-27T07:43:10+00:00",
        "updated_at": "2025-03-29T22:46:29+00:00",
        "version": 4
      }
    },
    "paper:arxiv.1802.09419": {
      "data": {
        "primary_id": "arxiv.1802.09419",
        "source": "arxiv",
        "sourceId": "1802.09419",
        "url": "https://arxiv.org/abs/1802.09419",
        "title": "Stochastic Hyperparameter Optimization through Hypernetworks",
        "authors": "Jonathan Lorraine, David Duvenaud",
        "abstract": "Machine learning models are often tuned by nesting optimization of model\nweights inside the optimization of hyperparameters. We give a method to\ncollapse this nested optimization into joint stochastic optimization of weights\nand hyperparameters. Our process trains a neural network to output\napproximately optimal weights as a function of hyperparameters. We show that\nour technique converges to locally optimal weights and hyperparameters for\nsufficiently large hypernetworks. We compare this method to standard\nhyperparameter optimization strategies and demonstrate its effectiveness for\ntuning thousands of hyperparameters.",
        "timestamp": "2025-02-27T07:43:06.215Z",
        "rating": "novote",
        "arxivId": "1802.09419",
        "arxiv_tags": [
          "cs.LG"
        ],
        "published_date": "2018-02-26T16:04:46Z"
      },
      "meta": {
        "created_at": "2025-02-27T07:43:06+00:00",
        "updated_at": "2025-03-29T22:46:29+00:00",
        "version": 2
      }
    },
    "paper:1802.09419": {
      "data": {
        "arxivId": "1802.09419",
        "url": "https://arxiv.org/abs/1802.09419",
        "title": "Stochastic Hyperparameter Optimization through Hypernetworks",
        "authors": "Jonathan Lorraine, David Duvenaud",
        "abstract": "Machine learning models are often tuned by nesting optimization of model\nweights inside the optimization of hyperparameters. We give a method to\ncollapse this nested optimization into joint stochastic optimization of weights\nand hyperparameters. Our process trains a neural network to output\napproximately optimal weights as a function of hyperparameters. We show that\nour technique converges to locally optimal weights and hyperparameters for\nsufficiently large hypernetworks. We compare this method to standard\nhyperparameter optimization strategies and demonstrate its effectiveness for\ntuning thousands of hyperparameters.",
        "timestamp": "2025-02-27T07:42:59.195Z",
        "rating": "novote",
        "published_date": "2018-02-26T16:04:46Z",
        "arxiv_tags": [
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-27T07:42:59+00:00",
        "updated_at": "2025-03-29T22:46:30+00:00",
        "version": 2
      }
    },
    "interactions:2102.06559": {
      "data": {
        "paper_id": "2102.06559",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-27T07:40:40.559Z",
            "data": {
              "session_id": "session_1740642026636_423owdl",
              "duration_seconds": 10,
              "idle_seconds": 0,
              "start_time": "2025-02-27T07:40:26.636Z",
              "end_time": "2025-02-27T07:40:36.846Z",
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-27T07:43:58.640Z",
            "data": {
              "session_id": "session_1740642232621_e8sbqrp",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-02-27T07:43:52.621Z",
              "end_time": "2025-02-27T07:43:58.040Z",
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-27T07:40:41+00:00",
        "updated_at": "2025-03-29T22:46:30+00:00",
        "version": 4
      }
    },
    "paper:2102.08431": {
      "data": {
        "arxivId": "2102.08431",
        "url": "https://arxiv.org/pdf/2102.08431",
        "title": "Complex Momentum for Optimization in Games",
        "authors": "Jonathan Lorraine, David Acuna, Paul Vicol, David Duvenaud",
        "abstract": "We generalize gradient descent with momentum for optimization in\ndifferentiable games to have complex-valued momentum. We give theoretical\nmotivation for our method by proving convergence on bilinear zero-sum games for\nsimultaneous and alternating updates. Our method gives real-valued parameter\nupdates, making it a drop-in replacement for standard optimizers. We\nempirically demonstrate that complex-valued momentum can improve convergence in\nrealistic adversarial games - like generative adversarial networks - by showing\nwe can find better solutions with an almost identical computational cost. We\nalso show a practical generalization to a complex-valued Adam variant, which we\nuse to train BigGAN to better inception scores on CIFAR-10.",
        "timestamp": "2025-02-27T07:40:37.469Z",
        "rating": "novote",
        "published_date": "2021-02-16T19:55:27Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.GT"
        ]
      },
      "meta": {
        "created_at": "2025-02-27T07:40:37+00:00",
        "updated_at": "2025-03-29T22:46:32+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2102.06559": {
      "data": {
        "primary_id": "arxiv.2102.06559",
        "source": "arxiv",
        "sourceId": "2102.06559",
        "url": "https://arxiv.org/abs/2102.06559",
        "title": "Infinitely Deep Bayesian Neural Networks with Stochastic Differential\n  Equations",
        "authors": "Winnie Xu, Ricky T. Q. Chen, Xuechen Li, David Duvenaud",
        "abstract": "We perform scalable approximate inference in continuous-depth Bayesian neural\nnetworks. In this model class, uncertainty about separate weights in each layer\ngives hidden units that follow a stochastic differential equation. We\ndemonstrate gradient-based stochastic variational inference in this\ninfinite-parameter setting, producing arbitrarily-flexible approximate\nposteriors. We also derive a novel gradient estimator that approaches zero\nvariance as the approximate posterior over weights approaches the true\nposterior. This approach brings continuous-depth Bayesian neural nets to a\ncompetitive comparison against discrete-depth alternatives, while inheriting\nthe memory-efficient training and tunable precision of Neural ODEs.",
        "timestamp": "2025-02-27T07:40:37.239Z",
        "rating": "novote",
        "arxivId": "2102.06559",
        "arxiv_tags": [
          "stat.ML",
          "cs.LG"
        ],
        "published_date": "2021-02-12T14:48:58Z"
      },
      "meta": {
        "created_at": "2025-02-27T07:40:37+00:00",
        "updated_at": "2025-03-29T22:46:31+00:00",
        "version": 2
      }
    },
    "paper:2102.06559": {
      "data": {
        "arxivId": "2102.06559",
        "url": "https://arxiv.org/abs/2102.06559",
        "title": "Infinitely Deep Bayesian Neural Networks with Stochastic Differential\n  Equations",
        "authors": "Winnie Xu, Ricky T. Q. Chen, Xuechen Li, David Duvenaud",
        "abstract": "We perform scalable approximate inference in continuous-depth Bayesian neural\nnetworks. In this model class, uncertainty about separate weights in each layer\ngives hidden units that follow a stochastic differential equation. We\ndemonstrate gradient-based stochastic variational inference in this\ninfinite-parameter setting, producing arbitrarily-flexible approximate\nposteriors. We also derive a novel gradient estimator that approaches zero\nvariance as the approximate posterior over weights approaches the true\nposterior. This approach brings continuous-depth Bayesian neural nets to a\ncompetitive comparison against discrete-depth alternatives, while inheriting\nthe memory-efficient training and tunable precision of Neural ODEs.",
        "timestamp": "2025-02-27T07:40:26.730Z",
        "rating": "novote",
        "published_date": "2021-02-12T14:48:58Z",
        "arxiv_tags": [
          "stat.ML",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-27T07:40:27+00:00",
        "updated_at": "2025-03-29T22:46:32+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2402.08733": {
      "data": {
        "primary_id": "arxiv.2402.08733",
        "source": "arxiv",
        "sourceId": "2402.08733",
        "url": "https://arxiv.org/abs/2402.08733",
        "title": "Experts Don't Cheat: Learning What You Don't Know By Predicting Pairs",
        "authors": "Daniel D. Johnson, Daniel Tarlow, David Duvenaud, Chris J. Maddison",
        "abstract": "Identifying how much a model ${\\widehat{p}}_{\\theta}(Y|X)$ knows about the\nstochastic real-world process $p(Y|X)$ it was trained on is important to ensure\nit avoids producing incorrect or \"hallucinated\" answers or taking unsafe\nactions. But this is difficult for generative models because probabilistic\npredictions do not distinguish between per-response noise (aleatoric\nuncertainty) and lack of knowledge about the process (epistemic uncertainty),\nand existing epistemic uncertainty quantification techniques tend to be\noverconfident when the model underfits. We propose a general strategy for\nteaching a model to both approximate $p(Y|X)$ and also estimate the remaining\ngaps between ${\\widehat{p}}_{\\theta}(Y|X)$ and $p(Y|X)$: train it to predict\npairs of independent responses drawn from the true conditional distribution,\nallow it to \"cheat\" by observing one response while predicting the other, then\nmeasure how much it cheats. Remarkably, we prove that being good at cheating\n(i.e. cheating whenever it improves your prediction) is equivalent to being\nsecond-order calibrated, a principled extension of ordinary calibration that\nallows us to construct provably-correct frequentist confidence intervals for\n$p(Y|X)$ and detect incorrect responses with high probability. We demonstrate\nempirically that our approach accurately estimates how much models don't know\nacross ambiguous image classification, (synthetic) language modeling, and\npartially-observable navigation tasks, outperforming existing techniques.",
        "timestamp": "2025-02-27T07:39:57.410Z",
        "rating": "novote",
        "arxivId": "2402.08733",
        "arxiv_tags": [
          "cs.LG"
        ],
        "published_date": "2024-02-13T19:01:45Z"
      },
      "meta": {
        "created_at": "2025-02-27T07:39:57+00:00",
        "updated_at": "2025-03-29T22:46:33+00:00",
        "version": 2
      }
    },
    "paper:2402.08733": {
      "data": {
        "arxivId": "2402.08733",
        "url": "https://arxiv.org/abs/2402.08733",
        "title": "Experts Don't Cheat: Learning What You Don't Know By Predicting Pairs",
        "authors": "Daniel D. Johnson, Daniel Tarlow, David Duvenaud, Chris J. Maddison",
        "abstract": "Identifying how much a model ${\\widehat{p}}_{\\theta}(Y|X)$ knows about the\nstochastic real-world process $p(Y|X)$ it was trained on is important to ensure\nit avoids producing incorrect or \"hallucinated\" answers or taking unsafe\nactions. But this is difficult for generative models because probabilistic\npredictions do not distinguish between per-response noise (aleatoric\nuncertainty) and lack of knowledge about the process (epistemic uncertainty),\nand existing epistemic uncertainty quantification techniques tend to be\noverconfident when the model underfits. We propose a general strategy for\nteaching a model to both approximate $p(Y|X)$ and also estimate the remaining\ngaps between ${\\widehat{p}}_{\\theta}(Y|X)$ and $p(Y|X)$: train it to predict\npairs of independent responses drawn from the true conditional distribution,\nallow it to \"cheat\" by observing one response while predicting the other, then\nmeasure how much it cheats. Remarkably, we prove that being good at cheating\n(i.e. cheating whenever it improves your prediction) is equivalent to being\nsecond-order calibrated, a principled extension of ordinary calibration that\nallows us to construct provably-correct frequentist confidence intervals for\n$p(Y|X)$ and detect incorrect responses with high probability. We demonstrate\nempirically that our approach accurately estimates how much models don't know\nacross ambiguous image classification, (synthetic) language modeling, and\npartially-observable navigation tasks, outperforming existing techniques.",
        "timestamp": "2025-02-27T07:39:53.849Z",
        "rating": "novote",
        "published_date": "2024-02-13T19:01:45Z",
        "arxiv_tags": [
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-27T07:39:54+00:00",
        "updated_at": "2025-03-29T22:46:34+00:00",
        "version": 2
      }
    },
    "paper:2411.02353": {
      "data": {
        "arxivId": "2411.02353",
        "url": "https://arxiv.org/abs/2411.02353",
        "title": "Social-RAG: Retrieving from Group Interactions to Socially Ground AI\n  Generation",
        "authors": "Ruotong Wang, Xinyi Zhou, Lin Qiu, Joseph Chee Chang, Jonathan Bragg, Amy X. Zhang",
        "abstract": "AI agents are increasingly tasked with making proactive suggestions in online\nspaces where groups collaborate, yet risk being unhelpful or even annoying if\nthey fail to match group preferences or behave in socially inappropriate ways.\nFortunately, group spaces have a rich history of prior interactions and\naffordances for social feedback that can support grounding an agent's\ngenerations to a group's interests and norms. We present Social-RAG, a workflow\nfor socially grounding agents that retrieves context from prior group\ninteractions, selects relevant social signals, and feeds them into a language\nmodel to generate messages in a socially aligned manner. We implement this in\n\\textsc{PaperPing}, a system for posting paper recommendations in group chat,\nleveraging social signals determined from formative studies with 39\nresearchers. From a three-month deployment in 18 channels reaching 500+\nresearchers, we observed PaperPing posted relevant messages in groups without\ndisrupting their existing social practices, fostering group common ground.",
        "timestamp": "2025-02-27T09:00:20.396Z",
        "rating": "novote",
        "published_date": "2024-11-04T18:21:53Z",
        "arxiv_tags": [
          "cs.HC"
        ]
      },
      "meta": {
        "created_at": "2025-02-27T09:00:20+00:00",
        "updated_at": "2025-03-29T22:46:26+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2502.18394": {
      "data": {
        "primary_id": "arxiv.2502.18394",
        "source": "arxiv",
        "sourceId": "2502.18394",
        "url": "https://arxiv.org/abs/2502.18394",
        "title": "The FFT Strikes Back: An Efficient Alternative to Self-Attention",
        "authors": "Jacob Fein-Ashley",
        "abstract": "Conventional self-attention mechanisms incur quadratic complexity, limiting\ntheir scalability on long sequences. We introduce FFTNet, an adaptive spectral\nfiltering framework that leverages the Fast Fourier Transform (FFT) to achieve\nglobal token mixing in $\\mathcal{O}(n\\log n)$ time. By transforming inputs into\nthe frequency domain, FFTNet exploits the orthogonality and energy preservation\nguaranteed by Parseval's theorem to capture long-range dependencies\nefficiently. A learnable spectral filter and modReLU activation dynamically\nemphasize salient frequency components, providing a rigorous and adaptive\nalternative to traditional self-attention. Experiments on the Long Range Arena\nand ImageNet benchmarks validate our theoretical insights and demonstrate\nsuperior performance over fixed Fourier and standard attention models.",
        "timestamp": "2025-02-27T09:00:20.127Z",
        "rating": "novote",
        "arxivId": "2502.18394",
        "arxiv_tags": [
          "cs.LG"
        ],
        "published_date": "2025-02-25T17:43:43Z"
      },
      "meta": {
        "created_at": "2025-02-27T09:00:20+00:00",
        "updated_at": "2025-03-29T22:46:26+00:00",
        "version": 2
      }
    },
    "interactions:1609.06783": {
      "data": {
        "paper_id": "1609.06783",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-27T18:22:55.012Z",
            "data": {
              "session_id": "session_1740678822834_swev5gh",
              "duration_seconds": 1748,
              "idle_seconds": 0,
              "start_time": "2025-02-27T17:53:42.834Z",
              "end_time": "2025-02-27T18:22:50.486Z",
              "total_elapsed_seconds": 1748
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-27T18:33:33.745Z",
            "data": {
              "session_id": "session_1740680583058_es74tdw",
              "duration_seconds": 630,
              "idle_seconds": 0,
              "start_time": "2025-02-27T18:23:03.058Z",
              "end_time": "2025-02-27T18:33:32.759Z",
              "total_elapsed_seconds": 630
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-28T08:50:13.982Z",
            "data": {
              "session_id": "session_1740732605420_y9fub91",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-02-28T08:50:05.420Z",
              "end_time": "2025-02-28T08:50:13.515Z",
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-27T18:22:55+00:00",
        "updated_at": "2025-03-29T22:46:25+00:00",
        "version": 5
      }
    },
    "paper:arxiv.1609.06783": {
      "data": {
        "primary_id": "arxiv.1609.06783",
        "source": "arxiv",
        "sourceId": "1609.06783",
        "url": "https://arxiv.org/abs/1609.06783",
        "title": "Nonparametric Bayesian Topic Modelling with the Hierarchical Pitman-Yor\n  Processes",
        "authors": "Kar Wai Lim, Wray Buntine, Changyou Chen, Lan Du",
        "abstract": "The Dirichlet process and its extension, the Pitman-Yor process, are\nstochastic processes that take probability distributions as a parameter. These\nprocesses can be stacked up to form a hierarchical nonparametric Bayesian\nmodel. In this article, we present efficient methods for the use of these\nprocesses in this hierarchical context, and apply them to latent variable\nmodels for text analytics. In particular, we propose a general framework for\ndesigning these Bayesian models, which are called topic models in the computer\nscience community. We then propose a specific nonparametric Bayesian topic\nmodel for modelling text from social media. We focus on tweets (posts on\nTwitter) in this article due to their ease of access. We find that our\nnonparametric model performs better than existing parametric models in both\ngoodness of fit and real world applications.",
        "timestamp": "2025-02-27T18:22:51.041Z",
        "rating": "novote",
        "arxivId": "1609.06783",
        "arxiv_tags": [
          "stat.ML",
          "cs.CL",
          "cs.LG"
        ],
        "published_date": "2016-09-22T00:10:16Z"
      },
      "meta": {
        "created_at": "2025-02-27T18:22:51+00:00",
        "updated_at": "2025-03-29T22:46:25+00:00",
        "version": 2
      }
    },
    "paper:1609.06783": {
      "data": {
        "arxivId": "1609.06783",
        "url": "https://arxiv.org/abs/1609.06783",
        "title": "Nonparametric Bayesian Topic Modelling with the Hierarchical Pitman-Yor\n  Processes",
        "authors": "Kar Wai Lim, Wray Buntine, Changyou Chen, Lan Du",
        "abstract": "The Dirichlet process and its extension, the Pitman-Yor process, are\nstochastic processes that take probability distributions as a parameter. These\nprocesses can be stacked up to form a hierarchical nonparametric Bayesian\nmodel. In this article, we present efficient methods for the use of these\nprocesses in this hierarchical context, and apply them to latent variable\nmodels for text analytics. In particular, we propose a general framework for\ndesigning these Bayesian models, which are called topic models in the computer\nscience community. We then propose a specific nonparametric Bayesian topic\nmodel for modelling text from social media. We focus on tweets (posts on\nTwitter) in this article due to their ease of access. We find that our\nnonparametric model performs better than existing parametric models in both\ngoodness of fit and real world applications.",
        "timestamp": "2025-02-27T17:53:43.134Z",
        "rating": "novote",
        "published_date": "2016-09-22T00:10:16Z",
        "arxiv_tags": [
          "stat.ML",
          "cs.CL",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-27T17:53:43+00:00",
        "updated_at": "2025-03-29T22:46:25+00:00",
        "version": 2
      }
    },
    "interactions:1210.6738": {
      "data": {
        "paper_id": "1210.6738",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-28T05:42:03.740Z",
            "data": {
              "session_id": "session_1740720638021_61ycvtd",
              "duration_seconds": 681,
              "idle_seconds": 0,
              "start_time": "2025-02-28T05:30:38.021Z",
              "end_time": "2025-02-28T05:41:59.324Z",
              "total_elapsed_seconds": 681
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-28T05:51:36.673Z",
            "data": {
              "session_id": "session_1740721331439_zyhid3a",
              "duration_seconds": 564,
              "idle_seconds": 0,
              "start_time": "2025-02-28T05:42:11.439Z",
              "end_time": "2025-02-28T05:51:35.724Z",
              "total_elapsed_seconds": 564
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-28T05:42:04+00:00",
        "updated_at": "2025-03-29T22:46:23+00:00",
        "version": 5
      }
    },
    "paper:arxiv.1210.6738": {
      "data": {
        "primary_id": "arxiv.1210.6738",
        "source": "arxiv",
        "sourceId": "1210.6738",
        "url": "https://arxiv.org/pdf/1210.6738",
        "title": "Nested Hierarchical Dirichlet Processes",
        "authors": "John Paisley, Chong Wang, David M. Blei, Michael I. Jordan",
        "abstract": "We develop a nested hierarchical Dirichlet process (nHDP) for hierarchical\ntopic modeling. The nHDP is a generalization of the nested Chinese restaurant\nprocess (nCRP) that allows each word to follow its own path to a topic node\naccording to a document-specific distribution on a shared tree. This alleviates\nthe rigid, single-path formulation of the nCRP, allowing a document to more\neasily express thematic borrowings as a random effect. We derive a stochastic\nvariational inference algorithm for the model, in addition to a greedy subtree\nselection method for each document, which allows for efficient inference using\nmassive collections of text documents. We demonstrate our algorithm on 1.8\nmillion documents from The New York Times and 3.3 million documents from\nWikipedia.",
        "timestamp": "2025-02-28T05:41:59.788Z",
        "rating": "novote",
        "arxivId": "1210.6738",
        "arxiv_tags": [
          "stat.ML",
          "cs.LG"
        ],
        "published_date": "2012-10-25T04:25:00Z"
      },
      "meta": {
        "created_at": "2025-02-28T05:42:00+00:00",
        "updated_at": "2025-03-29T22:46:23+00:00",
        "version": 2
      }
    },
    "paper:1210.6738": {
      "data": {
        "arxivId": "1210.6738",
        "url": "https://arxiv.org/pdf/1210.6738",
        "title": "Nested Hierarchical Dirichlet Processes",
        "authors": "John Paisley, Chong Wang, David M. Blei, Michael I. Jordan",
        "abstract": "We develop a nested hierarchical Dirichlet process (nHDP) for hierarchical\ntopic modeling. The nHDP is a generalization of the nested Chinese restaurant\nprocess (nCRP) that allows each word to follow its own path to a topic node\naccording to a document-specific distribution on a shared tree. This alleviates\nthe rigid, single-path formulation of the nCRP, allowing a document to more\neasily express thematic borrowings as a random effect. We derive a stochastic\nvariational inference algorithm for the model, in addition to a greedy subtree\nselection method for each document, which allows for efficient inference using\nmassive collections of text documents. We demonstrate our algorithm on 1.8\nmillion documents from The New York Times and 3.3 million documents from\nWikipedia.",
        "timestamp": "2025-02-28T05:30:35.760Z",
        "rating": "novote",
        "published_date": "2012-10-25T04:25:00Z",
        "arxiv_tags": [
          "stat.ML",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-28T05:30:36+00:00",
        "updated_at": "2025-03-29T22:46:24+00:00",
        "version": 2
      }
    },
    "interactions:1206.5270": {
      "data": {
        "paper_id": "1206.5270",
        "interactions": [
          {
            "type": "rating",
            "timestamp": "2025-02-28T08:44:26.313Z",
            "data": {
              "rating": "thumbsup"
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-28T09:00:22.029Z",
            "data": {
              "session_id": "session_1740733201473_dynz0m3",
              "duration_seconds": 18,
              "idle_seconds": 0,
              "start_time": "2025-02-28T09:00:01.473Z",
              "end_time": "2025-02-28T09:00:19.887Z",
              "total_elapsed_seconds": 18
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-28T09:13:14.413Z",
            "data": {
              "session_id": "session_1740733971461_e2iipb8",
              "duration_seconds": 20,
              "idle_seconds": 0,
              "start_time": "2025-02-28T09:12:51.461Z",
              "end_time": "2025-02-28T09:13:11.478Z",
              "total_elapsed_seconds": 20
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-01T20:31:04.350Z",
            "data": {
              "session_id": "session_1740861034557_1ie4bmx",
              "duration_seconds": 30,
              "idle_seconds": 0,
              "start_time": "2025-03-01T20:30:34.557Z",
              "end_time": "2025-03-01T20:31:04.144Z",
              "total_elapsed_seconds": 30
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-28T08:44:27+00:00",
        "updated_at": "2025-03-29T22:46:21+00:00",
        "version": 8
      }
    },
    "paper:arxiv.1206.5270": {
      "data": {
        "primary_id": "arxiv.1206.5270",
        "source": "arxiv",
        "sourceId": "1206.5270",
        "url": "https://arxiv.org/pdf/1206.5270",
        "title": "Nonparametric Bayes Pachinko Allocation",
        "authors": "Wei Li, David Blei, Andrew McCallum",
        "abstract": "Recent advances in topic models have explored complicated structured\ndistributions to represent topic correlation. For example, the pachinko\nallocation model (PAM) captures arbitrary, nested, and possibly sparse\ncorrelations between topics using a directed acyclic graph (DAG). While PAM\nprovides more flexibility and greater expressive power than previous models\nlike latent Dirichlet allocation (LDA), it is also more difficult to determine\nthe appropriate topic structure for a specific dataset. In this paper, we\npropose a nonparametric Bayesian prior for PAM based on a variant of the\nhierarchical Dirichlet process (HDP). Although the HDP can capture topic\ncorrelations defined by nested data structure, it does not automatically\ndiscover such correlations from unstructured data. By assuming an HDP-based\nprior for PAM, we are able to learn both the number of topics and how the\ntopics are correlated. We evaluate our model on synthetic and real-world text\ndatasets, and show that nonparametric PAM achieves performance matching the\nbest of PAM without manually tuning the number of topics.",
        "timestamp": "2025-02-28T08:44:16.653Z",
        "rating": "thumbsup",
        "arxivId": "1206.5270",
        "arxiv_tags": [
          "cs.IR",
          "cs.LG",
          "stat.ML"
        ],
        "published_date": "2012-06-20T15:04:47Z"
      },
      "meta": {
        "created_at": "2025-02-28T08:44:17+00:00",
        "updated_at": "2025-03-29T22:46:22+00:00",
        "version": 3
      }
    },
    "paper:1206.5270": {
      "data": {
        "arxivId": "1206.5270",
        "url": "https://arxiv.org/pdf/1206.5270",
        "title": "Nonparametric Bayes Pachinko Allocation",
        "authors": "Wei Li, David Blei, Andrew McCallum",
        "abstract": "Recent advances in topic models have explored complicated structured\ndistributions to represent topic correlation. For example, the pachinko\nallocation model (PAM) captures arbitrary, nested, and possibly sparse\ncorrelations between topics using a directed acyclic graph (DAG). While PAM\nprovides more flexibility and greater expressive power than previous models\nlike latent Dirichlet allocation (LDA), it is also more difficult to determine\nthe appropriate topic structure for a specific dataset. In this paper, we\npropose a nonparametric Bayesian prior for PAM based on a variant of the\nhierarchical Dirichlet process (HDP). Although the HDP can capture topic\ncorrelations defined by nested data structure, it does not automatically\ndiscover such correlations from unstructured data. By assuming an HDP-based\nprior for PAM, we are able to learn both the number of topics and how the\ntopics are correlated. We evaluate our model on synthetic and real-world text\ndatasets, and show that nonparametric PAM achieves performance matching the\nbest of PAM without manually tuning the number of topics.",
        "timestamp": "2025-02-28T08:42:30.716Z",
        "rating": "novote",
        "published_date": "2012-06-20T15:04:47Z",
        "arxiv_tags": [
          "cs.IR",
          "cs.LG",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-02-28T08:42:31+00:00",
        "updated_at": "2025-03-29T22:46:22+00:00",
        "version": 2
      }
    },
    "paper:openreview.VSFR5eBP7h": {
      "data": {
        "primary_id": "openreview.VSFR5eBP7h",
        "source": "openreview",
        "sourceId": "VSFR5eBP7h",
        "url": "https://openreview.net/forum?id=VSFR5eBP7h",
        "title": "OPENREVIEW Paper: VSFR5eBP7h",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-02-28T22:04:17.313Z",
        "rating": "novote",
        "identifiers": {
          "original": "VSFR5eBP7h",
          "url": "https://openreview.net/forum?id=VSFR5eBP7h"
        }
      },
      "meta": {
        "created_at": "2025-02-28T22:04:17+00:00",
        "updated_at": "2025-03-29T22:46:19+00:00",
        "version": 2
      }
    },
    "interactions:2410.01131": {
      "data": {
        "paper_id": "2410.01131",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-28T22:03:12.961Z",
            "data": {
              "session_id": "session_1740780162850_4yq9clx",
              "duration_seconds": 29,
              "idle_seconds": 0,
              "start_time": "2025-02-28T22:02:42.850Z",
              "end_time": "2025-02-28T22:03:12.096Z",
              "total_elapsed_seconds": 29
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-28T22:03:14+00:00",
        "updated_at": "2025-03-29T22:46:20+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2410.01131": {
      "data": {
        "primary_id": "arxiv.2410.01131",
        "source": "arxiv",
        "sourceId": "2410.01131",
        "url": "https://arxiv.org/abs/2410.01131",
        "title": "nGPT: Normalized Transformer with Representation Learning on the\n  Hypersphere",
        "authors": "Ilya Loshchilov, Cheng-Ping Hsieh, Simeng Sun, Boris Ginsburg",
        "abstract": "We propose a novel neural network architecture, the normalized Transformer\n(nGPT) with representation learning on the hypersphere. In nGPT, all vectors\nforming the embeddings, MLP, attention matrices and hidden states are unit norm\nnormalized. The input stream of tokens travels on the surface of a hypersphere,\nwith each layer contributing a displacement towards the target output\npredictions. These displacements are defined by the MLP and attention blocks,\nwhose vector components also reside on the same hypersphere. Experiments show\nthat nGPT learns much faster, reducing the number of training steps required to\nachieve the same accuracy by a factor of 4 to 20, depending on the sequence\nlength.",
        "timestamp": "2025-02-28T22:02:42.541Z",
        "rating": "novote",
        "arxivId": "2410.01131",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI"
        ],
        "published_date": "2024-10-01T23:50:09Z"
      },
      "meta": {
        "created_at": "2025-02-28T22:02:43+00:00",
        "updated_at": "2025-03-29T22:46:20+00:00",
        "version": 2
      }
    },
    "interactions:2110.09456": {
      "data": {
        "paper_id": "2110.09456",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-28T22:02:35.133Z",
            "data": {
              "session_id": "session_1740780149736_caiibdt",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-02-28T22:02:29.736Z",
              "end_time": "2025-02-28T22:02:35.123Z",
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-28T22:02:07+00:00",
        "updated_at": "2025-03-29T22:46:21+00:00",
        "version": 5
      }
    },
    "paper:arxiv.2110.09456": {
      "data": {
        "primary_id": "arxiv.2110.09456",
        "source": "arxiv",
        "sourceId": "2110.09456",
        "url": "https://arxiv.org/abs/2110.09456",
        "title": "NormFormer: Improved Transformer Pretraining with Extra Normalization",
        "authors": "Sam Shleifer, Jason Weston, Myle Ott",
        "abstract": "During pretraining, the Pre-LayerNorm transformer suffers from a gradient\nmagnitude mismatch: gradients at early layers are much larger than at later\nlayers. These issues can be alleviated by our proposed NormFormer architecture,\nwhich adds three normalization operations to each layer: a Layer Norm after\nself attention, head-wise scaling of self-attention outputs, and a Layer Norm\nafter the first fully connected layer. The extra operations incur negligible\ncompute cost (+0.4% parameter increase), but improve pretraining perplexity and\ndownstream task performance for both causal and masked language models ranging\nfrom 125 Million to 2.7 Billion parameters. For example, adding NormFormer on\ntop of our strongest 1.3B parameter baseline can reach equal perplexity 24%\nfaster, or converge 0.27 perplexity better in the same compute budget. This\nmodel reaches GPT3-Large (1.3B) zero shot performance 60% faster. For masked\nlanguage modeling, NormFormer improves fine-tuned GLUE performance by 1.9% on\naverage. Code to train NormFormer models is available in fairseq\nhttps://github.com/pytorch/fairseq/tree/main/examples/normformer .",
        "timestamp": "2025-02-28T22:02:02.816Z",
        "rating": "novote",
        "arxivId": "2110.09456",
        "arxiv_tags": [
          "cs.CL",
          "cs.AI"
        ],
        "published_date": "2021-10-18T16:47:45Z"
      },
      "meta": {
        "created_at": "2025-02-28T22:02:03+00:00",
        "updated_at": "2025-03-29T22:46:21+00:00",
        "version": 2
      }
    },
    "interactions:1112.5016": {
      "data": {
        "paper_id": "1112.5016",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-01T00:45:07.182Z",
            "data": {
              "session_id": "session_1740789894318_5oucrsh",
              "duration_seconds": 12,
              "idle_seconds": 0,
              "start_time": "2025-03-01T00:44:54.318Z",
              "end_time": "2025-03-01T00:45:06.385Z",
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-01T00:46:32.410Z",
            "data": {
              "session_id": "session_1740789966895_hxlopri",
              "duration_seconds": 25,
              "idle_seconds": 0,
              "start_time": "2025-03-01T00:46:06.895Z",
              "end_time": "2025-03-01T00:46:31.943Z",
              "total_elapsed_seconds": 25
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-01T04:34:42.661Z",
            "data": {
              "session_id": "session_1740803661385_10461ew",
              "duration_seconds": 19,
              "idle_seconds": 0,
              "start_time": "2025-03-01T04:34:21.385Z",
              "end_time": "2025-03-01T04:34:40.590Z",
              "total_elapsed_seconds": 19
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-01T00:45:08+00:00",
        "updated_at": "2025-03-29T22:46:18+00:00",
        "version": 8
      }
    },
    "paper:arxiv.1112.5016": {
      "data": {
        "primary_id": "arxiv.1112.5016",
        "source": "arxiv",
        "sourceId": "1112.5016",
        "url": "https://arxiv.org/abs/1112.5016",
        "title": "A Scalable Bootstrap for Massive Data",
        "authors": "Ariel Kleiner, Ameet Talwalkar, Purnamrita Sarkar, Michael I. Jordan",
        "abstract": "The bootstrap provides a simple and powerful means of assessing the quality\nof estimators. However, in settings involving large datasets---which are\nincreasingly prevalent---the computation of bootstrap-based quantities can be\nprohibitively demanding computationally. While variants such as subsampling and\nthe $m$ out of $n$ bootstrap can be used in principle to reduce the cost of\nbootstrap computations, we find that these methods are generally not robust to\nspecification of hyperparameters (such as the number of subsampled data\npoints), and they often require use of more prior information (such as rates of\nconvergence of estimators) than the bootstrap. As an alternative, we introduce\nthe Bag of Little Bootstraps (BLB), a new procedure which incorporates features\nof both the bootstrap and subsampling to yield a robust, computationally\nefficient means of assessing the quality of estimators. BLB is well suited to\nmodern parallel and distributed computing architectures and furthermore retains\nthe generic applicability and statistical efficiency of the bootstrap. We\ndemonstrate BLB's favorable statistical performance via a theoretical analysis\nelucidating the procedure's properties, as well as a simulation study comparing\nBLB to the bootstrap, the $m$ out of $n$ bootstrap, and subsampling. In\naddition, we present results from a large-scale distributed implementation of\nBLB demonstrating its computational superiority on massive data, a method for\nadaptively selecting BLB's hyperparameters, an empirical study applying BLB to\nseveral real datasets, and an extension of BLB to time series data.",
        "timestamp": "2025-03-01T00:44:52.424Z",
        "rating": "novote",
        "arxivId": "1112.5016",
        "arxiv_tags": [
          "stat.ME",
          "stat.CO",
          "stat.ML"
        ],
        "published_date": "2011-12-21T13:18:57Z"
      },
      "meta": {
        "created_at": "2025-03-01T00:44:52+00:00",
        "updated_at": "2025-03-29T22:46:18+00:00",
        "version": 2
      }
    },
    "interactions:openreview.R4xpvDTWkV": {
      "data": {
        "paper_id": "openreview.R4xpvDTWkV",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-03-01T07:44:48+00:00",
        "updated_at": "2025-03-29T22:46:00+00:00",
        "version": 2
      }
    },
    "paper:openreview.R4xpvDTWkV": {
      "data": {
        "primary_id": "openreview.R4xpvDTWkV",
        "source": "openreview",
        "sourceId": "R4xpvDTWkV",
        "url": "https://openreview.net/forum?id=R4xpvDTWkV",
        "title": "OPENREVIEW Paper: R4xpvDTWkV",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-01T07:44:36.526Z",
        "rating": "novote",
        "identifiers": {
          "original": "R4xpvDTWkV",
          "url": "https://openreview.net/forum?id=R4xpvDTWkV"
        }
      },
      "meta": {
        "created_at": "2025-03-01T07:44:36+00:00",
        "updated_at": "2025-03-30T08:07:11+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2407.17465": {
      "data": {
        "primary_id": "arxiv.2407.17465",
        "source": "arxiv",
        "sourceId": "2407.17465",
        "url": "https://arxiv.org/abs/2407.17465v2",
        "title": "u-$\u03bc$P: The Unit-Scaled Maximal Update Parametrization",
        "authors": "Charlie Blake, Constantin Eichenberg, Josef Dean, Lukas Balles, Luke Y. Prince, Bj\u00f6rn Deiseroth, Andres Felipe Cruz-Salinas, Carlo Luschi, Samuel Weinbach, Douglas Orr",
        "abstract": "The Maximal Update Parametrization ($\\mu$P) aims to make the optimal\nhyperparameters (HPs) of a model independent of its size, allowing them to be\nswept using a cheap proxy model rather than the full-size target model. We\npresent a new scheme, u-$\\mu$P, which improves upon $\\mu$P by combining it with\nUnit Scaling, a method for designing models that makes them easy to train in\nlow-precision. The two techniques have a natural affinity: $\\mu$P ensures that\nthe scale of activations is independent of model size, and Unit Scaling ensures\nthat activations, weights and gradients begin training with a scale of one.\nThis synthesis opens the door to a simpler scheme, whose default values are\nnear-optimal. This in turn facilitates a more efficient sweeping strategy, with\nu-$\\mu$P models reaching a loss that is equal to or lower than comparable\n$\\mu$P models and working out-of-the-box in FP8.",
        "timestamp": "2025-03-01T07:43:24.368Z",
        "rating": "novote",
        "arxivId": "2407.17465",
        "arxiv_tags": [
          "cs.LG"
        ],
        "published_date": "2024-07-24T17:58:42Z"
      },
      "meta": {
        "created_at": "2025-03-01T07:43:24+00:00",
        "updated_at": "2025-03-29T22:46:17+00:00",
        "version": 2
      }
    },
    "paper:openreview.Vota6rFhBQ": {
      "data": {
        "primary_id": "openreview.Vota6rFhBQ",
        "source": "openreview",
        "sourceId": "Vota6rFhBQ",
        "url": "https://openreview.net/forum?id=Vota6rFhBQ",
        "title": "OPENREVIEW Paper: Vota6rFhBQ",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-01T18:05:11.255Z",
        "rating": "novote",
        "identifiers": {
          "original": "Vota6rFhBQ",
          "url": "https://openreview.net/forum?id=Vota6rFhBQ"
        }
      },
      "meta": {
        "created_at": "2025-03-01T18:05:11+00:00",
        "updated_at": "2025-03-29T22:45:59+00:00",
        "version": 2
      }
    },
    "paper:arxiv.1503.03585": {
      "data": {
        "primary_id": "arxiv.1503.03585",
        "source": "arxiv",
        "sourceId": "1503.03585",
        "url": "https://arxiv.org/pdf/1503.03585",
        "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
        "authors": "Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli",
        "abstract": "A central problem in machine learning involves modeling complex data-sets\nusing highly flexible families of probability distributions in which learning,\nsampling, inference, and evaluation are still analytically or computationally\ntractable. Here, we develop an approach that simultaneously achieves both\nflexibility and tractability. The essential idea, inspired by non-equilibrium\nstatistical physics, is to systematically and slowly destroy structure in a\ndata distribution through an iterative forward diffusion process. We then learn\na reverse diffusion process that restores structure in data, yielding a highly\nflexible and tractable generative model of the data. This approach allows us to\nrapidly learn, sample from, and evaluate probabilities in deep generative\nmodels with thousands of layers or time steps, as well as to compute\nconditional and posterior probabilities under the learned model. We\nadditionally release an open source reference implementation of the algorithm.",
        "timestamp": "2025-03-01T18:02:51.681Z",
        "rating": "novote",
        "arxivId": "1503.03585",
        "arxiv_tags": [
          "cs.LG",
          "cond-mat.dis-nn",
          "q-bio.NC",
          "stat.ML"
        ],
        "published_date": "2015-03-12T04:51:37Z"
      },
      "meta": {
        "created_at": "2025-03-01T18:02:52+00:00",
        "updated_at": "2025-03-29T22:46:00+00:00",
        "version": 2
      }
    },
    "interactions:openreview.Vota6rFhBQ": {
      "data": {
        "paper_id": "openreview.Vota6rFhBQ",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-01T18:06:02.449Z",
            "data": {
              "session_id": "session_1740852338275_8x6iwun",
              "duration_seconds": 24,
              "idle_seconds": 0,
              "start_time": "2025-03-01T18:05:38.275Z",
              "end_time": "2025-03-01T18:06:02.433Z",
              "total_elapsed_seconds": 24
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-01T18:05:32+00:00",
        "updated_at": "2025-03-29T22:45:58+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2107.09133": {
      "data": {
        "primary_id": "arxiv.2107.09133",
        "source": "arxiv",
        "sourceId": "2107.09133",
        "url": "https://arxiv.org/abs/2107.09133",
        "title": "The Limiting Dynamics of SGD: Modified Loss, Phase Space Oscillations,\n  and Anomalous Diffusion",
        "authors": "Daniel Kunin, Javier Sagastuy-Brena, Lauren Gillespie, Eshed Margalit, Hidenori Tanaka, Surya Ganguli, Daniel L. K. Yamins",
        "abstract": "In this work we explore the limiting dynamics of deep neural networks trained\nwith stochastic gradient descent (SGD). As observed previously, long after\nperformance has converged, networks continue to move through parameter space by\na process of anomalous diffusion in which distance travelled grows as a power\nlaw in the number of gradient updates with a nontrivial exponent. We reveal an\nintricate interaction between the hyperparameters of optimization, the\nstructure in the gradient noise, and the Hessian matrix at the end of training\nthat explains this anomalous diffusion. To build this understanding, we first\nderive a continuous-time model for SGD with finite learning rates and batch\nsizes as an underdamped Langevin equation. We study this equation in the\nsetting of linear regression, where we can derive exact, analytic expressions\nfor the phase space dynamics of the parameters and their instantaneous\nvelocities from initialization to stationarity. Using the Fokker-Planck\nequation, we show that the key ingredient driving these dynamics is not the\noriginal training loss, but rather the combination of a modified loss, which\nimplicitly regularizes the velocity, and probability currents, which cause\noscillations in phase space. We identify qualitative and quantitative\npredictions of this theory in the dynamics of a ResNet-18 model trained on\nImageNet. Through the lens of statistical physics, we uncover a mechanistic\norigin for the anomalous limiting dynamics of deep neural networks trained with\nSGD.",
        "timestamp": "2025-03-01T20:28:38.002Z",
        "rating": "novote",
        "arxivId": "2107.09133",
        "arxiv_tags": [
          "cs.LG",
          "cond-mat.stat-mech",
          "q-bio.NC",
          "stat.ML"
        ],
        "published_date": "2021-07-19T20:18:57Z"
      },
      "meta": {
        "created_at": "2025-03-01T20:28:38+00:00",
        "updated_at": "2025-03-29T22:45:58+00:00",
        "version": 2
      }
    },
    "interactions:2502.02737": {
      "data": {
        "paper_id": "arxiv.2502.02737",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-02T08:48:33.075Z",
            "data": {
              "session_id": "session_1740905307632_7w0ik1q",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-03-02T08:48:27.632Z",
              "end_time": "2025-03-02T08:48:32.750Z",
              "total_elapsed_seconds": 5
            }
          }
        ],
        "legacy_id": "2502.02737"
      },
      "meta": {
        "created_at": "2025-03-02T08:48:33+00:00",
        "updated_at": "2025-03-29T22:45:57+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2502.02737": {
      "data": {
        "primary_id": "arxiv.2502.02737",
        "source": "arxiv",
        "sourceId": "2502.02737",
        "url": "https://arxiv.org/abs/2502.02737",
        "title": "SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language\n  Model",
        "authors": "Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Mart\u00edn Bl\u00e1zquez, Guilherme Penedo, Lewis Tunstall, Andr\u00e9s Marafioti, Hynek Kydl\u00ed\u010dek, Agust\u00edn Piqueres Lajar\u00edn, Vaibhav Srivastav, Joshua Lochner, Caleb Fahlgren, Xuan-Son Nguyen, Cl\u00e9mentine Fourrier, Ben Burtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandro von Werra, Thomas Wolf",
        "abstract": "While large language models have facilitated breakthroughs in many\napplications of artificial intelligence, their inherent largeness makes them\ncomputationally expensive and challenging to deploy in resource-constrained\nsettings. In this paper, we document the development of SmolLM2, a\nstate-of-the-art \"small\" (1.7 billion parameter) language model (LM). To attain\nstrong performance, we overtrain SmolLM2 on ~11 trillion tokens of data using a\nmulti-stage training process that mixes web text with specialized math, code,\nand instruction-following data. We additionally introduce new specialized\ndatasets (FineMath, Stack-Edu, and SmolTalk) at stages where we found existing\ndatasets to be problematically small or low-quality. To inform our design\ndecisions, we perform both small-scale ablations as well as a manual refinement\nprocess that updates the dataset mixing rates at each stage based on the\nperformance at the previous stage. Ultimately, we demonstrate that SmolLM2\noutperforms other recent small LMs including Qwen2.5-1.5B and Llama3.2-1B. To\nfacilitate future research on LM development as well as applications of small\nLMs, we release both SmolLM2 as well as all of the datasets we prepared in the\ncourse of this project.",
        "timestamp": "2025-03-02T08:43:26.219Z",
        "rating": "novote",
        "arxivId": "2502.02737",
        "arxiv_tags": [
          "cs.CL"
        ],
        "published_date": "2025-02-04T21:43:16Z"
      },
      "meta": {
        "created_at": "2025-03-02T08:43:26+00:00",
        "updated_at": "2025-03-29T22:45:58+00:00",
        "version": 2
      }
    },
    "interactions:2305.04641": {
      "data": {
        "paper_id": "arxiv.2305.04641",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-03T00:40:40.974Z",
            "data": {
              "session_id": "session_1740962423329_84m208v",
              "duration_seconds": 13,
              "idle_seconds": 0,
              "start_time": "2025-03-03T00:40:23.329Z",
              "end_time": "2025-03-03T00:40:36.115Z",
              "total_elapsed_seconds": 13
            }
          }
        ],
        "legacy_id": "2305.04641"
      },
      "meta": {
        "created_at": "2025-03-03T00:40:37+00:00",
        "updated_at": "2025-03-29T22:45:56+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2305.04641": {
      "data": {
        "primary_id": "arxiv.2305.04641",
        "source": "arxiv",
        "sourceId": "2305.04641",
        "url": "https://arxiv.org/abs/2305.04641",
        "title": "The Cure is in the Cause: A Filesystem for Container Debloating",
        "authors": "Huaifeng Zhang, Mohannad Alhanahnah, Philipp Leitner, Ahmed Ali-Eldin",
        "abstract": "Containers have become a standard for deploying applications due to their\nconvenience, but they often suffer from significant software bloat-unused files\nthat inflate image sizes, increase provisioning times, and waste resources.\nThese inefficiencies are particularly problematic in serverless and edge\ncomputing scenarios, where resources are constrained, and performance is\ncritical. Existing debloating tools are limited in scope and effectiveness,\nfailing to address the widespread issue of container bloat at scale. In this\npaper, we conduct a large-scale evaluation of container bloat, analyzing the\ntop 20 most downloaded containers on DockerHub. We evaluate two\nstate-of-the-art debloating tools, identify their limitations, and propose a\nnovel solution, BAFFS, which addresses bloat at the filesystem level by\nintroducing a flexible debloating layer that preserves the layered structure of\ncontainer filesystems. The debloating layer can be organized in different ways\nto meet diverse requirements. Our evaluation demonstrates that over 50% of the\ntop-downloaded containers have more than 60% bloat, and BAFFS reduces container\nsizes significantly while maintaining functionality. For serverless functions,\nBAFFS reduces cold start latency by up to 68%. Additionally, when combined with\nlazy-loading snapshotters, BAFFS enhances provisioning efficiency, reducing\nconversion times by up to 93% and provisioning times by up to 19%.",
        "timestamp": "2025-03-03T00:04:16.046Z",
        "rating": "novote",
        "arxivId": "2305.04641",
        "arxiv_tags": [
          "cs.SE"
        ],
        "published_date": "2023-05-08T11:41:30Z"
      },
      "meta": {
        "created_at": "2025-03-03T00:04:16+00:00",
        "updated_at": "2025-03-29T22:45:56+00:00",
        "version": 2
      }
    },
    "interactions:2502.12018": {
      "data": {
        "paper_id": "arxiv.2502.12018",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-03T02:58:07.293Z",
            "data": {
              "session_id": "session_1740970676892_9t0a0yr",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-03-03T02:57:56.892Z",
              "end_time": "2025-03-03T02:58:00.439Z",
              "total_elapsed_seconds": 4
            }
          }
        ],
        "legacy_id": "2502.12018"
      },
      "meta": {
        "created_at": "2025-03-03T02:58:02+00:00",
        "updated_at": "2025-03-29T22:45:55+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2502.12018": {
      "data": {
        "primary_id": "arxiv.2502.12018",
        "source": "arxiv",
        "sourceId": "2502.12018",
        "url": "https://arxiv.org/abs/2502.12018",
        "title": "Atom of Thoughts for Markov LLM Test-Time Scaling",
        "authors": "Fengwei Teng, Zhaoyang Yu, Quan Shi, Jiayi Zhang, Chenglin Wu, Yuyu Luo",
        "abstract": "Large Language Models (LLMs) achieve superior performance through\ntraining-time scaling, and test-time scaling further enhances their\ncapabilities by conducting effective reasoning during inference. However, as\nthe scale of reasoning increases, existing test-time scaling methods suffer\nfrom accumulated historical information, which not only wastes computational\nresources but also interferes with effective reasoning. To address this issue,\nwe observe that complex reasoning progress is often achieved by solving a\nsequence of independent subquestions, each being self-contained and verifiable.\nThese subquestions are essentially atomic questions, relying primarily on their\ncurrent state rather than accumulated history, similar to the memoryless\ntransitions in a Markov process. Based on this observation, we propose Atom of\nThoughts (AoT), where each state transition in the reasoning process consists\nof decomposing the current question into a dependency-based directed acyclic\ngraph and contracting its subquestions, forming a new atomic question state.\nThis iterative decomposition-contraction process continues until reaching\ndirectly solvable atomic questions, naturally realizing Markov transitions\nbetween question states. Furthermore, these atomic questions can be seamlessly\nintegrated into existing test-time scaling methods, enabling AoT to serve as a\nplug-in enhancement for improving reasoning capabilities. Experiments across\nsix benchmarks demonstrate the effectiveness of AoT both as a standalone\nframework and a plug-in enhancement. Notably, on HotpotQA, when applied to\ngpt-4o-mini, AoT achieves an 80.6% F1 score, surpassing o3-mini by 3.4% and\nDeepSeek-R1 by 10.6%. The code will be available at\nhttps://github.com/qixucen/atom.",
        "timestamp": "2025-03-03T02:57:57.436Z",
        "rating": "novote",
        "arxivId": "2502.12018",
        "arxiv_tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG"
        ],
        "published_date": "2025-02-17T16:52:42Z"
      },
      "meta": {
        "created_at": "2025-03-03T02:57:57+00:00",
        "updated_at": "2025-03-29T22:45:55+00:00",
        "version": 2
      }
    },
    "interactions:2502.21231": {
      "data": {
        "paper_id": "arxiv.2502.21231",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-03T21:11:21.899Z",
            "data": {
              "session_id": "session_1741036272711_nlmwegu",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-03-03T21:11:12.711Z",
              "end_time": "2025-03-03T21:11:21.162Z",
              "total_elapsed_seconds": 8
            }
          }
        ],
        "legacy_id": "2502.21231"
      },
      "meta": {
        "created_at": "2025-03-03T21:11:22+00:00",
        "updated_at": "2025-03-29T22:45:54+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2502.21231": {
      "data": {
        "primary_id": "arxiv.2502.21231",
        "source": "arxiv",
        "sourceId": "2502.21231",
        "url": "https://arxiv.org/abs/2502.21231",
        "title": "ByteScale: Efficient Scaling of LLM Training with a 2048K Context Length\n  on More Than 12,000 GPUs",
        "authors": "Hao Ge, Junda Feng, Qi Huang, Fangcheng Fu, Xiaonan Nie, Lei Zuo, Haibin Lin, Bin Cui, Xin Liu",
        "abstract": "Scaling long-context ability is essential for Large Language Models (LLMs).\nTo amortize the memory consumption across multiple devices in long-context\ntraining, inter-data partitioning (a.k.a. Data Parallelism) and intra-data\npartitioning (a.k.a. Context Parallelism) are commonly used. Current training\nframeworks predominantly treat the two techniques as orthogonal, and establish\nstatic communication groups to organize the devices as a static mesh (e.g., a\n2D mesh). However, the sequences for LLM training typically vary in lengths, no\nmatter for texts, multi-modalities or reinforcement learning. The mismatch\nbetween data heterogeneity and static mesh causes redundant communication and\nimbalanced computation, degrading the training efficiency.\n  In this work, we introduce ByteScale, an efficient, flexible, and scalable\nLLM training framework for large-scale mixed training of long and short\nsequences. The core of ByteScale is a novel parallelism strategy, namely Hybrid\nData Parallelism (HDP), which unifies the inter- and intra-data partitioning\nwith a dynamic mesh design. In particular, we build a communication optimizer,\nwhich eliminates the redundant communication for short sequences by data-aware\nsharding and dynamic communication, and further compresses the communication\ncost for long sequences by selective offloading. Besides, we also develop a\nbalance scheduler to mitigate the imbalanced computation by parallelism-aware\ndata assignment. We evaluate ByteScale with the model sizes ranging from 7B to\n141B, context lengths from 256K to 2048K, on a production cluster with more\nthan 12,000 GPUs. Experiment results show that ByteScale outperforms the\nstate-of-the-art training system by up to 7.89x.",
        "timestamp": "2025-03-03T21:11:13.328Z",
        "rating": "novote",
        "arxivId": "2502.21231",
        "arxiv_tags": [
          "cs.DC",
          "cs.AI",
          "cs.LG"
        ],
        "published_date": "2025-02-28T17:01:03Z"
      },
      "meta": {
        "created_at": "2025-03-03T21:11:14+00:00",
        "updated_at": "2025-03-29T22:45:54+00:00",
        "version": 2
      }
    },
    "interactions:1511.07948": {
      "data": {
        "paper_id": "arxiv.1511.07948",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-05T07:28:42.634Z",
            "data": {
              "session_id": "session_1741159712079_jh2znhl",
              "duration_seconds": 10,
              "idle_seconds": 0,
              "start_time": "2025-03-05T07:28:32.079Z",
              "end_time": "2025-03-05T07:28:41.920Z",
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-05T16:17:57.004Z",
            "data": {
              "session_id": "session_1741191472230_s93v0ht",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-03-05T16:17:52.230Z",
              "end_time": "2025-03-05T16:17:56.820Z",
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-05T16:17:57.460Z",
            "data": {
              "session_id": "session_1741191472230_s93v0ht",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-03-05T16:17:52.230Z",
              "end_time": "2025-03-05T16:17:56.820Z",
              "total_elapsed_seconds": 5
            }
          }
        ],
        "legacy_id": "1511.07948"
      },
      "meta": {
        "created_at": "2025-03-05T07:28:43+00:00",
        "updated_at": "2025-03-29T22:45:51+00:00",
        "version": 5
      }
    },
    "paper:arxiv.1511.07948": {
      "data": {
        "primary_id": "arxiv.1511.07948",
        "source": "arxiv",
        "sourceId": "1511.07948",
        "url": "https://arxiv.org/abs/1511.07948",
        "title": "Learning Halfspaces and Neural Networks with Random Initialization",
        "authors": "Yuchen Zhang, Jason D. Lee, Martin J. Wainwright, Michael I. Jordan",
        "abstract": "We study non-convex empirical risk minimization for learning halfspaces and\nneural networks. For loss functions that are $L$-Lipschitz continuous, we\npresent algorithms to learn halfspaces and multi-layer neural networks that\nachieve arbitrarily small excess risk $\\epsilon&gt;0$. The time complexity is\npolynomial in the input dimension $d$ and the sample size $n$, but exponential\nin the quantity $(L/\\epsilon^2)\\log(L/\\epsilon)$. These algorithms run multiple\nrounds of random initialization followed by arbitrary optimization steps. We\nfurther show that if the data is separable by some neural network with constant\nmargin $\\gamma&gt;0$, then there is a polynomial-time algorithm for learning a\nneural network that separates the training data with margin $\\Omega(\\gamma)$.\nAs a consequence, the algorithm achieves arbitrary generalization error\n$\\epsilon&gt;0$ with ${\\rm poly}(d,1/\\epsilon)$ sample and time complexity. We\nestablish the same learnability result when the labels are randomly flipped\nwith probability $\\eta&lt;1/2$.",
        "timestamp": "2025-03-05T07:28:32.623Z",
        "rating": "novote",
        "arxivId": "1511.07948",
        "arxiv_tags": [
          "cs.LG"
        ],
        "published_date": "2015-11-25T04:41:20Z"
      },
      "meta": {
        "created_at": "2025-03-05T07:28:33+00:00",
        "updated_at": "2025-03-29T22:45:51+00:00",
        "version": 2
      }
    },
    "paper:arxiv.1805.10694": {
      "data": {
        "primary_id": "arxiv.1805.10694",
        "source": "arxiv",
        "sourceId": "1805.10694",
        "url": "https://arxiv.org/abs/1805.10694",
        "title": "Exponential convergence rates for Batch Normalization: The power of\n  length-direction decoupling in non-convex optimization",
        "authors": "Jonas Kohler, Hadi Daneshmand, Aurelien Lucchi, Ming Zhou, Klaus Neymeyr, Thomas Hofmann",
        "abstract": "Normalization techniques such as Batch Normalization have been applied\nsuccessfully for training deep neural networks. Yet, despite its apparent\nempirical benefits, the reasons behind the success of Batch Normalization are\nmostly hypothetical. We here aim to provide a more thorough theoretical\nunderstanding from a classical optimization perspective. Our main contribution\ntowards this goal is the identification of various problem instances in the\nrealm of machine learning where % -- under certain assumptions-- Batch\nNormalization can provably accelerate optimization. We argue that this\nacceleration is due to the fact that Batch Normalization splits the\noptimization task into optimizing length and direction of the parameters\nseparately. This allows gradient-based methods to leverage a favourable global\nstructure in the loss landscape that we prove to exist in Learning Halfspace\nproblems and neural network training with Gaussian inputs. We thereby turn\nBatch Normalization from an effective practical heuristic into a provably\nconverging algorithm for these settings. Furthermore, we substantiate our\nanalysis with empirical evidence that suggests the validity of our theoretical\nresults in a broader context.",
        "timestamp": "2025-03-05T07:26:53.574Z",
        "rating": "novote",
        "arxivId": "1805.10694",
        "arxiv_tags": [
          "stat.ML",
          "cs.LG"
        ],
        "published_date": "2018-05-27T21:33:37Z"
      },
      "meta": {
        "created_at": "2025-03-05T07:26:54+00:00",
        "updated_at": "2025-03-29T22:45:52+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2401.12053": {
      "data": {
        "primary_id": "arxiv.2401.12053",
        "source": "arxiv",
        "sourceId": "2401.12053",
        "url": "https://arxiv.org/abs/2401.12053",
        "title": "From trust in news to disagreement: is misinformation more\n  controversial?",
        "authors": "Donald Ruggiero Lo Sardo, Emanuele Brugnoli, Enrico Ubaldi, Pietro Gravino, Vittorio Loreto",
        "abstract": "The growing prevalence of fruitless disagreement threatens social cohesion\nand constructive public discourse. While polarised discussions often reflect\ndistrust in the news, the link between disagreement and misinformation remains\nunclear. In this study, we used data from \"Cartesio\", an online experiment\nrating the trustworthiness of Italian news articles annotated for reliability\nby experts, to develop a disagreement metric that accounts for differences in\nmean trust values. Our findings show that while misinformation is rated as less\ntrustworthy, it is not more controversial. Furthermore, disagreement correlates\nwith increased commenting on Facebook. This suggests that combating\nmisinformation alone may not reduce polarisation. Disagreement focuses more on\nthe divergence of opinions, trust, and their effects on social cohesion. Our\nstudy lays the groundwork for unsupervised news analysis and highlights the\nneed for platform design that promotes constructive interactions and reduces\ndivisiveness.",
        "timestamp": "2025-03-04T18:29:19.310Z",
        "rating": "novote",
        "arxivId": "2401.12053",
        "arxiv_tags": [
          "physics.soc-ph"
        ],
        "published_date": "2024-01-22T15:43:14Z"
      },
      "meta": {
        "created_at": "2025-03-04T18:29:19+00:00",
        "updated_at": "2025-03-29T22:45:52+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.00710": {
      "data": {
        "primary_id": "arxiv.2503.00710",
        "source": "arxiv",
        "sourceId": "2503.00710",
        "url": "https://arxiv.org/abs/2503.00710",
        "title": "Proteina: Scaling Flow-based Protein Structure Generative Models",
        "authors": "Tomas Geffner, Kieran Didi, Zuobai Zhang, Danny Reidenbach, Zhonglin Cao, Jason Yim, Mario Geiger, Christian Dallago, Emine Kucukbenli, Arash Vahdat, Karsten Kreis",
        "abstract": "Recently, diffusion- and flow-based generative models of protein structures\nhave emerged as a powerful tool for de novo protein design. Here, we develop\nProteina, a new large-scale flow-based protein backbone generator that utilizes\nhierarchical fold class labels for conditioning and relies on a tailored\nscalable transformer architecture with up to 5x as many parameters as previous\nmodels. To meaningfully quantify performance, we introduce a new set of metrics\nthat directly measure the distributional similarity of generated proteins with\nreference sets, complementing existing metrics. We further explore scaling\ntraining data to millions of synthetic protein structures and explore improved\ntraining and sampling recipes adapted to protein backbone generation. This\nincludes fine-tuning strategies like LoRA for protein backbones, new guidance\nmethods like classifier-free guidance and autoguidance for protein backbones,\nand new adjusted training objectives. Proteina achieves state-of-the-art\nperformance on de novo protein backbone design and produces diverse and\ndesignable proteins at unprecedented length, up to 800 residues. The\nhierarchical conditioning offers novel control, enabling high-level\nsecondary-structure guidance as well as low-level fold-specific generation.",
        "timestamp": "2025-03-04T18:17:53.077Z",
        "rating": "novote",
        "arxivId": "2503.00710",
        "arxiv_tags": [
          "cs.LG"
        ],
        "published_date": "2025-03-02T03:21:49Z"
      },
      "meta": {
        "created_at": "2025-03-04T18:17:53+00:00",
        "updated_at": "2025-03-29T22:45:53+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.01776": {
      "data": {
        "primary_id": "arxiv.2503.01776",
        "source": "arxiv",
        "sourceId": "2503.01776",
        "url": "https://arxiv.org/abs/2503.01776",
        "title": "Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation",
        "authors": "Tiansheng Wen, Yifei Wang, Zequn Zeng, Zhong Peng, Yudi Su, Xinyang Liu, Bo Chen, Hongwei Liu, Stefanie Jegelka, Chenyu You",
        "abstract": "Many large-scale systems rely on high-quality deep representations\n(embeddings) to facilitate tasks like retrieval, search, and generative\nmodeling. Matryoshka Representation Learning (MRL) recently emerged as a\nsolution for adaptive embedding lengths, but it requires full model retraining\nand suffers from noticeable performance degradations at short lengths. In this\npaper, we show that sparse coding offers a compelling alternative for achieving\nadaptive representation with minimal overhead and higher fidelity. We propose\nContrastive Sparse Representation (CSR), a method that sparsifies pre-trained\nembeddings into a high-dimensional but selectively activated feature space. By\nleveraging lightweight autoencoding and task-aware contrastive objectives, CSR\npreserves semantic quality while allowing flexible, cost-effective inference at\ndifferent sparsity levels. Extensive experiments on image, text, and multimodal\nbenchmarks demonstrate that CSR consistently outperforms MRL in terms of both\naccuracy and retrieval speed-often by large margins-while also cutting training\ntime to a fraction of that required by MRL. Our results establish sparse coding\nas a powerful paradigm for adaptive representation learning in real-world\napplications where efficiency and fidelity are both paramount. Code is\navailable at https://github.com/neilwen987/CSR_Adaptive_Rep",
        "timestamp": "2025-03-04T06:13:31.788Z",
        "rating": "novote",
        "arxivId": "2503.01776",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI",
          "cs.CV",
          "cs.IR"
        ],
        "published_date": "2025-03-03T17:59:48Z"
      },
      "meta": {
        "created_at": "2025-03-04T06:13:32+00:00",
        "updated_at": "2025-03-29T22:45:53+00:00",
        "version": 2
      }
    },
    "interactions:2502.17019": {
      "data": {
        "paper_id": "arxiv.2502.17019",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-05T19:10:58.279Z",
            "data": {
              "session_id": "session_1741201852357_ipgyb2p",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-03-05T19:10:52.357Z",
              "end_time": "2025-03-05T19:10:57.496Z",
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-05T19:10:59.473Z",
            "data": {
              "session_id": "session_1741201852357_ipgyb2p",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-03-05T19:10:52.357Z",
              "end_time": "2025-03-05T19:10:57.496Z",
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-05T19:28:11.106Z",
            "data": {
              "session_id": "session_1741202862669_qa0gf6t",
              "duration_seconds": 28,
              "idle_seconds": 0,
              "start_time": "2025-03-05T19:27:42.669Z",
              "end_time": "2025-03-05T19:28:10.781Z",
              "total_elapsed_seconds": 28
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-06T08:43:19.112Z",
            "data": {
              "session_id": "session_1741250594430_yxuczrf",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-03-06T08:43:14.430Z",
              "end_time": "2025-03-06T08:43:19.103Z",
              "total_elapsed_seconds": 5
            }
          }
        ],
        "legacy_id": "2502.17019"
      },
      "meta": {
        "created_at": "2025-03-05T19:10:59+00:00",
        "updated_at": "2025-03-29T22:45:50+00:00",
        "version": 7
      }
    },
    "paper:arxiv.2502.17019": {
      "data": {
        "primary_id": "arxiv.2502.17019",
        "source": "arxiv",
        "sourceId": "2502.17019",
        "url": "https://arxiv.org/abs/2502.17019",
        "title": "Erwin: A Tree-based Hierarchical Transformer for Large-scale Physical\n  Systems",
        "authors": "Maksim Zhdanov, Max Welling, Jan-Willem van de Meent",
        "abstract": "Large-scale physical systems defined on irregular grids pose significant\nscalability challenges for deep learning methods, especially in the presence of\nlong-range interactions and multi-scale coupling. Traditional approaches that\ncompute all pairwise interactions, such as attention, become computationally\nprohibitive as they scale quadratically with the number of nodes. We present\nErwin, a hierarchical transformer inspired by methods from computational\nmany-body physics, which combines the efficiency of tree-based algorithms with\nthe expressivity of attention mechanisms. Erwin employs ball tree partitioning\nto organize computation, which enables linear-time attention by processing\nnodes in parallel within local neighborhoods of fixed size. Through progressive\ncoarsening and refinement of the ball tree structure, complemented by a novel\ncross-ball interaction mechanism, it captures both fine-grained local details\nand global features. We demonstrate Erwin's effectiveness across multiple\ndomains, including cosmology, molecular dynamics, and particle fluid dynamics,\nwhere it consistently outperforms baseline methods both in accuracy and\ncomputational efficiency.",
        "timestamp": "2025-03-05T19:10:52.918Z",
        "rating": "novote",
        "arxivId": "2502.17019",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI",
          "cs.CV"
        ],
        "published_date": "2025-02-24T10:16:55Z"
      },
      "meta": {
        "created_at": "2025-03-05T19:10:53+00:00",
        "updated_at": "2025-03-29T22:45:50+00:00",
        "version": 2
      }
    },
    "interactions:2010.01185": {
      "data": {
        "paper_id": "arxiv.2010.01185",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-06T02:37:44.404Z",
            "data": {
              "session_id": "session_1741228660638_20zwb59",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-03-06T02:37:40.638Z",
              "end_time": "2025-03-06T02:37:44.031Z",
              "total_elapsed_seconds": 3
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-06T02:37:45.092Z",
            "data": {
              "session_id": "session_1741228660638_20zwb59",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-03-06T02:37:40.638Z",
              "end_time": "2025-03-06T02:37:44.031Z",
              "total_elapsed_seconds": 3
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-06T02:37:46.024Z",
            "data": {
              "session_id": "session_1741228660638_20zwb59",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-03-06T02:37:40.638Z",
              "end_time": "2025-03-06T02:37:44.031Z",
              "total_elapsed_seconds": 3
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-06T02:37:47.089Z",
            "data": {
              "session_id": "session_1741228660638_20zwb59",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-03-06T02:37:40.638Z",
              "end_time": "2025-03-06T02:37:44.031Z",
              "total_elapsed_seconds": 3
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-06T02:37:48.411Z",
            "data": {
              "session_id": "session_1741228660638_20zwb59",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-03-06T02:37:40.638Z",
              "end_time": "2025-03-06T02:37:44.031Z",
              "total_elapsed_seconds": 3
            }
          }
        ],
        "legacy_id": "2010.01185"
      },
      "meta": {
        "created_at": "2025-03-06T02:37:45+00:00",
        "updated_at": "2025-03-29T22:45:48+00:00",
        "version": 7
      }
    },
    "paper:arxiv.2103.02096": {
      "data": {
        "primary_id": "arxiv.2103.02096",
        "source": "arxiv",
        "sourceId": "2103.02096",
        "url": "https://arxiv.org/abs/2103.02096",
        "title": "An Alternative Practice of Tropical Convolution to Traditional\n  Convolutional Neural Networks",
        "authors": "Shiqing Fan, Liu Liying, Ye Luo",
        "abstract": "Convolutional neural networks (CNNs) have been used in many machine learning\nfields. In practical applications, the computational cost of convolutional\nneural networks is often high with the deepening of the network and the growth\nof data volume, mostly due to a large amount of multiplication operations of\nfloating-point numbers in convolution operations. To reduce the amount of\nmultiplications, we propose a new type of CNNs called Tropical Convolutional\nNeural Networks (TCNNs) which are built on tropical convolutions in which the\nmultiplications and additions in conventional convolutional layers are replaced\nby additions and min/max operations respectively. In addition, since tropical\nconvolution operators are essentially nonlinear operators, we expect TCNNs to\nhave higher nonlinear fitting ability than conventional CNNs. In the\nexperiments, we test and analyze several different architectures of TCNNs for\nimage classification tasks in comparison with similar-sized conventional CNNs.\nThe results show that TCNN can achieve higher expressive power than ordinary\nconvolutional layers on the MNIST and CIFAR10 image data set. In different\nnoise environments, there are wins and losses in the robustness of TCNN and\nordinary CNNs.",
        "timestamp": "2025-03-06T02:06:18.648Z",
        "rating": "thumbsup",
        "arxivId": "2103.02096",
        "arxiv_tags": [
          "cs.CV",
          "14T90",
          "G.2.1; I.2.6; I.5.1"
        ],
        "published_date": "2021-03-03T00:13:30Z"
      },
      "meta": {
        "created_at": "2025-03-06T02:06:19+00:00",
        "updated_at": "2025-03-29T22:45:49+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2010.01185": {
      "data": {
        "primary_id": "arxiv.2010.01185",
        "source": "arxiv",
        "sourceId": "2010.01185",
        "url": "https://arxiv.org/abs/2010.01185",
        "title": "Compressing Images by Encoding Their Latent Representations with\n  Relative Entropy Coding",
        "authors": "Gergely Flamich, Marton Havasi, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato",
        "abstract": "Variational Autoencoders (VAEs) have seen widespread use in learned image\ncompression. They are used to learn expressive latent representations on which\ndownstream compression methods can operate with high efficiency. Recently\nproposed 'bits-back' methods can indirectly encode the latent representation of\nimages with codelength close to the relative entropy between the latent\nposterior and the prior. However, due to the underlying algorithm, these\nmethods can only be used for lossless compression, and they only achieve their\nnominal efficiency when compressing multiple images simultaneously; they are\ninefficient for compressing single images. As an alternative, we propose a\nnovel method, Relative Entropy Coding (REC), that can directly encode the\nlatent representation with codelength close to the relative entropy for single\nimages, supported by our empirical results obtained on the Cifar10, ImageNet32\nand Kodak datasets. Moreover, unlike previous bits-back methods, REC is\nimmediately applicable to lossy compression, where it is competitive with the\nstate-of-the-art on the Kodak dataset.",
        "timestamp": "2025-03-06T02:06:16.637Z",
        "rating": "novote",
        "arxivId": "2010.01185",
        "arxiv_tags": [
          "cs.IT",
          "eess.IV",
          "math.IT",
          "stat.ML",
          "94A08 (Primary) 94A34 (Secondary)",
          "E.4; G.3; H.1.1"
        ],
        "published_date": "2020-10-02T20:23:22Z"
      },
      "meta": {
        "created_at": "2025-03-06T02:06:17+00:00",
        "updated_at": "2025-03-29T22:45:49+00:00",
        "version": 2
      }
    },
    "interactions:2411.19108": {
      "data": {
        "paper_id": "arxiv.2411.19108",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-06T19:16:42.250Z",
            "data": {
              "session_id": "session_1741288593011_nw0daea",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-03-06T19:16:33.011Z",
              "end_time": "2025-03-06T19:16:41.013Z",
              "total_elapsed_seconds": 8
            }
          }
        ],
        "legacy_id": "2411.19108"
      },
      "meta": {
        "created_at": "2025-03-06T19:16:26+00:00",
        "updated_at": "2025-03-29T22:45:46+00:00",
        "version": 6
      }
    },
    "paper:arxiv.2411.19108": {
      "data": {
        "primary_id": "arxiv.2411.19108",
        "source": "arxiv",
        "sourceId": "2411.19108",
        "url": "https://arxiv.org/abs/2411.19108",
        "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
        "authors": "Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Ye, Fang Wan",
        "abstract": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality.",
        "timestamp": "2025-03-06T19:16:21.213Z",
        "rating": "novote",
        "arxivId": "2411.19108",
        "arxiv_tags": [
          "cs.CV"
        ],
        "published_date": "2024-11-28T12:50:05Z"
      },
      "meta": {
        "created_at": "2025-03-06T19:16:21+00:00",
        "updated_at": "2025-03-29T22:45:46+00:00",
        "version": 2
      }
    },
    "interactions:2503.01307": {
      "data": {
        "paper_id": "arxiv.2503.01307",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-06T19:14:45.770Z",
            "data": {
              "session_id": "session_1741288480983_1n4fzk9",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-03-06T19:14:40.983Z",
              "end_time": "2025-03-06T19:14:44.893Z",
              "total_elapsed_seconds": 4
            }
          }
        ],
        "legacy_id": "2503.01307"
      },
      "meta": {
        "created_at": "2025-03-06T19:14:46+00:00",
        "updated_at": "2025-03-29T22:45:47+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.01307": {
      "data": {
        "primary_id": "arxiv.2503.01307",
        "source": "arxiv",
        "sourceId": "2503.01307",
        "url": "https://arxiv.org/abs/2503.01307",
        "title": "Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four\n  Habits of Highly Effective STaRs",
        "authors": "Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, Noah D. Goodman",
        "abstract": "Test-time inference has emerged as a powerful paradigm for enabling language\nmodels to ``think'' longer and more carefully about complex challenges, much\nlike skilled human experts. While reinforcement learning (RL) can drive\nself-improvement in language models on verifiable tasks, some models exhibit\nsubstantial gains while others quickly plateau. For instance, we find that\nQwen-2.5-3B far exceeds Llama-3.2-3B under identical RL training for the game\nof Countdown. This discrepancy raises a critical question: what intrinsic\nproperties enable effective self-improvement? We introduce a framework to\ninvestigate this question by analyzing four key cognitive behaviors --\nverification, backtracking, subgoal setting, and backward chaining -- that both\nexpert human problem solvers and successful language models employ. Our study\nreveals that Qwen naturally exhibits these reasoning behaviors, whereas Llama\ninitially lacks them. In systematic experimentation with controlled behavioral\ndatasets, we find that priming Llama with examples containing these reasoning\nbehaviors enables substantial improvements during RL, matching or exceeding\nQwen's performance. Importantly, the presence of reasoning behaviors, rather\nthan correctness of answers, proves to be the critical factor -- models primed\nwith incorrect solutions containing proper reasoning patterns achieve\ncomparable performance to those trained on correct solutions. Finally,\nleveraging continued pretraining with OpenWebMath data, filtered to amplify\nreasoning behaviors, enables the Llama model to match Qwen's self-improvement\ntrajectory. Our findings establish a fundamental relationship between initial\nreasoning behaviors and the capacity for improvement, explaining why some\nlanguage models effectively utilize additional computation while others\nplateau.",
        "timestamp": "2025-03-06T19:14:41.546Z",
        "rating": "novote",
        "arxivId": "2503.01307",
        "arxiv_tags": [
          "cs.CL",
          "cs.LG"
        ],
        "published_date": "2025-03-03T08:46:22Z"
      },
      "meta": {
        "created_at": "2025-03-06T19:14:42+00:00",
        "updated_at": "2025-03-29T22:45:47+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.02812": {
      "data": {
        "primary_id": "arxiv.2503.02812",
        "source": "arxiv",
        "sourceId": "2503.02812",
        "url": "https://arxiv.org/abs/2503.02812",
        "title": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression",
        "authors": "Nathan Godey, Alessio Devoto, Yu Zhao, Simone Scardapane, Pasquale Minervini, \u00c9ric de la Clergerie, Beno\u00eet Sagot",
        "abstract": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM.",
        "timestamp": "2025-03-06T17:00:20.510Z",
        "rating": "novote",
        "arxivId": "2503.02812",
        "arxiv_tags": [
          "cs.CL",
          "cs.AI"
        ],
        "published_date": "2025-03-04T17:37:49Z"
      },
      "meta": {
        "created_at": "2025-03-06T17:00:21+00:00",
        "updated_at": "2025-03-29T22:45:48+00:00",
        "version": 2
      }
    },
    "interactions:2411.10958": {
      "data": {
        "paper_id": "arxiv.2411.10958",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-06T19:22:28.470Z",
            "data": {
              "session_id": "session_1741288943701_4mwa1pd",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-03-06T19:22:23.701Z",
              "end_time": "2025-03-06T19:22:27.723Z",
              "total_elapsed_seconds": 4
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-06T19:22:32.284Z",
            "data": {
              "session_id": "session_1741288943701_4mwa1pd",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-03-06T19:22:23.701Z",
              "end_time": "2025-03-06T19:22:27.723Z",
              "total_elapsed_seconds": 4
            }
          }
        ],
        "legacy_id": "2411.10958"
      },
      "meta": {
        "created_at": "2025-03-06T19:22:29+00:00",
        "updated_at": "2025-03-29T22:45:44+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2411.10958": {
      "data": {
        "primary_id": "arxiv.2411.10958",
        "source": "arxiv",
        "sourceId": "2411.10958",
        "url": "https://arxiv.org/abs/2411.10958",
        "title": "SageAttention2: Efficient Attention with Thorough Outlier Smoothing and\n  Per-thread INT4 Quantization",
        "authors": "Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, Jianfei Chen",
        "abstract": "Although quantization for linear layers has been widely used, its application\nto accelerate the attention process remains limited. To further enhance the\nefficiency of attention computation compared to SageAttention while maintaining\nprecision, we propose SageAttention2, which utilizes significantly faster 4-bit\nmatrix multiplication (Matmul) alongside additional precision-enhancing\ntechniques. First, we propose to quantize matrices $(Q, K)$ to INT4 in a\nhardware-friendly thread-level granularity and quantize matrices $(\\widetilde\nP, V)$ to FP8. Second, we propose a method to smooth $Q$, enhancing the\naccuracy of INT4 $QK^\\top$. Third, we propose a two-level accumulation strategy\nfor $\\widetilde PV$ to enhance the accuracy of FP8 $\\widetilde PV$. The\noperations per second (OPS) of SageAttention2 surpass FlashAttention2 and\nxformers by about 3x and 4.5x on RTX4090, respectively. Moreover,\nSageAttention2 matches the speed of FlashAttention3(fp8) on the Hopper GPUs,\nwhile delivering much higher accuracy. Comprehensive experiments confirm that\nour approach incurs negligible end-to-end metrics loss across diverse models,\nincluding those for language, image, and video generation. The code is\navailable at https://github.com/thu-ml/SageAttention.",
        "timestamp": "2025-03-06T19:22:24.293Z",
        "rating": "novote",
        "arxivId": "2411.10958",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI",
          "cs.CV",
          "cs.NE",
          "cs.PF"
        ],
        "published_date": "2024-11-17T04:35:49Z"
      },
      "meta": {
        "created_at": "2025-03-06T19:22:24+00:00",
        "updated_at": "2025-03-29T22:45:44+00:00",
        "version": 2
      }
    },
    "interactions:2410.02367": {
      "data": {
        "paper_id": "arxiv.2410.02367",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-06T19:20:17.621Z",
            "data": {
              "session_id": "session_1741288806527_3xdplqc",
              "duration_seconds": 10,
              "idle_seconds": 0,
              "start_time": "2025-03-06T19:20:06.527Z",
              "end_time": "2025-03-06T19:20:16.857Z",
              "total_elapsed_seconds": 10
            }
          }
        ],
        "legacy_id": "2410.02367"
      },
      "meta": {
        "created_at": "2025-03-06T19:20:18+00:00",
        "updated_at": "2025-03-29T22:45:45+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2410.02367": {
      "data": {
        "primary_id": "arxiv.2410.02367",
        "source": "arxiv",
        "sourceId": "2410.02367",
        "url": "https://arxiv.org/abs/2410.02367",
        "title": "SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference\n  Acceleration",
        "authors": "Jintao Zhang, Jia wei, Haofeng Huang, Pengle Zhang, Jun Zhu, Jianfei Chen",
        "abstract": "The transformer architecture predominates across various models. As the heart\nof the transformer, attention has a computational complexity of O(N^2),\ncompared to O(N) for linear transformations. When handling large sequence\nlengths, attention becomes the primary time-consuming component. Although\nquantization has proven to be an effective method for accelerating model\ninference, existing quantization methods primarily focus on optimizing the\nlinear layer. In response, we first analyze the feasibility of quantization in\nattention detailedly. Following that, we propose SageAttention, a highly\nefficient and accurate quantization method for attention. The OPS (operations\nper second) of our approach outperforms FlashAttention2 and xformers by about\n2.1 times and 2.7 times, respectively. SageAttention also achieves superior\naccuracy performance over FlashAttention3. Comprehensive experiments confirm\nthat our approach incurs almost no end-to-end metrics loss across diverse\nmodels, including those for large language processing, image generation, and\nvideo generation. The codes are available at\nhttps://github.com/thu-ml/SageAttention.",
        "timestamp": "2025-03-06T19:20:07.058Z",
        "rating": "novote",
        "arxivId": "2410.02367",
        "arxiv_tags": [
          "cs.LG"
        ],
        "published_date": "2024-10-03T10:25:23Z"
      },
      "meta": {
        "created_at": "2025-03-06T19:20:07+00:00",
        "updated_at": "2025-03-29T22:45:46+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2102.12321": {
      "data": {
        "primary_id": "arxiv.2102.12321",
        "source": "arxiv",
        "sourceId": "2102.12321",
        "url": "https://arxiv.org/pdf/2102.12321",
        "title": "AGENT: A Benchmark for Core Psychological Reasoning",
        "authors": "Tianmin Shu, Abhishek Bhandwaldar, Chuang Gan, Kevin A. Smith, Shari Liu, Dan Gutfreund, Elizabeth Spelke, Joshua B. Tenenbaum, Tomer D. Ullman",
        "abstract": "For machine agents to successfully interact with humans in real-world\nsettings, they will need to develop an understanding of human mental life.\nIntuitive psychology, the ability to reason about hidden mental variables that\ndrive observable actions, comes naturally to people: even pre-verbal infants\ncan tell agents from objects, expecting agents to act efficiently to achieve\ngoals given constraints. Despite recent interest in machine agents that reason\nabout other agents, it is not clear if such agents learn or hold the core\npsychology principles that drive human reasoning. Inspired by cognitive\ndevelopment studies on intuitive psychology, we present a benchmark consisting\nof a large dataset of procedurally generated 3D animations, AGENT (Action,\nGoal, Efficiency, coNstraint, uTility), structured around four scenarios (goal\npreferences, action efficiency, unobserved constraints, and cost-reward\ntrade-offs) that probe key concepts of core intuitive psychology. We validate\nAGENT with human-ratings, propose an evaluation protocol emphasizing\ngeneralization, and compare two strong baselines built on Bayesian inverse\nplanning and a Theory of Mind neural network. Our results suggest that to pass\nthe designed tests of core intuitive psychology at human levels, a model must\nacquire or have built-in representations of how agents plan, combining utility\ncomputations and core knowledge of objects and physics.",
        "timestamp": "2025-03-06T19:49:50.136Z",
        "rating": "novote",
        "arxivId": "2102.12321",
        "arxiv_tags": [
          "cs.AI",
          "cs.CV",
          "cs.LG"
        ],
        "published_date": "2021-02-24T14:58:23Z"
      },
      "meta": {
        "created_at": "2025-03-06T19:49:50+00:00",
        "updated_at": "2025-03-29T22:45:43+00:00",
        "version": 2
      }
    },
    "interactions:2502.21098": {
      "data": {
        "paper_id": "arxiv.2502.21098",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-06T19:47:38.463Z",
            "data": {
              "session_id": "session_1741290432635_5ey9e1n",
              "duration_seconds": 25,
              "idle_seconds": 0,
              "start_time": "2025-03-06T19:47:12.635Z",
              "end_time": "2025-03-06T19:47:38.072Z",
              "total_elapsed_seconds": 25
            }
          }
        ],
        "legacy_id": "2502.21098"
      },
      "meta": {
        "created_at": "2025-03-06T19:47:39+00:00",
        "updated_at": "2025-03-29T22:45:43+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2502.21098": {
      "data": {
        "primary_id": "arxiv.2502.21098",
        "source": "arxiv",
        "sourceId": "2502.21098",
        "url": "https://arxiv.org/abs/2502.21098",
        "title": "Re-evaluating Theory of Mind evaluation in large language models",
        "authors": "Jennifer Hu, Felix Sosa, Tomer Ullman",
        "abstract": "The question of whether large language models (LLMs) possess Theory of Mind\n(ToM) -- often defined as the ability to reason about others' mental states --\nhas sparked significant scientific and public interest. However, the evidence\nas to whether LLMs possess ToM is mixed, and the recent growth in evaluations\nhas not resulted in a convergence. Here, we take inspiration from cognitive\nscience to re-evaluate the state of ToM evaluation in LLMs. We argue that a\nmajor reason for the disagreement on whether LLMs have ToM is a lack of clarity\non whether models should be expected to match human behaviors, or the\ncomputations underlying those behaviors. We also highlight ways in which\ncurrent evaluations may be deviating from \"pure\" measurements of ToM abilities,\nwhich also contributes to the confusion. We conclude by discussing several\ndirections for future research, including the relationship between ToM and\npragmatic communication, which could advance our understanding of artificial\nsystems as well as human cognition.",
        "timestamp": "2025-03-06T19:45:02.464Z",
        "rating": "novote",
        "arxivId": "2502.21098",
        "arxiv_tags": [
          "cs.AI",
          "cs.CL"
        ],
        "published_date": "2025-02-28T14:36:57Z"
      },
      "meta": {
        "created_at": "2025-03-06T19:45:03+00:00",
        "updated_at": "2025-03-29T22:45:44+00:00",
        "version": 2
      }
    },
    "interactions:2102.12321": {
      "data": {
        "paper_id": "arxiv.2102.12321",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-06T19:50:05.820Z",
            "data": {
              "session_id": "session_1741290589452_bsgiqnm",
              "duration_seconds": 16,
              "idle_seconds": 0,
              "start_time": "2025-03-06T19:49:49.452Z",
              "end_time": "2025-03-06T19:50:05.018Z",
              "total_elapsed_seconds": 16
            }
          }
        ],
        "legacy_id": "2102.12321"
      },
      "meta": {
        "created_at": "2025-03-06T19:50:07+00:00",
        "updated_at": "2025-03-29T22:45:42+00:00",
        "version": 3
      }
    },
    "interactions:2403.07183": {
      "data": {
        "paper_id": "arxiv.2403.07183",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-06T19:56:15.711Z",
            "data": {
              "session_id": "session_1741290965718_dhtlfgr",
              "duration_seconds": 9,
              "idle_seconds": 0,
              "start_time": "2025-03-06T19:56:05.718Z",
              "end_time": "2025-03-06T19:56:14.977Z",
              "total_elapsed_seconds": 9
            }
          }
        ],
        "legacy_id": "2403.07183"
      },
      "meta": {
        "created_at": "2025-03-06T19:56:16+00:00",
        "updated_at": "2025-03-29T22:45:41+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2403.07183": {
      "data": {
        "primary_id": "arxiv.2403.07183",
        "source": "arxiv",
        "sourceId": "2403.07183",
        "url": "https://arxiv.org/abs/2403.07183",
        "title": "Monitoring AI-Modified Content at Scale: A Case Study on the Impact of\n  ChatGPT on AI Conference Peer Reviews",
        "authors": "Weixin Liang, Zachary Izzo, Yaohui Zhang, Haley Lepp, Hancheng Cao, Xuandong Zhao, Lingjiao Chen, Haotian Ye, Sheng Liu, Zhi Huang, Daniel A. McFarland, James Y. Zou",
        "abstract": "We present an approach for estimating the fraction of text in a large corpus\nwhich is likely to be substantially modified or produced by a large language\nmodel (LLM). Our maximum likelihood model leverages expert-written and\nAI-generated reference texts to accurately and efficiently examine real-world\nLLM-use at the corpus level. We apply this approach to a case study of\nscientific peer review in AI conferences that took place after the release of\nChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest\nthat between 6.5% and 16.9% of text submitted as peer reviews to these\nconferences could have been substantially modified by LLMs, i.e. beyond\nspell-checking or minor writing updates. The circumstances in which generated\ntext occurs offer insight into user behavior: the estimated fraction of\nLLM-generated text is higher in reviews which report lower confidence, were\nsubmitted close to the deadline, and from reviewers who are less likely to\nrespond to author rebuttals. We also observe corpus-level trends in generated\ntext which may be too subtle to detect at the individual level, and discuss the\nimplications of such trends on peer review. We call for future\ninterdisciplinary work to examine how LLM use is changing our information and\nknowledge practices.",
        "timestamp": "2025-03-06T19:56:06.153Z",
        "rating": "novote",
        "arxivId": "2403.07183",
        "arxiv_tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG",
          "cs.SI",
          "I.2.7"
        ],
        "published_date": "2024-03-11T21:51:39Z"
      },
      "meta": {
        "created_at": "2025-03-06T19:56:06+00:00",
        "updated_at": "2025-03-29T22:45:41+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2502.09747": {
      "data": {
        "primary_id": "arxiv.2502.09747",
        "source": "arxiv",
        "sourceId": "2502.09747",
        "url": "https://arxiv.org/abs/2502.09747",
        "title": "The Widespread Adoption of Large Language Model-Assisted Writing Across\n  Society",
        "authors": "Weixin Liang, Yaohui Zhang, Mihai Codreanu, Jiayu Wang, Hancheng Cao, James Zou",
        "abstract": "The recent advances in large language models (LLMs) attracted significant\npublic and policymaker interest in its adoption patterns. In this paper, we\nsystematically analyze LLM-assisted writing across four domains-consumer\ncomplaints, corporate communications, job postings, and international\norganization press releases-from January 2022 to September 2024. Our dataset\nincludes 687,241 consumer complaints, 537,413 corporate press releases, 304.3\nmillion job postings, and 15,919 United Nations (UN) press releases. Using a\nrobust population-level statistical framework, we find that LLM usage surged\nfollowing the release of ChatGPT in November 2022. By late 2024, roughly 18% of\nfinancial consumer complaint text appears to be LLM-assisted, with adoption\npatterns spread broadly across regions and slightly higher in urban areas. For\ncorporate press releases, up to 24% of the text is attributable to LLMs. In job\npostings, LLM-assisted writing accounts for just below 10% in small firms, and\nis even more common among younger firms. UN press releases also reflect this\ntrend, with nearly 14% of content being generated or modified by LLMs. Although\nadoption climbed rapidly post-ChatGPT, growth appears to have stabilized by\n2024, reflecting either saturation in LLM adoption or increasing subtlety of\nmore advanced models. Our study shows the emergence of a new reality in which\nfirms, consumers and even international organizations substantially rely on\ngenerative AI for communications.",
        "timestamp": "2025-03-06T19:52:48.985Z",
        "rating": "novote",
        "arxivId": "2502.09747",
        "arxiv_tags": [
          "cs.CL"
        ],
        "published_date": "2025-02-13T20:07:03Z"
      },
      "meta": {
        "created_at": "2025-03-06T19:52:49+00:00",
        "updated_at": "2025-03-29T22:45:42+00:00",
        "version": 2
      }
    },
    "interactions:1408.6944": {
      "data": {
        "paper_id": "arxiv.1408.6944",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-06T20:28:04.968Z",
            "data": {
              "session_id": "session_1741292860029_v5h9nh8",
              "duration_seconds": 24,
              "idle_seconds": 0,
              "start_time": "2025-03-06T20:27:40.029Z",
              "end_time": "2025-03-06T20:28:04.121Z",
              "total_elapsed_seconds": 24
            }
          }
        ],
        "legacy_id": "1408.6944"
      },
      "meta": {
        "created_at": "2025-03-06T20:28:05+00:00",
        "updated_at": "2025-03-29T22:45:39+00:00",
        "version": 3
      }
    },
    "paper:arxiv.1408.6944": {
      "data": {
        "primary_id": "arxiv.1408.6944",
        "source": "arxiv",
        "sourceId": "1408.6944",
        "url": "https://arxiv.org/abs/1408.6944",
        "title": "An introduction to Mandelbrot cascades",
        "authors": "Yanick Heurteaux",
        "abstract": "In this course, we propose an elementary and self-contained introduction to\ncanonical Mandelbrot random cascades. The multiplicative construction is\nexplained and the necessary and sufficient condition of non-degeneracy is\nproved. Then, we discuss the problem of the existence of moments and the link\nwith nondegeneracy. We also calculate the almost sure dimension of the\nmeasures. Finally, we give an outline on multifractal analysis of Mandelbrot\ncascades. This course was delivered in september 2013 during a meeting of the\n\"Multifractal Analysis GDR\" (GDR no 3475 of the french CNRS).",
        "timestamp": "2025-03-06T20:27:40.602Z",
        "rating": "novote",
        "arxivId": "1408.6944",
        "arxiv_tags": [
          "math.PR"
        ],
        "published_date": "2014-08-29T07:57:08Z"
      },
      "meta": {
        "created_at": "2025-03-06T20:27:41+00:00",
        "updated_at": "2025-03-29T22:45:39+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.02921": {
      "data": {
        "primary_id": "arxiv.2503.02921",
        "source": "arxiv",
        "sourceId": "2503.02921",
        "url": "https://arxiv.org/abs/2503.02921",
        "title": "Applications of Entropy in Data Analysis and Machine Learning: A Review",
        "authors": "Salom\u00e9 A. Sep\u00faveda Fontaine, Jos\u00e9 M. Amig\u00f3",
        "abstract": "Since its origin in the thermodynamics of the 19th century, the concept of\nentropy has also permeated other fields of physics and mathematics, such as\nClassical and Quantum Statistical Mechanics, Information Theory, Probability\nTheory, Ergodic Theory and the Theory of Dynamical Systems. Specifically, we\nare referring to the classical entropies: the Boltzmann-Gibbs, von Neumann,\nShannon, Kolmogorov-Sinai and topological entropies. In addition to their\ncommon name, which is historically justified (as we briefly describe in this\nreview), other commonality of the classical entropies is the important role\nthat they have played and are still playing in the theory and applications of\ntheir respective fields and beyond. Therefore, it is not surprising that, in\nthe course of time, many other instances of the overarching concept of entropy\nhave been proposed, most of them tailored to specific purposes. Following the\ncurrent usage, we will refer to all of them, whether classical or new, simply\nas entropies. Precisely, the subject of this review is their applications in\ndata analysis and machine learning. The reason for these particular\napplications is that entropies are very well suited to characterize probability\nmass distributions, typically generated by finite-state processes or symbolized\nsignals. Therefore, we will focus on entropies defined as positive functionals\non probability mass distributions and provide an axiomatic characterization\nthat goes back to Shannon and Khinchin. Given the plethora of entropies in the\nliterature, we have selected a representative group, including the classical\nones. The applications summarized in this review finely illustrate the power\nand versatility of entropy in data analysis and machine learning.",
        "timestamp": "2025-03-06T19:59:35.421Z",
        "rating": "novote",
        "arxivId": "2503.02921",
        "arxiv_tags": [
          "stat.ML",
          "cs.LG",
          "math.PR",
          "Primary: 94A16, Secondary: 62R07"
        ],
        "published_date": "2025-03-04T17:43:48Z"
      },
      "meta": {
        "created_at": "2025-03-06T19:59:35+00:00",
        "updated_at": "2025-03-29T22:45:40+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2501.11457": {
      "data": {
        "primary_id": "arxiv.2501.11457",
        "source": "arxiv",
        "sourceId": "2501.11457",
        "url": "https://arxiv.org/abs/2501.11457",
        "title": "Governance of Generative AI in Creative Work: Consent, Credit,\n  Compensation, and Beyond",
        "authors": "Lin Kyi, Amruta Mahuli, M. Six Silberman, Reuben Binns, Jun Zhao, Asia J. Biega",
        "abstract": "Since the emergence of generative AI, creative workers have spoken up about\nthe career-based harms they have experienced arising from this new technology.\nA common theme in these accounts of harm is that generative AI models are\ntrained on workers' creative output without their consent and without giving\ncredit or compensation to the original creators.\n  This paper reports findings from 20 interviews with creative workers in three\ndomains: visual art and design, writing, and programming. We investigate the\ngaps between current AI governance strategies, what creative workers want out\nof generative AI governance, and the nuanced role of creative workers' consent,\ncompensation and credit for training AI models on their work. Finally, we make\nrecommendations for how generative AI can be governed and how operators of\ngenerative AI systems might more ethically train models on creative output in\nthe future.",
        "timestamp": "2025-03-06T20:55:11.910Z",
        "rating": "novote",
        "arxivId": "2501.11457",
        "arxiv_tags": [
          "cs.HC"
        ],
        "published_date": "2025-01-20T12:44:13Z"
      },
      "meta": {
        "created_at": "2025-03-06T20:55:12+00:00",
        "updated_at": "2025-03-29T22:45:37+00:00",
        "version": 2
      }
    },
    "interactions:2411.04368": {
      "data": {
        "paper_id": "arxiv.2411.04368",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-06T20:53:22.875Z",
            "data": {
              "session_id": "session_1741294376775_wtb2ewg",
              "duration_seconds": 25,
              "idle_seconds": 0,
              "start_time": "2025-03-06T20:52:56.775Z",
              "end_time": "2025-03-06T20:53:22.048Z",
              "total_elapsed_seconds": 25
            }
          }
        ],
        "legacy_id": "2411.04368"
      },
      "meta": {
        "created_at": "2025-03-06T20:53:23+00:00",
        "updated_at": "2025-03-29T22:45:38+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2411.04368": {
      "data": {
        "primary_id": "arxiv.2411.04368",
        "source": "arxiv",
        "sourceId": "2411.04368",
        "url": "https://arxiv.org/pdf/2411.04368",
        "title": "Measuring short-form factuality in large language models",
        "authors": "Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, William Fedus",
        "abstract": "We present SimpleQA, a benchmark that evaluates the ability of language\nmodels to answer short, fact-seeking questions. We prioritized two properties\nin designing this eval. First, SimpleQA is challenging, as it is adversarially\ncollected against GPT-4 responses. Second, responses are easy to grade, because\nquestions are created such that there exists only a single, indisputable\nanswer. Each answer in SimpleQA is graded as either correct, incorrect, or not\nattempted. A model with ideal behavior would get as many questions correct as\npossible while not attempting the questions for which it is not confident it\nknows the correct answer. SimpleQA is a simple, targeted evaluation for whether\nmodels \"know what they know,\" and our hope is that this benchmark will remain\nrelevant for the next few generations of frontier models. SimpleQA can be found\nat https://github.com/openai/simple-evals.",
        "timestamp": "2025-03-06T20:52:57.347Z",
        "rating": "novote",
        "arxivId": "2411.04368",
        "arxiv_tags": [
          "cs.CL"
        ],
        "published_date": "2024-11-07T01:58:42Z"
      },
      "meta": {
        "created_at": "2025-03-06T20:52:57+00:00",
        "updated_at": "2025-03-29T22:45:38+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.03312": {
      "data": {
        "primary_id": "arxiv.2503.03312",
        "source": "arxiv",
        "sourceId": "2503.03312",
        "url": "https://arxiv.org/abs/2503.03312",
        "title": "How manipulable are prediction markets?",
        "authors": "Itzhak Rasooly, Roberto Rozzi",
        "abstract": "In this paper, we conduct a large-scale field experiment to investigate the\nmanipulability of prediction markets. The main experiment involves randomly\nshocking prices across 817 separate markets; we then collect hourly price data\nto examine whether the effects of these shocks persist over time. We find that\nprediction markets can be manipulated: the effects of our trades are visible\neven 60 days after they have occurred. However, as predicted by our model, the\neffects of the manipulations somewhat fade over time. Markets with more\ntraders, greater trading volume, and an external source of probability\nestimates are harder to manipulate.",
        "timestamp": "2025-03-06T20:59:45.013Z",
        "rating": "novote",
        "arxivId": "2503.03312",
        "arxiv_tags": [
          "econ.GN",
          "q-fin.EC"
        ],
        "published_date": "2025-03-05T09:44:56Z"
      },
      "meta": {
        "created_at": "2025-03-06T20:59:45+00:00",
        "updated_at": "2025-03-29T22:45:37+00:00",
        "version": 2
      }
    },
    "interactions:2501.11457": {
      "data": {
        "paper_id": "arxiv.2501.11457",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-06T20:58:17.156Z",
            "data": {
              "session_id": "session_1741294672856_h7q5b5o",
              "duration_seconds": 24,
              "idle_seconds": 0,
              "start_time": "2025-03-06T20:57:52.856Z",
              "end_time": "2025-03-06T20:58:17.139Z",
              "total_elapsed_seconds": 24
            }
          }
        ],
        "legacy_id": "2501.11457"
      },
      "meta": {
        "created_at": "2025-03-06T20:58:18+00:00",
        "updated_at": "2025-03-29T22:45:37+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2304.03442": {
      "data": {
        "primary_id": "arxiv.2304.03442",
        "source": "arxiv",
        "sourceId": "2304.03442",
        "url": "https://arxiv.org/abs/2304.03442",
        "title": "Generative Agents: Interactive Simulacra of Human Behavior",
        "authors": "Joon Sung Park, Joseph C. O'Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, Michael S. Bernstein",
        "abstract": "Believable proxies of human behavior can empower interactive applications\nranging from immersive environments to rehearsal spaces for interpersonal\ncommunication to prototyping tools. In this paper, we introduce generative\nagents--computational software agents that simulate believable human behavior.\nGenerative agents wake up, cook breakfast, and head to work; artists paint,\nwhile authors write; they form opinions, notice each other, and initiate\nconversations; they remember and reflect on days past as they plan the next\nday. To enable generative agents, we describe an architecture that extends a\nlarge language model to store a complete record of the agent's experiences\nusing natural language, synthesize those memories over time into higher-level\nreflections, and retrieve them dynamically to plan behavior. We instantiate\ngenerative agents to populate an interactive sandbox environment inspired by\nThe Sims, where end users can interact with a small town of twenty five agents\nusing natural language. In an evaluation, these generative agents produce\nbelievable individual and emergent social behaviors: for example, starting with\nonly a single user-specified notion that one agent wants to throw a Valentine's\nDay party, the agents autonomously spread invitations to the party over the\nnext two days, make new acquaintances, ask each other out on dates to the\nparty, and coordinate to show up for the party together at the right time. We\ndemonstrate through ablation that the components of our agent\narchitecture--observation, planning, and reflection--each contribute critically\nto the believability of agent behavior. By fusing large language models with\ncomputational, interactive agents, this work introduces architectural and\ninteraction patterns for enabling believable simulations of human behavior.",
        "timestamp": "2025-03-06T21:02:03.006Z",
        "rating": "novote",
        "arxivId": "2304.03442",
        "arxiv_tags": [
          "cs.HC",
          "cs.AI",
          "cs.LG"
        ],
        "published_date": "2023-04-07T01:55:19Z"
      },
      "meta": {
        "created_at": "2025-03-06T21:02:04+00:00",
        "updated_at": "2025-03-29T22:45:36+00:00",
        "version": 2
      }
    },
    "interactions:2503.03312": {
      "data": {
        "paper_id": "arxiv.2503.03312",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-06T21:00:01.437Z",
            "data": {
              "session_id": "session_1741294784606_yos1fxw",
              "duration_seconds": 15,
              "idle_seconds": 0,
              "start_time": "2025-03-06T20:59:44.606Z",
              "end_time": "2025-03-06T21:00:00.081Z",
              "total_elapsed_seconds": 15
            }
          }
        ],
        "legacy_id": "2503.03312"
      },
      "meta": {
        "created_at": "2025-03-06T21:00:02+00:00",
        "updated_at": "2025-03-29T22:45:36+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2502.15815": {
      "data": {
        "primary_id": "arxiv.2502.15815",
        "source": "arxiv",
        "sourceId": "2502.15815",
        "url": "https://arxiv.org/abs/2502.15815",
        "title": "Theoretical Physics Benchmark (TPBench) -- a Dataset and Study of AI\n  Reasoning Capabilities in Theoretical Physics",
        "authors": "Daniel J. H. Chung, Zhiqi Gao, Yurii Kvasiuk, Tianyi Li, Moritz M\u00fcnchmeyer, Maja Rudolph, Frederic Sala, Sai Chaitanya Tadepalli",
        "abstract": "We introduce a benchmark to evaluate the capability of AI to solve problems\nin theoretical physics, focusing on high-energy theory and cosmology. The first\niteration of our benchmark consists of 57 problems of varying difficulty, from\nundergraduate to research level. These problems are novel in the sense that\nthey do not come from public problem collections. We evaluate our data set on\nvarious open and closed language models, including o3-mini, o1, DeepSeek-R1,\nGPT-4o and versions of Llama and Qwen. While we find impressive progress in\nmodel performance with the most recent models, our research-level difficulty\nproblems are mostly unsolved. We address challenges of auto-verifiability and\ngrading, and discuss common failure modes. While currently state-of-the art\nmodels are still of limited use for researchers, our results show that AI\nassisted theoretical physics research may become possible in the near future.\nWe discuss the main obstacles towards this goal and possible strategies to\novercome them. The public problems and solutions, results for various models,\nand updates to the data set and score distribution, are available on the\nwebsite of the dataset tpbench.org.",
        "timestamp": "2025-03-06T21:08:16.326Z",
        "rating": "novote",
        "arxivId": "2502.15815",
        "arxiv_tags": [
          "cs.LG",
          "astro-ph.CO",
          "cs.AI",
          "hep-ph",
          "hep-th"
        ],
        "published_date": "2025-02-19T19:00:00Z"
      },
      "meta": {
        "created_at": "2025-03-06T21:08:16+00:00",
        "updated_at": "2025-03-29T22:45:33+00:00",
        "version": 2
      }
    },
    "interactions:2503.01328": {
      "data": {
        "paper_id": "arxiv.2503.01328",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-06T21:06:42.615Z",
            "data": {
              "session_id": "session_1741295188702_oa11m4q",
              "duration_seconds": 13,
              "idle_seconds": 0,
              "start_time": "2025-03-06T21:06:28.702Z",
              "end_time": "2025-03-06T21:06:41.780Z",
              "total_elapsed_seconds": 13
            }
          }
        ],
        "legacy_id": "2503.01328"
      },
      "meta": {
        "created_at": "2025-03-06T21:06:43+00:00",
        "updated_at": "2025-03-29T22:45:33+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.01328": {
      "data": {
        "primary_id": "arxiv.2503.01328",
        "source": "arxiv",
        "sourceId": "2503.01328",
        "url": "https://arxiv.org/abs/2503.01328",
        "title": "PipeOffload: Improving Scalability of Pipeline Parallelism with Memory\n  Optimization",
        "authors": "Xinyi Wan, Penghui Qi, Guangxing Huang, Jialin Li, Min Lin",
        "abstract": "Pipeline parallelism (PP) is widely used for training large language models\n(LLMs), yet its scalability is often constrained by high activation memory\nconsumption as the number of in-flight microbatches grows with the degree of\nPP. In this paper, we focus on addressing this challenge by leveraging the\nunder-explored memory offload strategy in PP. With empirical study, we discover\nthat in the majority of standard configurations, at least half, and potentially\nall, of the activations can be offloaded with negligible overhead. In the cases\nwhere full overload is not possible, we introduce a novel selective offload\nstrategy that decreases peak activation memory in a better-than-linear manner.\nFurthermore, we integrate memory offload with other techniques to jointly\nconsider overall throughput and memory limitation. Our experiments proves that\nthe per-device activation memory effectively reduces with the total number of\nstages, making PP a stronger alternative than TP, offering up to a 19\\%\nacceleration with even lower memory consumption. The implementation is\nopen-sourced at\n\\href{https://github.com/sail-sg/zero-bubble-pipeline-parallelism}{this url}.",
        "timestamp": "2025-03-06T21:06:29.168Z",
        "rating": "novote",
        "arxivId": "2503.01328",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI",
          "cs.DC"
        ],
        "published_date": "2025-03-03T09:11:06Z"
      },
      "meta": {
        "created_at": "2025-03-06T21:06:29+00:00",
        "updated_at": "2025-03-29T22:45:34+00:00",
        "version": 2
      }
    },
    "interactions:2503.03730": {
      "data": {
        "paper_id": "arxiv.2503.03730",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-06T21:04:29.962Z",
            "data": {
              "session_id": "session_1741295055085_4f2r3qw",
              "duration_seconds": 14,
              "idle_seconds": 0,
              "start_time": "2025-03-06T21:04:15.085Z",
              "end_time": "2025-03-06T21:04:29.220Z",
              "total_elapsed_seconds": 14
            }
          }
        ],
        "legacy_id": "2503.03730"
      },
      "meta": {
        "created_at": "2025-03-06T21:04:30+00:00",
        "updated_at": "2025-03-29T22:45:34+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.03730": {
      "data": {
        "primary_id": "arxiv.2503.03730",
        "source": "arxiv",
        "sourceId": "2503.03730",
        "url": "https://arxiv.org/abs/2503.03730",
        "title": "Towards Understanding Distilled Reasoning Models: A Representational\n  Approach",
        "authors": "David D. Baek, Max Tegmark",
        "abstract": "In this paper, we investigate how model distillation impacts the development\nof reasoning features in large language models (LLMs). To explore this, we\ntrain a crosscoder on Qwen-series models and their fine-tuned variants. Our\nresults suggest that the crosscoder learns features corresponding to various\ntypes of reasoning, including self-reflection and computation verification.\nMoreover, we observe that distilled models contain unique reasoning feature\ndirections, which could be used to steer the model into over-thinking or\nincisive-thinking mode. In particular, we perform analysis on four specific\nreasoning categories: (a) self-reflection, (b) deductive reasoning, (c)\nalternative reasoning, and (d) contrastive reasoning. Finally, we examine the\nchanges in feature geometry resulting from the distillation process and find\nindications that larger distilled models may develop more structured\nrepresentations, which correlate with enhanced distillation performance. By\nproviding insights into how distillation modifies the model, our study\ncontributes to enhancing the transparency and reliability of AI systems.",
        "timestamp": "2025-03-06T21:04:15.628Z",
        "rating": "novote",
        "arxivId": "2503.03730",
        "arxiv_tags": [
          "cs.LG"
        ],
        "published_date": "2025-03-05T18:40:19Z"
      },
      "meta": {
        "created_at": "2025-03-06T21:04:16+00:00",
        "updated_at": "2025-03-29T22:45:34+00:00",
        "version": 2
      }
    },
    "interactions:2304.03442": {
      "data": {
        "paper_id": "arxiv.2304.03442",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-06T21:03:21.722Z",
            "data": {
              "session_id": "session_1741294981941_zy9ycv8",
              "duration_seconds": 20,
              "idle_seconds": 0,
              "start_time": "2025-03-06T21:03:01.941Z",
              "end_time": "2025-03-06T21:03:21.712Z",
              "total_elapsed_seconds": 20
            }
          }
        ],
        "legacy_id": "2304.03442"
      },
      "meta": {
        "created_at": "2025-03-06T21:03:22+00:00",
        "updated_at": "2025-03-29T22:45:35+00:00",
        "version": 3
      }
    },
    "interactions:2502.15815": {
      "data": {
        "paper_id": "arxiv.2502.15815",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-06T21:08:24.296Z",
            "data": {
              "session_id": "session_1741295295961_vp4402i",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-03-06T21:08:15.961Z",
              "end_time": "2025-03-06T21:08:23.500Z",
              "total_elapsed_seconds": 8
            }
          }
        ],
        "legacy_id": "2502.15815"
      },
      "meta": {
        "created_at": "2025-03-06T21:08:25+00:00",
        "updated_at": "2025-03-29T22:45:32+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2412.10924": {
      "data": {
        "primary_id": "arxiv.2412.10924",
        "source": "arxiv",
        "sourceId": "2412.10924",
        "url": "https://arxiv.org/abs/2412.10924",
        "title": "Tokens, the oft-overlooked appetizer: Large language models, the\n  distributional hypothesis, and meaning",
        "authors": "Julia Witte Zimmerman, Denis Hudon, Kathryn Cramer, Alejandro J. Ruiz, Calla Beauregard, Ashley Fehr, Mikaela Irene Fudolig, Bradford Demarest, Yoshi Meke Bird, Milo Z. Trujillo, Christopher M. Danforth, Peter Sheridan Dodds",
        "abstract": "Tokenization is a necessary component within the current architecture of many\nlanguage models, including the transformer-based large language models (LLMs)\nof Generative AI, yet its impact on the model's cognition is often overlooked.\nWe argue that LLMs demonstrate that the Distributional Hypothesis (DH) is\nsufficient for reasonably human-like language performance, and that the\nemergence of human-meaningful linguistic units among tokens motivates\nlinguistically-informed interventions in existing, linguistically-agnostic\ntokenization techniques, particularly with respect to their roles as (1)\nsemantic primitives and as (2) vehicles for conveying salient distributional\npatterns from human language to the model. We explore tokenizations from a BPE\ntokenizer; extant model vocabularies obtained from Hugging Face and tiktoken;\nand the information in exemplar token vectors as they move through the layers\nof a RoBERTa (large) model. Besides creating sub-optimal semantic building\nblocks and obscuring the model's access to the necessary distributional\npatterns, we describe how tokenization pretraining can be a backdoor for bias\nand other unwanted content, which current alignment practices may not\nremediate. Additionally, we relay evidence that the tokenization algorithm's\nobjective function impacts the LLM's cognition, despite being meaningfully\ninsulated from the main system intelligence.",
        "timestamp": "2025-03-06T21:28:57.560Z",
        "rating": "novote",
        "arxivId": "2412.10924",
        "arxiv_tags": [
          "cs.CL",
          "cs.AI"
        ],
        "published_date": "2024-12-14T18:18:52Z"
      },
      "meta": {
        "created_at": "2025-03-06T21:28:58+00:00",
        "updated_at": "2025-03-29T22:45:30+00:00",
        "version": 2
      }
    },
    "interactions:2305.13707": {
      "data": {
        "paper_id": "arxiv.2305.13707",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-06T21:28:20.714Z",
            "data": {
              "session_id": "session_1741296481022_o1juz19",
              "duration_seconds": 19,
              "idle_seconds": 0,
              "start_time": "2025-03-06T21:28:01.022Z",
              "end_time": "2025-03-06T21:28:20.008Z",
              "total_elapsed_seconds": 19
            }
          }
        ],
        "legacy_id": "2305.13707"
      },
      "meta": {
        "created_at": "2025-03-06T21:28:21+00:00",
        "updated_at": "2025-03-29T22:45:31+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2305.13707": {
      "data": {
        "primary_id": "arxiv.2305.13707",
        "source": "arxiv",
        "sourceId": "2305.13707",
        "url": "https://arxiv.org/abs/2305.13707",
        "title": "Do All Languages Cost the Same? Tokenization in the Era of Commercial\n  Language Models",
        "authors": "Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David R. Mortensen, Noah A. Smith, Yulia Tsvetkov",
        "abstract": "Language models have graduated from being research prototypes to\ncommercialized products offered as web APIs, and recent works have highlighted\nthe multilingual capabilities of these products. The API vendors charge their\nusers based on usage, more specifically on the number of ``tokens'' processed\nor generated by the underlying language models. What constitutes a token,\nhowever, is training data and model dependent with a large variance in the\nnumber of tokens required to convey the same information in different\nlanguages. In this work, we analyze the effect of this non-uniformity on the\nfairness of an API's pricing policy across languages. We conduct a systematic\nanalysis of the cost and utility of OpenAI's language model API on multilingual\nbenchmarks in 22 typologically diverse languages. We show evidence that\nspeakers of a large number of the supported languages are overcharged while\nobtaining poorer results. These speakers tend to also come from regions where\nthe APIs are less affordable to begin with. Through these analyses, we aim to\nincrease transparency around language model APIs' pricing policies and\nencourage the vendors to make them more equitable.",
        "timestamp": "2025-03-06T21:28:01.531Z",
        "rating": "novote",
        "arxivId": "2305.13707",
        "arxiv_tags": [
          "cs.CL"
        ],
        "published_date": "2023-05-23T05:46:45Z"
      },
      "meta": {
        "created_at": "2025-03-06T21:28:02+00:00",
        "updated_at": "2025-03-29T22:45:31+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2501.02393": {
      "data": {
        "primary_id": "arxiv.2501.02393",
        "source": "arxiv",
        "sourceId": "2501.02393",
        "url": "https://arxiv.org/abs/2501.02393",
        "title": "Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers",
        "authors": "Markus J. Buehler",
        "abstract": "We present an approach to modifying Transformer architectures by integrating\ngraph-aware relational reasoning into the attention mechanism, merging concepts\nfrom graph neural networks and language modeling. Building on the inherent\nconnection between attention and graph theory, we reformulate the Transformer's\nattention mechanism as a graph operation and propose Graph-Aware Isomorphic\nAttention. This method leverages advanced graph modeling strategies, including\nGraph Isomorphism Networks (GIN) and Principal Neighborhood Aggregation (PNA),\nto enrich the representation of relational structures. Our approach captures\ncomplex dependencies and generalizes across tasks, as evidenced by a reduced\ngeneralization gap and improved learning performance. Additionally, we expand\nthe concept of graph-aware attention to introduce Sparse GIN-Attention, a\nfine-tuning approach that employs sparse GINs. By interpreting attention\nmatrices as sparse adjacency graphs, this technique enhances the adaptability\nof pre-trained foundational models with minimal computational overhead,\nendowing them with graph-aware capabilities. Sparse GIN-Attention fine-tuning\nachieves improved training dynamics and better generalization compared to\nalternative methods like low-rank adaption (LoRA). We discuss latent graph-like\nstructures within traditional attention mechanisms, offering a new lens through\nwhich Transformers can be understood. By evolving Transformers as hierarchical\nGIN models for relational reasoning. This perspective suggests profound\nimplications for foundational model development, enabling the design of\narchitectures that dynamically adapt to both local and global dependencies.\nApplications in bioinformatics, materials science, language modeling, and\nbeyond could benefit from this synthesis of relational and sequential data\nmodeling, setting the stage for interpretable and generalizable modeling\nstrategies.",
        "timestamp": "2025-03-06T21:14:39.253Z",
        "rating": "novote",
        "arxivId": "2501.02393",
        "arxiv_tags": [
          "cs.LG",
          "cond-mat.mes-hall",
          "cond-mat.mtrl-sci",
          "cs.AI",
          "cs.CL"
        ],
        "published_date": "2025-01-04T22:30:21Z"
      },
      "meta": {
        "created_at": "2025-03-06T21:14:39+00:00",
        "updated_at": "2025-03-29T22:45:32+00:00",
        "version": 2
      }
    },
    "interactions:2403.03737": {
      "data": {
        "paper_id": "arxiv.2403.03737",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-06T22:10:39.570Z",
            "data": {
              "session_id": "session_1741299020621_tvnzmlh",
              "duration_seconds": 19,
              "idle_seconds": 0,
              "start_time": "2025-03-06T22:10:20.621Z",
              "end_time": "2025-03-06T22:10:39.564Z",
              "total_elapsed_seconds": 19
            }
          }
        ],
        "legacy_id": "2403.03737"
      },
      "meta": {
        "created_at": "2025-03-06T22:10:40+00:00",
        "updated_at": "2025-03-29T22:45:28+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2403.03737": {
      "data": {
        "primary_id": "arxiv.2403.03737",
        "source": "arxiv",
        "sourceId": "2403.03737",
        "url": "https://arxiv.org/pdf/2403.03737",
        "title": "Probabilistic Topic Modelling with Transformer Representations",
        "authors": "Arik Reuter, Anton Thielmann, Christoph Weisser, Benjamin S\u00e4fken, Thomas Kneib",
        "abstract": "Topic modelling was mostly dominated by Bayesian graphical models during the\nlast decade. With the rise of transformers in Natural Language Processing,\nhowever, several successful models that rely on straightforward clustering\napproaches in transformer-based embedding spaces have emerged and consolidated\nthe notion of topics as clusters of embedding vectors. We propose the\nTransformer-Representation Neural Topic Model (TNTM), which combines the\nbenefits of topic representations in transformer-based embedding spaces and\nprobabilistic modelling. Therefore, this approach unifies the powerful and\nversatile notion of topics based on transformer embeddings with fully\nprobabilistic modelling, as in models such as Latent Dirichlet Allocation\n(LDA). We utilize the variational autoencoder (VAE) framework for improved\ninference speed and modelling flexibility. Experimental results show that our\nproposed model achieves results on par with various state-of-the-art approaches\nin terms of embedding coherence while maintaining almost perfect topic\ndiversity. The corresponding source code is available at\nhttps://github.com/ArikReuter/TNTM.",
        "timestamp": "2025-03-06T22:07:43.010Z",
        "rating": "novote",
        "arxivId": "2403.03737",
        "arxiv_tags": [
          "cs.LG",
          "cs.CL"
        ],
        "published_date": "2024-03-06T14:27:29Z"
      },
      "meta": {
        "created_at": "2025-03-06T22:07:43+00:00",
        "updated_at": "2025-03-29T22:45:29+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2408.08541": {
      "data": {
        "primary_id": "arxiv.2408.08541",
        "source": "arxiv",
        "sourceId": "2408.08541",
        "url": "https://arxiv.org/pdf/2408.08541",
        "title": "Where is the signal in tokenization space?",
        "authors": "Renato Lui Geh, Honghua Zhang, Kareem Ahmed, Benjie Wang, Guy Van den Broeck",
        "abstract": "Large Language Models (LLMs) are typically shipped with tokenizers that\ndeterministically encode text into so-called canonical token sequences, to\nwhich the LLMs assign probability values. One common assumption is that the\nprobability of a piece of text is the probability of its canonical token\nsequence. However, the tokenization of a string is not unique: e.g., the Llama2\ntokenizer encodes Tokens as [Tok,ens], but [Tok,en,s] also represents the same\ntext. In this paper, we study non-canonical tokenizations. We prove that, given\na string, it is computationally hard to find the most likely tokenization for\nan autoregressive LLM, as well as to compute the marginal probability over all\npossible tokenizations. We then show how the marginal is, in most cases,\nindistinguishable from the canonical probability. Surprisingly, we then\nempirically demonstrate the existence of a significant amount of signal hidden\nwithin tokenization space. Notably, by simply aggregating the probabilities of\nnon-canonical tokenizations, we achieve improvements across a range of LLM\nevaluation benchmarks for a variety of architectures, including transformers\nand state space models.",
        "timestamp": "2025-03-06T21:38:06.846Z",
        "rating": "novote",
        "arxivId": "2408.08541",
        "arxiv_tags": [
          "cs.CL",
          "cs.LG"
        ],
        "published_date": "2024-08-16T05:56:10Z"
      },
      "meta": {
        "created_at": "2025-03-06T21:38:07+00:00",
        "updated_at": "2025-03-29T22:45:29+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2502.15343": {
      "data": {
        "primary_id": "arxiv.2502.15343",
        "source": "arxiv",
        "sourceId": "2502.15343",
        "url": "https://arxiv.org/abs/2502.15343",
        "title": "Tokenization is Sensitive to Language Variation",
        "authors": "Anna Wegmann, Dong Nguyen, David Jurgens",
        "abstract": "Variation in language is ubiquitous and often systematically linked to\nregional, social, and contextual factors. Tokenizers split texts into smaller\nunits and might behave differently for less common linguistic forms. This might\naffect downstream LLM performance differently on two types of tasks: Tasks\nwhere the model should be robust to language variation (e.g., for semantic\ntasks like NLI, labels do not depend on whether a text uses British or American\nspelling) and tasks where the model should be sensitive to language variation\n(e.g., for form-based tasks like authorship verification, labels depend on\nwhether a text uses British or American spelling). We pre-train BERT base\nmodels for the popular Byte-Pair Encoding algorithm to investigate how key\nalgorithmic design choices impact downstream models' performances: fitting\ncorpus, pre-tokenizer and vocabulary size. We find that the best tokenizer\nvaries on the two task types -- with the pre-tokenizer having the biggest\nimpact on performance. Further, we introduce a new approach to estimate\ntokenizer impact on downstream LLM performance, showing significant improvement\nover techniques like R\\'enyi efficiency. We encourage more work on language\nvariation and its relation to tokenizers and thus LLM performance.",
        "timestamp": "2025-03-06T21:37:25.435Z",
        "rating": "novote",
        "arxivId": "2502.15343",
        "arxiv_tags": [
          "cs.CL"
        ],
        "published_date": "2025-02-21T09:58:54Z"
      },
      "meta": {
        "created_at": "2025-03-06T21:37:25+00:00",
        "updated_at": "2025-03-29T22:45:30+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2502.19413": {
      "data": {
        "primary_id": "arxiv.2502.19413",
        "source": "arxiv",
        "sourceId": "2502.19413",
        "url": "https://arxiv.org/abs/2502.19413",
        "title": "Project Alexandria: Towards Freeing Scientific Knowledge from Copyright\n  Burdens via LLMs",
        "authors": "Christoph Schuhmann, Gollam Rabby, Ameya Prabhu, Tawsif Ahmed, Andreas Hochlehnert, Huu Nguyen, Nick Akinci Heidrich, Ludwig Schmidt, Robert Kaczmarczyk, S\u00f6ren Auer, Jenia Jitsev, Matthias Bethge",
        "abstract": "Paywalls, licenses and copyright rules often restrict the broad dissemination\nand reuse of scientific knowledge. We take the position that it is both legally\nand technically feasible to extract the scientific knowledge in scholarly\ntexts. Current methods, like text embeddings, fail to reliably preserve factual\ncontent, and simple paraphrasing may not be legally sound. We urge the\ncommunity to adopt a new idea: convert scholarly documents into Knowledge Units\nusing LLMs. These units use structured data capturing entities, attributes and\nrelationships without stylistic content. We provide evidence that Knowledge\nUnits: (1) form a legally defensible framework for sharing knowledge from\ncopyrighted research texts, based on legal analyses of German copyright law and\nU.S. Fair Use doctrine, and (2) preserve most (~95%) factual knowledge from\noriginal text, measured by MCQ performance on facts from the original\ncopyrighted text across four research domains. Freeing scientific knowledge\nfrom copyright promises transformative benefits for scientific research and\neducation by allowing language models to reuse important facts from copyrighted\ntext. To support this, we share open-source tools for converting research\ndocuments into Knowledge Units. Overall, our work posits the feasibility of\ndemocratizing access to scientific knowledge while respecting copyright.",
        "timestamp": "2025-03-06T22:21:09.991Z",
        "rating": "novote",
        "arxivId": "2502.19413",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI",
          "cs.CL"
        ],
        "published_date": "2025-02-26T18:56:52Z"
      },
      "meta": {
        "created_at": "2025-03-06T22:21:10+00:00",
        "updated_at": "2025-03-29T22:45:28+00:00",
        "version": 2
      }
    },
    "interactions:2502.19413": {
      "data": {
        "paper_id": "arxiv.2502.19413",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-06T22:22:12.255Z",
            "data": {
              "session_id": "session_1741299727814_njhyfqm",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-03-06T22:22:07.814Z",
              "end_time": "2025-03-06T22:22:11.468Z",
              "total_elapsed_seconds": 4
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-06T22:24:20.704Z",
            "data": {
              "session_id": "session_1741299850874_qxc7t5e",
              "duration_seconds": 7,
              "idle_seconds": 0,
              "start_time": "2025-03-06T22:24:10.874Z",
              "end_time": "2025-03-06T22:24:18.222Z",
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-06T22:27:34.368Z",
            "data": {
              "session_id": "session_1741300044147_ovg68yn",
              "duration_seconds": 10,
              "idle_seconds": 0,
              "start_time": "2025-03-06T22:27:24.147Z",
              "end_time": "2025-03-06T22:27:34.184Z",
              "total_elapsed_seconds": 10
            }
          }
        ],
        "legacy_id": "2502.19413"
      },
      "meta": {
        "created_at": "2025-03-06T22:22:13+00:00",
        "updated_at": "2025-03-29T22:45:27+00:00",
        "version": 6
      }
    },
    "interactions:2502.16732": {
      "data": {
        "paper_id": "arxiv.2502.16732",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-07T02:42:12.424Z",
            "data": {
              "session_id": "session_1741315320424_qhqlc2o",
              "duration_seconds": 12,
              "idle_seconds": 0,
              "start_time": "2025-03-07T02:42:00.424Z",
              "end_time": "2025-03-07T02:42:12.411Z",
              "total_elapsed_seconds": 12
            }
          }
        ],
        "legacy_id": "2502.16732"
      },
      "meta": {
        "created_at": "2025-03-07T02:41:19+00:00",
        "updated_at": "2025-03-29T22:45:26+00:00",
        "version": 5
      }
    },
    "paper:arxiv.2502.16732": {
      "data": {
        "primary_id": "arxiv.2502.16732",
        "source": "arxiv",
        "sourceId": "2502.16732",
        "url": "https://arxiv.org/abs/2502.16732",
        "title": "DeepSeek reshaping healthcare in China's tertiary hospitals",
        "authors": "Jishizhan Chen, Qingzeng Zhang",
        "abstract": "The rapid integration of artificial intelligence (AI) into healthcare is\ntransforming clinical decision-making and hospital operations. DeepSeek has\nemerged as a leading AI system, widely deployed across China's tertiary\nhospitals since January 2025. Initially implemented in Shanghai's major medical\ninstitutions, it has since expanded nationwide, enhancing diagnostic accuracy,\nstreamlining workflows, and improving patient management. AI-powered pathology,\nimaging analysis, and clinical decision support systems have demonstrated\nsignificant potential in optimizing medical processes and reducing the\ncognitive burden on healthcare professionals. However, the widespread adoption\nof AI in healthcare raises critical regulatory and ethical challenges,\nparticularly regarding accountability in AI-assisted diagnosis and the risk of\nautomation bias. The absence of a well-defined liability framework underscores\nthe need for policies that ensure AI functions as an assistive tool rather than\nan autonomous decision-maker. With continued technological advancements, AI is\nexpected to integrate multimodal data sources, such as genomics and radiomics,\npaving the way for precision medicine and personalized treatment strategies.\nThe future of AI in healthcare depends on the development of transparent\nregulatory structures, industry collaboration, and adaptive governance\nframeworks that balance innovation with responsibility, ensuring equitable and\neffective AI-driven medical services.",
        "timestamp": "2025-03-07T02:41:03.938Z",
        "rating": "novote",
        "arxivId": "2502.16732",
        "arxiv_tags": [
          "cs.CY",
          "cs.AI"
        ],
        "published_date": "2025-02-23T22:09:17Z"
      },
      "meta": {
        "created_at": "2025-03-07T02:41:04+00:00",
        "updated_at": "2025-03-29T22:45:26+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.03206": {
      "data": {
        "primary_id": "arxiv.2503.03206",
        "source": "arxiv",
        "sourceId": "2503.03206",
        "url": "https://arxiv.org/abs/2503.03206",
        "title": "An Analytical Theory of Power Law Spectral Bias in the Learning Dynamics\n  of Diffusion Models",
        "authors": "Binxu Wang",
        "abstract": "We developed an analytical framework for understanding how the learned\ndistribution evolves during diffusion model training. Leveraging the Gaussian\nequivalence principle, we derived exact solutions for the gradient-flow\ndynamics of weights in one- or two-layer linear denoiser settings with\narbitrary data. Remarkably, these solutions allowed us to derive the generated\ndistribution in closed form and its KL divergence through training. These\nanalytical results expose a pronounced power-law spectral bias, i.e., for\nweights and distributions, the convergence time of a mode follows an inverse\npower law of its variance. Empirical experiments on both Gaussian and image\ndatasets demonstrate that the power-law spectral bias remains robust even when\nusing deeper or convolutional architectures. Our results underscore the\nimportance of the data covariance in dictating the order and rate at which\ndiffusion models learn different modes of the data, providing potential\nexplanations for why earlier stopping could lead to incorrect details in image\ngenerative models.",
        "timestamp": "2025-03-07T02:39:26.172Z",
        "rating": "novote",
        "arxivId": "2503.03206",
        "arxiv_tags": [
          "cs.LG",
          "cs.CV",
          "math.ST",
          "stat.ML",
          "stat.TH",
          "68T07, 60G15",
          "F.2.2; G.1.2; G.3; I.2.6"
        ],
        "published_date": "2025-03-05T05:50:38Z"
      },
      "meta": {
        "created_at": "2025-03-07T02:39:26+00:00",
        "updated_at": "2025-03-29T22:45:26+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2402.07754": {
      "data": {
        "primary_id": "arxiv.2402.07754",
        "source": "arxiv",
        "sourceId": "2402.07754",
        "url": "https://arxiv.org/abs/2402.07754",
        "title": "Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language\n  Models",
        "authors": "Jiacheng Ye, Shansan Gong, Liheng Chen, Lin Zheng, Jiahui Gao, Han Shi, Chuan Wu, Xin Jiang, Zhenguo Li, Wei Bi, Lingpeng Kong",
        "abstract": "Recently, diffusion models have garnered significant interest in the field of\ntext processing due to their many potential advantages compared to conventional\nautoregressive models. In this work, we propose Diffusion-of-Thought (DoT), a\nnovel approach that integrates diffusion models with Chain-of-Thought, a\nwell-established technique for improving the reasoning ability of\nautoregressive language models. In contrast to autoregressive language models\nthat make decisions in a left-to-right, token-by-token manner, DoT allows\nreasoning steps to diffuse over time through a diffusion language model and\noffers greater flexibility in trading-off computation for reasoning\nperformance. Our experimental results demonstrate the effectiveness of DoT in\nmulti-digit multiplication, boolean logic, and grade school math problems, with\na small diffusion model outperforming a much larger autoregressive model in\nboth efficiency and accuracy. In addition to that, DoT showcases promising\nself-correction abilities and benefits from existing reasoning-enhancing\ntechniques like self-consistency decoding. Our findings contribute to the\nunderstanding and development of reasoning with diffusion language models.",
        "timestamp": "2025-03-07T02:47:45.411Z",
        "rating": "novote",
        "arxivId": "2402.07754",
        "arxiv_tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG"
        ],
        "published_date": "2024-02-12T16:23:28Z"
      },
      "meta": {
        "created_at": "2025-03-07T02:47:45+00:00",
        "updated_at": "2025-03-29T22:45:24+00:00",
        "version": 2
      }
    },
    "interactions:2304.04661": {
      "data": {
        "paper_id": "arxiv.2304.04661",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-07T02:45:43.073Z",
            "data": {
              "session_id": "session_1741315533811_wi0d2hh",
              "duration_seconds": 9,
              "idle_seconds": 0,
              "start_time": "2025-03-07T02:45:33.811Z",
              "end_time": "2025-03-07T02:45:42.326Z",
              "total_elapsed_seconds": 9
            }
          }
        ],
        "legacy_id": "2304.04661"
      },
      "meta": {
        "created_at": "2025-03-07T02:45:43+00:00",
        "updated_at": "2025-03-29T22:45:25+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2304.04661": {
      "data": {
        "primary_id": "arxiv.2304.04661",
        "source": "arxiv",
        "sourceId": "2304.04661",
        "url": "https://arxiv.org/pdf/2304.04661",
        "title": "AI for IT Operations (AIOps) on Cloud Platforms: Reviews, Opportunities\n  and Challenges",
        "authors": "Qian Cheng, Doyen Sahoo, Amrita Saha, Wenzhuo Yang, Chenghao Liu, Gerald Woo, Manpreet Singh, Silvio Saverese, Steven C. H. Hoi",
        "abstract": "Artificial Intelligence for IT operations (AIOps) aims to combine the power\nof AI with the big data generated by IT Operations processes, particularly in\ncloud infrastructures, to provide actionable insights with the primary goal of\nmaximizing availability. There are a wide variety of problems to address, and\nmultiple use-cases, where AI capabilities can be leveraged to enhance\noperational efficiency. Here we provide a review of the AIOps vision, trends\nchallenges and opportunities, specifically focusing on the underlying AI\ntechniques. We discuss in depth the key types of data emitted by IT Operations\nactivities, the scale and challenges in analyzing them, and where they can be\nhelpful. We categorize the key AIOps tasks as - incident detection, failure\nprediction, root cause analysis and automated actions. We discuss the problem\nformulation for each task, and then present a taxonomy of techniques to solve\nthese problems. We also identify relatively under explored topics, especially\nthose that could significantly benefit from advances in AI literature. We also\nprovide insights into the trends in this field, and what are the key investment\nopportunities.",
        "timestamp": "2025-03-07T02:45:34.214Z",
        "rating": "novote",
        "arxivId": "2304.04661",
        "arxiv_tags": [
          "cs.LG",
          "cs.DC",
          "cs.SE"
        ],
        "published_date": "2023-04-10T15:38:12Z"
      },
      "meta": {
        "created_at": "2025-03-07T02:45:34+00:00",
        "updated_at": "2025-03-29T22:45:25+00:00",
        "version": 2
      }
    },
    "interactions:2402.07754": {
      "data": {
        "paper_id": "arxiv.2402.07754",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-07T02:47:54.518Z",
            "data": {
              "session_id": "session_1741315664986_948liwn",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-03-07T02:47:44.986Z",
              "end_time": "2025-03-07T02:47:48.315Z",
              "total_elapsed_seconds": 3
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-07T02:47:55.126Z",
            "data": {
              "session_id": "session_1741315664986_948liwn",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-03-07T02:47:44.986Z",
              "end_time": "2025-03-07T02:47:48.315Z",
              "total_elapsed_seconds": 3
            }
          }
        ],
        "legacy_id": "2402.07754"
      },
      "meta": {
        "created_at": "2025-03-07T02:47:49+00:00",
        "updated_at": "2025-03-29T22:45:24+00:00",
        "version": 5
      }
    },
    "interactions:arxiv.2408.08541": {
      "data": {
        "paper_id": "arxiv.2408.08541",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-07T04:33:54.841Z",
            "data": {
              "session_id": "session_1741321978258_8aiq383",
              "duration_seconds": 56,
              "idle_seconds": 0,
              "start_time": "2025-03-07T04:32:58.258Z",
              "end_time": "2025-03-07T04:33:53.956Z",
              "total_elapsed_seconds": 56
            }
          }
        ],
        "legacy_id": "2408.08541"
      },
      "meta": {
        "created_at": "2025-03-07T04:33:56+00:00",
        "updated_at": "2025-03-29T22:45:23+00:00",
        "version": 3
      }
    },
    "interactions:arxiv.2103.02096": {
      "data": {
        "paper_id": "arxiv.2103.02096",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-07T04:36:05.422Z",
            "data": {
              "session_id": "session_1741322158352_loc15le",
              "duration_seconds": 7,
              "idle_seconds": 0,
              "start_time": "2025-03-07T04:35:58.352Z",
              "end_time": "2025-03-07T04:36:05.407Z",
              "total_elapsed_seconds": 7
            }
          }
        ],
        "legacy_id": "2103.02096"
      },
      "meta": {
        "created_at": "2025-03-07T04:35:52+00:00",
        "updated_at": "2025-03-29T22:45:23+00:00",
        "version": 5
      }
    },
    "interactions:arxiv.2503.00691": {
      "data": {
        "paper_id": "arxiv.2503.00691",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-07T06:06:37.418Z",
            "data": {
              "session_id": "session_1741327579701_ew6wlcq",
              "duration_seconds": 17,
              "idle_seconds": 0,
              "start_time": "2025-03-07T06:06:19.701Z",
              "end_time": "2025-03-07T06:06:36.680Z",
              "total_elapsed_seconds": 17
            }
          }
        ],
        "legacy_id": "2503.00691"
      },
      "meta": {
        "created_at": "2025-03-07T06:06:38+00:00",
        "updated_at": "2025-03-29T22:45:22+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.00691": {
      "data": {
        "primary_id": "arxiv.2503.00691",
        "source": "arxiv",
        "sourceId": "2503.00691",
        "url": "https://arxiv.org/html/2503.00691v1",
        "title": "How Diversely Can Language Models Solve Problems? Exploring the\n  Algorithmic Diversity of Model-Generated Code",
        "authors": "Seonghyeon Lee, Heejae Chon, Joonwon Jang, Dongha Lee, Hwanjo Yu",
        "abstract": "Language models (LMs) have exhibited impressive abilities in generating code\nfrom natural language requirements. In this work, we highlight the diversity of\ncode generated by LMs as a critical criterion for evaluating their code\ngeneration capabilities. There is a lack of studies focused on assessing the\ndiversity of generated code, which overlooks its importance in code LMs.\nTherefore, we propose a systematic approach to evaluate code diversity,\nintroducing various metrics with inter-code similarity. Specifically, we\nintroduce code clustering methods that leverages LMs' capabilities in code\nunderstanding and reasoning, resulting in a set of metrics that represent the\nnumber of algorithms in model-generated solutions. We extensively investigate\nthe property of model-generated solutions by contrasting them with\nhuman-written ones and quantifying the impact of various factors on code\ndiversity: model size, temperature, instruction tuning, and problem complexity.\nOur analysis demonstrates that model-generated solutions exhibit low\nalgorithmic diversity, which was neglected by the research community. Moreover,\nwe explore methods to increase code diversity by combining solutions from\ndifferent models and increasing sampling temperatures. Our findings highlight\nthat code diversity can be enhanced with the help of heterogeneous models and\nsetting temperature beyond 1.0 that has not been fully explored due to the\nfunctional correctness degradation. To facilitate our research direction, we\npublicly share our code and datasets through open-source repositories.",
        "timestamp": "2025-03-07T06:06:20.292Z",
        "rating": "novote",
        "arxivId": "2503.00691",
        "arxiv_tags": [
          "cs.SE",
          "cs.AI",
          "cs.CL"
        ],
        "published_date": "2025-03-02T02:04:58Z"
      },
      "meta": {
        "created_at": "2025-03-07T06:06:20+00:00",
        "updated_at": "2025-03-29T22:45:23+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2412.08194": {
      "data": {
        "paper_id": "arxiv.2412.08194",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-07T08:17:49.380Z",
            "data": {
              "session_id": "session_1741335465810_qa6bh3z",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-03-07T08:17:45.810Z",
              "end_time": "2025-03-07T08:17:49.004Z",
              "total_elapsed_seconds": 3
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-07T08:17:54.176Z",
            "data": {
              "session_id": "session_1741335465810_qa6bh3z",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-03-07T08:17:45.810Z",
              "end_time": "2025-03-07T08:17:49.004Z",
              "total_elapsed_seconds": 3
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-07T08:50:09.355Z",
            "data": {
              "session_id": "session_1741337384411_lfeza2h",
              "duration_seconds": 22,
              "idle_seconds": 0,
              "start_time": "2025-03-07T08:49:44.411Z",
              "end_time": "2025-03-07T08:50:06.681Z",
              "total_elapsed_seconds": 22
            }
          }
        ],
        "legacy_id": "2412.08194"
      },
      "meta": {
        "created_at": "2025-03-07T08:17:50+00:00",
        "updated_at": "2025-03-29T22:45:21+00:00",
        "version": 6
      }
    },
    "paper:arxiv.2412.08194": {
      "data": {
        "primary_id": "arxiv.2412.08194",
        "source": "arxiv",
        "sourceId": "2412.08194",
        "url": "https://arxiv.org/abs/2412.08194",
        "title": "Magneto: Combining Small and Large Language Models for Schema Matching",
        "authors": "Yurong Liu, Eduardo Pena, Aecio Santos, Eden Wu, Juliana Freire",
        "abstract": "Recent advances in language models opened new opportunities to address\ncomplex schema matching tasks. Schema matching approaches have been proposed\nthat demonstrate the usefulness of language models, but they have also\nuncovered important limitations: Small language models (SLMs) require training\ndata (which can be both expensive and challenging to obtain), and large\nlanguage models (LLMs) often incur high computational costs and must deal with\nconstraints imposed by context windows. We present Magneto, a cost-effective\nand accurate solution for schema matching that combines the advantages of SLMs\nand LLMs to address their limitations. By structuring the schema matching\npipeline in two phases, retrieval and reranking, Magneto can use\ncomputationally efficient SLM-based strategies to derive candidate matches\nwhich can then be reranked by LLMs, thus making it possible to reduce runtime\nwithout compromising matching accuracy. We propose a self-supervised approach\nto fine-tune SLMs which uses LLMs to generate syntactically diverse training\ndata, and prompting strategies that are effective for reranking. We also\nintroduce a new benchmark, developed in collaboration with domain experts,\nwhich includes real biomedical datasets and presents new challenges to schema\nmatching methods. Through a detailed experimental evaluation, using both our\nnew and existing benchmarks, we show that Magneto is scalable and attains high\naccuracy for datasets from different domains.",
        "timestamp": "2025-03-07T07:26:53.882Z",
        "rating": "novote",
        "arxivId": "2412.08194",
        "arxiv_tags": [
          "cs.DB",
          "cs.LG"
        ],
        "published_date": "2024-12-11T08:35:56Z"
      },
      "meta": {
        "created_at": "2025-03-07T07:26:54+00:00",
        "updated_at": "2025-03-29T22:45:21+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2204.05149": {
      "data": {
        "primary_id": "arxiv.2204.05149",
        "source": "arxiv",
        "sourceId": "2204.05149",
        "url": "https://arxiv.org/pdf/2204.05149",
        "title": "The Carbon Footprint of Machine Learning Training Will Plateau, Then\n  Shrink",
        "authors": "David Patterson, Joseph Gonzalez, Urs H\u00f6lzle, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, Jeff Dean",
        "abstract": "Machine Learning (ML) workloads have rapidly grown in importance, but raised\nconcerns about their carbon footprint. Four best practices can reduce ML\ntraining energy by up to 100x and CO2 emissions up to 1000x. By following best\npractices, overall ML energy use (across research, development, and production)\nheld steady at &lt;15% of Google's total energy use for the past three years. If\nthe whole ML field were to adopt best practices, total carbon emissions from\ntraining would reduce. Hence, we recommend that ML papers include emissions\nexplicitly to foster competition on more than just model quality. Estimates of\nemissions in papers that omitted them have been off 100x-100,000x, so\npublishing emissions has the added benefit of ensuring accurate accounting.\nGiven the importance of climate change, we must get the numbers right to make\ncertain that we work on its biggest challenges.",
        "timestamp": "2025-03-07T07:22:40.809Z",
        "rating": "novote",
        "arxivId": "2204.05149",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI",
          "cs.GL"
        ],
        "published_date": "2022-04-11T14:30:27Z"
      },
      "meta": {
        "created_at": "2025-03-07T07:22:41+00:00",
        "updated_at": "2025-03-29T22:45:22+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2309.13101": {
      "data": {
        "primary_id": "arxiv.2309.13101",
        "source": "arxiv",
        "sourceId": "2309.13101",
        "url": "https://arxiv.org/abs/2309.13101",
        "title": "Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene\n  Reconstruction",
        "authors": "Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, Xiaogang Jin",
        "abstract": "Implicit neural representation has paved the way for new approaches to\ndynamic scene reconstruction and rendering. Nonetheless, cutting-edge dynamic\nneural rendering methods rely heavily on these implicit representations, which\nfrequently struggle to capture the intricate details of objects in the scene.\nFurthermore, implicit methods have difficulty achieving real-time rendering in\ngeneral dynamic scenes, limiting their use in a variety of tasks. To address\nthe issues, we propose a deformable 3D Gaussians Splatting method that\nreconstructs scenes using 3D Gaussians and learns them in canonical space with\na deformation field to model monocular dynamic scenes. We also introduce an\nannealing smoothing training mechanism with no extra overhead, which can\nmitigate the impact of inaccurate poses on the smoothness of time interpolation\ntasks in real-world datasets. Through a differential Gaussian rasterizer, the\ndeformable 3D Gaussians not only achieve higher rendering quality but also\nreal-time rendering speed. Experiments show that our method outperforms\nexisting methods significantly in terms of both rendering quality and speed,\nmaking it well-suited for tasks such as novel-view synthesis, time\ninterpolation, and real-time rendering.",
        "timestamp": "2025-03-07T08:32:49.683Z",
        "rating": "novote",
        "arxivId": "2309.13101",
        "arxiv_tags": [
          "cs.CV"
        ],
        "published_date": "2023-09-22T16:04:02Z"
      },
      "meta": {
        "created_at": "2025-03-07T08:32:50+00:00",
        "updated_at": "2025-03-29T22:45:19+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.04079": {
      "data": {
        "paper_id": "arxiv.2503.04079",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-07T08:31:24.743Z",
            "data": {
              "session_id": "session_1741336256535_uua2ibp",
              "duration_seconds": 27,
              "idle_seconds": 0,
              "start_time": "2025-03-07T08:30:56.535Z",
              "end_time": "2025-03-07T08:31:23.111Z",
              "total_elapsed_seconds": 27
            }
          }
        ],
        "legacy_id": "2503.04079"
      },
      "meta": {
        "created_at": "2025-03-07T08:30:29+00:00",
        "updated_at": "2025-03-29T22:45:20+00:00",
        "version": 5
      }
    },
    "paper:arxiv.2503.04079": {
      "data": {
        "primary_id": "arxiv.2503.04079",
        "source": "arxiv",
        "sourceId": "2503.04079",
        "url": "https://arxiv.org/abs/2503.04079",
        "title": "Surgical Gaussian Surfels: Highly Accurate Real-time Surgical Scene\n  Rendering",
        "authors": "Idris O. Sunmola, Zhenjun Zhao, Samuel Schmidgall, Yumeng Wang, Paul Maria Scheikl, Axel Krieger",
        "abstract": "Accurate geometric reconstruction of deformable tissues in monocular\nendoscopic video remains a fundamental challenge in robot-assisted minimally\ninvasive surgery. Although recent volumetric and point primitive methods based\non neural radiance fields (NeRF) and 3D Gaussian primitives have efficiently\nrendered surgical scenes, they still struggle with handling artifact-free tool\nocclusions and preserving fine anatomical details. These limitations stem from\nunrestricted Gaussian scaling and insufficient surface alignment constraints\nduring reconstruction. To address these issues, we introduce Surgical Gaussian\nSurfels (SGS), which transforms anisotropic point primitives into\nsurface-aligned elliptical splats by constraining the scale component of the\nGaussian covariance matrix along the view-aligned axis. We predict accurate\nsurfel motion fields using a lightweight Multi-Layer Perceptron (MLP) coupled\nwith locality constraints to handle complex tissue deformations. We use\nhomodirectional view-space positional gradients to capture fine image details\nby splitting Gaussian Surfels in over-reconstructed regions. In addition, we\ndefine surface normals as the direction of the steepest density change within\neach Gaussian surfel primitive, enabling accurate normal estimation without\nrequiring monocular normal priors. We evaluate our method on two in-vivo\nsurgical datasets, where it outperforms current state-of-the-art methods in\nsurface geometry, normal map quality, and rendering efficiency, while remaining\ncompetitive in real-time rendering performance. We make our code available at\nhttps://github.com/aloma85/SurgicalGaussianSurfels",
        "timestamp": "2025-03-07T08:30:19.935Z",
        "rating": "novote",
        "arxivId": "2503.04079",
        "arxiv_tags": [
          "cs.CV"
        ],
        "published_date": "2025-03-06T04:33:19Z"
      },
      "meta": {
        "created_at": "2025-03-07T08:30:20+00:00",
        "updated_at": "2025-03-29T22:45:20+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2404.17774": {
      "data": {
        "primary_id": "arxiv.2404.17774",
        "source": "arxiv",
        "sourceId": "2404.17774",
        "url": "https://arxiv.org/abs/2404.17774",
        "title": "High-quality Surface Reconstruction using Gaussian Surfels",
        "authors": "Pinxuan Dai, Jiamin Xu, Wenxiang Xie, Xinguo Liu, Huamin Wang, Weiwei Xu",
        "abstract": "We propose a novel point-based representation, Gaussian surfels, to combine\nthe advantages of the flexible optimization procedure in 3D Gaussian points and\nthe surface alignment property of surfels. This is achieved by directly setting\nthe z-scale of 3D Gaussian points to 0, effectively flattening the original 3D\nellipsoid into a 2D ellipse. Such a design provides clear guidance to the\noptimizer. By treating the local z-axis as the normal direction, it greatly\nimproves optimization stability and surface alignment. While the derivatives to\nthe local z-axis computed from the covariance matrix are zero in this setting,\nwe design a self-supervised normal-depth consistency loss to remedy this issue.\nMonocular normal priors and foreground masks are incorporated to enhance the\nquality of the reconstruction, mitigating issues related to highlights and\nbackground. We propose a volumetric cutting method to aggregate the information\nof Gaussian surfels so as to remove erroneous points in depth maps generated by\nalpha blending. Finally, we apply screened Poisson reconstruction method to the\nfused depth maps to extract the surface mesh. Experimental results show that\nour method demonstrates superior performance in surface reconstruction compared\nto state-of-the-art neural volume rendering and point-based rendering methods.",
        "timestamp": "2025-03-07T08:34:31.369Z",
        "rating": "novote",
        "arxivId": "2404.17774",
        "arxiv_tags": [
          "cs.CV",
          "cs.GR"
        ],
        "published_date": "2024-04-27T04:13:39Z"
      },
      "meta": {
        "created_at": "2025-03-07T08:34:31+00:00",
        "updated_at": "2025-03-29T22:45:17+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2310.08528": {
      "data": {
        "paper_id": "arxiv.2310.08528",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-07T08:34:01.447Z",
            "data": {
              "session_id": "session_1741336437081_adff4l0",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-03-07T08:33:57.081Z",
              "end_time": "2025-03-07T08:34:00.290Z",
              "total_elapsed_seconds": 3
            }
          }
        ],
        "legacy_id": "2310.08528"
      },
      "meta": {
        "created_at": "2025-03-07T08:34:02+00:00",
        "updated_at": "2025-03-29T22:45:18+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2310.08528": {
      "data": {
        "primary_id": "arxiv.2310.08528",
        "source": "arxiv",
        "sourceId": "2310.08528",
        "url": "https://arxiv.org/abs/2310.08528",
        "title": "4D Gaussian Splatting for Real-Time Dynamic Scene Rendering",
        "authors": "Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, Xinggang Wang",
        "abstract": "Representing and rendering dynamic scenes has been an important but\nchallenging task. Especially, to accurately model complex motions, high\nefficiency is usually hard to guarantee. To achieve real-time dynamic scene\nrendering while also enjoying high training and storage efficiency, we propose\n4D Gaussian Splatting (4D-GS) as a holistic representation for dynamic scenes\nrather than applying 3D-GS for each individual frame. In 4D-GS, a novel\nexplicit representation containing both 3D Gaussians and 4D neural voxels is\nproposed. A decomposed neural voxel encoding algorithm inspired by HexPlane is\nproposed to efficiently build Gaussian features from 4D neural voxels and then\na lightweight MLP is applied to predict Gaussian deformations at novel\ntimestamps. Our 4D-GS method achieves real-time rendering under high\nresolutions, 82 FPS at an 800$\\times$800 resolution on an RTX 3090 GPU while\nmaintaining comparable or better quality than previous state-of-the-art\nmethods. More demos and code are available at\nhttps://guanjunwu.github.io/4dgs/.",
        "timestamp": "2025-03-07T08:33:57.559Z",
        "rating": "novote",
        "arxivId": "2310.08528",
        "arxiv_tags": [
          "cs.CV",
          "cs.GR"
        ],
        "published_date": "2023-10-12T17:21:41Z"
      },
      "meta": {
        "created_at": "2025-03-07T08:33:58+00:00",
        "updated_at": "2025-03-29T22:45:18+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2309.13101": {
      "data": {
        "paper_id": "arxiv.2309.13101",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-07T08:33:00.257Z",
            "data": {
              "session_id": "session_1741336369252_sqnan6h",
              "duration_seconds": 10,
              "idle_seconds": 0,
              "start_time": "2025-03-07T08:32:49.252Z",
              "end_time": "2025-03-07T08:32:59.435Z",
              "total_elapsed_seconds": 10
            }
          }
        ],
        "legacy_id": "2309.13101"
      },
      "meta": {
        "created_at": "2025-03-07T08:33:01+00:00",
        "updated_at": "2025-03-29T22:45:19+00:00",
        "version": 3
      }
    },
    "interactions:arxiv.2502.20581": {
      "data": {
        "paper_id": "arxiv.2502.20581",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-07T09:01:38.633Z",
            "data": {
              "session_id": "session_1741338082320_4cx9frk",
              "duration_seconds": 15,
              "idle_seconds": 0,
              "start_time": "2025-03-07T09:01:22.320Z",
              "end_time": "2025-03-07T09:01:37.758Z",
              "total_elapsed_seconds": 15
            }
          }
        ],
        "legacy_id": "2502.20581"
      },
      "meta": {
        "created_at": "2025-03-07T09:01:39+00:00",
        "updated_at": "2025-03-29T22:45:16+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2502.20581": {
      "data": {
        "primary_id": "arxiv.2502.20581",
        "source": "arxiv",
        "sourceId": "2502.20581",
        "url": "https://arxiv.org/abs/2502.20581",
        "title": "The Noisy Path from Source to Citation: Measuring How Scholars Engage\n  with Past Research",
        "authors": "Hong Chen, Misha Teplitskiy, David Jurgens",
        "abstract": "Academic citations are widely used for evaluating research and tracing\nknowledge flows. Such uses typically rely on raw citation counts and neglect\nvariability in citation types. In particular, citations can vary in their\nfidelity as original knowledge from cited studies may be paraphrased,\nsummarized, or reinterpreted, possibly wrongly, leading to variation in how\nmuch information changes from cited to citing paper. In this study, we\nintroduce a computational pipeline to quantify citation fidelity at scale.\nUsing full texts of papers, the pipeline identifies citations in citing papers\nand the corresponding claims in cited papers, and applies supervised models to\nmeasure fidelity at the sentence level. Analyzing a large-scale\nmulti-disciplinary dataset of approximately 13 million citation sentence pairs,\nwe find that citation fidelity is higher when authors cite papers that are 1)\nmore recent and intellectually close, 2) more accessible, and 3) the first\nauthor has a lower H-index and the author team is medium-sized. Using a\nquasi-experiment, we establish the \"telephone effect\" - when citing papers have\nlow fidelity to the original claim, future papers that cite the citing paper\nand the original have lower fidelity to the original. Our work reveals\nsystematic differences in citation fidelity, underscoring the limitations of\nanalyses that rely on citation quantity alone and the potential for distortion\nof evidence.",
        "timestamp": "2025-03-07T09:01:22.960Z",
        "rating": "novote",
        "arxivId": "2502.20581",
        "arxiv_tags": [
          "cs.CL"
        ],
        "published_date": "2025-02-27T22:47:03Z"
      },
      "meta": {
        "created_at": "2025-03-07T09:01:23+00:00",
        "updated_at": "2025-03-29T22:45:16+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.1503.03585": {
      "data": {
        "paper_id": "arxiv.1503.03585",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-08T01:39:16.793Z",
            "data": {
              "session_id": "session_1741397943278_q1ncqya",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-03-08T01:39:03.278Z",
              "end_time": "2025-03-08T01:39:09.771Z",
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-08T01:42:32.832Z",
            "data": {
              "session_id": "session_1741398138806_1vee16s",
              "duration_seconds": 14,
              "idle_seconds": 0,
              "start_time": "2025-03-08T01:42:18.806Z",
              "end_time": "2025-03-08T01:42:32.642Z",
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-08T02:08:43.039Z",
            "data": {
              "session_id": "session_1741399715783_gxdrirg",
              "duration_seconds": 7,
              "idle_seconds": 0,
              "start_time": "2025-03-08T02:08:35.783Z",
              "end_time": "2025-03-08T02:08:43.028Z",
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-08T02:13:26.820Z",
            "data": {
              "session_id": "session_1741400001677_e6b0ur7",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-03-08T02:13:21.677Z",
              "end_time": "2025-03-08T02:13:26.603Z",
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-08T02:15:10.783Z",
            "data": {
              "session_id": "session_1741400103941_j5cyupy",
              "duration_seconds": 7,
              "idle_seconds": 0,
              "start_time": "2025-03-08T02:15:03.941Z",
              "end_time": "2025-03-08T02:15:10.776Z",
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-08T04:28:11.661Z",
            "data": {
              "session_id": "session_1741408073408_hf93b1y",
              "duration_seconds": 18,
              "idle_seconds": 0,
              "start_time": "2025-03-08T04:27:53.408Z",
              "end_time": "2025-03-08T04:28:11.461Z",
              "total_elapsed_seconds": 18
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-08T06:14:00.611Z",
            "data": {
              "session_id": "session_1741414431766_af2yvme",
              "duration_seconds": 9,
              "idle_seconds": 0,
              "start_time": "2025-03-08T06:13:51.766Z",
              "end_time": "2025-03-08T06:14:00.393Z",
              "total_elapsed_seconds": 9
            }
          }
        ],
        "legacy_id": "1503.03585"
      },
      "meta": {
        "created_at": "2025-03-08T01:39:11+00:00",
        "updated_at": "2025-03-29T22:45:15+00:00",
        "version": 12
      }
    },
    "interactions:arxiv.2011.13456": {
      "data": {
        "paper_id": "arxiv.2011.13456",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-08T02:13:18.147Z",
            "data": {
              "session_id": "session_1741399976723_44v39sw",
              "duration_seconds": 20,
              "idle_seconds": 0,
              "start_time": "2025-03-08T02:12:56.723Z",
              "end_time": "2025-03-08T02:13:17.204Z",
              "total_elapsed_seconds": 20
            }
          }
        ],
        "legacy_id": "2011.13456"
      },
      "meta": {
        "created_at": "2025-03-08T02:12:50+00:00",
        "updated_at": "2025-03-29T22:45:14+00:00",
        "version": 5
      }
    },
    "paper:arxiv.2011.13456": {
      "data": {
        "primary_id": "arxiv.2011.13456",
        "source": "arxiv",
        "sourceId": "2011.13456",
        "url": "https://arxiv.org/abs/2011.13456",
        "title": "Score-Based Generative Modeling through Stochastic Differential\n  Equations",
        "authors": "Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole",
        "abstract": "Creating noise from data is easy; creating data from noise is generative\nmodeling. We present a stochastic differential equation (SDE) that smoothly\ntransforms a complex data distribution to a known prior distribution by slowly\ninjecting noise, and a corresponding reverse-time SDE that transforms the prior\ndistribution back into the data distribution by slowly removing the noise.\nCrucially, the reverse-time SDE depends only on the time-dependent gradient\nfield (\\aka, score) of the perturbed data distribution. By leveraging advances\nin score-based generative modeling, we can accurately estimate these scores\nwith neural networks, and use numerical SDE solvers to generate samples. We\nshow that this framework encapsulates previous approaches in score-based\ngenerative modeling and diffusion probabilistic modeling, allowing for new\nsampling procedures and new modeling capabilities. In particular, we introduce\na predictor-corrector framework to correct errors in the evolution of the\ndiscretized reverse-time SDE. We also derive an equivalent neural ODE that\nsamples from the same distribution as the SDE, but additionally enables exact\nlikelihood computation, and improved sampling efficiency. In addition, we\nprovide a new way to solve inverse problems with score-based models, as\ndemonstrated with experiments on class-conditional generation, image\ninpainting, and colorization. Combined with multiple architectural\nimprovements, we achieve record-breaking performance for unconditional image\ngeneration on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a\ncompetitive likelihood of 2.99 bits/dim, and demonstrate high fidelity\ngeneration of 1024 x 1024 images for the first time from a score-based\ngenerative model.",
        "timestamp": "2025-03-08T02:12:29.792Z",
        "rating": "novote",
        "arxivId": "2011.13456",
        "arxiv_tags": [
          "cs.LG",
          "stat.ML"
        ],
        "published_date": "2020-11-26T19:39:10Z"
      },
      "meta": {
        "created_at": "2025-03-08T02:12:30+00:00",
        "updated_at": "2025-03-29T22:45:15+00:00",
        "version": 2
      }
    },
    "interactions:openreview.y7rbX884LZrX": {
      "data": {
        "paper_id": "openreview.y7rbX884LZrX",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-08T02:25:49.095Z",
            "data": {
              "session_id": "session_1741400735605_je3anep",
              "duration_seconds": 13,
              "idle_seconds": 0,
              "start_time": "2025-03-08T02:25:35.605Z",
              "end_time": "2025-03-08T02:25:48.324Z",
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-08T02:25:49.095Z",
            "data": {
              "session_id": "session_1741400735605_je3anep",
              "duration_seconds": 13,
              "idle_seconds": 0,
              "start_time": "2025-03-08T02:25:35.605Z",
              "end_time": "2025-03-08T02:25:48.324Z",
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-08T02:25:52.542Z",
            "data": {
              "session_id": "session_1741400735605_je3anep",
              "duration_seconds": 13,
              "idle_seconds": 0,
              "start_time": "2025-03-08T02:25:35.605Z",
              "end_time": "2025-03-08T02:25:48.324Z",
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-08T03:37:54.596Z",
            "data": {
              "session_id": "session_1741405061993_rfol4ry",
              "duration_seconds": 12,
              "idle_seconds": 0,
              "start_time": "2025-03-08T03:37:41.993Z",
              "end_time": "2025-03-08T03:37:54.379Z",
              "total_elapsed_seconds": 12
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-08T02:25:49+00:00",
        "updated_at": "2025-03-29T22:45:13+00:00",
        "version": 7
      }
    },
    "paper:openreview.y7rbX884LZrX": {
      "data": {
        "primary_id": "openreview.y7rbX884LZrX",
        "source": "openreview",
        "sourceId": "y7rbX884LZrX",
        "url": "https://openreview.net/forum?id=y7rbX884LZrX&referrer=%5Bthe%20profile%20of%20Lichuan%20Xiang%5D(%2Fprofile%3Fid%3D~Lichuan_Xiang1)",
        "title": "OPENREVIEW Paper: y7rbX884LZrX",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-08T02:25:36.110Z",
        "rating": "novote",
        "identifiers": {
          "original": "y7rbX884LZrX",
          "url": "https://openreview.net/forum?id=y7rbX884LZrX&referrer=%5Bthe%20profile%20of%20Lichuan%20Xiang%5D(%2Fprofile%3Fid%3D~Lichuan_Xiang1)"
        }
      },
      "meta": {
        "created_at": "2025-03-08T02:25:36+00:00",
        "updated_at": "2025-03-29T22:45:14+00:00",
        "version": 2
      }
    },
    "paper:openreview.A9FUg5vrw7Y6": {
      "data": {
        "primary_id": "openreview.A9FUg5vrw7Y6",
        "source": "openreview",
        "sourceId": "A9FUg5vrw7Y6",
        "url": "https://openreview.net/forum?id=A9FUg5vrw7Y6",
        "title": "OPENREVIEW Paper: A9FUg5vrw7Y6",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-08T03:38:59.724Z",
        "rating": "novote",
        "identifiers": {
          "original": "A9FUg5vrw7Y6",
          "url": "https://openreview.net/forum?id=A9FUg5vrw7Y6"
        }
      },
      "meta": {
        "created_at": "2025-03-08T03:39:00+00:00",
        "updated_at": "2025-03-29T22:45:12+00:00",
        "version": 2
      }
    },
    "interactions:openreview.A9FUg5vrw7Y6": {
      "data": {
        "paper_id": "openreview.A9FUg5vrw7Y6",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-08T03:39:20.114Z",
            "data": {
              "session_id": "session_1741405139196_emf3u8s",
              "duration_seconds": 20,
              "idle_seconds": 0,
              "start_time": "2025-03-08T03:38:59.196Z",
              "end_time": "2025-03-08T03:39:19.395Z",
              "total_elapsed_seconds": 20
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-08T03:39:20+00:00",
        "updated_at": "2025-03-29T22:45:12+00:00",
        "version": 3
      }
    },
    "paper:openreview.aWXnKanInf": {
      "data": {
        "primary_id": "openreview.aWXnKanInf",
        "source": "openreview",
        "sourceId": "aWXnKanInf",
        "url": "https://openreview.net/pdf?id=aWXnKanInf",
        "title": "OPENREVIEW Paper: aWXnKanInf",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-08T04:31:09.337Z",
        "rating": "novote",
        "identifiers": {
          "original": "aWXnKanInf",
          "url": "https://openreview.net/pdf?id=aWXnKanInf"
        }
      },
      "meta": {
        "created_at": "2025-03-08T04:31:09+00:00",
        "updated_at": "2025-03-29T22:45:11+00:00",
        "version": 2
      }
    },
    "interactions:openreview.aWXnKanInf": {
      "data": {
        "paper_id": "openreview.aWXnKanInf",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-08T04:35:18.066Z",
            "data": {
              "session_id": "session_1741408500457_0hdibj8",
              "duration_seconds": 17,
              "idle_seconds": 0,
              "start_time": "2025-03-08T04:35:00.457Z",
              "end_time": "2025-03-08T04:35:17.490Z",
              "total_elapsed_seconds": 17
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-08T04:58:03.219Z",
            "data": {
              "session_id": "session_1741409872232_4pn6qhw",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-03-08T04:57:52.232Z",
              "end_time": "2025-03-08T04:58:00.615Z",
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-08T04:33:10+00:00",
        "updated_at": "2025-03-29T22:45:10+00:00",
        "version": 9
      }
    },
    "interactions:openreview.eHehzSDUFp": {
      "data": {
        "paper_id": "openreview.eHehzSDUFp",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-08T04:53:32.113Z",
            "data": {
              "session_id": "session_1741409546272_cuhhrgq",
              "duration_seconds": 65,
              "idle_seconds": 0,
              "start_time": "2025-03-08T04:52:26.272Z",
              "end_time": "2025-03-08T04:53:31.210Z",
              "total_elapsed_seconds": 65
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-08T04:57:49.993Z",
            "data": {
              "session_id": "session_1741409619202_5e9ydxr",
              "duration_seconds": 249,
              "idle_seconds": 0,
              "start_time": "2025-03-08T04:53:39.202Z",
              "end_time": "2025-03-08T04:57:47.984Z",
              "total_elapsed_seconds": 249
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-08T06:59:04.941Z",
            "data": {
              "session_id": "session_1741417137618_g57ion3",
              "duration_seconds": 7,
              "idle_seconds": 0,
              "start_time": "2025-03-08T06:58:57.618Z",
              "end_time": "2025-03-08T06:59:04.750Z",
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-08T04:53:32+00:00",
        "updated_at": "2025-03-29T22:45:08+00:00",
        "version": 6
      }
    },
    "paper:openreview.eHehzSDUFp": {
      "data": {
        "primary_id": "openreview.eHehzSDUFp",
        "source": "openreview",
        "sourceId": "eHehzSDUFp",
        "url": "https://openreview.net/forum?id=eHehzSDUFp",
        "title": "OPENREVIEW Paper: eHehzSDUFp",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-08T04:52:26.764Z",
        "rating": "novote",
        "identifiers": {
          "original": "eHehzSDUFp",
          "url": "https://openreview.net/forum?id=eHehzSDUFp"
        }
      },
      "meta": {
        "created_at": "2025-03-08T04:52:27+00:00",
        "updated_at": "2025-03-29T22:45:09+00:00",
        "version": 2
      }
    },
    "paper:openreview.is4nCVkSFA": {
      "data": {
        "primary_id": "openreview.is4nCVkSFA",
        "source": "openreview",
        "sourceId": "is4nCVkSFA",
        "url": "https://openreview.net/pdf?id=is4nCVkSFA",
        "title": "OPENREVIEW Paper: is4nCVkSFA",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-08T04:59:50.821Z",
        "rating": "novote",
        "identifiers": {
          "original": "is4nCVkSFA",
          "url": "https://openreview.net/pdf?id=is4nCVkSFA"
        }
      },
      "meta": {
        "created_at": "2025-03-08T04:59:51+00:00",
        "updated_at": "2025-03-29T22:45:07+00:00",
        "version": 2
      }
    },
    "interactions:openreview.is4nCVkSFA": {
      "data": {
        "paper_id": "openreview.is4nCVkSFA",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-08T05:02:54.301Z",
            "data": {
              "session_id": "session_1741409990301_ndjhytz",
              "duration_seconds": 183,
              "idle_seconds": 0,
              "start_time": "2025-03-08T04:59:50.301Z",
              "end_time": "2025-03-08T05:02:53.409Z",
              "total_elapsed_seconds": 183
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-08T05:02:56.771Z",
            "data": {
              "session_id": "session_1741409990301_ndjhytz",
              "duration_seconds": 183,
              "idle_seconds": 0,
              "start_time": "2025-03-08T04:59:50.301Z",
              "end_time": "2025-03-08T05:02:53.409Z",
              "total_elapsed_seconds": 183
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-08T05:02:55+00:00",
        "updated_at": "2025-03-29T22:45:07+00:00",
        "version": 4
      }
    },
    "interactions:openreview.tyEyYT267x": {
      "data": {
        "paper_id": "openreview.tyEyYT267x",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-08T05:08:52.450Z",
            "data": {
              "session_id": "session_1741410527185_ncuze73",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-03-08T05:08:47.185Z",
              "end_time": "2025-03-08T05:08:52.439Z",
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-08T06:11:43.195Z",
            "data": {
              "session_id": "session_1741414295772_8849l8v",
              "duration_seconds": 7,
              "idle_seconds": 0,
              "start_time": "2025-03-08T06:11:35.772Z",
              "end_time": "2025-03-08T06:11:42.989Z",
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-08T06:16:24.380Z",
            "data": {
              "session_id": "session_1741414576542_qv3nqn4",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-03-08T06:16:16.542Z",
              "end_time": "2025-03-08T06:16:24.373Z",
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-08T06:17:44.065Z",
            "data": {
              "session_id": "session_1741414659814_cwjfcrx",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-03-08T06:17:39.814Z",
              "end_time": "2025-03-08T06:17:42.884Z",
              "total_elapsed_seconds": 3
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-08T05:08:11+00:00",
        "updated_at": "2025-03-29T22:45:06+00:00",
        "version": 8
      }
    },
    "paper:openreview.tyEyYT267x": {
      "data": {
        "primary_id": "openreview.tyEyYT267x",
        "source": "openreview",
        "sourceId": "tyEyYT267x",
        "url": "https://openreview.net/forum?id=tyEyYT267x",
        "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models",
        "authors": "Marianne Arriola",
        "abstract": "Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences. We provide the code, along with the model weights and blog post on the project page: https://mariannearriola.github.io/bd3-lms",
        "timestamp": "2025-03-08T05:08:04.332Z",
        "rating": "novote",
        "identifiers": {
          "original": "tyEyYT267x",
          "url": "https://openreview.net/forum?id=tyEyYT267x"
        }
      },
      "meta": {
        "created_at": "2025-03-08T05:08:04+00:00",
        "updated_at": "2025-03-29T22:45:06+00:00",
        "version": 2
      }
    },
    "paper:openreview.WJaUkwci9o": {
      "data": {
        "primary_id": "openreview.WJaUkwci9o",
        "source": "openreview",
        "sourceId": "WJaUkwci9o",
        "url": "https://openreview.net/forum?id=WJaUkwci9o",
        "title": "Self-Improvement in Language Models: The Sharpening Mechanism",
        "authors": "Audrey Huang",
        "abstract": "Recent work in language modeling has raised the possibility of \u201cself-improvement,\u201d where an LLM evaluates and refines its own generations to achieve higher performance without external feedback. It is impossible for this self-improvement to create information that is not already in the model, so why should we expect that this will lead to improved capabilities? We offer a new theoretical perspective on the capabilities of self-improvement through a lens we refer to as \u201csharpening.\u201d Motivated by the observation that language models are often better at verifying response quality than they are at generating correct responses, we formalize self-improvement as using the model itself as a verifier during post-training in order to \u2018sharpen\u2019 the model to one placing large mass on high-quality sequences, thereby amortizing the expensive inference-time computation of generating good sequences. We begin by introducing a new statistical framework for sharpening in which the learner has sample access to a pre-trained base policy. Then, we analyze two natural families of self improvement algorithms based on SFT and RLHF. We find that (i) the SFT-based approach is minimax optimal whenever the initial model has sufficient coverage, but (ii) the RLHF-based approach can improve over SFT-based self- improvement by leveraging online exploration, bypassing the need for coverage. We view these findings as a starting point toward a foundational understanding that can guide the design and evaluation of self-improvement algorithms.",
        "timestamp": "2025-03-08T06:12:26.065Z",
        "rating": "novote",
        "identifiers": {
          "original": "WJaUkwci9o",
          "url": "https://openreview.net/forum?id=WJaUkwci9o"
        }
      },
      "meta": {
        "created_at": "2025-03-08T06:12:26+00:00",
        "updated_at": "2025-03-29T22:45:05+00:00",
        "version": 2
      }
    },
    "interactions:openreview.WJaUkwci9o": {
      "data": {
        "paper_id": "openreview.WJaUkwci9o",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-08T06:12:34.988Z",
            "data": {
              "session_id": "session_1741414345541_hfs9288",
              "duration_seconds": 9,
              "idle_seconds": 0,
              "start_time": "2025-03-08T06:12:25.541Z",
              "end_time": "2025-03-08T06:12:34.156Z",
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-08T06:14:31.513Z",
            "data": {
              "session_id": "session_1741414460721_qfs54l4",
              "duration_seconds": 11,
              "idle_seconds": 0,
              "start_time": "2025-03-08T06:14:20.721Z",
              "end_time": "2025-03-08T06:14:31.505Z",
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T03:20:59.707Z",
            "data": {
              "session_id": "session_1741490450414_92bymav",
              "duration_seconds": 9,
              "idle_seconds": 0,
              "start_time": "2025-03-09T03:20:50.414Z",
              "end_time": "2025-03-09T03:20:59.526Z",
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T03:35:18.464Z",
            "data": {
              "session_id": "session_1741491314688_wsca1dd",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-03-09T03:35:14.688Z",
              "end_time": "2025-03-09T03:35:18.262Z",
              "total_elapsed_seconds": 4
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-08T06:12:35+00:00",
        "updated_at": "2025-03-29T22:45:05+00:00",
        "version": 7
      }
    },
    "interactions:arxiv.2211.09619": {
      "data": {
        "paper_id": "arxiv.2211.09619",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-08T07:17:35.704Z",
            "data": {
              "session_id": "session_1741418240286_dtl3aq5",
              "duration_seconds": 9,
              "idle_seconds": 0,
              "start_time": "2025-03-08T07:17:20.286Z",
              "end_time": "2025-03-08T07:17:29.576Z",
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T02:19:40.735Z",
            "data": {
              "session_id": "session_1741486776755_7dyc4yq",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-03-09T02:19:36.755Z",
              "end_time": "2025-03-09T02:19:40.550Z",
              "total_elapsed_seconds": 4
            }
          }
        ],
        "legacy_id": "2211.09619"
      },
      "meta": {
        "created_at": "2025-03-08T07:17:31+00:00",
        "updated_at": "2025-03-29T22:45:03+00:00",
        "version": 5
      }
    },
    "paper:arxiv.2211.09619": {
      "data": {
        "primary_id": "arxiv.2211.09619",
        "source": "arxiv",
        "sourceId": "2211.09619",
        "url": "https://arxiv.org/abs/2211.09619",
        "title": "Introduction to Online Control",
        "authors": "Elad Hazan, Karan Singh",
        "abstract": "This text presents an introduction to an emerging paradigm in control of\ndynamical systems and differentiable reinforcement learning called online\nnonstochastic control. The new approach applies techniques from online convex\noptimization and convex relaxations to obtain new methods with provable\nguarantees for classical settings in optimal and robust control.\n  The primary distinction between online nonstochastic control and other\nframeworks is the objective. In optimal control, robust control, and other\ncontrol methodologies that assume stochastic noise, the goal is to perform\ncomparably to an offline optimal strategy. In online nonstochastic control,\nboth the cost functions as well as the perturbations from the assumed dynamical\nmodel are chosen by an adversary. Thus the optimal policy is not defined a\npriori. Rather, the target is to attain low regret against the best policy in\nhindsight from a benchmark class of policies.\n  This objective suggests the use of the decision making framework of online\nconvex optimization as an algorithmic methodology. The resulting methods are\nbased on iterative mathematical optimization algorithms, and are accompanied by\nfinite-time regret and computational complexity guarantees.",
        "timestamp": "2025-03-08T07:17:20.703Z",
        "rating": "novote",
        "arxivId": "2211.09619",
        "arxiv_tags": [
          "cs.LG",
          "cs.RO",
          "cs.SY",
          "eess.SY",
          "math.OC",
          "stat.ML"
        ],
        "published_date": "2022-11-17T16:12:45Z"
      },
      "meta": {
        "created_at": "2025-03-08T07:17:21+00:00",
        "updated_at": "2025-03-29T22:45:04+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2502.13130": {
      "data": {
        "paper_id": "arxiv.2502.13130",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-08T21:19:58.942Z",
            "data": {
              "session_id": "session_1741468787933_bi3934c",
              "duration_seconds": 10,
              "idle_seconds": 0,
              "start_time": "2025-03-08T21:19:47.933Z",
              "end_time": "2025-03-08T21:19:58.150Z",
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-08T21:20:00.024Z",
            "data": {
              "session_id": "session_1741468787933_bi3934c",
              "duration_seconds": 10,
              "idle_seconds": 0,
              "start_time": "2025-03-08T21:19:47.933Z",
              "end_time": "2025-03-08T21:19:58.150Z",
              "total_elapsed_seconds": 10
            }
          }
        ],
        "legacy_id": "2502.13130"
      },
      "meta": {
        "created_at": "2025-03-08T21:19:59+00:00",
        "updated_at": "2025-03-29T22:45:02+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2502.13130": {
      "data": {
        "primary_id": "arxiv.2502.13130",
        "source": "arxiv",
        "sourceId": "2502.13130",
        "url": "https://arxiv.org/pdf/2502.13130v1",
        "title": "Magma: A Foundation Model for Multimodal AI Agents",
        "authors": "Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng, Yongyuan Liang, Yu Gu, Mu Cai, Seonghyeon Ye, Joel Jang, Yuquan Deng, Lars Liden, Jianfeng Gao",
        "abstract": "We present Magma, a foundation model that serves multimodal AI agentic tasks\nin both the digital and physical worlds. Magma is a significant extension of\nvision-language (VL) models in that it not only retains the VL understanding\nability (verbal intelligence) of the latter, but is also equipped with the\nability to plan and act in the visual-spatial world (spatial-temporal\nintelligence) and complete agentic tasks ranging from UI navigation to robot\nmanipulation. To endow the agentic capabilities, Magma is pretrained on large\namounts of heterogeneous datasets spanning from images, videos to robotics\ndata, where the actionable visual objects (e.g., clickable buttons in GUI) in\nimages are labeled by Set-of-Mark (SoM) for action grounding, and the object\nmovements (e.g., the trace of human hands or robotic arms) in videos are\nlabeled by Trace-of-Mark (ToM) for action planning. Extensive experiments show\nthat SoM and ToM reach great synergy and facilitate the acquisition of\nspatial-temporal intelligence for our Magma model, which is fundamental to a\nwide range of tasks as shown in Fig.1. In particular, Magma creates new\nstate-of-the-art results on UI navigation and robotic manipulation tasks,\noutperforming previous models that are specifically tailored to these tasks. On\nimage and video-related multimodal tasks, Magma also compares favorably to\npopular large multimodal models that are trained on much larger datasets. We\nmake our model and code public for reproducibility at\nhttps://microsoft.github.io/Magma.",
        "timestamp": "2025-03-08T21:19:49.124Z",
        "rating": "novote",
        "arxivId": "2502.13130",
        "arxiv_tags": [
          "cs.CV",
          "cs.AI",
          "cs.HC",
          "cs.LG",
          "cs.RO"
        ],
        "published_date": "2025-02-18T18:55:21Z"
      },
      "meta": {
        "created_at": "2025-03-08T21:19:49+00:00",
        "updated_at": "2025-03-29T22:45:03+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2502.17019": {
      "data": {
        "paper_id": "arxiv.2502.17019",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T03:20:38.137Z",
            "data": {
              "session_id": "session_1741490408019_do13zbj",
              "duration_seconds": 25,
              "idle_seconds": 0,
              "start_time": "2025-03-09T03:20:08.019Z",
              "end_time": "2025-03-09T03:20:32.663Z",
              "total_elapsed_seconds": 25
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-09T03:20:33+00:00",
        "updated_at": "2025-03-29T22:45:02+00:00",
        "version": 4
      }
    },
    "paper:openreview.0vtftmYQGV": {
      "data": {
        "primary_id": "openreview.0vtftmYQGV",
        "source": "openreview",
        "sourceId": "0vtftmYQGV",
        "url": "https://openreview.net/forum?id=0vtftmYQGV",
        "title": "OpenReview Paper: 0vtftmYQGV",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-09T05:00:11.819Z",
        "rating": "novote",
        "source_specific_metadata": {
          "forum_id": "0vtftmYQGV"
        },
        "identifiers": {
          "original": "0vtftmYQGV",
          "url": "https://openreview.net/forum?id=0vtftmYQGV"
        }
      },
      "meta": {
        "created_at": "2025-03-09T05:00:12+00:00",
        "updated_at": "2025-03-29T22:45:00+00:00",
        "version": 2
      }
    },
    "interactions:openreview.rwqShzb9li": {
      "data": {
        "paper_id": "openreview.rwqShzb9li",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T03:52:40.096Z",
            "data": {
              "session_id": "session_1741492354910_5z9hkvq",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-03-09T03:52:34.910Z",
              "end_time": "2025-03-09T03:52:40.079Z",
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T03:52:40.096Z",
            "data": {
              "session_id": "session_1741492354910_5z9hkvq",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-03-09T03:52:34.910Z",
              "end_time": "2025-03-09T03:52:40.079Z",
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T03:53:46.219Z",
            "data": {
              "session_id": "session_1741492362420_xnve2v9",
              "duration_seconds": 63,
              "idle_seconds": 0,
              "start_time": "2025-03-09T03:52:42.420Z",
              "end_time": "2025-03-09T03:53:45.680Z",
              "total_elapsed_seconds": 63
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-09T03:52:29+00:00",
        "updated_at": "2025-03-29T22:45:00+00:00",
        "version": 7
      }
    },
    "paper:openreview.rwqShzb9li": {
      "data": {
        "primary_id": "openreview.rwqShzb9li",
        "source": "openreview",
        "sourceId": "rwqShzb9li",
        "url": "https://openreview.net/forum?id=rwqShzb9li",
        "title": "OpenReview Paper: rwqShzb9li",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-09T03:52:19.930Z",
        "rating": "novote",
        "source_specific_metadata": {
          "forum_id": "rwqShzb9li"
        },
        "identifiers": {
          "original": "rwqShzb9li",
          "url": "https://openreview.net/forum?id=rwqShzb9li"
        }
      },
      "meta": {
        "created_at": "2025-03-09T03:52:20+00:00",
        "updated_at": "2025-03-29T22:45:01+00:00",
        "version": 2
      }
    },
    "interactions:openreview.0vtftmYQGV": {
      "data": {
        "paper_id": "openreview.0vtftmYQGV",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T05:00:19.527Z",
            "data": {
              "session_id": "session_1741496411377_jl90po6",
              "duration_seconds": 7,
              "idle_seconds": 0,
              "start_time": "2025-03-09T05:00:11.377Z",
              "end_time": "2025-03-09T05:00:18.806Z",
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T05:02:22.058Z",
            "data": {
              "session_id": "session_1741496532914_qime230",
              "duration_seconds": 9,
              "idle_seconds": 0,
              "start_time": "2025-03-09T05:02:12.914Z",
              "end_time": "2025-03-09T05:02:22.041Z",
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T05:02:22.148Z",
            "data": {
              "session_id": "session_1741496532914_qime230",
              "duration_seconds": 9,
              "idle_seconds": 0,
              "start_time": "2025-03-09T05:02:12.914Z",
              "end_time": "2025-03-09T05:02:22.041Z",
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-09T05:00:20+00:00",
        "updated_at": "2025-03-29T22:44:59+00:00",
        "version": 5
      }
    },
    "interactions:openreview.9DrPvYCETp": {
      "data": {
        "paper_id": "openreview.9DrPvYCETp",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T05:15:32.984Z",
            "data": {
              "session_id": "session_1741497286858_93r4ho4",
              "duration_seconds": 44,
              "idle_seconds": 0,
              "start_time": "2025-03-09T05:14:46.858Z",
              "end_time": "2025-03-09T05:15:30.914Z",
              "total_elapsed_seconds": 44
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-09T05:14:40+00:00",
        "updated_at": "2025-03-29T22:44:59+00:00",
        "version": 6
      }
    },
    "paper:openreview.9DrPvYCETp": {
      "data": {
        "primary_id": "openreview.9DrPvYCETp",
        "source": "openreview",
        "sourceId": "9DrPvYCETp",
        "url": "https://openreview.net/forum?id=9DrPvYCETp",
        "title": "OpenReview Paper: 9DrPvYCETp",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-09T05:14:25.032Z",
        "rating": "novote",
        "source_specific_metadata": {
          "forum_id": "9DrPvYCETp"
        },
        "identifiers": {
          "original": "9DrPvYCETp",
          "url": "https://openreview.net/forum?id=9DrPvYCETp"
        }
      },
      "meta": {
        "created_at": "2025-03-09T05:14:25+00:00",
        "updated_at": "2025-03-29T22:44:59+00:00",
        "version": 2
      }
    },
    "paper:openreview.u1b1dJtyxc": {
      "data": {
        "primary_id": "openreview.u1b1dJtyxc",
        "source": "openreview",
        "sourceId": "u1b1dJtyxc",
        "url": "https://openreview.net/forum?id=u1b1dJtyxc",
        "title": "OpenReview Paper: u1b1dJtyxc",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-09T05:56:27.928Z",
        "rating": "novote",
        "source_specific_metadata": {
          "forum_id": "u1b1dJtyxc"
        },
        "identifiers": {
          "original": "u1b1dJtyxc",
          "url": "https://openreview.net/forum?id=u1b1dJtyxc"
        }
      },
      "meta": {
        "created_at": "2025-03-09T05:56:28+00:00",
        "updated_at": "2025-03-29T22:44:55+00:00",
        "version": 2
      }
    },
    "paper:openreview.aVh9KRZdRk": {
      "data": {
        "primary_id": "openreview.aVh9KRZdRk",
        "source": "openreview",
        "sourceId": "aVh9KRZdRk",
        "url": "https://openreview.net/forum?id=aVh9KRZdRk",
        "title": "OpenReview Paper: aVh9KRZdRk",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-09T05:55:38.723Z",
        "rating": "novote",
        "source_specific_metadata": {
          "forum_id": "aVh9KRZdRk"
        },
        "identifiers": {
          "original": "aVh9KRZdRk",
          "url": "https://openreview.net/forum?id=aVh9KRZdRk"
        }
      },
      "meta": {
        "created_at": "2025-03-09T05:55:39+00:00",
        "updated_at": "2025-03-29T22:44:56+00:00",
        "version": 2
      }
    },
    "interactions:openreview.owR9ofvkFQ": {
      "data": {
        "paper_id": "openreview.owR9ofvkFQ",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T05:20:23.678Z",
            "data": {
              "session_id": "session_1741497612033_haj2hd6",
              "duration_seconds": 11,
              "idle_seconds": 0,
              "start_time": "2025-03-09T05:20:12.033Z",
              "end_time": "2025-03-09T05:20:22.910Z",
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T05:20:24.214Z",
            "data": {
              "session_id": "session_1741497612033_haj2hd6",
              "duration_seconds": 11,
              "idle_seconds": 0,
              "start_time": "2025-03-09T05:20:12.033Z",
              "end_time": "2025-03-09T05:20:22.910Z",
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T05:20:26.686Z",
            "data": {
              "session_id": "session_1741497612033_haj2hd6",
              "duration_seconds": 11,
              "idle_seconds": 0,
              "start_time": "2025-03-09T05:20:12.033Z",
              "end_time": "2025-03-09T05:20:22.910Z",
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-09T05:20:24+00:00",
        "updated_at": "2025-03-29T22:44:57+00:00",
        "version": 5
      }
    },
    "paper:openreview.owR9ofvkFQ": {
      "data": {
        "primary_id": "openreview.owR9ofvkFQ",
        "source": "openreview",
        "sourceId": "owR9ofvkFQ",
        "url": "https://openreview.net/forum?id=owR9ofvkFQ",
        "title": "OpenReview Paper: owR9ofvkFQ",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-09T05:20:12.358Z",
        "rating": "novote",
        "source_specific_metadata": {
          "forum_id": "owR9ofvkFQ"
        },
        "identifiers": {
          "original": "owR9ofvkFQ",
          "url": "https://openreview.net/forum?id=owR9ofvkFQ"
        }
      },
      "meta": {
        "created_at": "2025-03-09T05:20:12+00:00",
        "updated_at": "2025-03-29T22:44:57+00:00",
        "version": 2
      }
    },
    "interactions:openreview.qK6U4Ahfms": {
      "data": {
        "paper_id": "openreview.qK6U4Ahfms",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T05:19:52.061Z",
            "data": {
              "session_id": "session_1741497571986_qoyoidb",
              "duration_seconds": 20,
              "idle_seconds": 0,
              "start_time": "2025-03-09T05:19:31.986Z",
              "end_time": "2025-03-09T05:19:52.046Z",
              "total_elapsed_seconds": 20
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T05:19:52.061Z",
            "data": {
              "session_id": "session_1741497571986_qoyoidb",
              "duration_seconds": 20,
              "idle_seconds": 0,
              "start_time": "2025-03-09T05:19:31.986Z",
              "end_time": "2025-03-09T05:19:52.046Z",
              "total_elapsed_seconds": 20
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-09T05:19:52+00:00",
        "updated_at": "2025-03-29T22:44:58+00:00",
        "version": 4
      }
    },
    "paper:openreview.qK6U4Ahfms": {
      "data": {
        "primary_id": "openreview.qK6U4Ahfms",
        "source": "openreview",
        "sourceId": "qK6U4Ahfms",
        "url": "https://openreview.net/forum?id=qK6U4Ahfms",
        "title": "OpenReview Paper: qK6U4Ahfms",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-09T05:19:27.154Z",
        "rating": "novote",
        "source_specific_metadata": {
          "forum_id": "qK6U4Ahfms"
        },
        "identifiers": {
          "original": "qK6U4Ahfms",
          "url": "https://openreview.net/forum?id=qK6U4Ahfms"
        }
      },
      "meta": {
        "created_at": "2025-03-09T05:19:27+00:00",
        "updated_at": "2025-03-29T22:44:58+00:00",
        "version": 2
      }
    },
    "interactions:openreview.u1b1dJtyxc": {
      "data": {
        "paper_id": "openreview.u1b1dJtyxc",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T05:57:57.742Z",
            "data": {
              "session_id": "session_1741499843863_8bfowny",
              "duration_seconds": 34,
              "idle_seconds": 0,
              "start_time": "2025-03-09T05:57:23.863Z",
              "end_time": "2025-03-09T05:57:57.729Z",
              "total_elapsed_seconds": 34
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-09T05:57:18+00:00",
        "updated_at": "2025-03-29T22:44:55+00:00",
        "version": 5
      }
    },
    "interactions:openreview.aVh9KRZdRk": {
      "data": {
        "paper_id": "openreview.aVh9KRZdRk",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T05:56:04.126Z",
            "data": {
              "session_id": "session_1741499755880_77v2ntb",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-03-09T05:55:55.880Z",
              "end_time": "2025-03-09T05:56:04.116Z",
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T05:58:07.648Z",
            "data": {
              "session_id": "session_1741499881888_lfact1e",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-03-09T05:58:01.888Z",
              "end_time": "2025-03-09T05:58:07.633Z",
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T05:58:07.648Z",
            "data": {
              "session_id": "session_1741499881888_lfact1e",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-03-09T05:58:01.888Z",
              "end_time": "2025-03-09T05:58:07.633Z",
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T05:59:21.473Z",
            "data": {
              "session_id": "session_1741499890720_2ol9rj3",
              "duration_seconds": 70,
              "idle_seconds": 0,
              "start_time": "2025-03-09T05:58:10.720Z",
              "end_time": "2025-03-09T05:59:21.026Z",
              "total_elapsed_seconds": 70
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-09T05:55:50+00:00",
        "updated_at": "2025-03-29T22:44:56+00:00",
        "version": 8
      }
    },
    "interactions:openreview.REIK4SZMJt": {
      "data": {
        "paper_id": "openreview.REIK4SZMJt",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T06:19:11.447Z",
            "data": {
              "session_id": "session_1741501143777_dk7x38v",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-03-09T06:19:03.777Z",
              "end_time": "2025-03-09T06:19:11.432Z",
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T06:19:11.552Z",
            "data": {
              "session_id": "session_1741501143777_dk7x38v",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-03-09T06:19:03.777Z",
              "end_time": "2025-03-09T06:19:11.432Z",
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T18:18:01.990Z",
            "data": {
              "session_id": "session_1741544258987_i96qrst",
              "duration_seconds": 21,
              "idle_seconds": 0,
              "start_time": "2025-03-09T18:17:38.987Z",
              "end_time": "2025-03-09T18:17:59.973Z",
              "total_elapsed_seconds": 21
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-09T06:18:42+00:00",
        "updated_at": "2025-03-29T22:44:54+00:00",
        "version": 7
      }
    },
    "paper:openreview.REIK4SZMJt": {
      "data": {
        "primary_id": "openreview.REIK4SZMJt",
        "source": "openreview",
        "sourceId": "REIK4SZMJt",
        "url": "https://openreview.net/forum?id=REIK4SZMJt",
        "title": "OpenReview Paper: REIK4SZMJt",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-09T06:18:31.502Z",
        "rating": "novote",
        "source_specific_metadata": {
          "forum_id": "REIK4SZMJt"
        },
        "identifiers": {
          "original": "REIK4SZMJt",
          "url": "https://openreview.net/forum?id=REIK4SZMJt"
        }
      },
      "meta": {
        "created_at": "2025-03-09T06:18:31+00:00",
        "updated_at": "2025-03-29T22:44:54+00:00",
        "version": 2
      }
    },
    "interactions:openreview.gojL67CfS8": {
      "data": {
        "paper_id": "openreview.gojL67CfS8",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T06:31:24.766Z",
            "data": {
              "session_id": "session_1741501873476_ip1ufkh",
              "duration_seconds": 11,
              "idle_seconds": 0,
              "start_time": "2025-03-09T06:31:13.476Z",
              "end_time": "2025-03-09T06:31:24.012Z",
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T06:34:03.974Z",
            "data": {
              "session_id": "session_1741502031048_6bmi6p1",
              "duration_seconds": 13,
              "idle_seconds": 0,
              "start_time": "2025-03-09T06:33:51.048Z",
              "end_time": "2025-03-09T06:34:03.959Z",
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T06:34:04.097Z",
            "data": {
              "session_id": "session_1741502031048_6bmi6p1",
              "duration_seconds": 13,
              "idle_seconds": 0,
              "start_time": "2025-03-09T06:33:51.048Z",
              "end_time": "2025-03-09T06:34:03.959Z",
              "total_elapsed_seconds": 13
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-09T06:31:07+00:00",
        "updated_at": "2025-03-29T22:44:53+00:00",
        "version": 8
      }
    },
    "paper:openreview.gojL67CfS8": {
      "data": {
        "primary_id": "openreview.gojL67CfS8",
        "source": "openreview",
        "sourceId": "gojL67CfS8",
        "url": "https://openreview.net/forum?id=gojL67CfS8",
        "title": "OpenReview Paper: gojL67CfS8",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-09T06:30:39.994Z",
        "rating": "novote",
        "source_specific_metadata": {
          "forum_id": "gojL67CfS8"
        },
        "identifiers": {
          "original": "gojL67CfS8",
          "url": "https://openreview.net/forum?id=gojL67CfS8"
        }
      },
      "meta": {
        "created_at": "2025-03-09T06:30:40+00:00",
        "updated_at": "2025-03-29T22:44:54+00:00",
        "version": 2
      }
    },
    "interactions:openreview.uNKlTQ8mBD": {
      "data": {
        "paper_id": "openreview.uNKlTQ8mBD",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T06:38:16.471Z",
            "data": {
              "session_id": "session_1741502291146_ozgvc9m",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-03-09T06:38:11.146Z",
              "end_time": "2025-03-09T06:38:15.350Z",
              "total_elapsed_seconds": 4
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-09T06:37:21+00:00",
        "updated_at": "2025-03-29T22:44:52+00:00",
        "version": 8
      }
    },
    "paper:openreview.uNKlTQ8mBD": {
      "data": {
        "primary_id": "openreview.uNKlTQ8mBD",
        "source": "openreview",
        "sourceId": "uNKlTQ8mBD",
        "url": "https://openreview.net/forum?id=uNKlTQ8mBD",
        "title": "OpenReview Paper: uNKlTQ8mBD",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-09T06:36:46.954Z",
        "rating": "novote",
        "source_specific_metadata": {
          "forum_id": "uNKlTQ8mBD"
        },
        "identifiers": {
          "original": "uNKlTQ8mBD",
          "url": "https://openreview.net/forum?id=uNKlTQ8mBD"
        }
      },
      "meta": {
        "created_at": "2025-03-09T06:36:47+00:00",
        "updated_at": "2025-03-29T22:44:53+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2502.21098v1": {
      "data": {
        "paper_id": "arxiv.2502.21098v1",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T21:22:22.289Z",
            "data": {
              "session_id": "session_1741555328715_a3xlioj",
              "duration_seconds": 13,
              "idle_seconds": 0,
              "start_time": "2025-03-09T21:22:08.715Z",
              "end_time": "2025-03-09T21:22:21.604Z",
              "total_elapsed_seconds": 13
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-09T21:22:23+00:00",
        "updated_at": "2025-03-29T22:44:51+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2502.21098v1": {
      "data": {
        "primary_id": "arxiv.2502.21098v1",
        "source": "arxiv",
        "sourceId": "2502.21098v1",
        "url": "https://arxiv.org/html/2502.21098v1",
        "title": "2502.21098v1",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-09T21:22:09.261Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [],
          "published_date": ""
        },
        "identifiers": {
          "original": "2502.21098v1",
          "url": "https://arxiv.org/html/2502.21098v1"
        }
      },
      "meta": {
        "created_at": "2025-03-09T21:22:09+00:00",
        "updated_at": "2025-03-29T22:44:52+00:00",
        "version": 2
      }
    },
    "paper:openreview.Q5RYn6jagC": {
      "data": {
        "primary_id": "openreview.Q5RYn6jagC",
        "source": "openreview",
        "sourceId": "Q5RYn6jagC",
        "url": "https://openreview.net/forum?id=Q5RYn6jagC",
        "title": "OpenReview Paper: Q5RYn6jagC",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-09T21:26:52.639Z",
        "rating": "novote",
        "source_specific_metadata": {
          "forum_id": "Q5RYn6jagC"
        },
        "identifiers": {
          "original": "Q5RYn6jagC",
          "url": "https://openreview.net/forum?id=Q5RYn6jagC"
        }
      },
      "meta": {
        "created_at": "2025-03-09T21:26:53+00:00",
        "updated_at": "2025-03-29T22:44:49+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2407.16551": {
      "data": {
        "paper_id": "arxiv.2407.16551",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T21:25:28.805Z",
            "data": {
              "session_id": "session_1741555501303_rhzmyus",
              "duration_seconds": 21,
              "idle_seconds": 0,
              "start_time": "2025-03-09T21:25:01.303Z",
              "end_time": "2025-03-09T21:25:22.669Z",
              "total_elapsed_seconds": 21
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-09T21:25:24+00:00",
        "updated_at": "2025-03-29T22:44:50+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2407.16551": {
      "data": {
        "primary_id": "arxiv.2407.16551",
        "source": "arxiv",
        "sourceId": "2407.16551",
        "url": "https://arxiv.org/abs/2407.16551",
        "title": "2407.16551",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-09T21:25:01.469Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [],
          "published_date": ""
        },
        "identifiers": {
          "original": "2407.16551",
          "url": "https://arxiv.org/abs/2407.16551"
        }
      },
      "meta": {
        "created_at": "2025-03-09T21:25:01+00:00",
        "updated_at": "2025-03-29T22:44:51+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2408.04681": {
      "data": {
        "paper_id": "arxiv.2408.04681",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T22:40:18.202Z",
            "data": {
              "session_id": "session_1741560008174_n6b3gpa",
              "duration_seconds": 9,
              "idle_seconds": 0,
              "start_time": "2025-03-09T22:40:08.174Z",
              "end_time": "2025-03-09T22:40:17.496Z",
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-09T22:40:19+00:00",
        "updated_at": "2025-03-29T22:44:48+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2408.04681": {
      "data": {
        "primary_id": "arxiv.2408.04681",
        "source": "arxiv",
        "sourceId": "2408.04681",
        "url": "https://arxiv.org/abs/2408.04681",
        "title": "2408.04681",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-09T22:40:08.698Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [],
          "published_date": ""
        },
        "identifiers": {
          "original": "2408.04681",
          "url": "https://arxiv.org/abs/2408.04681"
        }
      },
      "meta": {
        "created_at": "2025-03-09T22:40:09+00:00",
        "updated_at": "2025-03-29T22:44:48+00:00",
        "version": 2
      }
    },
    "interactions:openreview.Q5RYn6jagC": {
      "data": {
        "paper_id": "openreview.Q5RYn6jagC",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T21:27:29.140Z",
            "data": {
              "session_id": "session_1741555636359_n6842k7",
              "duration_seconds": 13,
              "idle_seconds": 0,
              "start_time": "2025-03-09T21:27:16.359Z",
              "end_time": "2025-03-09T21:27:29.120Z",
              "total_elapsed_seconds": 13
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-09T21:27:03+00:00",
        "updated_at": "2025-03-29T22:44:49+00:00",
        "version": 4
      }
    },
    "interactions:openreview.HRnSVflpgt": {
      "data": {
        "paper_id": "openreview.HRnSVflpgt",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T22:44:14.641Z",
            "data": {
              "session_id": "session_1741560232777_t6004ez",
              "duration_seconds": 21,
              "idle_seconds": 0,
              "start_time": "2025-03-09T22:43:52.777Z",
              "end_time": "2025-03-09T22:44:13.934Z",
              "total_elapsed_seconds": 21
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-09T23:06:17.252Z",
            "data": {
              "session_id": "session_1741561571363_s4uo67h",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-03-09T23:06:11.363Z",
              "end_time": "2025-03-09T23:06:17.059Z",
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-09T22:44:15+00:00",
        "updated_at": "2025-03-29T22:44:47+00:00",
        "version": 4
      }
    },
    "paper:openreview.HRnSVflpgt": {
      "data": {
        "primary_id": "openreview.HRnSVflpgt",
        "source": "openreview",
        "sourceId": "HRnSVflpgt",
        "url": "https://openreview.net/forum?id=HRnSVflpgt",
        "title": "OpenReview Paper: HRnSVflpgt",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-09T22:43:53.280Z",
        "rating": "novote",
        "source_specific_metadata": {
          "forum_id": "HRnSVflpgt"
        },
        "identifiers": {
          "original": "HRnSVflpgt",
          "url": "https://openreview.net/forum?id=HRnSVflpgt"
        }
      },
      "meta": {
        "created_at": "2025-03-09T22:43:53+00:00",
        "updated_at": "2025-03-29T22:44:47+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2502.05082": {
      "data": {
        "primary_id": "arxiv.2502.05082",
        "source": "arxiv",
        "sourceId": "2502.05082",
        "url": "https://arxiv.org/abs/2502.05082",
        "title": "2502.05082",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-10T03:57:40.626Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [],
          "published_date": ""
        },
        "identifiers": {
          "original": "2502.05082",
          "url": "https://arxiv.org/abs/2502.05082"
        }
      },
      "meta": {
        "created_at": "2025-03-10T03:57:41+00:00",
        "updated_at": "2025-03-29T22:44:45+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.05003": {
      "data": {
        "paper_id": "arxiv.2503.05003",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T03:56:32.582Z",
            "data": {
              "session_id": "session_1741578974828_9tdhtei",
              "duration_seconds": 17,
              "idle_seconds": 0,
              "start_time": "2025-03-10T03:56:14.828Z",
              "end_time": "2025-03-10T03:56:31.864Z",
              "total_elapsed_seconds": 17
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-10T03:56:33+00:00",
        "updated_at": "2025-03-29T22:44:46+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.05003": {
      "data": {
        "primary_id": "arxiv.2503.05003",
        "source": "arxiv",
        "sourceId": "2503.05003",
        "url": "https://arxiv.org/abs/2503.05003",
        "title": "2503.05003",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-10T03:56:15.414Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [],
          "published_date": ""
        },
        "identifiers": {
          "original": "2503.05003",
          "url": "https://arxiv.org/abs/2503.05003"
        }
      },
      "meta": {
        "created_at": "2025-03-10T03:56:15+00:00",
        "updated_at": "2025-03-29T22:44:46+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2502.13065v1": {
      "data": {
        "primary_id": "arxiv.2502.13065v1",
        "source": "arxiv",
        "sourceId": "2502.13065v1",
        "url": "https://arxiv.org/abs/2502.13065v1",
        "title": "2502.13065v1",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-10T04:01:29.752Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [],
          "published_date": ""
        },
        "identifiers": {
          "original": "2502.13065v1",
          "url": "https://arxiv.org/abs/2502.13065v1"
        }
      },
      "meta": {
        "created_at": "2025-03-10T04:01:30+00:00",
        "updated_at": "2025-03-29T22:44:44+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2502.05082": {
      "data": {
        "paper_id": "arxiv.2502.05082",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T04:00:01.374Z",
            "data": {
              "session_id": "session_1741579190335_ugh8ecq",
              "duration_seconds": 11,
              "idle_seconds": 0,
              "start_time": "2025-03-10T03:59:50.335Z",
              "end_time": "2025-03-10T04:00:01.362Z",
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T04:12:05.125Z",
            "data": {
              "session_id": "session_1741579917073_qnikiqv",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-03-10T04:11:57.073Z",
              "end_time": "2025-03-10T04:12:04.931Z",
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-10T03:59:28+00:00",
        "updated_at": "2025-03-29T22:44:45+00:00",
        "version": 6
      }
    },
    "interactions:arxiv.2502.13065v1": {
      "data": {
        "paper_id": "arxiv.2502.13065v1",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T04:01:36.498Z",
            "data": {
              "session_id": "session_1741579289322_zp2t0ch",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-03-10T04:01:29.322Z",
              "end_time": "2025-03-10T04:01:35.729Z",
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T04:11:47.681Z",
            "data": {
              "session_id": "session_1741579889453_8qf2s1k",
              "duration_seconds": 16,
              "idle_seconds": 0,
              "start_time": "2025-03-10T04:11:29.453Z",
              "end_time": "2025-03-10T04:11:45.227Z",
              "total_elapsed_seconds": 16
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T04:11:47.730Z",
            "data": {
              "session_id": "session_1741579889453_8qf2s1k",
              "duration_seconds": 16,
              "idle_seconds": 0,
              "start_time": "2025-03-10T04:11:29.453Z",
              "end_time": "2025-03-10T04:11:45.227Z",
              "total_elapsed_seconds": 16
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-10T04:01:37+00:00",
        "updated_at": "2025-03-29T22:44:44+00:00",
        "version": 6
      }
    },
    "paper:arxiv.2503.05182": {
      "data": {
        "primary_id": "arxiv.2503.05182",
        "source": "arxiv",
        "sourceId": "2503.05182",
        "url": "https://arxiv.org/abs/2503.05182",
        "title": "2503.05182",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-10T06:04:31.311Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [],
          "published_date": ""
        },
        "identifiers": {
          "original": "2503.05182",
          "url": "https://arxiv.org/abs/2503.05182"
        }
      },
      "meta": {
        "created_at": "2025-03-10T06:04:31+00:00",
        "updated_at": "2025-03-29T22:44:42+00:00",
        "version": 2
      }
    },
    "paper:openreview.sbmp55k6iE": {
      "data": {
        "primary_id": "openreview.sbmp55k6iE",
        "source": "openreview",
        "sourceId": "sbmp55k6iE",
        "url": "https://openreview.net/forum?id=sbmp55k6iE",
        "title": "OpenReview Paper: sbmp55k6iE",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-10T04:45:32.274Z",
        "rating": "novote",
        "source_specific_metadata": {
          "forum_id": "sbmp55k6iE"
        },
        "identifiers": {
          "original": "sbmp55k6iE",
          "url": "https://openreview.net/forum?id=sbmp55k6iE"
        }
      },
      "meta": {
        "created_at": "2025-03-10T04:45:32+00:00",
        "updated_at": "2025-03-29T22:44:43+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.05425": {
      "data": {
        "paper_id": "arxiv.2503.05425",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T06:07:41.675Z",
            "data": {
              "session_id": "session_1741586847366_oz5dwt1",
              "duration_seconds": 14,
              "idle_seconds": 0,
              "start_time": "2025-03-10T06:07:27.366Z",
              "end_time": "2025-03-10T06:07:40.881Z",
              "total_elapsed_seconds": 14
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-10T06:07:42+00:00",
        "updated_at": "2025-03-29T22:44:40+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.05425": {
      "data": {
        "primary_id": "arxiv.2503.05425",
        "source": "arxiv",
        "sourceId": "2503.05425",
        "url": "https://arxiv.org/abs/2503.05425",
        "title": "2503.05425",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-10T06:07:27.457Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [],
          "published_date": ""
        },
        "identifiers": {
          "original": "2503.05425",
          "url": "https://arxiv.org/abs/2503.05425"
        }
      },
      "meta": {
        "created_at": "2025-03-10T06:07:27+00:00",
        "updated_at": "2025-03-29T22:44:41+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.05182": {
      "data": {
        "paper_id": "arxiv.2503.05182",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T06:04:42.351Z",
            "data": {
              "session_id": "session_1741586670979_i9mnfvg",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-03-10T06:04:30.979Z",
              "end_time": "2025-03-10T06:04:35.117Z",
              "total_elapsed_seconds": 4
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T06:06:55.658Z",
            "data": {
              "session_id": "session_1741586811829_fass7p6",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-03-10T06:06:51.829Z",
              "end_time": "2025-03-10T06:06:55.647Z",
              "total_elapsed_seconds": 4
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-10T06:04:36+00:00",
        "updated_at": "2025-03-29T22:44:41+00:00",
        "version": 6
      }
    },
    "paper:arxiv.2503.04034": {
      "data": {
        "primary_id": "arxiv.2503.04034",
        "source": "arxiv",
        "sourceId": "2503.04034",
        "url": "https://arxiv.org/abs/2503.04034",
        "title": "2503.04034",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-10T07:55:21.521Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [],
          "published_date": ""
        },
        "identifiers": {
          "original": "2503.04034",
          "url": "https://arxiv.org/abs/2503.04034"
        }
      },
      "meta": {
        "created_at": "2025-03-10T07:55:21+00:00",
        "updated_at": "2025-03-29T22:44:37+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.03945": {
      "data": {
        "paper_id": "arxiv.2503.03945",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T07:53:36.919Z",
            "data": {
              "session_id": "session_1741593211841_z6ht2kj",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-03-10T07:53:31.841Z",
              "end_time": "2025-03-10T07:53:36.121Z",
              "total_elapsed_seconds": 4
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-10T07:53:37+00:00",
        "updated_at": "2025-03-29T22:44:38+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.03945": {
      "data": {
        "primary_id": "arxiv.2503.03945",
        "source": "arxiv",
        "sourceId": "2503.03945",
        "url": "https://arxiv.org/abs/2503.03945",
        "title": "2503.03945",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-10T07:53:32.195Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [],
          "published_date": ""
        },
        "identifiers": {
          "original": "2503.03945",
          "url": "https://arxiv.org/abs/2503.03945"
        }
      },
      "meta": {
        "created_at": "2025-03-10T07:53:32+00:00",
        "updated_at": "2025-03-29T22:44:38+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2502.15651": {
      "data": {
        "paper_id": "arxiv.2502.15651",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T07:54:17.137Z",
            "data": {
              "session_id": "session_1741593253426_0c8jd3i",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-03-10T07:54:13.426Z",
              "end_time": "2025-03-10T07:54:17.120Z",
              "total_elapsed_seconds": 4
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-10T07:53:23+00:00",
        "updated_at": "2025-03-29T22:44:39+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2502.15651": {
      "data": {
        "primary_id": "arxiv.2502.15651",
        "source": "arxiv",
        "sourceId": "2502.15651",
        "url": "https://arxiv.org/abs/2502.15651",
        "title": "2502.15651",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-10T07:52:04.587Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [],
          "published_date": ""
        },
        "identifiers": {
          "original": "2502.15651",
          "url": "https://arxiv.org/abs/2502.15651"
        }
      },
      "meta": {
        "created_at": "2025-03-10T07:52:05+00:00",
        "updated_at": "2025-03-29T22:44:39+00:00",
        "version": 2
      }
    },
    "paper:openreview.T86jIuSi5A": {
      "data": {
        "primary_id": "openreview.T86jIuSi5A",
        "source": "openreview",
        "sourceId": "T86jIuSi5A",
        "url": "https://openreview.net/forum?id=T86jIuSi5A",
        "title": "OpenReview Paper: T86jIuSi5A",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-10T08:17:27.548Z",
        "rating": "novote",
        "source_specific_metadata": {
          "forum_id": "T86jIuSi5A"
        },
        "identifiers": {
          "original": "T86jIuSi5A",
          "url": "https://openreview.net/forum?id=T86jIuSi5A"
        }
      },
      "meta": {
        "created_at": "2025-03-10T08:17:27+00:00",
        "updated_at": "2025-03-29T22:44:36+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.04316": {
      "data": {
        "primary_id": "arxiv.2503.04316",
        "source": "arxiv",
        "sourceId": "2503.04316",
        "url": "https://arxiv.org/abs/2503.04316",
        "title": "2503.04316",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-10T08:17:10.098Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [],
          "published_date": ""
        },
        "identifiers": {
          "original": "2503.04316",
          "url": "https://arxiv.org/abs/2503.04316"
        }
      },
      "meta": {
        "created_at": "2025-03-10T08:17:10+00:00",
        "updated_at": "2025-03-29T22:44:36+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2404.11596": {
      "data": {
        "paper_id": "arxiv.2404.11596",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T08:16:50.284Z",
            "data": {
              "session_id": "session_1741594603170_6ro5jel",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-03-10T08:16:43.170Z",
              "end_time": "2025-03-10T08:16:49.496Z",
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-10T08:16:51+00:00",
        "updated_at": "2025-03-29T22:44:37+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2404.11596": {
      "data": {
        "primary_id": "arxiv.2404.11596",
        "source": "arxiv",
        "sourceId": "2404.11596",
        "url": "https://arxiv.org/abs/2404.11596",
        "title": "2404.11596",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-10T08:16:43.779Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [],
          "published_date": ""
        },
        "identifiers": {
          "original": "2404.11596",
          "url": "https://arxiv.org/abs/2404.11596"
        }
      },
      "meta": {
        "created_at": "2025-03-10T08:16:44+00:00",
        "updated_at": "2025-03-29T22:44:37+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.04020": {
      "data": {
        "paper_id": "arxiv.2503.04020",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T08:22:01.182Z",
            "data": {
              "session_id": "session_1741594854160_f92ppub",
              "duration_seconds": 66,
              "idle_seconds": 0,
              "start_time": "2025-03-10T08:20:54.160Z",
              "end_time": "2025-03-10T08:22:00.262Z",
              "total_elapsed_seconds": 66
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-10T08:22:02+00:00",
        "updated_at": "2025-03-29T22:44:34+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.04020": {
      "data": {
        "primary_id": "arxiv.2503.04020",
        "source": "arxiv",
        "sourceId": "2503.04020",
        "url": "https://arxiv.org/abs/2503.04020",
        "title": "2503.04020",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-10T08:20:54.744Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [],
          "published_date": ""
        },
        "identifiers": {
          "original": "2503.04020",
          "url": "https://arxiv.org/abs/2503.04020"
        }
      },
      "meta": {
        "created_at": "2025-03-10T08:20:55+00:00",
        "updated_at": "2025-03-29T22:44:34+00:00",
        "version": 2
      }
    },
    "interactions:openreview.T86jIuSi5A": {
      "data": {
        "paper_id": "openreview.T86jIuSi5A",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T08:20:19.529Z",
            "data": {
              "session_id": "session_1741594769546_kvu5i25",
              "duration_seconds": 49,
              "idle_seconds": 0,
              "start_time": "2025-03-10T08:19:29.546Z",
              "end_time": "2025-03-10T08:20:18.852Z",
              "total_elapsed_seconds": 49
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T08:24:26.613Z",
            "data": {
              "session_id": "session_1741595056389_4hckbg2",
              "duration_seconds": 7,
              "idle_seconds": 0,
              "start_time": "2025-03-10T08:24:16.389Z",
              "end_time": "2025-03-10T08:24:23.646Z",
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T15:03:59.088Z",
            "data": {
              "session_id": "session_1741619035126_vmdm6fv",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-03-10T15:03:55.126Z",
              "end_time": "2025-03-10T15:03:58.882Z",
              "total_elapsed_seconds": 4
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-10T08:20:20+00:00",
        "updated_at": "2025-03-29T22:44:35+00:00",
        "version": 7
      }
    },
    "interactions:arxiv.2503.04316": {
      "data": {
        "paper_id": "arxiv.2503.04316",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T08:17:27.893Z",
            "data": {
              "session_id": "session_1741594629745_17gomk2",
              "duration_seconds": 17,
              "idle_seconds": 0,
              "start_time": "2025-03-10T08:17:09.745Z",
              "end_time": "2025-03-10T08:17:27.034Z",
              "total_elapsed_seconds": 17
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T08:19:11.268Z",
            "data": {
              "session_id": "session_1741594715067_6wdj3uf",
              "duration_seconds": 36,
              "idle_seconds": 0,
              "start_time": "2025-03-10T08:18:35.067Z",
              "end_time": "2025-03-10T08:19:11.263Z",
              "total_elapsed_seconds": 36
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-10T08:17:28+00:00",
        "updated_at": "2025-03-29T22:44:35+00:00",
        "version": 4
      }
    },
    "interactions:arxiv.2502.13993": {
      "data": {
        "paper_id": "arxiv.2502.13993",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T08:37:13.134Z",
            "data": {
              "session_id": "session_1741595817408_hsqtcgl",
              "duration_seconds": 15,
              "idle_seconds": 0,
              "start_time": "2025-03-10T08:36:57.408Z",
              "end_time": "2025-03-10T08:37:12.325Z",
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T08:41:07.635Z",
            "data": {
              "session_id": "session_1741596062940_7upv4we",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-03-10T08:41:02.940Z",
              "end_time": "2025-03-10T08:41:07.629Z",
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-10T08:37:14+00:00",
        "updated_at": "2025-03-29T22:44:32+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2402.06176": {
      "data": {
        "primary_id": "arxiv.2402.06176",
        "source": "arxiv",
        "sourceId": "2402.06176",
        "url": "https://arxiv.org/abs/2402.06176",
        "title": "2402.06176",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-10T08:37:12.760Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [],
          "published_date": ""
        },
        "identifiers": {
          "original": "2402.06176",
          "url": "https://arxiv.org/abs/2402.06176"
        }
      },
      "meta": {
        "created_at": "2025-03-10T08:37:13+00:00",
        "updated_at": "2025-03-29T22:44:33+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2502.13993": {
      "data": {
        "primary_id": "arxiv.2502.13993",
        "source": "arxiv",
        "sourceId": "2502.13993",
        "url": "https://arxiv.org/abs/2502.13993",
        "title": "2502.13993",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-10T08:36:57.873Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [],
          "published_date": ""
        },
        "identifiers": {
          "original": "2502.13993",
          "url": "https://arxiv.org/abs/2502.13993"
        }
      },
      "meta": {
        "created_at": "2025-03-10T08:36:58+00:00",
        "updated_at": "2025-03-29T22:44:33+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2502.08552": {
      "data": {
        "primary_id": "arxiv.2502.08552",
        "source": "arxiv",
        "sourceId": "2502.08552",
        "url": "https://arxiv.org/abs/2502.08552",
        "title": "2502.08552",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-10T08:42:27.299Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [],
          "published_date": ""
        },
        "identifiers": {
          "original": "2502.08552",
          "url": "https://arxiv.org/abs/2502.08552"
        }
      },
      "meta": {
        "created_at": "2025-03-10T08:42:27+00:00",
        "updated_at": "2025-03-29T22:44:29+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2502.06626": {
      "data": {
        "paper_id": "arxiv.2502.06626",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T08:42:15.746Z",
            "data": {
              "session_id": "session_1741596129246_gag87bi",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-03-10T08:42:09.246Z",
              "end_time": "2025-03-10T08:42:15.734Z",
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-10T08:42:10+00:00",
        "updated_at": "2025-03-29T22:44:29+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2502.06626": {
      "data": {
        "primary_id": "arxiv.2502.06626",
        "source": "arxiv",
        "sourceId": "2502.06626",
        "url": "https://arxiv.org/abs/2502.06626",
        "title": "2502.06626",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-10T08:41:48.829Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [],
          "published_date": ""
        },
        "identifiers": {
          "original": "2502.06626",
          "url": "https://arxiv.org/abs/2502.06626"
        }
      },
      "meta": {
        "created_at": "2025-03-10T08:41:49+00:00",
        "updated_at": "2025-03-29T22:44:29+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2502.14555": {
      "data": {
        "paper_id": "arxiv.2502.14555",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T08:41:28.698Z",
            "data": {
              "session_id": "session_1741596076177_df1s5ad",
              "duration_seconds": 12,
              "idle_seconds": 0,
              "start_time": "2025-03-10T08:41:16.177Z",
              "end_time": "2025-03-10T08:41:27.987Z",
              "total_elapsed_seconds": 12
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-10T08:41:29+00:00",
        "updated_at": "2025-03-29T22:44:30+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2502.14555": {
      "data": {
        "primary_id": "arxiv.2502.14555",
        "source": "arxiv",
        "sourceId": "2502.14555",
        "url": "https://arxiv.org/abs/2502.14555?context=nlin.AO",
        "title": "2502.14555",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-10T08:41:16.340Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [],
          "published_date": ""
        },
        "identifiers": {
          "original": "2502.14555",
          "url": "https://arxiv.org/abs/2502.14555?context=nlin.AO"
        }
      },
      "meta": {
        "created_at": "2025-03-10T08:41:16+00:00",
        "updated_at": "2025-03-29T22:44:31+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2402.06176": {
      "data": {
        "paper_id": "arxiv.2402.06176",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T08:40:32.008Z",
            "data": {
              "session_id": "session_1741595895624_nd1gf0x",
              "duration_seconds": 136,
              "idle_seconds": 0,
              "start_time": "2025-03-10T08:38:15.624Z",
              "end_time": "2025-03-10T08:40:31.470Z",
              "total_elapsed_seconds": 136
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-10T08:40:32+00:00",
        "updated_at": "2025-03-29T22:44:32+00:00",
        "version": 3
      }
    },
    "interactions:arxiv.2502.08552": {
      "data": {
        "paper_id": "arxiv.2502.08552",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T08:43:36.377Z",
            "data": {
              "session_id": "session_1741596146910_6lzozfg",
              "duration_seconds": 68,
              "idle_seconds": 0,
              "start_time": "2025-03-10T08:42:26.910Z",
              "end_time": "2025-03-10T08:43:35.402Z",
              "total_elapsed_seconds": 68
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T08:44:57.962Z",
            "data": {
              "session_id": "session_1741596289046_b24nu9l",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-03-10T08:44:49.046Z",
              "end_time": "2025-03-10T08:44:56.839Z",
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-10T08:43:37+00:00",
        "updated_at": "2025-03-29T22:44:28+00:00",
        "version": 5
      }
    },
    "interactions:arxiv.2503.04034": {
      "data": {
        "paper_id": "arxiv.2503.04034",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T08:58:29.642Z",
            "data": {
              "session_id": "session_1741597105331_7cepklf",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-03-10T08:58:25.331Z",
              "end_time": "2025-03-10T08:58:29.253Z",
              "total_elapsed_seconds": 4
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-10T08:58:30+00:00",
        "updated_at": "2025-03-29T22:44:28+00:00",
        "version": 3
      }
    },
    "interactions:arxiv.2503.00450": {
      "data": {
        "paper_id": "arxiv.2503.00450",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T14:57:33.934Z",
            "data": {
              "session_id": "session_1741618497029_ufhn7wa",
              "duration_seconds": 156,
              "idle_seconds": 0,
              "start_time": "2025-03-10T14:54:57.029Z",
              "end_time": "2025-03-10T14:57:33.079Z",
              "total_elapsed_seconds": 156
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-10T14:57:35+00:00",
        "updated_at": "2025-03-29T22:44:27+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.00450": {
      "data": {
        "primary_id": "arxiv.2503.00450",
        "source": "arxiv",
        "sourceId": "2503.00450",
        "url": "https://arxiv.org/pdf/2503.00450",
        "title": "2503.00450",
        "authors": [
          "Joshua Talks",
          "Anna Kreshuk"
        ],
        "abstract": "Model transfer presents a solution to the challenges of segmentation in the\nmicroscopy community, where the immense cost of labelling sufficient training\ndata is a major bottleneck in the use of deep learning. With large quantities\nof imaging data produced across a wide range of imaging conditions, institutes\nalso produce many bespoke models trained on specific source data which then get\ncollected in model banks or zoos. As the number of available models grows, so\ndoes the need for an efficient and reliable model selection method for a\nspecific target dataset of interest. We focus on the unsupervised regime where\nno labels are available for the target dataset. Building on previous work\nlinking model generalisation and consistency under perturbation, we propose the\nfirst unsupervised transferability estimator for semantic and instance\nsegmentation tasks which doesn't require access to source training data or\ntarget domain labels. We evaluate the method on multiple segmentation problems\nacross microscopy modalities, finding a strong correlation between the rankings\nbased on our estimator and rankings based on target dataset performance.",
        "timestamp": "2025-03-10T14:54:57.554Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [],
          "published_date": ""
        },
        "identifiers": {
          "original": "2503.00450",
          "url": "https://arxiv.org/pdf/2503.00450"
        },
        "publishedDate": "2025-03-01T11:11:06+00:00",
        "doi": null,
        "tags": [
          "cs.CV"
        ]
      },
      "meta": {
        "issue_number": 1739,
        "object_id": "paper:arxiv.2503.00450",
        "created_at": "2025-03-10T14:54:58+00:00",
        "updated_at": "2025-04-10T05:35:54+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.05244": {
      "data": {
        "primary_id": "arxiv.2502.05244",
        "source": "arxiv",
        "sourceId": "2502.05244",
        "url": "https://arxiv.org/abs/2502.05244",
        "title": "2502.05244",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-10T15:02:03.217Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [],
          "published_date": ""
        },
        "identifiers": {
          "original": "2502.05244",
          "url": "https://arxiv.org/abs/2502.05244"
        }
      },
      "meta": {
        "created_at": "2025-03-10T15:02:03+00:00",
        "updated_at": "2025-03-29T22:44:25+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2502.17540": {
      "data": {
        "paper_id": "arxiv.2502.17540",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T15:00:34.084Z",
            "data": {
              "session_id": "session_1741618814962_6j1we7u",
              "duration_seconds": 18,
              "idle_seconds": 0,
              "start_time": "2025-03-10T15:00:14.962Z",
              "end_time": "2025-03-10T15:00:33.124Z",
              "total_elapsed_seconds": 18
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-10T15:00:35+00:00",
        "updated_at": "2025-03-29T22:44:26+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2502.17540": {
      "data": {
        "primary_id": "arxiv.2502.17540",
        "source": "arxiv",
        "sourceId": "2502.17540",
        "url": "https://arxiv.org/abs/2502.17540",
        "title": "2502.17540",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-10T15:00:15.525Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [],
          "published_date": ""
        },
        "identifiers": {
          "original": "2502.17540",
          "url": "https://arxiv.org/abs/2502.17540"
        }
      },
      "meta": {
        "created_at": "2025-03-10T15:00:16+00:00",
        "updated_at": "2025-03-29T22:44:26+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2502.05244": {
      "data": {
        "paper_id": "arxiv.2502.05244",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T15:02:59.255Z",
            "data": {
              "session_id": "session_1741618922618_uf6aafs",
              "duration_seconds": 56,
              "idle_seconds": 0,
              "start_time": "2025-03-10T15:02:02.618Z",
              "end_time": "2025-03-10T15:02:58.374Z",
              "total_elapsed_seconds": 56
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-10T15:03:00+00:00",
        "updated_at": "2025-03-29T22:44:25+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2412.04466": {
      "data": {
        "primary_id": "arxiv.2412.04466",
        "source": "arxiv",
        "sourceId": "2412.04466",
        "url": "https://arxiv.org/abs/2412.04466",
        "title": "ARXIV Paper: 2412.04466",
        "authors": "Sophie Greenwood, Sudalakshmee Chiniah, Nikhil Garg",
        "abstract": "In the basic recommendation paradigm, the most (predicted) relevant item is\nrecommended to each user. This may result in some items receiving lower\nexposure than they \"should\"; to counter this, several algorithmic approaches\nhave been developed to ensure item fairness. These approaches necessarily\ndegrade recommendations for some users to improve outcomes for items, leading\nto user fairness concerns. In turn, a recent line of work has focused on\ndeveloping algorithms for multi-sided fairness, to jointly optimize user\nfairness, item fairness, and overall recommendation quality. This induces the\nquestion: what is the tradeoff between these objectives, and what are the\ncharacteristics of (multi-objective) optimal solutions? Theoretically, we\ndevelop a model of recommendations with user and item fairness objectives and\ncharacterize the solutions of fairness-constrained optimization. We identify\ntwo phenomena: (a) when user preferences are diverse, there is \"free\" item and\nuser fairness; and (b) users whose preferences are misestimated can be\nespecially disadvantaged by item fairness constraints. Empirically, we\nprototype a recommendation system for preprints on arXiv and implement our\nframework, measuring the phenomena in practice and showing how these phenomena\ninform the design of markets with recommendation systems-intermediated\nmatching.",
        "timestamp": "2025-03-10T17:36:12.780Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [
            "cs.IR",
            "cs.CY"
          ],
          "published_date": "2024-12-05T18:59:51Z"
        },
        "identifiers": {
          "original": "2412.04466",
          "url": "https://arxiv.org/abs/2412.04466"
        }
      },
      "meta": {
        "created_at": "2025-03-10T17:36:13+00:00",
        "updated_at": "2025-03-29T22:44:24+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2412.04466": {
      "data": {
        "paper_id": "arxiv.2412.04466",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T17:36:25.708Z",
            "data": {
              "session_id": "session_1741628172554_kn7z83j",
              "duration_seconds": 12,
              "idle_seconds": 0,
              "start_time": "2025-03-10T17:36:12.554Z",
              "end_time": "2025-03-10T17:36:24.911Z",
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T17:38:01.719Z",
            "data": {
              "session_id": "session_1741628270093_99ijq12",
              "duration_seconds": 12,
              "idle_seconds": 0,
              "start_time": "2025-03-10T17:37:50.093Z",
              "end_time": "2025-03-10T17:38:01.692Z",
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T17:46:48.270Z",
            "data": {
              "session_id": "session_1741628794724_6g3j06o",
              "duration_seconds": 11,
              "idle_seconds": 0,
              "start_time": "2025-03-10T17:46:34.724Z",
              "end_time": "2025-03-10T17:46:45.923Z",
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-10T17:36:26+00:00",
        "updated_at": "2025-03-29T22:44:23+00:00",
        "version": 7
      }
    },
    "interactions:arxiv.2503.00089": {
      "data": {
        "paper_id": "arxiv.2503.00089",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T18:12:00.665Z",
            "data": {
              "session_id": "session_1741630308304_iliak8u",
              "duration_seconds": 11,
              "idle_seconds": 0,
              "start_time": "2025-03-10T18:11:48.304Z",
              "end_time": "2025-03-10T18:11:59.680Z",
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T18:21:14.199Z",
            "data": {
              "session_id": "session_1741630731859_yjlntbz",
              "duration_seconds": 142,
              "idle_seconds": 0,
              "start_time": "2025-03-10T18:18:51.859Z",
              "end_time": "2025-03-10T18:21:13.739Z",
              "total_elapsed_seconds": 142
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-10T18:12:02+00:00",
        "updated_at": "2025-03-29T22:44:21+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2503.00089": {
      "data": {
        "primary_id": "arxiv.2503.00089",
        "source": "arxiv",
        "sourceId": "2503.00089",
        "url": "https://arxiv.org/abs/2503.00089",
        "title": "ARXIV Paper: 2503.00089",
        "authors": "Xinyu Yuan, Zichen Wang, Marcus Collins, Huzefa Rangwala",
        "abstract": "Recent years have witnessed a surge in the development of protein structural\ntokenization methods, which chunk protein 3D structures into discrete or\ncontinuous representations. Structure tokenization enables the direct\napplication of powerful techniques like language modeling for protein\nstructures, and large multimodal models to integrate structures with protein\nsequences and functional texts. Despite the progress, the capabilities and\nlimitations of these methods remain poorly understood due to the lack of a\nunified evaluation framework. We first introduce StructTokenBench, a framework\nthat comprehensively evaluates the quality and efficiency of structure\ntokenizers, focusing on fine-grained local substructures rather than global\nstructures, as typical in existing benchmarks. Our evaluations reveal that no\nsingle model dominates all benchmarking perspectives. Observations of codebook\nunder-utilization led us to develop AminoAseed, a simple yet effective strategy\nthat enhances codebook gradient updates and optimally balances codebook size\nand dimension for improved tokenizer utilization and quality. Compared to the\nleading model ESM3, our method achieves an average of 6.31% performance\nimprovement across 24 supervised tasks, with sensitivity and utilization rates\nincreased by 12.83% and 124.03%, respectively.",
        "timestamp": "2025-03-10T18:11:48.848Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [
            "q-bio.QM",
            "cs.AI",
            "cs.LG"
          ],
          "published_date": "2025-02-28T15:14:33Z"
        },
        "identifiers": {
          "original": "2503.00089",
          "url": "https://arxiv.org/abs/2503.00089"
        }
      },
      "meta": {
        "created_at": "2025-03-10T18:11:49+00:00",
        "updated_at": "2025-03-29T22:44:21+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2409.15644": {
      "data": {
        "primary_id": "arxiv.2409.15644",
        "source": "arxiv",
        "sourceId": "2409.15644",
        "url": "https://arxiv.org/abs/2409.15644",
        "title": "ARXIV Paper: 2409.15644",
        "authors": "Tzu-Sheng Kuo, Quan Ze Chen, Amy X. Zhang, Jane Hsieh, Haiyi Zhu, Kenneth Holstein",
        "abstract": "Community and organizational policies are typically designed in a top-down,\ncentralized fashion, with limited input from impacted stakeholders. This can\nresult in policies that are misaligned with community needs or perceived as\nillegitimate. How can we support more collaborative, participatory approaches\nto policy design? In this paper, we present PolicyCraft, a system that\nstructures collaborative policy design through case-grounded deliberation.\nBuilding on past research that highlights the value of concrete cases in\nestablishing common ground, PolicyCraft supports users in collaboratively\nproposing, critiquing, and revising policies through discussion and voting on\ncases. A field study across two university courses showed that students using\nPolicyCraft reached greater consensus and developed better-supported course\npolicies, compared with those using a baseline system that did not scaffold\ntheir use of concrete cases. Reflecting on our findings, we discuss\nopportunities for future HCI systems to help groups more effectively bridge\nbetween abstract policies and concrete cases.",
        "timestamp": "2025-03-10T18:09:06.574Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [
            "cs.HC"
          ],
          "published_date": "2024-09-24T01:16:43Z"
        },
        "identifiers": {
          "original": "2409.15644",
          "url": "https://arxiv.org/abs/2409.15644"
        }
      },
      "meta": {
        "created_at": "2025-03-10T18:09:07+00:00",
        "updated_at": "2025-03-29T22:44:22+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.04957": {
      "data": {
        "paper_id": "arxiv.2503.04957",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T19:10:21.467Z",
            "data": {
              "session_id": "session_1741633790772_i34kx5y",
              "duration_seconds": 23,
              "idle_seconds": 0,
              "start_time": "2025-03-10T19:09:50.772Z",
              "end_time": "2025-03-10T19:10:13.723Z",
              "total_elapsed_seconds": 23
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-10T19:10:16+00:00",
        "updated_at": "2025-03-29T22:44:20+00:00",
        "version": 6
      }
    },
    "paper:arxiv.2503.04957": {
      "data": {
        "primary_id": "arxiv.2503.04957",
        "source": "arxiv",
        "sourceId": "2503.04957",
        "url": "https://arxiv.org/abs/2503.04957",
        "title": "2503.04957",
        "authors": "Ada Defne Tur, Nicholas Meade, Xing Han L\u00f9, Alejandra Zambrano, Arkil Patel, Esin Durmus, Spandana Gella, Karolina Sta\u0144czak, Siva Reddy",
        "abstract": "LLM-based agents are becoming increasingly proficient at solving web-based\ntasks. With this capability comes a greater risk of misuse for malicious\npurposes, such as posting misinformation in an online forum or selling illicit\nsubstances on a website. To evaluate these risks, we propose SafeArena, the\nfirst benchmark to focus on the deliberate misuse of web agents. SafeArena\ncomprises 250 safe and 250 harmful tasks across four websites. We classify the\nharmful tasks into five harm categories -- misinformation, illegal activity,\nharassment, cybercrime, and social bias, designed to assess realistic misuses\nof web agents. We evaluate leading LLM-based web agents, including GPT-4o,\nClaude-3.5 Sonnet, Qwen-2-VL 72B, and Llama-3.2 90B, on our benchmark. To\nsystematically assess their susceptibility to harmful tasks, we introduce the\nAgent Risk Assessment framework that categorizes agent behavior across four\nrisk levels. We find agents are surprisingly compliant with malicious requests,\nwith GPT-4o and Qwen-2 completing 34.7% and 27.3% of harmful requests,\nrespectively. Our findings highlight the urgent need for safety alignment\nprocedures for web agents. Our benchmark is available here:\nhttps://safearena.github.io",
        "timestamp": "2025-03-10T19:09:51.260Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
          ],
          "published_date": "2025-03-06T20:43:14Z"
        },
        "identifiers": {
          "original": "2503.04957",
          "url": "https://arxiv.org/abs/2503.04957"
        }
      },
      "meta": {
        "created_at": "2025-03-10T19:09:51+00:00",
        "updated_at": "2025-03-29T22:44:20+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2406.08129": {
      "data": {
        "paper_id": "arxiv.2406.08129",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T19:30:01.242Z",
            "data": {
              "session_id": "session_1741634947839_ispszsd",
              "duration_seconds": 47,
              "idle_seconds": 0,
              "start_time": "2025-03-10T19:29:07.839Z",
              "end_time": "2025-03-10T19:29:54.795Z",
              "total_elapsed_seconds": 47
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-10T19:29:56+00:00",
        "updated_at": "2025-03-29T22:44:18+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2406.08129": {
      "data": {
        "primary_id": "arxiv.2406.08129",
        "source": "arxiv",
        "sourceId": "2406.08129",
        "url": "https://arxiv.org/abs/2406.08129",
        "title": "2406.08129",
        "authors": "K. Trachenko, B. Monserrat, M. Hutcheon, C. J. Pickard",
        "abstract": "Fundamental physical constants govern key effects in high-energy particle\nphysics and astrophysics, including the stability of particles, nuclear\nreactions, formation and evolution of stars, synthesis of heavy nuclei and\nemergence of stable molecular structures. Here, we show that fundamental\nconstants also set an upper bound for the frequency of phonons in condensed\nmatter phases, or how rapidly an atom can vibrate. This bound is in agreement\nwith \\textit{ab initio} simulations of atomic hydrogen and high-temperature\nhydride superconductors, and implies an upper limit to the superconducting\ntransition temperature $T_c$ in condensed matter. Fundamental constants set\nthis limit to the order of 10$^2-10^3$ K. This range is consistent with our\ncalculations of $T_c$ from optimal Eliashberg functions. As a corollary, we\nobserve that the very existence of the current research of finding\n$T_{\\mathrm{c}}$ at and above $300$ K is due to the observed values of\nfundamental constants. We finally discuss how fundamental constants affect the\nobservability and operation of other effects and phenomena including phase\ntransitions.",
        "timestamp": "2025-03-10T19:29:07.994Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [
            "cond-mat.supr-con",
            "cond-mat.mtrl-sci",
            "cond-mat.stat-mech"
          ],
          "published_date": "2024-06-12T12:15:49Z"
        },
        "identifiers": {
          "original": "2406.08129",
          "url": "https://arxiv.org/abs/2406.08129"
        }
      },
      "meta": {
        "created_at": "2025-03-10T19:29:08+00:00",
        "updated_at": "2025-03-29T22:44:19+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.05040": {
      "data": {
        "paper_id": "arxiv.2503.05040",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T20:26:29.096Z",
            "data": {
              "session_id": "session_1741638379454_vnzqyiw",
              "duration_seconds": 9,
              "idle_seconds": 0,
              "start_time": "2025-03-10T20:26:19.454Z",
              "end_time": "2025-03-10T20:26:28.351Z",
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T21:18:43.793Z",
            "data": {
              "session_id": "session_1741641496904_lato1rk",
              "duration_seconds": 24,
              "idle_seconds": 0,
              "start_time": "2025-03-10T21:18:16.904Z",
              "end_time": "2025-03-10T21:18:40.813Z",
              "total_elapsed_seconds": 24
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-10T20:26:30+00:00",
        "updated_at": "2025-03-29T22:44:17+00:00",
        "version": 5
      }
    },
    "paper:arxiv.2503.05040": {
      "data": {
        "primary_id": "arxiv.2503.05040",
        "source": "arxiv",
        "sourceId": "2503.05040",
        "url": "https://arxiv.org/abs/2503.05040",
        "title": "2503.05040",
        "authors": "John C. Flournoy, Carol S. Lee, Catherine M. Hicks, Maggie Wu",
        "abstract": "Understanding factors that influence software development velocity is crucial\nfor engineering teams and organizations, yet empirical evidence at scale\nremains limited. A more robust understanding of the dynamics of cycle time may\nhelp practitioners avoid pitfalls in relying on velocity measures while\nevaluating software work. We analyze cycle time, a widely-used metric measuring\ntime from ticket creation to completion, using a dataset of over 55,000\nobservations across 216 organizations. Through Bayesian hierarchical modeling\nthat appropriately separates individual and organizational variation, we\nexamine how coding time, task scoping, and collaboration patterns affect cycle\ntime while characterizing its substantial variability across contexts. We find\nprecise but modest associations between cycle time and factors including coding\ndays per week, number of merged pull requests, and degree of collaboration.\nHowever, these effects are set against considerable unexplained variation both\nbetween and within individuals. Our findings suggest that while common\nworkplace factors do influence cycle time in expected directions, any single\nobservation provides limited signal about typical performance. This work\ndemonstrates methods for analyzing complex operational metrics at scale while\nhighlighting potential pitfalls in using such measurements to drive\ndecision-making. We conclude that improving software delivery velocity likely\nrequires systems-level thinking rather than individual-focused interventions.",
        "timestamp": "2025-03-10T20:26:19.753Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [
            "cs.SE"
          ],
          "published_date": "2025-03-06T23:32:53Z"
        },
        "identifiers": {
          "original": "2503.05040",
          "url": "https://arxiv.org/abs/2503.05040"
        }
      },
      "meta": {
        "created_at": "2025-03-10T20:26:20+00:00",
        "updated_at": "2025-03-29T22:44:17+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.1611.03530": {
      "data": {
        "paper_id": "arxiv.1611.03530",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T21:42:10.459Z",
            "data": {
              "session_id": "session_1741642926420_y7sghab",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-03-10T21:42:06.420Z",
              "end_time": "2025-03-10T21:42:09.677Z",
              "total_elapsed_seconds": 3
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-10T21:42:11+00:00",
        "updated_at": "2025-03-29T22:44:16+00:00",
        "version": 3
      }
    },
    "paper:arxiv.1611.03530": {
      "data": {
        "primary_id": "arxiv.1611.03530",
        "source": "arxiv",
        "sourceId": "1611.03530",
        "url": "https://arxiv.org/abs/1611.03530",
        "title": "1611.03530",
        "authors": "Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals",
        "abstract": "Despite their massive size, successful deep artificial neural networks can\nexhibit a remarkably small difference between training and test performance.\nConventional wisdom attributes small generalization error either to properties\nof the model family, or to the regularization techniques used during training.\n  Through extensive systematic experiments, we show how these traditional\napproaches fail to explain why large neural networks generalize well in\npractice. Specifically, our experiments establish that state-of-the-art\nconvolutional networks for image classification trained with stochastic\ngradient methods easily fit a random labeling of the training data. This\nphenomenon is qualitatively unaffected by explicit regularization, and occurs\neven if we replace the true images by completely unstructured random noise. We\ncorroborate these experimental findings with a theoretical construction showing\nthat simple depth two neural networks already have perfect finite sample\nexpressivity as soon as the number of parameters exceeds the number of data\npoints as it usually does in practice.\n  We interpret our experimental findings by comparison with traditional models.",
        "timestamp": "2025-03-10T21:42:06.808Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [
            "cs.LG"
          ],
          "published_date": "2016-11-10T22:02:36Z"
        },
        "identifiers": {
          "original": "1611.03530",
          "url": "https://arxiv.org/abs/1611.03530"
        }
      },
      "meta": {
        "created_at": "2025-03-10T21:42:07+00:00",
        "updated_at": "2025-03-29T22:44:16+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.02113": {
      "data": {
        "primary_id": "arxiv.2503.02113",
        "source": "arxiv",
        "sourceId": "2503.02113",
        "url": "https://arxiv.org/abs/2503.02113",
        "title": "2503.02113",
        "authors": "Andrew Gordon Wilson",
        "abstract": "Deep neural networks are often seen as different from other model classes by\ndefying conventional notions of generalization. Popular examples of anomalous\ngeneralization behaviour include benign overfitting, double descent, and the\nsuccess of overparametrization. We argue that these phenomena are not distinct\nto neural networks, or particularly mysterious. Moreover, this generalization\nbehaviour can be intuitively understood, and rigorously characterized using\nlong-standing generalization frameworks such as PAC-Bayes and countable\nhypothesis bounds. We present soft inductive biases as a key unifying principle\nin explaining these phenomena: rather than restricting the hypothesis space to\navoid overfitting, embrace a flexible hypothesis space, with a soft preference\nfor simpler solutions that are consistent with the data. This principle can be\nencoded in many model classes, and thus deep learning is not as mysterious or\ndifferent from other model classes as it might seem. However, we also highlight\nhow deep learning is relatively distinct in other ways, such as its ability for\nrepresentation learning, phenomena such as mode connectivity, and its relative\nuniversality.",
        "timestamp": "2025-03-10T21:42:05.532Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [
            "cs.LG",
            "stat.ML"
          ],
          "published_date": "2025-03-03T22:56:04Z"
        },
        "identifiers": {
          "original": "2503.02113",
          "url": "https://arxiv.org/abs/2503.02113"
        }
      },
      "meta": {
        "created_at": "2025-03-10T21:42:06+00:00",
        "updated_at": "2025-03-29T22:44:16+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.05025": {
      "data": {
        "paper_id": "arxiv.2503.05025",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T21:53:05.855Z",
            "data": {
              "session_id": "session_1741643572358_nim10k6",
              "duration_seconds": 13,
              "idle_seconds": 0,
              "start_time": "2025-03-10T21:52:52.359Z",
              "end_time": "2025-03-10T21:53:05.054Z",
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T21:53:08.748Z",
            "data": {
              "session_id": "session_1741643572358_nim10k6",
              "duration_seconds": 13,
              "idle_seconds": 0,
              "start_time": "2025-03-10T21:52:52.359Z",
              "end_time": "2025-03-10T21:53:05.054Z",
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T21:55:50.766Z",
            "data": {
              "session_id": "session_1741643735447_sufezjk",
              "duration_seconds": 15,
              "idle_seconds": 0,
              "start_time": "2025-03-10T21:55:35.447Z",
              "end_time": "2025-03-10T21:55:50.753Z",
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-10T21:53:06+00:00",
        "updated_at": "2025-03-29T22:44:14+00:00",
        "version": 5
      }
    },
    "paper:arxiv.2503.05025": {
      "data": {
        "primary_id": "arxiv.2503.05025",
        "source": "arxiv",
        "sourceId": "2503.05025",
        "url": "https://arxiv.org/abs/2503.05025",
        "title": "2503.05025",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-10T21:52:52.531Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [],
          "published_date": ""
        },
        "identifiers": {
          "original": "2503.05025",
          "url": "https://arxiv.org/abs/2503.05025"
        }
      },
      "meta": {
        "created_at": "2025-03-10T21:52:53+00:00",
        "updated_at": "2025-03-29T22:44:15+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2502.20332": {
      "data": {
        "paper_id": "arxiv.2502.20332",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-10T22:32:34.786Z",
            "data": {
              "session_id": "session_1741645915768_43qymi4",
              "duration_seconds": 38,
              "idle_seconds": 0,
              "start_time": "2025-03-10T22:31:55.768Z",
              "end_time": "2025-03-10T22:32:33.858Z",
              "total_elapsed_seconds": 38
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-10T22:32:35+00:00",
        "updated_at": "2025-03-29T22:44:13+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2502.20332": {
      "data": {
        "primary_id": "arxiv.2502.20332",
        "source": "arxiv",
        "sourceId": "2502.20332",
        "url": "https://arxiv.org/abs/2502.20332",
        "title": "2502.20332",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-10T22:31:56.409Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [],
          "published_date": ""
        },
        "identifiers": {
          "original": "2502.20332",
          "url": "https://arxiv.org/abs/2502.20332"
        }
      },
      "meta": {
        "created_at": "2025-03-10T22:31:56+00:00",
        "updated_at": "2025-03-29T22:44:14+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.06334": {
      "data": {
        "paper_id": "arxiv.2503.06334",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-11T05:39:48.601Z",
            "data": {
              "session_id": "session_1741671583658_ya59ayq",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-03-11T05:39:43.658Z",
              "end_time": "2025-03-11T05:39:47.879Z",
              "total_elapsed_seconds": 4
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-11T05:39:51.811Z",
            "data": {
              "session_id": "session_1741671583658_ya59ayq",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-03-11T05:39:43.658Z",
              "end_time": "2025-03-11T05:39:47.879Z",
              "total_elapsed_seconds": 4
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-11T05:39:49+00:00",
        "updated_at": "2025-03-29T22:44:11+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2503.06334": {
      "data": {
        "primary_id": "arxiv.2503.06334",
        "source": "arxiv",
        "sourceId": "2503.06334",
        "url": "https://arxiv.org/abs/2503.06334",
        "title": "2503.06334",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-11T05:39:44.236Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [],
          "published_date": ""
        },
        "identifiers": {
          "original": "2503.06334",
          "url": "https://arxiv.org/abs/2503.06334"
        }
      },
      "meta": {
        "created_at": "2025-03-11T05:39:44+00:00",
        "updated_at": "2025-03-29T22:44:12+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.07134": {
      "data": {
        "primary_id": "arxiv.2503.07134",
        "source": "arxiv",
        "sourceId": "2503.07134",
        "url": "https://arxiv.org/abs/2503.07134",
        "title": "2503.07134",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-11T05:13:29.789Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [],
          "published_date": ""
        },
        "identifiers": {
          "original": "2503.07134",
          "url": "https://arxiv.org/abs/2503.07134"
        }
      },
      "meta": {
        "created_at": "2025-03-11T05:13:30+00:00",
        "updated_at": "2025-03-29T22:44:13+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.06520": {
      "data": {
        "paper_id": "arxiv.2503.06520",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-11T07:34:55.919Z",
            "data": {
              "session_id": "session_1741678487046_hhu7y1h",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-03-11T07:34:47.046Z",
              "end_time": "2025-03-11T07:34:55.136Z",
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-11T07:34:57+00:00",
        "updated_at": "2025-03-29T22:44:10+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.06520": {
      "data": {
        "primary_id": "arxiv.2503.06520",
        "source": "arxiv",
        "sourceId": "2503.06520",
        "url": "https://arxiv.org/abs/2503.06520",
        "title": "ARXIV Paper: 2503.06520",
        "authors": "Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, Jiaya Jia",
        "abstract": "Traditional methods for reasoning segmentation rely on supervised fine-tuning\nwith categorical labels and simple descriptions, limiting its out-of-domain\ngeneralization and lacking explicit reasoning processes. To address these\nlimitations, we propose Seg-Zero, a novel framework that demonstrates\nremarkable generalizability and derives explicit chain-of-thought reasoning\nthrough cognitive reinforcement. Seg-Zero introduces a decoupled architecture\nconsisting of a reasoning model and a segmentation model. The reasoning model\ninterprets user intentions, generates explicit reasoning chains, and produces\npositional prompts, which are subsequently used by the segmentation model to\ngenerate precious pixel-level masks. We design a sophisticated reward mechanism\nthat integrates both format and accuracy rewards to effectively guide\noptimization directions. Trained exclusively via reinforcement learning with\nGRPO and without explicit reasoning data, Seg-Zero achieves robust zero-shot\ngeneralization and exhibits emergent test-time reasoning capabilities.\nExperiments show that Seg-Zero-7B achieves a zero-shot performance of 57.5 on\nthe ReasonSeg benchmark, surpassing the prior LISA-7B by 18\\%. This significant\nimprovement highlights Seg-Zero's ability to generalize across domains while\npresenting an explicit reasoning process. Code is available at\nhttps://github.com/dvlab-research/Seg-Zero.",
        "timestamp": "2025-03-11T07:34:47.360Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [
            "cs.CV",
            "cs.MM"
          ],
          "published_date": "2025-03-09T08:48:51Z"
        },
        "identifiers": {
          "original": "2503.06520",
          "url": "https://arxiv.org/abs/2503.06520"
        }
      },
      "meta": {
        "created_at": "2025-03-11T07:34:47+00:00",
        "updated_at": "2025-03-29T22:44:11+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.06698": {
      "data": {
        "paper_id": "arxiv.2503.06698",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-11T07:51:15.246Z",
            "data": {
              "session_id": "session_1741679435987_zo2oiud",
              "duration_seconds": 38,
              "idle_seconds": 0,
              "start_time": "2025-03-11T07:50:35.987Z",
              "end_time": "2025-03-11T07:51:13.541Z",
              "total_elapsed_seconds": 38
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-11T09:03:12.446Z",
            "data": {
              "session_id": "session_1741683775912_irh839j",
              "duration_seconds": 17,
              "idle_seconds": 0,
              "start_time": "2025-03-11T09:02:55.912Z",
              "end_time": "2025-03-11T09:03:12.434Z",
              "total_elapsed_seconds": 17
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-11T18:06:00.206Z",
            "data": {
              "session_id": "session_1741716349766_4war4en",
              "duration_seconds": 10,
              "idle_seconds": 0,
              "start_time": "2025-03-11T18:05:49.766Z",
              "end_time": "2025-03-11T18:05:59.987Z",
              "total_elapsed_seconds": 10
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-11T07:51:16+00:00",
        "updated_at": "2025-03-29T22:44:09+00:00",
        "version": 5
      }
    },
    "paper:arxiv.2503.06698": {
      "data": {
        "primary_id": "arxiv.2503.06698",
        "source": "arxiv",
        "sourceId": "2503.06698",
        "url": "https://arxiv.org/abs/2503.06698",
        "title": "ARXIV Paper: 2503.06698",
        "authors": "Xavier Thomas, Deepti Ghadiyaram",
        "abstract": "Domain Generalization aims to develop models that can generalize to novel and\nunseen data distributions. In this work, we study how model architectures and\npre-training objectives impact feature richness and propose a method to\neffectively leverage them for domain generalization. Specifically, given a\npre-trained feature space, we first discover latent domain structures, referred\nto as pseudo-domains, that capture domain-specific variations in an\nunsupervised manner. Next, we augment existing classifiers with these\ncomplementary pseudo-domain representations making them more amenable to\ndiverse unseen test domains. We analyze how different pre-training feature\nspaces differ in the domain-specific variances they capture. Our empirical\nstudies reveal that features from diffusion models excel at separating domains\nin the absence of explicit domain labels and capture nuanced domain-specific\ninformation. On 5 datasets, we show that our very simple framework improves\ngeneralization to unseen domains by a maximum test accuracy improvement of over\n4% compared to the standard baseline Empirical Risk Minimization (ERM).\nCrucially, our method outperforms most algorithms that access domain labels\nduring training.",
        "timestamp": "2025-03-11T07:50:36.406Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [
            "cs.LG",
            "cs.CV"
          ],
          "published_date": "2025-03-09T17:29:01Z"
        },
        "identifiers": {
          "original": "2503.06698",
          "url": "https://arxiv.org/abs/2503.06698"
        }
      },
      "meta": {
        "created_at": "2025-03-11T07:50:36+00:00",
        "updated_at": "2025-03-29T22:44:09+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2410.13061": {
      "data": {
        "paper_id": "arxiv.2410.13061",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-11T09:13:35.148Z",
            "data": {
              "session_id": "session_1741684388072_u9tk3mj",
              "duration_seconds": 26,
              "idle_seconds": 0,
              "start_time": "2025-03-11T09:13:08.072Z",
              "end_time": "2025-03-11T09:13:34.318Z",
              "total_elapsed_seconds": 26
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-11T09:13:36+00:00",
        "updated_at": "2025-03-29T22:44:06+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2410.13061": {
      "data": {
        "primary_id": "arxiv.2410.13061",
        "source": "arxiv",
        "sourceId": "2410.13061",
        "url": "https://arxiv.org/abs/2410.13061",
        "title": "ARXIV Paper: 2410.13061",
        "authors": "Adrian Ciotinga, YooJung Choi",
        "abstract": "We introduce a novel optimal transport framework for probabilistic circuits\n(PCs). While it has been shown recently that divergences between distributions\nrepresented as certain classes of PCs can be computed tractably, to the best of\nour knowledge, there is no existing approach to compute the Wasserstein\ndistance between probability distributions given by PCs. We propose a\nWasserstein-type distance that restricts the coupling measure of the associated\noptimal transport problem to be a probabilistic circuit. We then develop an\nalgorithm for computing this distance by solving a series of small linear\nprograms and derive the circuit conditions under which this is tractable.\nFurthermore, we show that we can easily retrieve the optimal transport plan\nbetween the PCs from the solutions to these linear programs. Lastly, we study\nthe empirical Wasserstein distance between a PC and a dataset, and show that we\ncan estimate the PC parameters to minimize this distance through an efficient\niterative algorithm.",
        "timestamp": "2025-03-11T09:13:08.269Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [
            "cs.AI",
            "cs.LG",
            "math.OC"
          ],
          "published_date": "2024-10-16T21:42:16Z"
        },
        "identifiers": {
          "original": "2410.13061",
          "url": "https://arxiv.org/abs/2410.13061"
        }
      },
      "meta": {
        "created_at": "2025-03-11T09:13:08+00:00",
        "updated_at": "2025-03-29T22:44:07+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.06462": {
      "data": {
        "primary_id": "arxiv.2503.06462",
        "source": "arxiv",
        "sourceId": "2503.06462",
        "url": "https://arxiv.org/abs/2503.06462",
        "title": "ARXIV Paper: 2503.06462",
        "authors": "Zexu Huang, Min Xu, Stuart Perry",
        "abstract": "Recent advancements in 3D reconstruction coupled with neural rendering\ntechniques have greatly improved the creation of photo-realistic 3D scenes,\ninfluencing both academic research and industry applications. The technique of\n3D Gaussian Splatting and its variants incorporate the strengths of both\nprimitive-based and volumetric representations, achieving superior rendering\nquality. While 3D Geometric Scattering (3DGS) and its variants have advanced\nthe field of 3D representation, they fall short in capturing the stochastic\nproperties of non-local structural information during the training process.\nAdditionally, the initialisation of spherical functions in 3DGS-based methods\noften fails to engage higher-order terms in early training rounds, leading to\nunnecessary computational overhead as training progresses. Furthermore, current\n3DGS-based approaches require training on higher resolution images to render\nhigher resolution outputs, significantly increasing memory demands and\nprolonging training durations. We introduce StructGS, a framework that enhances\n3D Gaussian Splatting (3DGS) for improved novel-view synthesis in 3D\nreconstruction. StructGS innovatively incorporates a patch-based SSIM loss,\ndynamic spherical harmonics initialisation and a Multi-scale Residual Network\n(MSRN) to address the above-mentioned limitations, respectively. Our framework\nsignificantly reduces computational redundancy, enhances detail capture and\nsupports high-resolution rendering from low-resolution inputs. Experimentally,\nStructGS demonstrates superior performance over state-of-the-art (SOTA) models,\nachieving higher quality and more detailed renderings with fewer artifacts.",
        "timestamp": "2025-03-11T09:10:56.193Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [
            "cs.CV",
            "cs.AI"
          ],
          "published_date": "2025-03-09T05:39:44Z"
        },
        "identifiers": {
          "original": "2503.06462",
          "url": "https://arxiv.org/abs/2503.06462"
        }
      },
      "meta": {
        "created_at": "2025-03-11T09:10:56+00:00",
        "updated_at": "2025-03-29T22:44:08+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2402.14740": {
      "data": {
        "paper_id": "arxiv.2402.14740",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-11T18:29:57.774Z",
            "data": {
              "session_id": "session_1741717782711_9fnvoxf",
              "duration_seconds": 14,
              "idle_seconds": 0,
              "start_time": "2025-03-11T18:29:42.711Z",
              "end_time": "2025-03-11T18:29:57.055Z",
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-11T18:59:15.618Z",
            "data": {
              "session_id": "session_1741719544888_9bao77w",
              "duration_seconds": 10,
              "idle_seconds": 0,
              "start_time": "2025-03-11T18:59:04.888Z",
              "end_time": "2025-03-11T18:59:14.500Z",
              "total_elapsed_seconds": 10
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-11T18:29:58+00:00",
        "updated_at": "2025-03-29T22:44:04+00:00",
        "version": 5
      }
    },
    "paper:arxiv.2402.14740": {
      "data": {
        "primary_id": "arxiv.2402.14740",
        "source": "arxiv",
        "sourceId": "2402.14740",
        "url": "https://arxiv.org/abs/2402.14740",
        "title": "ARXIV Paper: 2402.14740",
        "authors": "Arash Ahmadian, Chris Cremer, Matthias Gall\u00e9, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet \u00dcst\u00fcn, Sara Hooker",
        "abstract": "AI alignment in the shape of Reinforcement Learning from Human Feedback\n(RLHF) is increasingly treated as a crucial ingredient for high performance\nlarge language models. Proximal Policy Optimization (PPO) has been positioned\nby recent literature as the canonical method for the RL part of RLHF. However,\nit involves both high computational cost and sensitive hyperparameter tuning.\nWe posit that most of the motivational principles that led to the development\nof PPO are less of a practical concern in RLHF and advocate for a less\ncomputationally expensive method that preserves and even increases performance.\nWe revisit the formulation of alignment from human preferences in the context\nof RL. Keeping simplicity as a guiding principle, we show that many components\nof PPO are unnecessary in an RLHF context and that far simpler REINFORCE-style\noptimization variants outperform both PPO and newly proposed \"RL-free\" methods\nsuch as DPO and RAFT. Our work suggests that careful adaptation to LLMs\nalignment characteristics enables benefiting from online RL optimization at low\ncost.",
        "timestamp": "2025-03-11T18:29:43.097Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [
            "cs.LG",
            "I.2.7"
          ],
          "published_date": "2024-02-22T17:52:34Z"
        },
        "identifiers": {
          "original": "2402.14740",
          "url": "https://arxiv.org/abs/2402.14740"
        }
      },
      "meta": {
        "created_at": "2025-03-11T18:29:43+00:00",
        "updated_at": "2025-03-29T22:44:05+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2405.06705": {
      "data": {
        "paper_id": "arxiv.2405.06705",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-11T18:30:10.678Z",
            "data": {
              "session_id": "session_1741717807140_ml7glyc",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-03-11T18:30:07.140Z",
              "end_time": "2025-03-11T18:30:10.666Z",
              "total_elapsed_seconds": 4
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-11T19:22:10.308Z",
            "data": {
              "session_id": "session_1741720925643_v4zw4dk",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-03-11T19:22:05.643Z",
              "end_time": "2025-03-11T19:22:10.080Z",
              "total_elapsed_seconds": 4
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-11T18:28:53+00:00",
        "updated_at": "2025-03-29T22:44:05+00:00",
        "version": 7
      }
    },
    "paper:arxiv.2405.06705": {
      "data": {
        "primary_id": "arxiv.2405.06705",
        "source": "arxiv",
        "sourceId": "2405.06705",
        "url": "https://arxiv.org/abs/2405.06705",
        "title": "ARXIV Paper: 2405.06705",
        "authors": "Zhuoxuan Jiang, Haoyuan Peng, Shanshan Feng, Fan Li, Dongsheng Li",
        "abstract": "Self-correction is emerging as a promising approach to mitigate the issue of\nhallucination in Large Language Models (LLMs). To facilitate effective\nself-correction, recent research has proposed mistake detection as its initial\nstep. However, current literature suggests that LLMs often struggle with\nreliably identifying reasoning mistakes when using simplistic prompting\nstrategies. To address this challenge, we introduce a unique prompting\nstrategy, termed the Pedagogical Chain-of-Thought (PedCoT), which is\nspecifically designed to guide the identification of reasoning mistakes,\nparticularly mathematical reasoning mistakes. PedCoT consists of pedagogical\nprinciples for prompts (PPP) design, two-stage interaction process (TIP) and\ngrounded PedCoT prompts, all inspired by the educational theory of the Bloom\nCognitive Model (BCM). We evaluate our approach on two public datasets\nfeaturing math problems of varying difficulty levels. The experiments\ndemonstrate that our zero-shot prompting strategy significantly outperforms\nstrong baselines. The proposed method can achieve the goal of reliable\nmathematical mistake identification and provide a foundation for automatic math\nanswer grading. The results underscore the significance of educational theory,\nserving as domain knowledge, in guiding prompting strategy design for\naddressing challenging tasks with LLMs effectively.",
        "timestamp": "2025-03-11T18:28:31.417Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [
            "cs.CL",
            "cs.AI"
          ],
          "published_date": "2024-05-09T07:37:34Z"
        },
        "identifiers": {
          "original": "2405.06705",
          "url": "https://arxiv.org/abs/2405.06705"
        }
      },
      "meta": {
        "created_at": "2025-03-11T18:28:31+00:00",
        "updated_at": "2025-03-29T22:44:06+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2002.06043": {
      "data": {
        "paper_id": "arxiv.2002.06043",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-11T18:58:50.561Z",
            "data": {
              "session_id": "session_1741719524521_ch8iwal",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-03-11T18:58:44.521Z",
              "end_time": "2025-03-11T18:58:49.810Z",
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-11T18:58:51+00:00",
        "updated_at": "2025-03-29T22:44:03+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2002.06043": {
      "data": {
        "primary_id": "arxiv.2002.06043",
        "source": "arxiv",
        "sourceId": "2002.06043",
        "url": "https://arxiv.org/abs/2002.06043",
        "title": "ARXIV Paper: 2002.06043",
        "authors": "Wouter Kool, Herke van Hoof, Max Welling",
        "abstract": "We derive an unbiased estimator for expectations over discrete random\nvariables based on sampling without replacement, which reduces variance as it\navoids duplicate samples. We show that our estimator can be derived as the\nRao-Blackwellization of three different estimators. Combining our estimator\nwith REINFORCE, we obtain a policy gradient estimator and we reduce its\nvariance using a built-in control variate which is obtained without additional\nmodel evaluations. The resulting estimator is closely related to other gradient\nestimators. Experiments with a toy problem, a categorical Variational\nAuto-Encoder and a structured prediction problem show that our estimator is the\nonly estimator that is consistently among the best estimators in both high and\nlow entropy settings.",
        "timestamp": "2025-03-11T18:58:44.945Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [
            "cs.LG",
            "stat.ML"
          ],
          "published_date": "2020-02-14T14:15:18Z"
        },
        "identifiers": {
          "original": "2002.06043",
          "url": "https://arxiv.org/abs/2002.06043"
        }
      },
      "meta": {
        "created_at": "2025-03-11T18:58:45+00:00",
        "updated_at": "2025-03-29T22:44:03+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.07154": {
      "data": {
        "paper_id": "arxiv.2503.07154",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-11T19:51:38.943Z",
            "data": {
              "session_id": "session_1741722691981_0ux6enh",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-03-11T19:51:31.981Z",
              "end_time": "2025-03-11T19:51:38.150Z",
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-11T20:49:17.004Z",
            "data": {
              "session_id": "session_1741726134334_r2bt1xa",
              "duration_seconds": 22,
              "idle_seconds": 0,
              "start_time": "2025-03-11T20:48:54.334Z",
              "end_time": "2025-03-11T20:49:16.762Z",
              "total_elapsed_seconds": 22
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-11T19:51:39+00:00",
        "updated_at": "2025-03-29T22:44:01+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2503.07154": {
      "data": {
        "primary_id": "arxiv.2503.07154",
        "source": "arxiv",
        "sourceId": "2503.07154",
        "url": "https://arxiv.org/abs/2503.07154",
        "title": "ARXIV Paper: 2503.07154",
        "authors": "Jiaming Song, Linqi Zhou",
        "abstract": "Recent years have seen significant advancements in foundation models through\ngenerative pre-training, yet algorithmic innovation in this space has largely\nstagnated around autoregressive models for discrete signals and diffusion\nmodels for continuous signals. This stagnation creates a bottleneck that\nprevents us from fully unlocking the potential of rich multi-modal data, which\nin turn limits the progress on multimodal intelligence. We argue that an\ninference-first perspective, which prioritizes scaling efficiency during\ninference time across sequence length and refinement steps, can inspire novel\ngenerative pre-training algorithms. Using Inductive Moment Matching (IMM) as a\nconcrete example, we demonstrate how addressing limitations in diffusion\nmodels' inference process through targeted modifications yields a stable,\nsingle-stage algorithm that achieves superior sample quality with over an order\nof magnitude greater inference efficiency.",
        "timestamp": "2025-03-11T19:51:32.394Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [
            "cs.LG",
            "cs.AI"
          ],
          "published_date": "2025-03-10T10:27:30Z"
        },
        "identifiers": {
          "original": "2503.07154",
          "url": "https://arxiv.org/abs/2503.07154"
        }
      },
      "meta": {
        "created_at": "2025-03-11T19:51:32+00:00",
        "updated_at": "2025-03-29T22:44:01+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.07565": {
      "data": {
        "paper_id": "arxiv.2503.07565",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-11T19:51:04.825Z",
            "data": {
              "session_id": "session_1741722660444_9fsvdr0",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-03-11T19:51:00.444Z",
              "end_time": "2025-03-11T19:51:04.072Z",
              "total_elapsed_seconds": 4
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-11T19:51:05+00:00",
        "updated_at": "2025-03-29T22:44:02+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.07565": {
      "data": {
        "primary_id": "arxiv.2503.07565",
        "source": "arxiv",
        "sourceId": "2503.07565",
        "url": "https://arxiv.org/abs/2503.07565",
        "title": "ARXIV Paper: 2503.07565",
        "authors": "Linqi Zhou, Stefano Ermon, Jiaming Song",
        "abstract": "Diffusion models and Flow Matching generate high-quality samples but are slow\nat inference, and distilling them into few-step models often leads to\ninstability and extensive tuning. To resolve these trade-offs, we propose\nInductive Moment Matching (IMM), a new class of generative models for one- or\nfew-step sampling with a single-stage training procedure. Unlike distillation,\nIMM does not require pre-training initialization and optimization of two\nnetworks; and unlike Consistency Models, IMM guarantees distribution-level\nconvergence and remains stable under various hyperparameters and standard model\narchitectures. IMM surpasses diffusion models on ImageNet-256x256 with 1.99 FID\nusing only 8 inference steps and achieves state-of-the-art 2-step FID of 1.98\non CIFAR-10 for a model trained from scratch.",
        "timestamp": "2025-03-11T19:51:01.062Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
          ],
          "published_date": "2025-03-10T17:37:39Z"
        },
        "identifiers": {
          "original": "2503.07565",
          "url": "https://arxiv.org/abs/2503.07565"
        }
      },
      "meta": {
        "created_at": "2025-03-11T19:51:01+00:00",
        "updated_at": "2025-03-29T22:44:02+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.08634": {
      "data": {
        "primary_id": "arxiv.2503.08634",
        "source": "arxiv",
        "sourceId": "2503.08634",
        "url": "https://arxiv.org/abs/2503.08634",
        "title": "ARXIV Paper: 2503.08634",
        "authors": "Mohammadjavad Ebrahimi, Yuyang Qiu, Shisheng Cui, Farzad Yousefian",
        "abstract": "We study a bilevel federated learning (FL) problem, where clients\ncooperatively seek to find among multiple optimal solutions of a primary\ndistributed learning problem, a solution that minimizes a secondary distributed\nglobal loss function. This problem is motivated by model selection in\nover-parameterized machine learning, in that the outer-level objective is a\nsuitably-defined regularizer and the inner-level objective is the training loss\nfunction. Despite recent progress in centralized settings,\ncommunication-efficient FL methods equipped with complexity guarantees for\nresolving this problem class are primarily absent. Motivated by this lacuna, we\nconsider the setting where the inner-level objective is convex and the\nouter-level objective is either convex or strongly convex. We propose a\nuniversal regularized scheme and derive promising error bounds in terms of both\nthe inner-level and outer-level loss functions. Leveraging this unifying\ntheory, we then enable two existing FL methods to address the corresponding\nsimple bilevel problem and derive novel communication complexity guarantees for\neach method. Additionally, we devise an FL method for addressing simple bilevel\noptimization problems with a nonconvex outer-level loss function. Through a\ntwo-loop scheme and by leveraging the universal theory, we derive new\ncomplexity bounds for the nonconvex setting. This appears to be the first time\nthat federated simple bilevel optimization problems are provably addressed with\nguarantees. We validate the theoretical findings on EMNIST and CIFAR-10\ndatasets.",
        "timestamp": "2025-03-12T04:27:36.424Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [
            "math.OC"
          ],
          "published_date": "2025-03-11T17:23:52Z"
        },
        "identifiers": {
          "original": "2503.08634",
          "url": "https://arxiv.org/abs/2503.08634"
        }
      },
      "meta": {
        "created_at": "2025-03-12T04:27:36+00:00",
        "updated_at": "2025-03-29T22:43:59+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.08494": {
      "data": {
        "paper_id": "arxiv.2503.08494",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-12T04:27:21.717Z",
            "data": {
              "session_id": "session_1741753633054_zybs0oo",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-03-12T04:27:13.054Z",
              "end_time": "2025-03-12T04:27:20.929Z",
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-12T04:27:22+00:00",
        "updated_at": "2025-03-29T22:43:59+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.08578": {
      "data": {
        "primary_id": "arxiv.2503.08578",
        "source": "arxiv",
        "sourceId": "2503.08578",
        "url": "https://arxiv.org/abs/2503.08578",
        "title": "ARXIV Paper: 2503.08578",
        "authors": "Hui Huang, Hicham Kouhkouh, Lukang Sun",
        "abstract": "We analyze the Consensus-Based Optimization (CBO) algorithm with a consensus\npoint rescaled by a small fixed parameter $\\kappa \\in (0,1)$. Under minimal\nassumptions on the objective function and the initial data, we establish its\nunconditional convergence to the global minimizer. Our results hold in the\nasymptotic regime where both the time--horizon $t \\to \\infty$ and the\ninverse--temperature $\\alpha \\to \\infty$, providing a rigorous theoretical\nfoundation for the algorithm's global convergence. Furthermore, our findings\nextend to the case of multiple and non--discrete set of minimizers.",
        "timestamp": "2025-03-12T04:27:21.294Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [
            "math.OC",
            "math.AP"
          ],
          "published_date": "2025-03-11T16:11:52Z"
        },
        "identifiers": {
          "original": "2503.08578",
          "url": "https://arxiv.org/abs/2503.08578"
        }
      },
      "meta": {
        "created_at": "2025-03-12T04:27:21+00:00",
        "updated_at": "2025-03-29T22:44:00+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.08494": {
      "data": {
        "primary_id": "arxiv.2503.08494",
        "source": "arxiv",
        "sourceId": "2503.08494",
        "url": "https://arxiv.org/abs/2503.08494",
        "title": "ARXIV Paper: 2503.08494",
        "authors": "Wenqing Zhao, Antai Xie, Yuchi Wu, Xinlei Yi, Xiaoqiang Ren",
        "abstract": "This paper studies the distributed generalized Nash equilibrium seeking\nproblem for aggregative games with coupling constraints, where each player\noptimizes its strategy depending on its local cost function and the estimated\nstrategy aggregation. The information transmission in distributed networks may\ngo beyond bandwidth capacity and eventuate communication bottlenecks.\nTherefore, we propose a novel communication-efficient distributed generalized\nNash equilibrium seeking algorithm, in which the communication efficiency is\nimproved by event-triggered communication and information compression methods.\nThe proposed algorithm saves the transmitted rounds and bits of communication\nsimultaneously. Specifically, by developing precise step size conditions, the\nproposed algorithm ensures provable convergence, and is proven to achieve\n$(0,\\delta)$-differential privacy with a stochastic quantization scheme. In the\nend, simulation results verify the effectiveness of the proposed algorithm.",
        "timestamp": "2025-03-12T04:27:13.499Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [
            "math.OC"
          ],
          "published_date": "2025-03-11T14:45:54Z"
        },
        "identifiers": {
          "original": "2503.08494",
          "url": "https://arxiv.org/abs/2503.08494"
        }
      },
      "meta": {
        "created_at": "2025-03-12T04:27:13+00:00",
        "updated_at": "2025-03-29T22:44:00+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.01017": {
      "data": {
        "paper_id": "arxiv.2503.01017",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-12T08:44:58.398Z",
            "data": {
              "session_id": "session_1741769074599_97ojf1x",
              "duration_seconds": 24,
              "idle_seconds": 0,
              "start_time": "2025-03-12T08:44:34.599Z",
              "end_time": "2025-03-12T08:44:58.165Z",
              "total_elapsed_seconds": 24
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-12T08:44:59+00:00",
        "updated_at": "2025-03-29T22:43:58+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.01017": {
      "data": {
        "primary_id": "arxiv.2503.01017",
        "source": "arxiv",
        "sourceId": "2503.01017",
        "url": "https://arxiv.org/abs/2503.01017",
        "title": "ARXIV Paper: 2503.01017",
        "authors": "Yuhang Zhang, Zhiyao Zhang, Junyi Ji, Marcos Qui\u00f1ones-Grueiro, William Barbour, Derek Gloudemans, Gergely Zach\u00e1r, Clay Weston, Gautam Biswas, Daniel B. Work",
        "abstract": "This article presents the first field deployment of a multi-agent\nreinforcement learning (MARL) based variable speed limit (VSL) control system\non Interstate 24 (I-24) near Nashville, Tennessee. We design and demonstrate a\nfull pipeline from training MARL agents in a traffic simulator to a field\ndeployment on a 17-mile segment of I-24 encompassing 67 VSL controllers. The\nsystem was launched on March 8th, 2024, and has made approximately 35 million\ndecisions on 28 million trips in six months of operation. We apply an invalid\naction masking mechanism and several safety guards to ensure real-world\nconstraints. The MARL-based implementation operates up to 98% of the time, with\nthe safety guards overriding the MARL decisions for the remaining time. We\nevaluate the performance of the MARL-based algorithm in comparison to a\npreviously deployed non-RL VSL benchmark algorithm on I-24. Results show that\nthe MARL-based VSL control system achieves a superior performance. The accuracy\nof correctly warning drivers about slowing traffic ahead is improved by 14% and\nthe response delay to non-recurrent congestion is reduced by 75%. The\npreliminary data shows that the VSL control system has reduced the crash rate\nby 26% and the secondary crash rate by 50%. We open-sourced the deployed\nMARL-based VSL algorithm at https://github.com/Lab-Work/marl-vsl-controller.",
        "timestamp": "2025-03-12T08:41:04.168Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [
            "eess.SY",
            "cs.SY"
          ],
          "published_date": "2025-03-02T21:09:16Z"
        },
        "identifiers": {
          "original": "2503.01017",
          "url": "https://arxiv.org/abs/2503.01017"
        }
      },
      "meta": {
        "created_at": "2025-03-12T08:41:04+00:00",
        "updated_at": "2025-03-29T22:43:58+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2405.16002": {
      "data": {
        "paper_id": "arxiv.2405.16002",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-12T08:51:31.101Z",
            "data": {
              "session_id": "session_1741769484892_hf5h9o5",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-03-12T08:51:24.892Z",
              "end_time": "2025-03-12T08:51:30.400Z",
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-12T08:51:31+00:00",
        "updated_at": "2025-03-29T22:43:54+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2405.16002": {
      "data": {
        "primary_id": "arxiv.2405.16002",
        "source": "arxiv",
        "sourceId": "2405.16002",
        "url": "https://arxiv.org/abs/2405.16002",
        "title": "ARXIV Paper: 2405.16002",
        "authors": "Minhak Song, Kwangjun Ahn, Chulhee Yun",
        "abstract": "Understanding the training dynamics of deep neural networks is challenging\ndue to their high-dimensional nature and intricate loss landscapes. Recent\nstudies have revealed that, along the training trajectory, the gradient\napproximately aligns with a low-rank top eigenspace of the training loss\nHessian, referred to as the dominant subspace. Given this alignment, this paper\nexplores whether neural networks can be trained within the dominant subspace,\nwhich, if feasible, could lead to more efficient training methods. Our primary\nobservation is that when the SGD update is projected onto the dominant\nsubspace, the training loss does not decrease further. This suggests that the\nobserved alignment between the gradient and the dominant subspace is spurious.\nSurprisingly, projecting out the dominant subspace proves to be just as\neffective as the original update, despite removing the majority of the original\nupdate component. We observe similar behavior across practical setups,\nincluding the large learning rate regime (also known as Edge of Stability),\nSharpness-Aware Minimization, momentum, and adaptive optimizers. We discuss the\nmain causes and implications of this spurious alignment, shedding light on the\ndynamics of neural network training.",
        "timestamp": "2025-03-12T08:51:25.306Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [
            "cs.LG",
            "math.OC",
            "stat.ML"
          ],
          "published_date": "2024-05-25T01:44:35Z"
        },
        "identifiers": {
          "original": "2405.16002",
          "url": "https://arxiv.org/abs/2405.16002"
        }
      },
      "meta": {
        "created_at": "2025-03-12T08:51:25+00:00",
        "updated_at": "2025-03-29T22:43:54+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2405.16397": {
      "data": {
        "paper_id": "arxiv.2405.16397",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-12T08:51:33.992Z",
            "data": {
              "session_id": "session_1741769490400_dv7gzmj",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-03-12T08:51:30.400Z",
              "end_time": "2025-03-12T08:51:33.983Z",
              "total_elapsed_seconds": 4
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-12T08:51:04+00:00",
        "updated_at": "2025-03-29T22:43:54+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2405.16397": {
      "data": {
        "primary_id": "arxiv.2405.16397",
        "source": "arxiv",
        "sourceId": "2405.16397",
        "url": "https://arxiv.org/abs/2405.16397",
        "title": "ARXIV Paper: 2405.16397",
        "authors": "Damien Martins Gomes, Yanlei Zhang, Eugene Belilovsky, Guy Wolf, Mahdi S. Hosseini",
        "abstract": "First-order optimization methods are currently the mainstream in training\ndeep neural networks (DNNs). Optimizers like Adam incorporate limited curvature\ninformation by employing the diagonal matrix preconditioning of the stochastic\ngradient during the training. Despite their widespread, second-order\noptimization algorithms exhibit superior convergence properties compared to\ntheir first-order counterparts e.g. Adam and SGD. However, their practicality\nin training DNNs is still limited due to increased per-iteration computations\ncompared to the first-order methods. We present \\emph{AdaFisher}--an adaptive\nsecond-order optimizer that leverages a \\emph{diagonal block-Kronecker}\napproximation of the Fisher information matrix for adaptive gradient\npreconditioning. AdaFisher aims to bridge the gap between enhanced\n\\emph{convergence/generalization} capabilities and computational efficiency in\nsecond-order optimization framework for training DNNs. Despite the slow pace of\nsecond-order optimizers, we showcase that AdaFisher can be reliably adopted for\nimage classification, language modeling and stands out for its stability and\nrobustness in hyper-parameter tuning. We demonstrate that AdaFisher\n\\textbf{outperforms the SOTA optimizers} in terms of both accuracy and\nconvergence speed. Code is available from\nhttps://github.com/AtlasAnalyticsLab/AdaFisher.",
        "timestamp": "2025-03-12T08:50:54.052Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [
            "cs.LG",
            "math.OC"
          ],
          "published_date": "2024-05-26T01:25:02Z"
        },
        "identifiers": {
          "original": "2405.16397",
          "url": "https://arxiv.org/abs/2405.16397"
        }
      },
      "meta": {
        "created_at": "2025-03-12T08:50:54+00:00",
        "updated_at": "2025-03-29T22:43:55+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2408.02572": {
      "data": {
        "paper_id": "arxiv.2408.02572",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-12T08:50:30.240Z",
            "data": {
              "session_id": "session_1741769420788_jql3y39",
              "duration_seconds": 9,
              "idle_seconds": 0,
              "start_time": "2025-03-12T08:50:20.788Z",
              "end_time": "2025-03-12T08:50:29.517Z",
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-12T08:50:31+00:00",
        "updated_at": "2025-03-29T22:43:56+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2408.02572": {
      "data": {
        "primary_id": "arxiv.2408.02572",
        "source": "arxiv",
        "sourceId": "2408.02572",
        "url": "https://arxiv.org/abs/2408.02572",
        "title": "ARXIV Paper: 2408.02572",
        "authors": "Mateus Ara\u00fajo, Andrew J. P. Garner, Miguel Navascues",
        "abstract": "Non-commutative polynomial optimization (NPO) problems seek to minimize the\nstate average of a polynomial of some operator variables, subject to polynomial\nconstraints, over all states and operators, as well as the Hilbert spaces where\nthose might be defined. Many of these problems are known to admit a complete\nhierarchy of semidefinite programming (SDP) relaxations. In this work, we\nconsider a variant of NPO problems where a subset of the operator variables\nsatisfies a system of ordinary differential equations. We find that, under mild\nconditions of operator boundedness, for every such problem one can construct a\nstandard NPO problem with the same solution. This allows us to define a\ncomplete hierarchy of SDPs to tackle the original differential problem. We\napply this method to bound averages of local observables in quantum spin\nsystems subject to a Hamiltonian evolution (i.e., a quench). We find that, even\nin the thermodynamic limit of infinitely many sites, low levels of the\nhierarchy provide very good approximations for reasonably long evolution times.",
        "timestamp": "2025-03-12T08:50:21.042Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [
            "quant-ph",
            "math.OC"
          ],
          "published_date": "2024-08-05T15:46:57Z"
        },
        "identifiers": {
          "original": "2408.02572",
          "url": "https://arxiv.org/abs/2408.02572"
        }
      },
      "meta": {
        "created_at": "2025-03-12T08:50:21+00:00",
        "updated_at": "2025-03-29T22:43:56+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2408.16543": {
      "data": {
        "primary_id": "arxiv.2408.16543",
        "source": "arxiv",
        "sourceId": "2408.16543",
        "url": "https://arxiv.org/abs/2408.16543",
        "title": "ARXIV Paper: 2408.16543",
        "authors": "Cl\u00e9mentine Chazal, Anna Korba, Francis Bach",
        "abstract": "In this paper, we study the statistical and geometrical properties of the\nKullback-Leibler divergence with kernel covariance operators (KKL) introduced\nby Bach [2022]. Unlike the classical Kullback-Leibler (KL) divergence that\ninvolves density ratios, the KKL compares probability distributions through\ncovariance operators (embeddings) in a reproducible kernel Hilbert space\n(RKHS), and compute the Kullback-Leibler quantum divergence. This novel\ndivergence hence shares parallel but different aspects with both the standard\nKullback-Leibler between probability distributions and kernel embeddings\nmetrics such as the maximum mean discrepancy. A limitation faced with the\noriginal KKL divergence is its inability to be defined for distributions with\ndisjoint supports. To solve this problem, we propose in this paper a\nregularised variant that guarantees that the divergence is well defined for all\ndistributions. We derive bounds that quantify the deviation of the regularised\nKKL to the original one, as well as finite-sample bounds. In addition, we\nprovide a closed-form expression for the regularised KKL, specifically\napplicable when the distributions consist of finite sets of points, which makes\nit implementable. Furthermore, we derive a Wasserstein gradient descent scheme\nof the KKL divergence in the case of discrete distributions, and study\nempirically its properties to transport a set of points to a target\ndistribution.",
        "timestamp": "2025-03-12T08:48:52.657Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [
            "stat.ML",
            "cs.LG",
            "math.FA",
            "math.OC"
          ],
          "published_date": "2024-08-29T14:01:30Z"
        },
        "identifiers": {
          "original": "2408.16543",
          "url": "https://arxiv.org/abs/2408.16543"
        }
      },
      "meta": {
        "created_at": "2025-03-12T08:48:53+00:00",
        "updated_at": "2025-03-29T22:43:57+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2408.16543": {
      "data": {
        "paper_id": "arxiv.2408.16543",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-12T21:36:14.609Z",
            "data": {
              "session_id": "session_1741815364571_i0pua8h",
              "duration_seconds": 10,
              "idle_seconds": 0,
              "start_time": "2025-03-12T21:36:04.571Z",
              "end_time": "2025-03-12T21:36:14.202Z",
              "total_elapsed_seconds": 10
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-12T21:36:15+00:00",
        "updated_at": "2025-03-29T22:43:53+00:00",
        "version": 3
      }
    },
    "interactions:arxiv.2503.07891": {
      "data": {
        "paper_id": "arxiv.2503.07891",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-12T21:47:50.005Z",
            "data": {
              "session_id": "session_1741816052293_8jl9wj7",
              "duration_seconds": 12,
              "idle_seconds": 0,
              "start_time": "2025-03-12T21:47:32.293Z",
              "end_time": "2025-03-12T21:47:44.366Z",
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-12T21:50:43.136Z",
            "data": {
              "session_id": "session_1741816233783_gt9nsfw",
              "duration_seconds": 9,
              "idle_seconds": 0,
              "start_time": "2025-03-12T21:50:33.783Z",
              "end_time": "2025-03-12T21:50:42.930Z",
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-12T21:47:46+00:00",
        "updated_at": "2025-03-29T22:43:52+00:00",
        "version": 5
      }
    },
    "paper:arxiv.2503.07891": {
      "data": {
        "primary_id": "arxiv.2503.07891",
        "source": "arxiv",
        "sourceId": "2503.07891",
        "url": "https://arxiv.org/abs/2503.07891",
        "title": "2503.07891",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-12T21:47:32.583Z",
        "rating": "novote",
        "source_specific_metadata": {
          "arxiv_tags": [],
          "published_date": ""
        },
        "identifiers": {
          "original": "2503.07891",
          "url": "https://arxiv.org/abs/2503.07891"
        }
      },
      "meta": {
        "created_at": "2025-03-12T21:47:33+00:00",
        "updated_at": "2025-03-29T22:43:53+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2311.00059": {
      "data": {
        "primary_id": "arxiv.2311.00059",
        "source": "arxiv",
        "sourceId": "2311.00059",
        "url": "https://arxiv.org/abs/2311.00059",
        "title": "The Generative AI Paradox: \"What It Can Create, It May Not Understand\"",
        "authors": "Peter West, Ximing Lu, Nouha Dziri, Faeze Brahman, Linjie Li, Jena D. Hwang, Liwei Jiang, Jillian Fisher, Abhilasha Ravichander, Khyathi Chandu, Benjamin Newman, Pang Wei Koh, Allyson Ettinger, Yejin Choi",
        "abstract": "The recent wave of generative AI has sparked unprecedented global attention,\nwith both excitement and concern over potentially superhuman levels of\nartificial intelligence: models now take only seconds to produce outputs that\nwould challenge or exceed the capabilities even of expert humans. At the same\ntime, models still show basic errors in understanding that would not be\nexpected even in non-expert humans. This presents us with an apparent paradox:\nhow do we reconcile seemingly superhuman capabilities with the persistence of\nerrors that few humans would make? In this work, we posit that this tension\nreflects a divergence in the configuration of intelligence in today's\ngenerative models relative to intelligence in humans. Specifically, we propose\nand test the Generative AI Paradox hypothesis: generative models, having been\ntrained directly to reproduce expert-like outputs, acquire generative\ncapabilities that are not contingent upon -- and can therefore exceed -- their\nability to understand those same types of outputs. This contrasts with humans,\nfor whom basic understanding almost always precedes the ability to generate\nexpert-level outputs. We test this hypothesis through controlled experiments\nanalyzing generation vs. understanding in generative models, across both\nlanguage and image modalities. Our results show that although models can\noutperform humans in generation, they consistently fall short of human\ncapabilities in measures of understanding, as well as weaker correlation\nbetween generation and understanding performance, and more brittleness to\nadversarial inputs. Our findings support the hypothesis that models' generative\ncapability may not be contingent upon understanding capability, and call for\ncaution in interpreting artificial intelligence by analogy to human\nintelligence.",
        "timestamp": "2025-03-12T21:52:45.657Z",
        "rating": "novote",
        "arxivId": "2311.00059",
        "arxiv_tags": [
          "cs.AI",
          "cs.CL",
          "cs.CV",
          "cs.LG"
        ],
        "published_date": "2023-10-31T18:07:07Z"
      },
      "meta": {
        "created_at": "2025-03-12T21:52:46+00:00",
        "updated_at": "2025-03-29T22:43:51+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2311.00059": {
      "data": {
        "paper_id": "arxiv.2311.00059",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-12T21:53:03.054Z",
            "data": {
              "session_id": "session_1741816365073_dbrlm09",
              "duration_seconds": 11,
              "idle_seconds": 0,
              "start_time": "2025-03-12T21:52:45.073Z",
              "end_time": "2025-03-12T21:52:56.047Z",
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-12T23:20:56.652Z",
            "data": {
              "session_id": "session_1741821638443_it3o78s",
              "duration_seconds": 16,
              "idle_seconds": 0,
              "start_time": "2025-03-12T23:20:38.443Z",
              "end_time": "2025-03-12T23:20:54.768Z",
              "total_elapsed_seconds": 16
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-12T23:48:44.307Z",
            "data": {
              "session_id": "session_1741823312413_nikj513",
              "duration_seconds": 12,
              "idle_seconds": 0,
              "start_time": "2025-03-12T23:48:32.413Z",
              "end_time": "2025-03-12T23:48:44.115Z",
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-13T04:31:32.474Z",
            "data": {
              "session_id": "session_1741840270884_5ato00q",
              "duration_seconds": 21,
              "idle_seconds": 0,
              "start_time": "2025-03-13T04:31:10.884Z",
              "end_time": "2025-03-13T04:31:32.061Z",
              "total_elapsed_seconds": 21
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-13T04:31:32.972Z",
            "data": {
              "session_id": "session_1741840270884_5ato00q",
              "duration_seconds": 21,
              "idle_seconds": 0,
              "start_time": "2025-03-13T04:31:10.884Z",
              "end_time": "2025-03-13T04:31:32.061Z",
              "total_elapsed_seconds": 21
            }
          }
        ],
        "legacy_id": "2311.00059"
      },
      "meta": {
        "created_at": "2025-03-12T21:52:57+00:00",
        "updated_at": "2025-03-29T22:43:51+00:00",
        "version": 10
      }
    },
    "interactions:arxiv.2503.08537": {
      "data": {
        "paper_id": "arxiv.2503.08537",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-13T03:33:08.817Z",
            "data": {
              "session_id": "session_1741836760350_f8rpsyp",
              "duration_seconds": 28,
              "idle_seconds": 0,
              "start_time": "2025-03-13T03:32:40.350Z",
              "end_time": "2025-03-13T03:33:08.376Z",
              "total_elapsed_seconds": 28
            }
          }
        ],
        "legacy_id": "2503.08537"
      },
      "meta": {
        "created_at": "2025-03-13T03:33:09+00:00",
        "updated_at": "2025-03-29T22:43:50+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.08537": {
      "data": {
        "primary_id": "arxiv.2503.08537",
        "source": "arxiv",
        "sourceId": "2503.08537",
        "url": "https://arxiv.org/abs/2503.08537",
        "title": "Chemical reasoning in LLMs unlocks steerable synthesis planning and\n  reaction mechanism elucidation",
        "authors": "Andres M Bran, Theo A Neukomm, Daniel P Armstrong, Zlatko Jon\u010dev, Philippe Schwaller",
        "abstract": "While machine learning algorithms have been shown to excel at specific\nchemical tasks, they have struggled to capture the strategic thinking that\ncharacterizes expert chemical reasoning, limiting their widespread adoption.\nHere we demonstrate that large language models (LLMs) can serve as powerful\nchemical reasoning engines when integrated with traditional search algorithms,\nenabling a new approach to computer-aided chemistry that mirrors human expert\nthinking. Rather than using LLMs to directly manipulate chemical structures, we\nleverage their ability to evaluate chemical strategies and guide search\nalgorithms toward chemically meaningful solutions. We demonstrate this paradigm\nthrough two fundamental challenges: strategy-aware retrosynthetic planning and\nmechanism elucidation. In retrosynthetic planning, our method allows chemists\nto specify desired synthetic strategies in natural language to find routes that\nsatisfy these constraints in vast searches. In mechanism elucidation, LLMs\nguide the search for plausible reaction mechanisms by combining chemical\nprinciples with systematic exploration. Our approach shows strong performance\nacross diverse chemical tasks, with larger models demonstrating increasingly\nsophisticated chemical reasoning. Our approach establishes a new paradigm for\ncomputer-aided chemistry that combines the strategic understanding of LLMs with\nthe precision of traditional chemical tools, opening possibilities for more\nintuitive and powerful chemical reasoning systems.",
        "timestamp": "2025-03-13T02:11:17.866Z",
        "rating": "novote",
        "arxivId": "2503.08537",
        "arxiv_tags": [
          "cs.AI",
          "cond-mat.mtrl-sci"
        ],
        "published_date": "2025-03-11T15:27:17Z"
      },
      "meta": {
        "created_at": "2025-03-13T02:11:18+00:00",
        "updated_at": "2025-03-29T22:43:50+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.09576": {
      "data": {
        "primary_id": "arxiv.2503.09576",
        "source": "arxiv",
        "sourceId": "2503.09576",
        "url": "https://arxiv.org/abs/2503.09576",
        "title": "Manify: A Python Library for Learning Non-Euclidean Representations",
        "authors": "Philippe Chlenski, Kaizhu Du, Dylan Satow, Itsik Pe'er",
        "abstract": "We present Manify, an open-source Python library for non-Euclidean\nrepresentation learning. Leveraging manifold learning techniques, Manify\nprovides tools for learning embeddings in (products of) non-Euclidean spaces,\nperforming classification and regression with data that lives in such spaces,\nand estimating the curvature of a manifold. Manify aims to advance research and\napplications in machine learning by offering a comprehensive suite of tools for\nmanifold-based data analysis. Our source code, examples, datasets, results, and\ndocumentation are available at https://github.com/pchlenski/manify",
        "timestamp": "2025-03-13T07:06:32.811Z",
        "rating": "novote",
        "arxivId": "2503.09576",
        "arxiv_tags": [
          "cs.LG"
        ],
        "published_date": "2025-03-12T17:44:40Z"
      },
      "meta": {
        "created_at": "2025-03-13T07:06:33+00:00",
        "updated_at": "2025-03-29T22:43:47+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.09541": {
      "data": {
        "primary_id": "arxiv.2503.09541",
        "source": "arxiv",
        "sourceId": "2503.09541",
        "url": "https://arxiv.org/abs/2503.09541",
        "title": "Neural Network-Based Change Point Detection for Large-Scale\n  Time-Evolving Data",
        "authors": "Jialiang Geng, George Michailidis",
        "abstract": "The paper studies the problem of detecting and locating change points in\nmultivariate time-evolving data. The problem has a long history in statistics\nand signal processing and various algorithms have been developed primarily for\nsimple parametric models. In this work, we focus on modeling the data through\nfeed-forward neural networks and develop a detection strategy based on the\nfollowing two-step procedure. In the first step, the neural network is trained\nover a prespecified window of the data, and its test error function is\ncalibrated over another prespecified window. Then, the test error function is\nused over a moving window to identify the change point. Once a change point is\ndetected, the procedure involving these two steps is repeated until all change\npoints are identified. The proposed strategy yields consistent estimates for\nboth the number and the locations of the change points under temporal\ndependence of the data-generating process. The effectiveness of the proposed\nstrategy is illustrated on synthetic data sets that provide insights on how to\nselect in practice tuning parameters of the algorithm and in real data sets.\nFinally, we note that although the detection strategy is general and can work\nwith different neural network architectures, the theoretical guarantees\nprovided are specific to feed-forward neural architectures.",
        "timestamp": "2025-03-13T07:05:16.806Z",
        "rating": "novote",
        "arxivId": "2503.09541",
        "arxiv_tags": [
          "stat.ML",
          "cs.LG",
          "stat.AP",
          "stat.CO",
          "stat.ME"
        ],
        "published_date": "2025-03-12T16:58:52Z"
      },
      "meta": {
        "created_at": "2025-03-13T07:05:17+00:00",
        "updated_at": "2025-03-29T22:43:48+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.09541": {
      "data": {
        "paper_id": "arxiv.2503.09541",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-13T07:05:20.512Z",
            "data": {
              "session_id": "session_1741849516303_iy9ng81",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-03-13T07:05:16.304Z",
              "end_time": "2025-03-13T07:05:19.740Z",
              "total_elapsed_seconds": 3
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-13T07:06:27.942Z",
            "data": {
              "session_id": "session_1741849574614_3bfjit8",
              "duration_seconds": 13,
              "idle_seconds": 0,
              "start_time": "2025-03-13T07:06:14.614Z",
              "end_time": "2025-03-13T07:06:27.163Z",
              "total_elapsed_seconds": 13
            }
          }
        ],
        "legacy_id": "2503.09541"
      },
      "meta": {
        "created_at": "2025-03-13T07:05:21+00:00",
        "updated_at": "2025-03-29T22:43:48+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2503.08944": {
      "data": {
        "primary_id": "arxiv.2503.08944",
        "source": "arxiv",
        "sourceId": "2503.08944",
        "url": "https://arxiv.org/pdf/2503.08944",
        "title": "Comment on \"Interferometric single-shot parity measurement in InAs-Al\n  hybrid devices\", Microsoft Quantum, Nature 638, 651-655 (2025)",
        "authors": "Henry F. Legg",
        "abstract": "We consider the 'parity readout' of a (topological) superconductor claimed in\nNature 638, 651-655 (2025). A prerequisite for this claim is the existence of a\nsuperconducting gap in the nanowire device. However, to determine the presence\nof a gap, Nature 638, 651-655 (2025) relied on the so-called topological gap\nprotocol (TGP). Here, we show that the TGP can report the regions where the\n'parity readout' occurred as either gapped or gapless, depending on data\nparameters such as magnetic field range and cutter pair (junction\ntransparency). Compounding these issues are inaccuracies in the presented TGP\noutcomes, which limited investigation of reproducibility. Since these\ninconsistent outcomes demonstrate that the TGP is not a reliable diagnostic\ntool for the presence of a superconducting gap, we instead investigate the\nconductance data for the studied regions -- data that were not presented in\nNature 638, 651-655 (2025), but are in the public data repository. These\nconductance data show that the regions where 'parity readout' occurred are in\nfact highly disordered and present no clear gap in the nanowire, i.e., the\nunderlying conductance data show that these regions are indeed gapless. That\nthese regions are gapless contradicts the claim that the reported measurements\nare of the parity of a superconducting nanowire, let alone the parity of a\ntopological superconducting nanowire. Taken together, these issues mean that\nthe core findings in Nature 638, 651-655 (2025) are not reliable and should be\nrevisited.",
        "timestamp": "2025-03-13T07:02:31.456Z",
        "rating": "novote",
        "arxivId": "2503.08944",
        "arxiv_tags": [
          "cond-mat.mes-hall",
          "cond-mat.supr-con",
          "quant-ph"
        ],
        "published_date": "2025-03-11T22:47:26Z"
      },
      "meta": {
        "created_at": "2025-03-13T07:02:31+00:00",
        "updated_at": "2025-03-29T22:43:49+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2402.11057": {
      "data": {
        "primary_id": "arxiv.2402.11057",
        "source": "arxiv",
        "sourceId": "2402.11057",
        "url": "https://arxiv.org/abs/2402.11057",
        "title": "CoMapGS: Covisibility Map-based Gaussian Splatting for Sparse Novel View\n  Synthesis",
        "authors": "Youngkyoon Jang, Eduardo P\u00e9rez-Pellitero",
        "abstract": "We propose Covisibility Map-based Gaussian Splatting (CoMapGS), designed to\nrecover underrepresented sparse regions in sparse novel view synthesis. CoMapGS\naddresses both high- and low-uncertainty regions by constructing covisibility\nmaps, enhancing initial point clouds, and applying uncertainty-aware weighted\nsupervision with a proximity classifier. Our contributions are threefold: (1)\nCoMapGS reframes novel view synthesis by leveraging covisibility maps as a core\ncomponent to address region-specific uncertainty levels; (2) Enhanced initial\npoint clouds for both low- and high-uncertainty regions compensate for sparse\nCOLMAP-derived point clouds, improving reconstruction quality and benefiting\nfew-shot 3DGS methods; (3) Adaptive supervision with covisibility-score-based\nweighting and proximity classification achieves consistent performance gains\nacross scenes with various sparsity scores derived from covisibility maps.\nExperimental results demonstrate that CoMapGS outperforms state-of-the-art\nmethods on datasets including Mip-NeRF 360 and LLFF.",
        "timestamp": "2025-03-13T07:02:28.727Z",
        "rating": "novote",
        "arxivId": "2402.11057",
        "arxiv_tags": [
          "cs.CV"
        ],
        "published_date": "2024-02-16T20:12:33Z"
      },
      "meta": {
        "created_at": "2025-03-13T07:02:29+00:00",
        "updated_at": "2025-03-29T22:43:49+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.09576": {
      "data": {
        "paper_id": "arxiv.2503.09576",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-13T07:08:38.979Z",
            "data": {
              "session_id": "session_1741849709915_61bgn2v",
              "duration_seconds": 9,
              "idle_seconds": 0,
              "start_time": "2025-03-13T07:08:29.915Z",
              "end_time": "2025-03-13T07:08:38.600Z",
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-13T07:08:39.617Z",
            "data": {
              "session_id": "session_1741849709915_61bgn2v",
              "duration_seconds": 9,
              "idle_seconds": 0,
              "start_time": "2025-03-13T07:08:29.915Z",
              "end_time": "2025-03-13T07:08:38.600Z",
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-13T07:08:41.300Z",
            "data": {
              "session_id": "session_1741849709915_61bgn2v",
              "duration_seconds": 9,
              "idle_seconds": 0,
              "start_time": "2025-03-13T07:08:29.915Z",
              "end_time": "2025-03-13T07:08:38.600Z",
              "total_elapsed_seconds": 9
            }
          }
        ],
        "legacy_id": "2503.09576"
      },
      "meta": {
        "created_at": "2025-03-13T07:08:39+00:00",
        "updated_at": "2025-03-29T22:43:47+00:00",
        "version": 5
      }
    },
    "interactions:arxiv.2404.02258": {
      "data": {
        "paper_id": "arxiv.2404.02258",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-14T00:03:52.588Z",
            "data": {
              "session_id": "session_1741910628747_yt5s9q8",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-03-14T00:03:48.747Z",
              "end_time": "2025-03-14T00:03:51.844Z",
              "total_elapsed_seconds": 3
            }
          }
        ],
        "legacy_id": "2404.02258"
      },
      "meta": {
        "created_at": "2025-03-14T00:03:53+00:00",
        "updated_at": "2025-03-29T22:43:46+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2404.02258": {
      "data": {
        "primary_id": "arxiv.2404.02258",
        "source": "arxiv",
        "sourceId": "2404.02258",
        "url": "https://arxiv.org/abs/2404.02258",
        "title": "Mixture-of-Depths: Dynamically allocating compute in transformer-based\n  language models",
        "authors": "David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, Adam Santoro",
        "abstract": "Transformer-based language models spread FLOPs uniformly across input\nsequences. In this work we demonstrate that transformers can instead learn to\ndynamically allocate FLOPs (or compute) to specific positions in a sequence,\noptimising the allocation along the sequence for different layers across the\nmodel depth. Our method enforces a total compute budget by capping the number\nof tokens ($k$) that can participate in the self-attention and MLP computations\nat a given layer. The tokens to be processed are determined by the network\nusing a top-$k$ routing mechanism. Since $k$ is defined a priori, this simple\nprocedure uses a static computation graph with known tensor sizes, unlike other\nconditional computation techniques. Nevertheless, since the identities of the\n$k$ tokens are fluid, this method can expend FLOPs non-uniformly across the\ntime and model depth dimensions. Thus, compute expenditure is entirely\npredictable in sum total, but dynamic and context-sensitive at the token-level.\nNot only do models trained in this way learn to dynamically allocate compute,\nthey do so efficiently. These models match baseline performance for equivalent\nFLOPS and wall-clock times to train, but require a fraction of the FLOPs per\nforward pass, and can be upwards of 50\\% faster to step during post-training\nsampling.",
        "timestamp": "2025-03-14T00:03:49.439Z",
        "rating": "novote",
        "arxivId": "2404.02258",
        "arxiv_tags": [
          "cs.LG",
          "cs.CL"
        ],
        "published_date": "2024-04-02T19:28:11Z"
      },
      "meta": {
        "created_at": "2025-03-14T00:03:49+00:00",
        "updated_at": "2025-03-29T22:43:46+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2310.00431": {
      "data": {
        "paper_id": "arxiv.2310.00431",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-14T04:08:12.176Z",
            "data": {
              "session_id": "session_1741925282628_unbv4tz",
              "duration_seconds": 9,
              "idle_seconds": 0,
              "start_time": "2025-03-14T04:08:02.628Z",
              "end_time": "2025-03-14T04:08:11.378Z",
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-14T04:09:22.297Z",
            "data": {
              "session_id": "session_1741925335495_kx65k4h",
              "duration_seconds": 26,
              "idle_seconds": 0,
              "start_time": "2025-03-14T04:08:55.495Z",
              "end_time": "2025-03-14T04:09:20.995Z",
              "total_elapsed_seconds": 26
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-14T04:09:22.896Z",
            "data": {
              "session_id": "session_1741925335495_kx65k4h",
              "duration_seconds": 26,
              "idle_seconds": 0,
              "start_time": "2025-03-14T04:08:55.495Z",
              "end_time": "2025-03-14T04:09:20.995Z",
              "total_elapsed_seconds": 26
            }
          }
        ],
        "legacy_id": "2310.00431"
      },
      "meta": {
        "created_at": "2025-03-14T04:07:56+00:00",
        "updated_at": "2025-03-29T22:43:42+00:00",
        "version": 8
      }
    },
    "paper:arxiv.2310.00431": {
      "data": {
        "primary_id": "arxiv.2310.00431",
        "source": "arxiv",
        "sourceId": "2310.00431",
        "url": "https://arxiv.org/abs/2310.00431",
        "title": "ResolvNet: A Graph Convolutional Network with multi-scale Consistency",
        "authors": "Christian Koke, Abhishek Saroha, Yuesong Shen, Marvin Eisenberger, Daniel Cremers",
        "abstract": "It is by now a well known fact in the graph learning community that the\npresence of bottlenecks severely limits the ability of graph neural networks to\npropagate information over long distances. What so far has not been appreciated\nis that, counter-intuitively, also the presence of strongly connected\nsub-graphs may severely restrict information flow in common architectures.\nMotivated by this observation, we introduce the concept of multi-scale\nconsistency. At the node level this concept refers to the retention of a\nconnected propagation graph even if connectivity varies over a given graph. At\nthe graph-level, multi-scale consistency refers to the fact that distinct\ngraphs describing the same object at different resolutions should be assigned\nsimilar feature vectors. As we show, both properties are not satisfied by\npoular graph neural network architectures. To remedy these shortcomings, we\nintroduce ResolvNet, a flexible graph neural network based on the mathematical\nconcept of resolvents. We rigorously establish its multi-scale consistency\ntheoretically and verify it in extensive experiments on real world data: Here\nnetworks based on this ResolvNet architecture prove expressive; out-performing\nbaselines significantly on many tasks; in- and outside the multi-scale setting.",
        "timestamp": "2025-03-14T04:07:25.791Z",
        "rating": "novote",
        "arxivId": "2310.00431",
        "arxiv_tags": [
          "cs.LG"
        ],
        "published_date": "2023-09-30T16:46:45Z"
      },
      "meta": {
        "created_at": "2025-03-14T04:07:26+00:00",
        "updated_at": "2025-03-29T22:43:43+00:00",
        "version": 2
      }
    },
    "interactions:openreview.SRCsyJafgP": {
      "data": {
        "paper_id": "openreview.SRCsyJafgP",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-14T04:07:10.773Z",
            "data": {
              "session_id": "session_1741925213011_71f0173",
              "duration_seconds": 17,
              "idle_seconds": 0,
              "start_time": "2025-03-14T04:06:53.011Z",
              "end_time": "2025-03-14T04:07:10.353Z",
              "total_elapsed_seconds": 17
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-14T04:09:57.266Z",
            "data": {
              "session_id": "session_1741925391117_sghhdvu",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-03-14T04:09:51.117Z",
              "end_time": "2025-03-14T04:09:57.246Z",
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-14T04:07:12+00:00",
        "updated_at": "2025-03-29T22:43:43+00:00",
        "version": 7
      }
    },
    "paper:openreview.SRCsyJafgP": {
      "data": {
        "primary_id": "openreview.SRCsyJafgP",
        "source": "openreview",
        "sourceId": "SRCsyJafgP",
        "url": "https://openreview.net/forum?id=SRCsyJafgP",
        "title": "On the successful Incorporation of Scale into Graph Neural Networks",
        "authors": "Christian Koke",
        "abstract": "Standard graph neural networks assign vastly different latent embeddings to graphs describing the same physical system at different resolution scales. This precludes consistency in applications and prevents generalization between scales as would fundamentally be needed in many scientific applications. We uncover the underlying obstruction, investigate its origin and show how to overcome it.",
        "timestamp": "2025-03-14T03:27:22.626Z",
        "rating": "novote",
        "identifiers": {
          "original": "SRCsyJafgP",
          "url": "https://openreview.net/forum?id=SRCsyJafgP"
        }
      },
      "meta": {
        "created_at": "2025-03-14T03:27:23+00:00",
        "updated_at": "2025-03-29T22:43:45+00:00",
        "version": 2
      }
    },
    "interactions:2309.14054": {
      "data": {
        "paper_id": "2309.14054",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-15T05:01:04.359Z",
            "data": {
              "session_id": "session_1742014853152_c4ah06c",
              "duration_seconds": 10,
              "idle_seconds": 0,
              "start_time": "2025-03-15T05:00:53.152Z",
              "end_time": "2025-03-15T05:01:03.486Z",
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-15T05:05:19.383Z",
            "data": {
              "session_id": "session_1742014872883_xwvlps5",
              "duration_seconds": 245,
              "idle_seconds": 0,
              "start_time": "2025-03-15T05:01:12.883Z",
              "end_time": "2025-03-15T05:05:17.407Z",
              "total_elapsed_seconds": 245
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-15T05:01:05+00:00",
        "updated_at": "2025-03-29T22:43:40+00:00",
        "version": 6
      }
    },
    "paper:2309.14054": {
      "data": {
        "arxivId": "2309.14054",
        "url": "https://arxiv.org/abs/2309.14054",
        "title": "Adapt then Unlearn: Exploring Parameter Space Semantics for Unlearning\n  in Generative Adversarial Networks",
        "authors": "Piyush Tiwary, Atri Guha, Subhodip Panda, Prathosh A. P",
        "abstract": "Owing to the growing concerns about privacy and regulatory compliance, it is\ndesirable to regulate the output of generative models. To that end, the\nobjective of this work is to prevent the generation of outputs containing\nundesired features from a pre-trained Generative Adversarial Network (GAN)\nwhere the underlying training data set is inaccessible. Our approach is\ninspired by the observation that the parameter space of GANs exhibits\nmeaningful directions that can be leveraged to suppress specific undesired\nfeatures. However, such directions usually result in the degradation of the\nquality of generated samples. Our proposed two-stage method, known as\n'Adapt-then-Unlearn,' excels at unlearning such undesirable features while also\nmaintaining the quality of generated samples. In the initial stage, we adapt a\npre-trained GAN on a set of negative samples (containing undesired features)\nprovided by the user. Subsequently, we train the original pre-trained GAN using\npositive samples, along with a repulsion regularizer. This regularizer\nencourages the learned model parameters to move away from the parameters of the\nadapted model (first stage) while not degrading the generation quality. We\nprovide theoretical insights into the proposed method. To the best of our\nknowledge, our approach stands as the first method addressing unlearning within\nthe realm of high-fidelity GANs (such as StyleGAN). We validate the\neffectiveness of our method through comprehensive experiments, encompassing\nboth class-level unlearning on the MNIST and AFHQ dataset and feature-level\nunlearning tasks on the CelebA-HQ dataset. Our code and implementation is\navailable at: https://github.com/atriguha/Adapt_Unlearn.",
        "timestamp": "2025-03-15T05:00:52.805Z",
        "rating": "novote",
        "published_date": "2023-09-25T11:36:20Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI",
          "cs.CV"
        ]
      },
      "meta": {
        "created_at": "2025-03-15T05:00:53+00:00",
        "updated_at": "2025-03-29T22:43:40+00:00",
        "version": 2
      }
    },
    "interactions:2503.09917": {
      "data": {
        "paper_id": "2503.09917",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-15T05:00:46.388Z",
            "data": {
              "session_id": "session_1742014840044_88mfs1j",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-03-15T05:00:40.044Z",
              "end_time": "2025-03-15T05:00:44.735Z",
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-15T05:00:33+00:00",
        "updated_at": "2025-03-29T22:43:41+00:00",
        "version": 5
      }
    },
    "paper:2503.09917": {
      "data": {
        "arxivId": "2503.09917",
        "url": "https://arxiv.org/abs/2503.09917",
        "title": "Introducing MareNostrum5: A European pre-exascale energy-efficient\n  system designed to serve a broad spectrum of scientific workloads",
        "authors": "Fabio Banchelli, Marta Garcia-Gasulla, Filippo Mantovani, Joan Vinyals, Josep Pocurull, David Vicente, Beatriz Eguzkitza, Flavio C. C. Galeazzo, Mario C. Acosta, Sergi Girona",
        "abstract": "MareNostrum5 is a pre-exascale supercomputer at the Barcelona Supercomputing\nCenter (BSC), part of the EuroHPC Joint Undertaking. With a peak performance of\n314 petaflops, MareNostrum5 features a hybrid architecture comprising Intel\nSapphire Rapids CPUs, NVIDIA Hopper GPUs, and DDR5 and high-bandwidth memory\n(HBM), organized into four partitions optimized for diverse workloads. This\ndocument evaluates MareNostrum5 through micro-benchmarks (floating-point\nperformance, memory bandwidth, interconnect throughput), HPC benchmarks (HPL\nand HPCG), and application studies using Alya, OpenFOAM, and IFS. It highlights\nMareNostrum5's scalability, efficiency, and energy performance, utilizing the\nEAR (Energy Aware Runtime) framework to assess power consumption and the\neffects of direct liquid cooling. Additionally, HBM and DDR5 configurations are\ncompared to examine memory performance trade-offs. Designed to complement\nstandard technical documentation, this study provides insights to guide both\nnew and experienced users in optimizing their workloads and maximizing\nMareNostrum5's computational capabilities.",
        "timestamp": "2025-03-15T05:00:28.298Z",
        "rating": "novote",
        "published_date": "2025-03-13T00:18:43Z",
        "arxiv_tags": [
          "cs.DC",
          "cs.PF"
        ]
      },
      "meta": {
        "created_at": "2025-03-15T05:00:28+00:00",
        "updated_at": "2025-03-29T22:43:42+00:00",
        "version": 2
      }
    },
    "interactions:2411.18864": {
      "data": {
        "paper_id": "2411.18864",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-15T05:13:45.159Z",
            "data": {
              "session_id": "session_1742015556130_9fr5040",
              "duration_seconds": 68,
              "idle_seconds": 0,
              "start_time": "2025-03-15T05:12:36.130Z",
              "end_time": "2025-03-15T05:13:44.100Z",
              "total_elapsed_seconds": 68
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-15T05:14:58.729Z",
            "data": {
              "session_id": "session_1742015632740_yo3okft",
              "duration_seconds": 63,
              "idle_seconds": 0,
              "start_time": "2025-03-15T05:13:52.740Z",
              "end_time": "2025-03-15T05:14:55.249Z",
              "total_elapsed_seconds": 63
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-15T05:13:46+00:00",
        "updated_at": "2025-03-29T22:43:39+00:00",
        "version": 6
      }
    },
    "paper:2411.18864": {
      "data": {
        "arxivId": "2411.18864",
        "url": "https://arxiv.org/abs/2411.18864",
        "title": "Redesigning the ensemble Kalman filter with a dedicated model of\n  epistemic uncertainty",
        "authors": "Chatchuea Kimchaiwong, Jeremie Houssineau, Adam M. Johansen",
        "abstract": "The problem of incorporating information from observations received serially\nin time is widespread in the field of uncertainty quantification. Within a\nprobabilistic framework, such problems can be addressed using standard\nfiltering techniques. However, in many real-world problems, some (or all) of\nthe uncertainty is epistemic, arising from a lack of knowledge, and is\ndifficult to model probabilistically. This paper introduces a possibilistic\nensemble Kalman filter designed for this setting and characterizes some of its\nproperties. Using possibility theory to describe epistemic uncertainty is\nappealing from a philosophical perspective, and it is easy to justify certain\nheuristics often employed in standard ensemble Kalman filters as principled\napproaches to capturing uncertainty within it. The possibilistic approach\nmotivates a robust mechanism for characterizing uncertainty which shows good\nperformance with small sample sizes, and can outperform standard ensemble\nKalman filters at given sample size, even when dealing with genuinely aleatoric\nuncertainty.",
        "timestamp": "2025-03-15T05:12:36.197Z",
        "rating": "novote",
        "published_date": "2024-11-28T02:11:23Z",
        "arxiv_tags": [
          "stat.ME",
          "cs.AI",
          "62F15, 65C35"
        ]
      },
      "meta": {
        "created_at": "2025-03-15T05:12:36+00:00",
        "updated_at": "2025-03-29T22:43:39+00:00",
        "version": 2
      }
    },
    "interactions:2307.02846": {
      "data": {
        "paper_id": "2307.02846",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-03-15T05:21:52+00:00",
        "updated_at": "2025-03-29T22:43:38+00:00",
        "version": 2
      }
    },
    "paper:2307.02846": {
      "data": {
        "arxivId": "2307.02846",
        "url": "https://arxiv.org/abs/2307.02846",
        "title": "Probability Metrics for Tropical Spaces of Different Dimensions",
        "authors": "Roan Talbut, Daniele Tramontano, Yueqi Cao, Mathias Drton, Anthea Monod",
        "abstract": "The problem of comparing probability distributions is at the heart of many\ntasks in statistics and machine learning. Established comparison methods treat\nthe standard setting that the distributions are supported in the same space.\nRecently, a new geometric solution has been proposed to address the more\nchallenging problem of comparing measures in Euclidean spaces of differing\ndimensions. Here, we study the same problem of comparing probability\ndistributions of different dimensions in the tropical setting, which is\nbecoming increasingly relevant in applications involving complex data\nstructures such as phylogenetic trees. Specifically, we construct a Wasserstein\ndistance between measures on different tropical projective tori -- the focal\nmetric spaces in both theory and applications of tropical geometry -- via\ntropical mappings between probability measures. We prove equivalence of the\ndirectionality of the maps, whether mapping from a low dimensional space to a\nhigh dimensional space or vice versa. As an important practical implication,\nour work provides a framework for comparing probability distributions on the\nspaces of phylogenetic trees with different leaf sets. We demonstrate the\ncomputational feasibility of our approach using existing optimisation\ntechniques on both simulated and real data.",
        "timestamp": "2025-03-15T05:21:25.025Z",
        "rating": "novote",
        "published_date": "2023-07-06T08:22:55Z",
        "arxiv_tags": [
          "math.MG",
          "math.CO",
          "math.ST",
          "q-bio.PE",
          "stat.TH"
        ]
      },
      "meta": {
        "created_at": "2025-03-15T05:21:25+00:00",
        "updated_at": "2025-03-29T22:43:38+00:00",
        "version": 2
      }
    },
    "interactions:2502.20642": {
      "data": {
        "paper_id": "2502.20642",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-15T05:27:23.085Z",
            "data": {
              "session_id": "session_1742016435507_xcpo7zv",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-03-15T05:27:15.507Z",
              "end_time": "2025-03-15T05:27:23.054Z",
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-15T05:27:02+00:00",
        "updated_at": "2025-03-29T22:43:34+00:00",
        "version": 6
      }
    },
    "paper:2502.20642": {
      "data": {
        "arxivId": "2502.20642",
        "url": "https://arxiv.org/abs/2502.20642",
        "title": "Fixed point theorem in metric spaces and its application to the Collatz\n  conjecture",
        "authors": "Toshiharu Kawasaki",
        "abstract": "In this paper, we show the new fixed point theorem in metric spaces.\nFurthermore, for this fixed point theorem, we apply to the Collatz conjecture.",
        "timestamp": "2025-03-15T05:26:48.021Z",
        "rating": "novote",
        "published_date": "2025-02-28T01:48:00Z",
        "arxiv_tags": [
          "math.GM",
          "math.FA",
          "math.MG",
          "47H10"
        ]
      },
      "meta": {
        "created_at": "2025-03-15T05:26:48+00:00",
        "updated_at": "2025-03-29T22:43:35+00:00",
        "version": 2
      }
    },
    "interactions:1805.12400": {
      "data": {
        "paper_id": "1805.12400",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-15T05:25:50.649Z",
            "data": {
              "session_id": "session_1742016326724_2tf1ebo",
              "duration_seconds": 24,
              "idle_seconds": 0,
              "start_time": "2025-03-15T05:25:26.724Z",
              "end_time": "2025-03-15T05:25:50.629Z",
              "total_elapsed_seconds": 24
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-15T05:25:20+00:00",
        "updated_at": "2025-03-29T22:43:36+00:00",
        "version": 4
      }
    },
    "paper:1805.12400": {
      "data": {
        "arxivId": "1805.12400",
        "url": "https://arxiv.org/abs/1805.12400v2",
        "title": "Tropical Geometry of Phylogenetic Tree Space: A Statistical Perspective",
        "authors": "Anthea Monod, Bo Lin, Ruriko Yoshida, Qiwen Kang",
        "abstract": "Phylogenetic trees are the fundamental mathematical representation of\nevolutionary processes in biology. They are also objects of interest in pure\nmathematics, such as algebraic geometry and combinatorics, due to their\ndiscrete geometry. Although they are important data structures, they face the\nsignificant challenge that sets of trees form a non-Euclidean phylogenetic tree\nspace, which means that standard computational and statistical methods cannot\nbe directly applied. In this work, we explore the statistical feasibility of a\npure mathematical representation of the set of all phylogenetic trees based on\ntropical geometry for both descriptive and inferential statistics, and\nunsupervised and supervised machine learning. Our exploration is both\ntheoretical and practical. We show that the tropical geometric phylogenetic\ntree space endowed with a generalized Hilbert projective metric exhibits\nanalytic, geometric, and topological properties that are desirable for\ntheoretical studies in probability and statistics and allow for well-defined\nquestions to be posed. We illustrate the statistical feasibility of the\ntropical geometric perspective for phylogenetic trees with an example of both a\ndescriptive and inferential statistical task. Moreover, this approach exhibits\nincreased computational efficiency and statistical performance over the current\nstate-of-the-art, which we illustrate with a real data example on seasonal\ninfluenza. Our results demonstrate the viability of the tropical geometric\nsetting for parametric statistical and probabilistic studies of sets of\nphylogenetic trees.",
        "timestamp": "2025-03-15T05:24:42.505Z",
        "rating": "novote",
        "published_date": "2018-05-31T09:52:30Z",
        "arxiv_tags": [
          "math.MG",
          "math.CO",
          "math.ST",
          "q-bio.PE",
          "stat.TH"
        ]
      },
      "meta": {
        "created_at": "2025-03-15T05:24:42+00:00",
        "updated_at": "2025-03-29T22:43:36+00:00",
        "version": 2
      }
    },
    "interactions:2406.18464": {
      "data": {
        "paper_id": "2406.18464",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-15T05:32:07.394Z",
            "data": {
              "session_id": "session_1742016716653_90jagv7",
              "duration_seconds": 10,
              "idle_seconds": 0,
              "start_time": "2025-03-15T05:31:56.653Z",
              "end_time": "2025-03-15T05:32:06.571Z",
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-15T05:34:50.170Z",
            "data": {
              "session_id": "session_1742016735207_jrw94zv",
              "duration_seconds": 154,
              "idle_seconds": 0,
              "start_time": "2025-03-15T05:32:15.207Z",
              "end_time": "2025-03-15T05:34:49.557Z",
              "total_elapsed_seconds": 154
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-15T05:32:08+00:00",
        "updated_at": "2025-03-29T22:43:33+00:00",
        "version": 5
      }
    },
    "paper:2406.18464": {
      "data": {
        "arxivId": "2406.18464",
        "url": "https://arxiv.org/abs/2406.18464",
        "title": "Bayesian inverse Navier-Stokes problems: joint flow field reconstruction\n  and parameter learning",
        "authors": "Alexandros Kontogiannis, Scott V. Elgersma, Andrew J. Sederman, Matthew P. Juniper",
        "abstract": "We formulate and solve a Bayesian inverse Navier-Stokes (N-S) problem that\nassimilates velocimetry data in order to jointly reconstruct a 3D flow field\nand learn the unknown N-S parameters, including the boundary position. By\nhardwiring a generalised N-S problem, and regularising its unknown parameters\nusing Gaussian prior distributions, we learn the most likely parameters in a\ncollapsed search space. The most likely flow field reconstruction is then the\nN-S solution that corresponds to the learned parameters. We develop the method\nin the variational setting and use a stabilised Nitsche weak form of the N-S\nproblem that permits the control of all N-S parameters. To regularise the\ninferred the geometry, we use a viscous signed distance field (vSDF) as an\nauxiliary variable, which is given as the solution of a viscous Eikonal\nboundary value problem. We devise an algorithm that solves this inverse\nproblem, and numerically implement it using an adjoint-consistent stabilised\ncut-cell finite element method. We then use this method to reconstruct magnetic\nresonance velocimetry (flow-MRI) data of a 3D steady laminar flow through a\nphysical model of an aortic arch for two different Reynolds numbers and\nsignal-to-noise ratio (SNR) levels (low/high). We find that the method can\naccurately i) reconstruct the low SNR data by filtering out the noise/artefacts\nand recovering flow features that are obscured by noise, and ii) reproduce the\nhigh SNR data without overfitting. Although the framework that we develop\napplies to 3D steady laminar flows in complex geometries, it readily extends to\ntime-dependent laminar and Reynolds-averaged turbulent flows, as well as\nnon-Newtonian (e.g. viscoelastic) fluids.",
        "timestamp": "2025-03-15T05:31:56.786Z",
        "rating": "novote",
        "published_date": "2024-06-26T16:16:36Z",
        "arxiv_tags": [
          "physics.flu-dyn",
          "cs.LG",
          "math.OC"
        ]
      },
      "meta": {
        "created_at": "2025-03-15T05:31:57+00:00",
        "updated_at": "2025-03-29T22:43:34+00:00",
        "version": 2
      }
    },
    "paper:2110.14020": {
      "data": {
        "arxivId": "2110.14020",
        "url": "https://arxiv.org/abs/2110.14020",
        "title": "The Difficulty of Passive Learning in Deep Reinforcement Learning",
        "authors": "Georg Ostrovski, Pablo Samuel Castro, Will Dabney",
        "abstract": "Learning to act from observational data without active environmental\ninteraction is a well-known challenge in Reinforcement Learning (RL). Recent\napproaches involve constraints on the learned policy or conservative updates,\npreventing strong deviations from the state-action distribution of the dataset.\nAlthough these methods are evaluated using non-linear function approximation,\ntheoretical justifications are mostly limited to the tabular or linear cases.\nGiven the impressive results of deep reinforcement learning, we argue for a\nneed to more clearly understand the challenges in this setting.\n  In the vein of Held &amp; Hein's classic 1963 experiment, we propose the \"tandem\nlearning\" experimental paradigm which facilitates our empirical analysis of the\ndifficulties in offline reinforcement learning. We identify function\napproximation in conjunction with fixed data distributions as the strongest\nfactors, thereby extending but also challenging hypotheses stated in past work.\nOur results provide relevant insights for offline deep reinforcement learning,\nwhile also shedding new light on phenomena observed in the online case of\nlearning control.",
        "timestamp": "2025-03-15T05:36:53.011Z",
        "rating": "novote",
        "published_date": "2021-10-26T20:50:49Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI"
        ]
      },
      "meta": {
        "created_at": "2025-03-15T05:36:53+00:00",
        "updated_at": "2025-03-29T22:43:33+00:00",
        "version": 2
      }
    },
    "interactions:2110.14020": {
      "data": {
        "paper_id": "2110.14020",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-15T05:38:02.672Z",
            "data": {
              "session_id": "session_1742017050878_8n6qwfd",
              "duration_seconds": 32,
              "idle_seconds": 0,
              "start_time": "2025-03-15T05:37:30.878Z",
              "end_time": "2025-03-15T05:38:02.378Z",
              "total_elapsed_seconds": 32
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-15T05:39:10.671Z",
            "data": {
              "session_id": "session_1742017144976_76xeltc",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-03-15T05:39:04.976Z",
              "end_time": "2025-03-15T05:39:10.652Z",
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-15T05:37:24+00:00",
        "updated_at": "2025-03-29T22:43:32+00:00",
        "version": 7
      }
    },
    "interactions:arxiv:2306.01462": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2306.01462",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-15T06:08:30.711Z",
            "data": {
              "session_id": "session_1742018895441_0mcm9du",
              "duration_seconds": 15,
              "idle_seconds": 0,
              "start_time": "2025-03-15T06:08:15.441Z",
              "end_time": "2025-03-15T06:08:30.702Z",
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-15T06:07:59+00:00",
        "updated_at": "2025-03-29T22:43:31+00:00",
        "version": 4
      }
    },
    "paper:arxiv:2306.01462": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2306.01462",
        "url": "https://arxiv.org/abs/2306.01462",
        "title": "Random eigenvalues of graphenes and the triangulation of plane",
        "authors": "Artur Bille, Victor Buchstaber, Simon Coste, Satoshi Kuriki, Evgeny Spodarev",
        "abstract": "We analyse the numbers of closed paths of length $k\\in\\mathbb{N}$ on two important regular lattices: the hexagonal lattice (also called $\\textit{graphene}$ in chemistry) and its dual triangular lattice. These numbers form a moment sequence of specific random variables connected to the distance of a position of a planar random flight (in three steps) from the origin. Here, we refer to such a random variable as a $\\textit{random eigenvalue}$ of the underlying lattice. Explicit formulas for the probability density and characteristic functions of these random eigenvalues are given for both the hexagonal and the triangular lattice. Furthermore, it is proven that both probability distributions can be approximated by a functional of the random variable uniformly distributed on increasing intervals $[0,b]$ as $b\\to\\infty$. This yields a straightforward method to simulate these random eigenvalues without generating graphene and triangular lattice graphs. To demonstrate this approximation, we first prove a key integral identity for a specific series containing the third powers of the modified Bessel functions $I_n$ of $n$th order, $n\\in\\mathbb{Z}$. Such series play a crucial role in various contexts, in particular, in analysis, combinatorics, and theoretical physics.",
        "timestamp": "2025-03-15T06:07:40.146Z",
        "rating": "novote",
        "publishedDate": "[Submitted on 2 Jun 2023 (v1\ud83d\udcdd), last revised 17 Jan 2025 (this version, v2)]",
        "tags": [
          "Spectral Theory (math.SP)",
          "Probability (math.PR)"
        ]
      },
      "meta": {
        "created_at": "2025-03-15T06:07:40+00:00",
        "updated_at": "2025-03-29T22:43:32+00:00",
        "version": 2
      }
    },
    "paper:arxiv:2501.09891": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.09891",
        "url": "https://arxiv.org/abs/2501.09891",
        "title": "Evolving Deeper LLM Thinking",
        "authors": "Kuang-Huei Lee, Ian Fischer, Yueh-Hua Wu, Dave Marwood, Shumeet Baluja, Dale Schuurmans, Xinyun Chen",
        "abstract": "We explore an evolutionary search strategy for scaling inference time compute in Large Language Models. The proposed approach, Mind Evolution, uses a language model to generate, recombine and refine candidate responses. The proposed approach avoids the need to formalize the underlying inference problem whenever a solution evaluator is available. Controlling for inference cost, we find that Mind Evolution significantly outperforms other inference strategies such as Best-of-N and Sequential Revision in natural language planning tasks. In the TravelPlanner and Natural Plan benchmarks, Mind Evolution solves more than 98% of the problem instances using Gemini 1.5 Pro without the use of a formal solver.",
        "timestamp": "2025-03-15T06:22:41.913Z",
        "rating": "novote",
        "publishedDate": "[Submitted on 17 Jan 2025]",
        "tags": [
          "Artificial Intelligence (cs.AI)"
        ]
      },
      "meta": {
        "created_at": "2025-03-15T06:22:42+00:00",
        "updated_at": "2025-03-29T22:43:29+00:00",
        "version": 2
      }
    },
    "interactions:arxiv:2502.17827": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.17827",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-15T06:21:21.688Z",
            "data": {
              "session_id": "session_1742019662429_pvlepco",
              "duration_seconds": 19,
              "idle_seconds": 0,
              "start_time": "2025-03-15T06:21:02.429Z",
              "end_time": "2025-03-15T06:21:21.019Z",
              "total_elapsed_seconds": 19
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-15T06:21:22+00:00",
        "updated_at": "2025-03-29T22:43:30+00:00",
        "version": 3
      }
    },
    "paper:arxiv:2502.17827": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.17827",
        "url": "https://arxiv.org/abs/2502.17827",
        "title": "DPGLM: A Semiparametric Bayesian GLM with Inhomogeneous Normalized Random Measures",
        "authors": "Entejar Alam, Paul J. Rathouz, Peter M\u00fcller",
        "abstract": "We introduce a varying weight dependent Dirichlet process (DDP) model to implement a semi-parametric GLM. The model extends a recently developed semi-parametric generalized linear model (SPGLM) by adding a nonparametric Bayesian prior on the baseline distribution of the GLM. We show that the resulting model takes the form of an inhomogeneous completely random measure that arises from exponential tilting of a normalized completely random measure. Building on familiar posterior simulation methods for mixtures with respect to normalized random measures we introduce posterior simulation in the resulting semi-parametric GLM model. The proposed methodology is validated through a series of simulation studies and is illustrated using data from a speech intelligibility study.",
        "timestamp": "2025-03-15T06:20:58.414Z",
        "rating": "novote",
        "publishedDate": "[Submitted on 25 Feb 2025]",
        "tags": [
          "Methodology (stat.ME)"
        ]
      },
      "meta": {
        "created_at": "2025-03-15T06:20:58+00:00",
        "updated_at": "2025-03-29T22:43:30+00:00",
        "version": 2
      }
    },
    "interactions:arxiv:2501.09891": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.09891",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-15T06:23:33.223Z",
            "data": {
              "session_id": "session_1742019783009_odves00",
              "duration_seconds": 30,
              "idle_seconds": 0,
              "start_time": "2025-03-15T06:23:03.009Z",
              "end_time": "2025-03-15T06:23:33.201Z",
              "total_elapsed_seconds": 30
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-15T06:23:34+00:00",
        "updated_at": "2025-03-29T22:43:29+00:00",
        "version": 3
      }
    },
    "paper:arxiv:2503.07588": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.07588",
        "url": "https://arxiv.org/abs/2503.07588",
        "title": "When Large Vision-Language Model Meets Large Remote Sensing Imagery:\n  Coarse-to-Fine Text-Guided Token Pruning",
        "authors": "Junwei Luo, Yingying Zhang, Xue Yang, Kang Wu, Qi Zhu, Lei Liang, Jingdong Chen, Yansheng Li",
        "abstract": "Efficient vision-language understanding of large Remote Sensing Images (RSIs)\nis meaningful but challenging. Current Large Vision-Language Models (LVLMs)\ntypically employ limited pre-defined grids to process images, leading to\ninformation loss when handling gigapixel RSIs. Conversely, using unlimited\ngrids significantly increases computational costs. To preserve image details\nwhile reducing computational complexity, we propose a text-guided token pruning\nmethod with Dynamic Image Pyramid (DIP) integration. Our method introduces: (i)\na Region Focus Module (RFM) that leverages text-aware region localization\ncapability to identify critical vision tokens, and (ii) a coarse-to-fine image\ntile selection and vision token pruning strategy based on DIP, which is guided\nby RFM outputs and avoids directly processing the entire large imagery.\nAdditionally, existing benchmarks for evaluating LVLMs' perception ability on\nlarge RSI suffer from limited question diversity and constrained image sizes.\nWe construct a new benchmark named LRS-VQA, which contains 7,333 QA pairs\nacross 8 categories, with image length up to 27,328 pixels. Our method\noutperforms existing high-resolution strategies on four datasets using the same\ndata. Moreover, compared to existing token reduction methods, our approach\ndemonstrates higher efficiency under high-resolution settings. Dataset and code\nare in https://github.com/VisionXLab/LRS-VQA.",
        "timestamp": "2025-03-15T07:12:00.059Z",
        "rating": "novote",
        "publishedDate": "2025-03-10T17:51:16Z",
        "tags": [
          "cs.CV",
          "cs.AI"
        ]
      },
      "meta": {
        "created_at": "2025-03-15T07:12:00+00:00",
        "updated_at": "2025-03-29T22:43:28+00:00",
        "version": 2
      }
    },
    "interactions:arxiv:1805.12400": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1805.12400",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-15T07:10:27.428Z",
            "data": {
              "session_id": "session_1742022389044_k9sxrtr",
              "duration_seconds": 238,
              "idle_seconds": 0,
              "start_time": "2025-03-15T07:06:29.044Z",
              "end_time": "2025-03-15T07:10:26.609Z",
              "total_elapsed_seconds": 238
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-15T07:10:28+00:00",
        "updated_at": "2025-03-29T22:43:28+00:00",
        "version": 3
      }
    },
    "paper:arxiv:1805.12400": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1805.12400",
        "url": "https://arxiv.org/abs/1805.12400v2",
        "title": "Tropical Foundations for Probability & Statistics on Phylogenetic Tree Space",
        "authors": "Bo Lin, Anthea Monod, Ruriko Yoshida",
        "abstract": "We introduce a novel framework for the statistical analysis of phylogenetic trees: Palm tree space is constructed on principles of tropical algebraic geometry, and represents phylogenetic trees as a point in a space endowed with the tropical metric. We show that palm tree space possesses a variety of properties that allow for the definition of probability measures, and thus expectations, variances, and other fundamental statistical quantities. This provides a new, tropical basis for a statistical treatment of evolutionary biological processes represented by phylogenetic trees. In particular, we show that a geometric approach to phylogenetic tree space --- first introduced by Billera, Holmes, and Vogtmann, which we reinterpret in this paper via tropical geometry --- results in analytic, geometric, and topological characteristics that are desirable for probability, statistics, and increased computational efficiency.",
        "timestamp": "2025-03-15T07:06:25.372Z",
        "rating": "novote",
        "publishedDate": "[Submitted on 31 May 2018 (v1\ud83d\udcdd), revised 4 Jun 2018 (this version, v2), latest version 29 Jun 2022 (v8\ud83d\udcdd)]",
        "tags": [
          "Metric Geometry (math.MG)",
          "Combinatorics (math.CO)",
          "Statistics Theory (math.ST)",
          "Populations and Evolution (q-bio.PE)"
        ]
      },
      "meta": {
        "created_at": "2025-03-15T07:06:25+00:00",
        "updated_at": "2025-03-29T22:43:28+00:00",
        "version": 2
      }
    },
    "interactions:arxiv:2503.07588": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.07588",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-15T07:12:22.346Z",
            "data": {
              "session_id": "session_1742022723293_ybg08pb",
              "duration_seconds": 18,
              "idle_seconds": 0,
              "start_time": "2025-03-15T07:12:03.293Z",
              "end_time": "2025-03-15T07:12:21.661Z",
              "total_elapsed_seconds": 18
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-15T07:12:23+00:00",
        "updated_at": "2025-03-29T22:43:27+00:00",
        "version": 3
      }
    },
    "interactions:arxiv:2503.06514": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.06514",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-15T14:49:55.013Z",
            "data": {
              "session_id": "session_1742050085651_zx2um15",
              "paper_id": "2503.06514",
              "source_id": "arxiv",
              "start_time": "2025-03-15T14:48:05.651Z",
              "end_time": "2025-03-15T14:49:53.878Z",
              "heartbeat_count": 21,
              "duration_seconds": 105,
              "idle_seconds": 3,
              "total_elapsed_seconds": 108
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-15T14:53:41.753Z",
            "data": {
              "session_id": "session_1742050372592_3t47dv3",
              "paper_id": "2503.06514",
              "source_id": "arxiv",
              "start_time": "2025-03-15T14:52:52.593Z",
              "end_time": "2025-03-15T14:53:41.711Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 4,
              "total_elapsed_seconds": 49
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-15T14:49:56+00:00",
        "updated_at": "2025-03-29T22:43:26+00:00",
        "version": 4
      }
    },
    "paper:arxiv:2503.06514": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.06514",
        "url": "https://arxiv.org/abs/2503.06514",
        "title": "GFlowVLM: Enhancing Multi-step Reasoning in Vision-Language Models with\n  Generative Flow Networks",
        "authors": "Haoqiang Kang, Enna Sachdeva, Piyush Gupta, Sangjae Bae, Kwonjoon Lee",
        "abstract": "Vision-Language Models (VLMs) have recently shown promising advancements in\nsequential decision-making tasks through task-specific fine-tuning. However,\ncommon fine-tuning methods, such as Supervised Fine-Tuning (SFT) and\nReinforcement Learning (RL) techniques like Proximal Policy Optimization (PPO),\npresent notable limitations: SFT assumes Independent and Identically\nDistributed (IID) data, while PPO focuses on maximizing cumulative rewards.\nThese limitations often restrict solution diversity and hinder generalization\nin multi-step reasoning tasks. To address these challenges, we introduce a\nnovel framework, GFlowVLM, a framework that fine-tune VLMs using Generative\nFlow Networks (GFlowNets) to promote generation of diverse solutions for\ncomplex reasoning tasks. GFlowVLM models the environment as a non-Markovian\ndecision process, allowing it to capture long-term dependencies essential for\nreal-world applications. It takes observations and task descriptions as inputs\nto prompt chain-of-thought (CoT) reasoning which subsequently guides action\nselection. We use task based rewards to fine-tune VLM with GFlowNets. This\napproach enables VLMs to outperform prior fine-tuning methods, including SFT\nand RL. Empirical results demonstrate the effectiveness of GFlowVLM on complex\ntasks such as card games (NumberLine, BlackJack) and embodied planning tasks\n(ALFWorld), showing enhanced training efficiency, solution diversity, and\nstronger generalization capabilities across both in-distribution and\nout-of-distribution scenarios.",
        "timestamp": "2025-03-15T14:48:06.243Z",
        "rating": "novote",
        "publishedDate": "2025-03-09T08:38:10Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.CV",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-03-15T14:48:06+00:00",
        "updated_at": "2025-03-29T22:43:27+00:00",
        "version": 2
      }
    },
    "paper:arxiv:2503.09617": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.09617",
        "url": "https://arxiv.org/abs/2503.09617",
        "title": "Factorio Learning Environment",
        "authors": "Jack Hopkins, Mart Bakler, Akbir Khan",
        "abstract": "Large Language Models (LLMs) are rapidly saturating existing benchmarks, necessitating new open-ended evaluations. We introduce the Factorio Learning Environment (FLE), based on the game of Factorio, that tests agents in long-term planning, program synthesis, and resource optimization. FLE provides exponentially scaling challenges -- from basic automation to complex factories processing millions of resource units per second. We provide two settings: (1) lab-play consisting of eight structured tasks with fixed resources, and (2) open-play with the unbounded task of building the largest factory on an procedurally generated map. We demonstrate across both settings that models still lack strong spatial reasoning. In lab-play, we find that LLMs exhibit promising short-horizon skills, yet are unable to operate effectively in constrained environments, reflecting limitations in error analysis. In open-play, while LLMs discover automation strategies that improve growth (e.g electric-powered drilling), they fail to achieve complex automation (e.g electronic-circuit manufacturing).",
        "timestamp": "2025-03-15T16:38:48.645Z",
        "rating": "novote",
        "publishedDate": "[Submitted on 6 Mar 2025]",
        "tags": [
          "Multiagent Systems (cs.MA)",
          "Computation and Language (cs.CL)",
          "Machine Learning (cs.LG)"
        ]
      },
      "meta": {
        "created_at": "2025-03-15T16:38:49+00:00",
        "updated_at": "2025-03-29T22:43:25+00:00",
        "version": 2
      }
    },
    "interactions:arxiv:2011.03069": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2011.03069",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-15T16:38:12.027Z",
            "data": {
              "session_id": "session_1742056670776_tw20jfl",
              "paper_id": "2011.03069",
              "source_id": "arxiv",
              "start_time": "2025-03-15T16:37:50.776Z",
              "end_time": "2025-03-15T16:38:11.183Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 0,
              "total_elapsed_seconds": 20
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-15T16:38:13+00:00",
        "updated_at": "2025-03-29T22:43:25+00:00",
        "version": 3
      }
    },
    "paper:arxiv:2011.03069": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2011.03069",
        "url": "https://arxiv.org/abs/2011.03069",
        "title": "Cosmological Trans-Planckian Conjectures are not Effective",
        "authors": "C.P. Burgess, S. P. de Alwis, F. Quevedo",
        "abstract": "It is remarkable that the primordial fluctuations as revealed by the CMB coincide with what quantum fluctuations would look like if they were stretched across the sky by accelerated cosmic expansion. It has been observed that this same stretching also brings very small -- even trans-Planckian -- length scales up to observable sizes if extrapolated far enough into the past. This potentially jeopardizes later descriptions of late-time cosmology by introducing uncontrolled trans-Planckian theoretical errors into all calculations. Recent speculations, such as the Trans-Planckian Censorship Conjecture (TCC), have been developed to avoid this problem. We revisit old arguments why the consistency of (and control over) the Effective Field Theory (EFT) governing late-time cosmology is not necessarily threatened by the descent of modes due to universal expansion, even if EFT methods may break down at much earlier times. Failure of EFT methods only poses a problem if late-time predictions rely on non-adiabatic behaviour at these early times (such as is often true for bouncing cosmologies, for example). We illustrate our arguments using simple non-gravitational examples such as slowly rolling scalar fields and the spacing between Landau levels for charged particles in slowly varying magnetic fields, for which similar issues arise and are easier to understand. We comment on issues associated with UV completions. Our arguments need not invalidate speculative ideas like the TCC but suggest they are not required by the present evidence.",
        "timestamp": "2025-03-15T16:37:51.300Z",
        "rating": "novote",
        "publishedDate": "[Submitted on 5 Nov 2020]",
        "tags": [
          "High Energy Physics - Theory (hep-th)",
          "General Relativity and Quantum Cosmology (gr-qc)",
          "High Energy Physics - Phenomenology (hep-ph)"
        ]
      },
      "meta": {
        "created_at": "2025-03-15T16:37:51+00:00",
        "updated_at": "2025-03-29T22:43:26+00:00",
        "version": 2
      }
    },
    "interactions:arxiv:2503.09617": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.09617",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-15T16:39:02.317Z",
            "data": {
              "session_id": "session_1742056728090_xwl50kh",
              "paper_id": "2503.09617",
              "source_id": "arxiv",
              "start_time": "2025-03-15T16:38:48.090Z",
              "end_time": "2025-03-15T16:39:01.535Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 3,
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-15T16:40:57.602Z",
            "data": {
              "session_id": "session_1742056853402_5k084cn",
              "paper_id": "2503.09617",
              "source_id": "arxiv",
              "start_time": "2025-03-15T16:40:53.402Z",
              "end_time": "2025-03-15T16:40:57.561Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 4,
              "total_elapsed_seconds": 4
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-15T16:39:03+00:00",
        "updated_at": "2025-03-29T22:43:24+00:00",
        "version": 5
      }
    },
    "interactions:arxiv:2405.21042": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2405.21042",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-15T17:46:47.684Z",
            "data": {
              "session_id": "session_1742060800995_xrwkulg",
              "paper_id": "2405.21042",
              "source_id": "arxiv",
              "start_time": "2025-03-15T17:46:40.995Z",
              "end_time": "2025-03-15T17:46:47.669Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-15T17:47:50.831Z",
            "data": {
              "session_id": "session_1742060817330_ysq5uym",
              "paper_id": "2405.21042",
              "source_id": "arxiv",
              "start_time": "2025-03-15T17:46:57.330Z",
              "end_time": "2025-03-15T17:47:50.247Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 3,
              "total_elapsed_seconds": 53
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-15T17:46:34+00:00",
        "updated_at": "2025-03-29T22:43:23+00:00",
        "version": 5
      }
    },
    "paper:arxiv:2405.21042": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2405.21042",
        "url": "https://arxiv.org/abs/2405.21042",
        "title": "Comparing the information content of probabilistic representation spaces",
        "authors": "Kieran A. Murphy, Sam Dillavou, Dani S. Bassett",
        "abstract": "Probabilistic representation spaces convey information about a dataset and are shaped by factors such as the training data, network architecture, and loss function. Comparing the information content of such spaces is crucial for understanding the learning process, yet most existing methods assume point-based representations, neglecting the distributional nature of probabilistic spaces. To address this gap, we propose two information-theoretic measures to compare general probabilistic representation spaces by extending classic methods to compare the information content of hard clustering assignments. Additionally, we introduce a lightweight method of estimation that is based on fingerprinting a representation space with a sample of the dataset, designed for scenarios where the communicated information is limited to a few bits. We demonstrate the utility of these measures in three case studies. First, in the context of unsupervised disentanglement, we identify recurring information fragments within individual latent dimensions of VAE and InfoGAN ensembles. Second, we compare the full latent spaces of models and reveal consistent information content across datasets and methods, despite variability during training. Finally, we leverage the differentiability of our measures to perform model fusion, synthesizing the information content of weak learners into a single, coherent representation. Across these applications, the direct comparison of information content offers a natural basis for characterizing the processing of information.",
        "timestamp": "2025-03-15T17:46:05.502Z",
        "rating": "novote",
        "publishedDate": "[Submitted on 31 May 2024 (v1\ud83d\udcdd), last revised 19 Feb 2025 (this version, v3)]",
        "tags": [
          "Machine Learning (cs.LG)"
        ]
      },
      "meta": {
        "created_at": "2025-03-15T17:46:06+00:00",
        "updated_at": "2025-03-29T22:43:24+00:00",
        "version": 2
      }
    },
    "interactions:arxiv:2503.09637": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.09637",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-15T20:36:02.512Z",
            "data": {
              "session_id": "session_1742070925007_mgc8r3n",
              "paper_id": "2503.09637",
              "source_id": "arxiv",
              "start_time": "2025-03-15T20:35:25.007Z",
              "end_time": "2025-03-15T20:36:02.465Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 2,
              "total_elapsed_seconds": 37
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-15T20:35:10+00:00",
        "updated_at": "2025-03-29T22:43:22+00:00",
        "version": 5
      }
    },
    "paper:arxiv:2503.09637": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.09637",
        "url": "https://arxiv.org/abs/2503.09637",
        "title": "Complementarity, Augmentation, or Substitutivity? The Impact of Generative Artificial Intelligence on the U.S. Federal Workforce",
        "authors": "William G. Resh, Yi Ming, Xinyao Xia, Michael Overton, Gul Nisa G\u00fcrb\u00fcz, Brandon De Breuhl",
        "abstract": "This study investigates the near-future impacts of generative artificial intelligence (AI) technologies on occupational competencies across the U.S. federal workforce. We develop a multi-stage Retrieval-Augmented Generation system to leverage large language models for predictive AI modeling that projects shifts in required competencies and to identify vulnerable occupations on a knowledge-by-skill-by-ability basis across the federal government workforce. This study highlights policy recommendations essential for workforce planning in the era of AI. We integrate several sources of detailed data on occupational requirements across the federal government from both centralized and decentralized human resource sources, including from the U.S. Office of Personnel Management (OPM) and various federal agencies. While our preliminary findings suggest some significant shifts in required competencies and potential vulnerability of certain roles to AI-driven changes, we provide nuanced insights that support arguments against abrupt or generic approaches to strategic human capital planning around the development of generative AI. The study aims to inform strategic workforce planning and policy development within federal agencies and demonstrates how this approach can be replicated across other large employment institutions and labor markets.",
        "timestamp": "2025-03-15T20:34:19.823Z",
        "rating": "novote",
        "publishedDate": "[Submitted on 12 Mar 2025]",
        "tags": [
          "Computers and Society (cs.CY)",
          "General Economics (econ.GN)"
        ]
      },
      "meta": {
        "created_at": "2025-03-15T20:34:20+00:00",
        "updated_at": "2025-03-29T22:43:23+00:00",
        "version": 2
      }
    },
    "interactions:arxiv:2503.07465": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.07465",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-15T20:55:13.431Z",
            "data": {
              "session_id": "session_1742072087792_8pz8t0n",
              "paper_id": "2503.07465",
              "source_id": "arxiv",
              "start_time": "2025-03-15T20:54:47.792Z",
              "end_time": "2025-03-15T20:55:13.384Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 1,
              "total_elapsed_seconds": 26
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-15T20:54:48+00:00",
        "updated_at": "2025-03-29T22:43:22+00:00",
        "version": 4
      }
    },
    "paper:arxiv:2503.07465": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.07465",
        "url": "https://arxiv.org/abs/2503.07465",
        "title": "YOLOE: Real-Time Seeing Anything",
        "authors": "Ao Wang, Lihao Liu, Hui Chen, Zijia Lin, Jungong Han, Guiguang Ding",
        "abstract": "Object detection and segmentation are widely employed in computer vision applications, yet conventional models like YOLO series, while efficient and accurate, are limited by predefined categories, hindering adaptability in open scenarios. Recent open-set methods leverage text prompts, visual cues, or prompt-free paradigm to overcome this, but often compromise between performance and efficiency due to high computational demands or deployment complexity. In this work, we introduce YOLOE, which integrates detection and segmentation across diverse open prompt mechanisms within a single highly efficient model, achieving real-time seeing anything. For text prompts, we propose Re-parameterizable Region-Text Alignment (RepRTA) strategy. It refines pretrained textual embeddings via a re-parameterizable lightweight auxiliary network and enhances visual-textual alignment with zero inference and transferring overhead. For visual prompts, we present Semantic-Activated Visual Prompt Encoder (SAVPE). It employs decoupled semantic and activation branches to bring improved visual embedding and accuracy with minimal complexity. For prompt-free scenario, we introduce Lazy Region-Prompt Contrast (LRPC) strategy. It utilizes a built-in large vocabulary and specialized embedding to identify all objects, avoiding costly language model dependency. Extensive experiments show YOLOE's exceptional zero-shot performance and transferability with high inference efficiency and low training cost. Notably, on LVIS, with 3$\\times$ less training cost and 1.4$\\times$ inference speedup, YOLOE-v8-S surpasses YOLO-Worldv2-S by 3.5 AP. When transferring to COCO, YOLOE-v8-L achieves 0.6 AP$^b$ and 0.4 AP$^m$ gains over closed-set YOLOv8-L with nearly 4$\\times$ less training time. Code and models are available at this https URL.",
        "timestamp": "2025-03-15T20:54:39.426Z",
        "rating": "novote",
        "publishedDate": "[Submitted on 10 Mar 2025]",
        "tags": [
          "Computer Vision and Pattern Recognition (cs.CV)"
        ]
      },
      "meta": {
        "created_at": "2025-03-15T20:54:39+00:00",
        "updated_at": "2025-03-29T22:43:22+00:00",
        "version": 2
      }
    },
    "interactions:arxiv:2501.12948": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.12948",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-15T22:00:09.960Z",
            "data": {
              "session_id": "session_1742076005288_gp0ibm8",
              "paper_id": "2501.12948",
              "source_id": "arxiv",
              "start_time": "2025-03-15T22:00:05.288Z",
              "end_time": "2025-03-15T22:00:09.959Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 5,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-15T21:59:50+00:00",
        "updated_at": "2025-03-29T22:43:21+00:00",
        "version": 5
      }
    },
    "paper:arxiv:2501.12948": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.12948",
        "url": "https://arxiv.org/abs/2501.12948",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "authors": "DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z.F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J.L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R.J. Chen, R.L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S.S. Li\n\n<!--\nfunction toggleAuthorList(whichLayer,toggleThis)\n{\n  var elem, vis, tempToggle;\n  tempToggle=toggleThis;\n  if( document.getElementById ) // standard\n      elem = document.getElementById( whichLayer );\n  else if( document.all ) // old msie versions\n      elem = document.all[whichLayer];\n  else if( document.layers ) // nn4\n      elem = document.layers[whichLayer];\n  vis = elem.style;\n  // if the style.display value is blank we try to figure it out here\n  if(vis.display==''&&elem.offsetWidth!=undefined&&elem.offsetHeight!=undefined)\n    vis.display = (elem.offsetWidth!=0&&elem.offsetHeight!=0)?'inline':'none';\n  vis.display = (vis.display==''||vis.display=='inline')?'none':'inline';\n\n  // toggle link inner text\n  status = vis.display;\n  if(status=='none'){\n      document.getElementById('toggle').innerHTML = tempToggle ;\n      document.getElementById('toggle').title = \"Show Entire Author List\";\n  }\n  else if(status=='inline'){\n      document.getElementById('toggle').innerHTML = \"(collapse list)\";\n      document.getElementById('toggle').title = \"Collapse Author List\";\n  }\n}\n//-->\n\n\n  \n        , Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W.L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X.Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y.K. Li, Y.Q. Wang, Y.X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y.X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z.Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, Zhen Zhang\n  \n  \n    et al. (100 additional authors not shown)\n  &nbsp;You must enable JavaScript to view entire author list.",
        "abstract": "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.",
        "timestamp": "2025-03-15T21:59:13.051Z",
        "rating": "novote",
        "publishedDate": "[Submitted on 22 Jan 2025]",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ]
      },
      "meta": {
        "created_at": "2025-03-15T21:59:13+00:00",
        "updated_at": "2025-03-29T22:43:21+00:00",
        "version": 2
      }
    },
    "interactions:arxiv:1503.03585": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1503.03585",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T00:12:47.421Z",
            "data": {
              "session_id": "session_1742083942414_lev44gu",
              "paper_id": "1503.03585",
              "source_id": "arxiv",
              "start_time": "2025-03-16T00:12:22.414Z",
              "end_time": "2025-03-16T00:12:47.421Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 15,
              "total_elapsed_seconds": 25
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-16T00:12:11+00:00",
        "updated_at": "2025-03-29T22:43:20+00:00",
        "version": 7
      }
    },
    "paper:arxiv:1503.03585": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1503.03585",
        "url": "https://arxiv.org/abs/1503.03585",
        "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
        "authors": "Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli",
        "abstract": "A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.",
        "timestamp": "2025-03-16T00:12:10.411Z",
        "rating": "novote",
        "publishedDate": "[Submitted on 12 Mar 2015 (v1\ud83d\udcdd), last revised 18 Nov 2015 (this version, v8)]",
        "tags": [
          "Machine Learning (cs.LG)",
          "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
          "Neurons and Cognition (q-bio.NC)",
          "Machine Learning (stat.ML)"
        ]
      },
      "meta": {
        "created_at": "2025-03-16T00:12:10+00:00",
        "updated_at": "2025-03-29T22:43:20+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2403.15711": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2403.15711",
        "url": "https://arxiv.org/abs/2403.15711",
        "title": "Identifiable Latent Neural Causal Models",
        "authors": "Yuhang Liu, Zhen Zhang, Dong Gong, Mingming Gong, Biwei Huang, Anton van den Hengel, Kun Zhang, Javen Qinfeng Shi",
        "abstract": "Causal representation learning seeks to uncover latent, high-level causal representations from low-level observed data. It is particularly good at predictions under unseen distribution shifts, because these shifts can generally be interpreted as consequences of interventions. Hence leveraging {seen} distribution shifts becomes a natural strategy to help identifying causal representations, which in turn benefits predictions where distributions are previously {unseen}. Determining the types (or conditions) of such distribution shifts that do contribute to the identifiability of causal representations is critical. This work establishes a {sufficient} and {necessary} condition characterizing the types of distribution shifts for identifiability in the context of latent additive noise models. Furthermore, we present partial identifiability results when only a portion of distribution shifts meets the condition. In addition, we extend our findings to latent post-nonlinear causal models. We translate our findings into a practical algorithm, allowing for the acquisition of reliable latent causal representations. Our algorithm, guided by our underlying theory, has demonstrated outstanding performance across a diverse range of synthetic and real-world datasets. The empirical observations align closely with the theoretical findings, affirming the robustness and effectiveness of our approach.",
        "timestamp": "2025-03-16T05:20:04.525Z",
        "rating": "novote",
        "publishedDate": "[Submitted on 23 Mar 2024]",
        "tags": [
          "Machine Learning (cs.LG)",
          "Methodology (stat.ME)",
          "Machine Learning (stat.ML)"
        ]
      },
      "meta": {
        "created_at": "2025-03-16T05:20:05+00:00",
        "updated_at": "2025-03-29T22:43:18+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2501.10846": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.10846",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T05:19:18.522Z",
            "data": {
              "session_id": "session_1742102314584_hsgtmth",
              "paper_id": "2501.10846",
              "source_id": "arxiv",
              "start_time": "2025-03-16T05:18:34.584Z",
              "end_time": "2025-03-16T05:19:18.521Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 4,
              "total_elapsed_seconds": 44
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-16T05:18:22+00:00",
        "updated_at": "2025-03-29T22:43:19+00:00",
        "version": 5
      }
    },
    "paper:arxiv.2501.10846": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.10846",
        "url": "https://arxiv.org/abs/2501.10846",
        "title": "The Missing Link: Identifying Digital Intermediaries in E-Government",
        "authors": "Sergio Toro-Maureira, Alejandro Olivares, Rocio Saez-Vergara, Sebastian Valenzuela, Macarena Valenzuela, Teresa Correa",
        "abstract": "The digitalization of public administration has advanced significantly on a global scale. Many governments now view digital platforms as essential for improving the delivery of public services and fostering direct communication between citizens and public institutions. However, this view overlooks the role played by digital intermediaries significantly shape the provision of e-government services. Using Chile as a case study, we analyze these intermediaries through a national survey on digitalization, we find five types of intermediaries: family members, peers, political figures, bureaucrats, and community leaders. The first two classes comprise close intermediaries, while the latter three comprise hierarchical intermediaries. Our findings suggest that all these intermediaries are a critical but underexplored element in the digitalization of public administration.",
        "timestamp": "2025-03-16T05:18:14.960Z",
        "rating": "novote",
        "publishedDate": "[Submitted on 18 Jan 2025]",
        "tags": [
          "General Economics (econ.GN)"
        ]
      },
      "meta": {
        "created_at": "2025-03-16T05:18:15+00:00",
        "updated_at": "2025-03-29T22:43:19+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2403.15711": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2403.15711",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T05:21:07.753Z",
            "data": {
              "session_id": "session_1742102437744_1f2ywa4",
              "paper_id": "2403.15711",
              "source_id": "arxiv",
              "start_time": "2025-03-16T05:20:37.744Z",
              "end_time": "2025-03-16T05:21:07.752Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 15,
              "total_elapsed_seconds": 30
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-16T05:20:38+00:00",
        "updated_at": "2025-03-29T22:43:18+00:00",
        "version": 4
      }
    },
    "interactions:arxiv.2503.06462": {
      "data": {
        "paper_id": "arxiv.2503.06462",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-11T09:11:16.531Z",
            "data": {
              "session_id": "session_1741684265601_rvy7ig2",
              "duration_seconds": 11,
              "idle_seconds": 0,
              "start_time": "2025-03-11T09:11:05.601Z",
              "end_time": "2025-03-11T09:11:16.518Z",
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-11T09:11:09+00:00",
        "updated_at": "2025-03-29T22:44:08+00:00",
        "version": 4
      }
    },
    "interactions:arxiv.2503.07134": {
      "data": {
        "paper_id": "arxiv.2503.07134",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-11T05:13:56.899Z",
            "data": {
              "session_id": "session_1741670009187_qeqm4fm",
              "duration_seconds": 27,
              "idle_seconds": 0,
              "start_time": "2025-03-11T05:13:29.187Z",
              "end_time": "2025-03-11T05:13:56.203Z",
              "total_elapsed_seconds": 27
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-11T05:13:57+00:00",
        "updated_at": "2025-03-29T22:44:12+00:00",
        "version": 3
      }
    },
    "interactions:arxiv.2503.05522": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.05522",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T07:51:56.119Z",
            "data": {
              "session_id": "session_1742111405615_fioct7y",
              "paper_id": "2503.05522",
              "source_id": "arxiv",
              "start_time": "2025-03-16T07:50:05.616Z",
              "end_time": "2025-03-16T07:51:56.118Z",
              "heartbeat_count": 22,
              "duration_seconds": 110,
              "idle_seconds": 1,
              "total_elapsed_seconds": 111
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-16T07:51:57+00:00",
        "updated_at": "2025-03-29T22:43:17+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.05522": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.05522",
        "url": "https://arxiv.org/abs/2503.05522",
        "title": "Post-Hoc Concept Disentanglement: From Correlated to Isolated Concept\n  Representations",
        "authors": "Eren Erogullari, Sebastian Lapuschkin, Wojciech Samek, Frederik Pahde",
        "abstract": "Concept Activation Vectors (CAVs) are widely used to model\nhuman-understandable concepts as directions within the latent space of neural\nnetworks. They are trained by identifying directions from the activations of\nconcept samples to those of non-concept samples. However, this method often\nproduces similar, non-orthogonal directions for correlated concepts, such as\n\"beard\" and \"necktie\" within the CelebA dataset, which frequently co-occur in\nimages of men. This entanglement complicates the interpretation of concepts in\nisolation and can lead to undesired effects in CAV applications, such as\nactivation steering. To address this issue, we introduce a post-hoc concept\ndisentanglement method that employs a non-orthogonality loss, facilitating the\nidentification of orthogonal concept directions while preserving directional\ncorrectness. We evaluate our approach with real-world and controlled correlated\nconcepts in CelebA and a synthetic FunnyBirds dataset with VGG16 and ResNet18\narchitectures. We further demonstrate the superiority of orthogonalized concept\nrepresentations in activation steering tasks, allowing (1) the insertion of\nisolated concepts into input images through generative models and (2) the\nremoval of concepts for effective shortcut suppression with reduced impact on\ncorrelated concepts in comparison to baseline CAVs.",
        "timestamp": "2025-03-16T07:50:05.458Z",
        "rating": "novote",
        "publishedDate": "2025-03-07T15:45:43Z",
        "tags": [
          "cs.CV",
          "cs.AI",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-03-16T07:50:06+00:00",
        "updated_at": "2025-03-29T22:43:17+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2402.17119": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.17119",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T16:47:45.102Z",
            "data": {
              "session_id": "session_1742143644624_pnou1ww",
              "paper_id": "2402.17119",
              "source_id": "arxiv",
              "start_time": "2025-03-16T16:47:24.624Z",
              "end_time": "2025-03-16T16:47:45.102Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 0,
              "total_elapsed_seconds": 20
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-16T16:46:45+00:00",
        "updated_at": "2025-03-29T22:43:16+00:00",
        "version": 5
      }
    },
    "paper:arxiv.2402.17119": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.17119",
        "url": "https://arxiv.org/abs/2402.17119",
        "title": "Creating Suspenseful Stories: Iterative Planning with Large Language Models",
        "authors": "Kaige Xie, Mark Riedl",
        "abstract": "Automated story generation has been one of the long-standing challenges in NLP. Among all dimensions of stories, suspense is very common in human-written stories but relatively under-explored in AI-generated stories. While recent advances in large language models (LLMs) have greatly promoted language generation in general, state-of-the-art LLMs are still unreliable when it comes to suspenseful story generation. We propose a novel iterative-prompting-based planning method that is grounded in two theoretical foundations of story suspense from cognitive psychology and narratology. This theory-grounded method works in a fully zero-shot manner and does not rely on any supervised story corpora. To the best of our knowledge, this paper is the first attempt at suspenseful story generation with LLMs. Extensive human evaluations of the generated suspenseful stories demonstrate the effectiveness of our method.",
        "timestamp": "2025-03-16T16:46:33.709Z",
        "rating": "novote",
        "publishedDate": "[Submitted on 27 Feb 2024]",
        "tags": [
          "Computation and Language (cs.CL)"
        ]
      },
      "meta": {
        "created_at": "2025-03-16T16:46:34+00:00",
        "updated_at": "2025-03-29T22:43:16+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.10633": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.10633",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T16:55:41.704Z",
            "data": {
              "session_id": "session_1742144137938_r1ui4sn",
              "paper_id": "2503.10633",
              "source_id": "arxiv",
              "start_time": "2025-03-16T16:55:37.938Z",
              "end_time": "2025-03-16T16:55:41.704Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 4,
              "total_elapsed_seconds": 4
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-16T16:55:02+00:00",
        "updated_at": "2025-03-29T22:43:15+00:00",
        "version": 5
      }
    },
    "paper:arxiv.2503.10633": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.10633",
        "url": "https://arxiv.org/abs/2503.10633",
        "title": "Charting and Navigating Hugging Face's Model Atlas",
        "authors": "Eliahu Horwitz, Nitzan Kurer, Jonathan Kahana, Liel Amar, Yedid Hoshen",
        "abstract": "As there are now millions of publicly available neural networks, searching and analyzing large model repositories becomes increasingly important. Navigating so many models requires an atlas, but as most models are poorly documented charting such an atlas is challenging. To explore the hidden potential of model repositories, we chart a preliminary atlas representing the documented fraction of Hugging Face. It provides stunning visualizations of the model landscape and evolution. We demonstrate several applications of this atlas including predicting model attributes (e.g., accuracy), and analyzing trends in computer vision models. However, as the current atlas remains incomplete, we propose a method for charting undocumented regions. Specifically, we identify high-confidence structural priors based on dominant real-world model training practices. Leveraging these priors, our approach enables accurate mapping of previously undocumented areas of the atlas. We publicly release our datasets, code, and interactive atlas.",
        "timestamp": "2025-03-16T16:55:00.673Z",
        "rating": "novote",
        "publishedDate": "[Submitted on 13 Mar 2025]",
        "tags": [
          "Machine Learning (cs.LG)",
          "Computation and Language (cs.CL)",
          "Computer Vision and Pattern Recognition (cs.CV)"
        ]
      },
      "meta": {
        "created_at": "2025-03-16T16:55:01+00:00",
        "updated_at": "2025-03-29T22:43:15+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.1906.01205": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1906.01205",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T17:38:46.628Z",
            "data": {
              "session_id": "session_1742146623911_ch9ej77",
              "paper_id": "1906.01205",
              "source_id": "arxiv",
              "start_time": "2025-03-16T17:37:03.911Z",
              "end_time": "2025-03-16T17:38:46.627Z",
              "heartbeat_count": 20,
              "duration_seconds": 100,
              "idle_seconds": 3,
              "total_elapsed_seconds": 103
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-16T17:38:47+00:00",
        "updated_at": "2025-03-29T22:43:14+00:00",
        "version": 3
      }
    },
    "paper:arxiv.1906.01205": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1906.01205",
        "url": "https://arxiv.org/abs/1906.01205",
        "title": "A Strong and Robust Baseline for Text-Image Matching",
        "authors": "Fangyu Liu, Rongtian Ye",
        "abstract": "We review the current schemes of text-image matching models and propose\nimprovements for both training and inference. First, we empirically show\nlimitations of two popular loss (sum and max-margin loss) widely used in\ntraining text-image embeddings and propose a trade-off: a kNN-margin loss which\n1) utilizes information from hard negatives and 2) is robust to noise as all\n$K$-most hardest samples are taken into account, tolerating \\emph{pseudo}\nnegatives and outliers. Second, we advocate the use of Inverted Softmax\n(\\textsc{Is}) and Cross-modal Local Scaling (\\textsc{Csls}) during inference to\nmitigate the so-called hubness problem in high-dimensional embedding space,\nenhancing scores of all metrics by a large margin.",
        "timestamp": "2025-03-16T17:36:40.649Z",
        "rating": "novote",
        "publishedDate": "2019-06-04T05:42:58Z",
        "tags": [
          "cs.LG",
          "cs.CL",
          "cs.CV"
        ]
      },
      "meta": {
        "created_at": "2025-03-16T17:36:41+00:00",
        "updated_at": "2025-03-29T22:43:15+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.1607.01759": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1607.01759",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T17:47:16.127Z",
            "data": {
              "session_id": "session_1742147230986_p2bthfk",
              "paper_id": "1607.01759",
              "source_id": "arxiv",
              "start_time": "2025-03-16T17:47:10.986Z",
              "end_time": "2025-03-16T17:47:16.127Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-16T17:46:26+00:00",
        "updated_at": "2025-03-29T22:43:13+00:00",
        "version": 5
      }
    },
    "paper:arxiv.1607.01759": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1607.01759",
        "url": "https://arxiv.org/abs/1607.01759",
        "title": "Bag of Tricks for Efficient Text Classification",
        "authors": "Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov",
        "abstract": "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute.",
        "timestamp": "2025-03-16T17:46:20.359Z",
        "rating": "novote",
        "publishedDate": "[Submitted on 6 Jul 2016 (v1\ud83d\udcdd), last revised 9 Aug 2016 (this version, v3)]",
        "tags": [
          "Computation and Language (cs.CL)"
        ]
      },
      "meta": {
        "created_at": "2025-03-16T17:46:20+00:00",
        "updated_at": "2025-03-29T22:43:14+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2502.19413": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.19413",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T18:25:54.313Z",
            "data": {
              "session_id": "session_1742149548668_oj8odh9",
              "paper_id": "2502.19413",
              "source_id": "arxiv",
              "start_time": "2025-03-16T18:25:48.668Z",
              "end_time": "2025-03-16T18:25:54.313Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T18:40:34.270Z",
            "data": {
              "session_id": "session_1742150346031_9vfon1w",
              "paper_id": "2502.19413",
              "source_id": "arxiv",
              "start_time": "2025-03-16T18:39:06.031Z",
              "end_time": "2025-03-16T18:40:34.269Z",
              "heartbeat_count": 17,
              "duration_seconds": 85,
              "idle_seconds": 3,
              "total_elapsed_seconds": 88
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-16T18:25:55+00:00",
        "updated_at": "2025-03-29T22:43:13+00:00",
        "version": 4
      }
    },
    "interactions:arxiv.2111.02114": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2111.02114",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T19:03:09.084Z",
            "data": {
              "session_id": "session_1742151780063_id1pn5w",
              "paper_id": "2111.02114",
              "source_id": "arxiv",
              "start_time": "2025-03-16T19:03:00.063Z",
              "end_time": "2025-03-16T19:03:09.083Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T19:09:42.996Z",
            "data": {
              "session_id": "session_1742152181897_pfto3bj",
              "paper_id": "2111.02114",
              "source_id": "arxiv",
              "start_time": "2025-03-16T19:09:41.897Z",
              "end_time": "2025-03-16T19:09:42.995Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 1,
              "total_elapsed_seconds": 1
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-16T19:03:10+00:00",
        "updated_at": "2025-03-29T22:43:11+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2111.02114": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2111.02114",
        "url": "https://arxiv.org/abs/2111.02114",
        "title": "LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs",
        "authors": "Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, Aran Komatsuzaki",
        "abstract": "Multi-modal language-vision models trained on hundreds of millions of image-text pairs (e.g. CLIP, DALL-E) gained a recent surge, showing remarkable capability to perform zero- or few-shot learning and transfer even in absence of per-sample labels on target image data. Despite this trend, to date there has been no publicly available datasets of sufficient scale for training such models from scratch. To address this issue, in a community effort we build and release for public LAION-400M, a dataset with CLIP-filtered 400 million image-text pairs, their CLIP embeddings and kNN indices that allow efficient similarity search.",
        "timestamp": "2025-03-16T19:03:00.468Z",
        "rating": "novote",
        "publishedDate": "[Submitted on 3 Nov 2021]",
        "tags": [
          "Computer Vision and Pattern Recognition (cs.CV)",
          "Computation and Language (cs.CL)",
          "Machine Learning (cs.LG)"
        ]
      },
      "meta": {
        "created_at": "2025-03-16T19:03:00+00:00",
        "updated_at": "2025-03-29T22:43:11+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2112.10752": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2112.10752",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T19:02:56.238Z",
            "data": {
              "session_id": "session_1742151749698_4750dwv",
              "paper_id": "2112.10752",
              "source_id": "arxiv",
              "start_time": "2025-03-16T19:02:29.698Z",
              "end_time": "2025-03-16T19:02:56.238Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 2,
              "total_elapsed_seconds": 27
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T19:09:41.872Z",
            "data": {
              "session_id": "session_1742152181069_6zth0z9",
              "paper_id": "2112.10752",
              "source_id": "arxiv",
              "start_time": "2025-03-16T19:09:41.069Z",
              "end_time": "2025-03-16T19:09:41.872Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 1,
              "total_elapsed_seconds": 1
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-16T19:02:29+00:00",
        "updated_at": "2025-03-29T22:43:12+00:00",
        "version": 5
      }
    },
    "paper:arxiv.2112.10752": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2112.10752",
        "url": "https://arxiv.org/abs/2112.10752",
        "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
        "authors": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer",
        "abstract": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at this https URL .",
        "timestamp": "2025-03-16T19:02:27.405Z",
        "rating": "novote",
        "publishedDate": "[Submitted on 20 Dec 2021 (v1\ud83d\udcdd), last revised 13 Apr 2022 (this version, v2)]",
        "tags": [
          "Computer Vision and Pattern Recognition (cs.CV)"
        ]
      },
      "meta": {
        "created_at": "2025-03-16T19:02:27+00:00",
        "updated_at": "2025-03-29T22:43:12+00:00",
        "version": 2
      }
    },
    "interactions:pdf.pdf.478253C1": {
      "data": {
        "sourceId": "pdf",
        "paperId": "pdf.478253C1",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T19:29:52.559Z",
            "data": {
              "session_id": "session_1742153377553_azpbujw",
              "paper_id": "pdf.478253C1",
              "source_id": "pdf",
              "start_time": "2025-03-16T19:29:37.553Z",
              "end_time": "2025-03-16T19:29:52.557Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-16T19:29:53+00:00",
        "updated_at": "2025-03-29T22:43:08+00:00",
        "version": 3
      }
    },
    "paper:pdf.pdf.478253C1": {
      "data": {
        "sourceId": "pdf",
        "paperId": "pdf.478253C1",
        "url": "https://www.research.unipd.it/bitstream/11577/3162912/2/2015_Kramer%20Bressan_Humans%20as%20superorganisms%20preprint.pdf",
        "title": "pdf.478253C1",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-16T19:29:33.967Z",
        "publishedDate": "",
        "tags": [],
        "rating": "novote"
      },
      "meta": {
        "created_at": "2025-03-16T19:29:34+00:00",
        "updated_at": "2025-03-29T22:43:08+00:00",
        "version": 2
      }
    },
    "interactions:url.url.14AC3C7E": {
      "data": {
        "sourceId": "url",
        "paperId": "url.14AC3C7E",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T19:28:57.140Z",
            "data": {
              "session_id": "session_1742153322133_s9bw042",
              "paper_id": "url.14AC3C7E",
              "source_id": "url",
              "start_time": "2025-03-16T19:28:42.133Z",
              "end_time": "2025-03-16T19:28:57.139Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-16T19:28:57+00:00",
        "updated_at": "2025-03-29T22:43:09+00:00",
        "version": 3
      }
    },
    "paper:url.url.14AC3C7E": {
      "data": {
        "sourceId": "url",
        "paperId": "url.14AC3C7E",
        "url": "https://firstmonday.org/ojs/index.php/fm/article/view/13630/11605",
        "title": "View of Automated decision-making as domination",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-16T19:28:38.365Z",
        "publishedDate": "",
        "tags": [],
        "rating": "novote"
      },
      "meta": {
        "created_at": "2025-03-16T19:28:38+00:00",
        "updated_at": "2025-03-29T22:43:09+00:00",
        "version": 2
      }
    },
    "interactions:url.url.6BAC17CA": {
      "data": {
        "sourceId": "url",
        "paperId": "url.6BAC17CA",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T19:27:46.285Z",
            "data": {
              "session_id": "session_1742153251275_ppo1ca1",
              "paper_id": "url.6BAC17CA",
              "source_id": "url",
              "start_time": "2025-03-16T19:27:31.275Z",
              "end_time": "2025-03-16T19:27:46.283Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-16T19:27:47+00:00",
        "updated_at": "2025-03-29T22:43:10+00:00",
        "version": 3
      }
    },
    "paper:url.url.6BAC17CA": {
      "data": {
        "sourceId": "url",
        "paperId": "url.6BAC17CA",
        "url": "https://firstmonday.org/ojs/index.php/fm/article/view/13630",
        "title": "Automated decision-making as domination | First Monday",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-16T19:27:27.432Z",
        "publishedDate": "",
        "tags": [],
        "rating": "novote"
      },
      "meta": {
        "created_at": "2025-03-16T19:27:27+00:00",
        "updated_at": "2025-03-29T22:43:10+00:00",
        "version": 2
      }
    },
    "interactions:url.6BAC17CA": {
      "data": {
        "sourceId": "url",
        "paperId": "6BAC17CA",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T19:41:01.694Z",
            "data": {
              "session_id": "session_1742154046691_ggs7a1a",
              "paper_id": "6BAC17CA",
              "source_id": "url",
              "start_time": "2025-03-16T19:40:46.691Z",
              "end_time": "2025-03-16T19:41:01.693Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T20:07:59.067Z",
            "data": {
              "session_id": "session_1742155664057_wfd8r8y",
              "paper_id": "6BAC17CA",
              "source_id": "url",
              "start_time": "2025-03-16T20:07:44.057Z",
              "end_time": "2025-03-16T20:07:59.065Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-16T19:41:02+00:00",
        "updated_at": "2025-03-29T22:43:07+00:00",
        "version": 4
      }
    },
    "paper:url.6BAC17CA": {
      "data": {
        "sourceId": "url",
        "paperId": "6BAC17CA",
        "url": "https://firstmonday.org/ojs/index.php/fm/article/view/13630",
        "title": "Automated decision-making as domination | First Monday",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-16T19:40:43.042Z",
        "publishedDate": "",
        "tags": [],
        "rating": "novote"
      },
      "meta": {
        "created_at": "2025-03-16T19:40:43+00:00",
        "updated_at": "2025-03-29T22:43:08+00:00",
        "version": 2
      }
    },
    "paper:url.2020268F": {
      "data": {
        "sourceId": "url",
        "paperId": "2020268F",
        "url": "https://openreview.net/forum?id=SyK00v5xx",
        "title": "A Simple but Tough-to-Beat Baseline for Sentence Embeddings",
        "authors": "Sanjeev Arora, Yingyu Liang, Tengyu Ma",
        "abstract": "\nThe success of neural network methods for computing word embeddings has motivated methods for generating semantic embeddings of longer pieces of text, such as sentences and paragraphs. Surprisingly, Wieting et al (ICLR'16) showed that such complicated methods are outperformed, especially in out-of-domain (transfer learning) settings, by simpler methods involving mild retraining of word embeddings and basic linear regression. The  method of Wieting et al. requires retraining with a substantial labeled dataset such as Paraphrase Database (Ganitkevitch et al., 2013). \n\nThe current paper goes further, showing that the following completely unsupervised sentence embedding is a formidable baseline: Use word embeddings computed using one of the popular methods on unlabeled corpus like Wikipedia, represent the sentence by a weighted average of the word vectors, and then modify them a bit using PCA/SVD. This weighting improves performance by about 10% to 30% in textual similarity tasks, and beats sophisticated supervised methods including RNN's and LSTM's. It even improves Wieting et al.'s embeddings. \n This simple method should be used as the baseline to beat in future, especially when labeled training data is scarce or nonexistent. \n\nThe paper also gives a theoretical explanation of the success of the above unsupervised method using a latent variable generative model for sentences, which is a simple extension of the model in Arora et al. (TACL'16) with new \"smoothing\" terms that allow for \nwords occurring out of context, as well as high probabilities for words like and, not in all contexts. ",
        "timestamp": "2025-03-16T20:09:44.775Z",
        "publishedDate": "",
        "tags": [],
        "rating": "novote"
      },
      "meta": {
        "created_at": "2025-03-16T20:09:45+00:00",
        "updated_at": "2025-03-29T22:43:06+00:00",
        "version": 2
      }
    },
    "interactions:url.2020268F": {
      "data": {
        "sourceId": "url",
        "paperId": "2020268F",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T20:10:03.562Z",
            "data": {
              "session_id": "session_1742155788554_jufy7bp",
              "paper_id": "2020268F",
              "source_id": "url",
              "start_time": "2025-03-16T20:09:48.554Z",
              "end_time": "2025-03-16T20:10:03.561Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-16T20:10:04+00:00",
        "updated_at": "2025-03-29T22:43:06+00:00",
        "version": 3
      }
    },
    "interactions:arxiv.2501.13928": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.13928",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T20:51:21.395Z",
            "data": {
              "session_id": "session_1742158277532_xeasxs8",
              "paper_id": "2501.13928",
              "source_id": "arxiv",
              "start_time": "2025-03-16T20:51:17.532Z",
              "end_time": "2025-03-16T20:51:21.395Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 4,
              "total_elapsed_seconds": 4
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T20:52:16.135Z",
            "data": {
              "session_id": "session_1742158325877_st3o3je",
              "paper_id": "2501.13928",
              "source_id": "arxiv",
              "start_time": "2025-03-16T20:52:05.877Z",
              "end_time": "2025-03-16T20:52:16.135Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 0,
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T20:53:59.844Z",
            "data": {
              "session_id": "session_1742158436628_whwqqvn",
              "paper_id": "2501.13928",
              "source_id": "arxiv",
              "start_time": "2025-03-16T20:53:56.628Z",
              "end_time": "2025-03-16T20:53:59.844Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 3,
              "total_elapsed_seconds": 3
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T21:52:35.917Z",
            "data": {
              "session_id": "session_1742161955831_aapnyjz",
              "paper_id": "2501.13928",
              "source_id": "arxiv",
              "start_time": "2025-03-16T21:52:35.831Z",
              "end_time": "2025-03-16T21:52:35.917Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 0,
              "total_elapsed_seconds": 0
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-16T20:50:44+00:00",
        "updated_at": "2025-03-29T22:43:05+00:00",
        "version": 9
      }
    },
    "interactions:url.30BB8D13": {
      "data": {
        "sourceId": "url",
        "paperId": "30BB8D13",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T21:26:00.991Z",
            "data": {
              "session_id": "session_1742160345984_f5m81yv",
              "paper_id": "30BB8D13",
              "source_id": "url",
              "start_time": "2025-03-16T21:25:45.984Z",
              "end_time": "2025-03-16T21:26:00.990Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-16T21:26:02+00:00",
        "updated_at": "2025-03-29T22:43:04+00:00",
        "version": 3
      }
    },
    "paper:url.30BB8D13": {
      "data": {
        "sourceId": "url",
        "paperId": "30BB8D13",
        "url": "https://www.mdpi.com/2076-3417/14/14/6171",
        "title": "MicroBERT: Distilling MoE-Based Knowledge from BERT into a Lighter Model",
        "authors": "Zheng, Dashun, Li, Jiaxuan, Yang, Yunchu, Wang, Yapeng, Pang, Patrick Cheong-Iao",
        "abstract": "Natural language-processing tasks have been improved greatly by large language models (LLMs). However, numerous parameters make their execution computationally expensive and difficult on resource-constrained devices. For this problem, as well as maintaining accuracy, some techniques such as distillation and quantization have been proposed. Unfortunately, current methods fail to integrate model pruning with downstream tasks and overlook sentence-level semantic modeling, resulting in reduced efficiency of distillation. To alleviate these limitations, we propose a novel distilled lightweight model for BERT named MicroBERT. This method can transfer the knowledge contained in the \u201cteacher\u201d BERT model to a \u201cstudent\u201d BERT model. The sentence-level feature alignment loss (FAL) distillation mechanism, guided by Mixture-of-Experts (MoE), captures comprehensive contextual semantic knowledge from the \u201cteacher\u201d model to enhance the \u201cstudent\u201d model\u2019s performance while reducing its parameters. To make the outputs of \u201cteacher\u201d and \u201cstudent\u201d models comparable, we introduce the idea of a generative adversarial network (GAN) to train a discriminator. Our experimental results based on four datasets show that all steps of our distillation mechanism are effective, and the MicroBERT (101.14%) model outperforms TinyBERT (99%) by 2.24% in terms of average distillation reductions in various tasks on the GLUE dataset.",
        "timestamp": "2025-03-16T21:25:41.885Z",
        "publishedDate": "",
        "tags": [],
        "rating": "novote"
      },
      "meta": {
        "created_at": "2025-03-16T21:25:42+00:00",
        "updated_at": "2025-03-29T22:43:05+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.08827": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.08827",
        "url": "https://arxiv.org/abs/2503.08827",
        "title": "Neural Network/de Sitter Space Correspondence",
        "authors": "Donghee Lee, Hye-Sung Lee, Jaeok Yi",
        "abstract": "Machine learning's remarkable practical successes have sparked extensive theoretical investigations, yet fundamental breakthroughs remain elusive. Here, we study neural network training via gradient descent and demonstrate that the resulting dynamics can be described by a field theory in de Sitter space. We illustrate this correspondence with a simple example of neural network in which the continuum limit is explicitly realized. Since de Sitter space is well-studied in both field theory and general relativity, our findings suggest new avenues for understanding neural network training and for leveraging established theoretical frameworks to advance machine learning theory.",
        "timestamp": "2025-03-16T21:48:01.798Z",
        "rating": "novote",
        "publishedDate": "[Submitted on 11 Mar 2025]",
        "tags": [
          "High Energy Physics - Theory (hep-th)",
          "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
          "High Energy Physics - Phenomenology (hep-ph)"
        ]
      },
      "meta": {
        "created_at": "2025-03-16T21:48:02+00:00",
        "updated_at": "2025-03-29T22:43:04+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.08827": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.08827",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T21:48:41.796Z",
            "data": {
              "session_id": "session_1742161719571_75vs6nh",
              "paper_id": "2503.08827",
              "source_id": "arxiv",
              "start_time": "2025-03-16T21:48:39.571Z",
              "end_time": "2025-03-16T21:48:41.796Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 2,
              "total_elapsed_seconds": 2
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T21:50:03.230Z",
            "data": {
              "session_id": "session_1742161802008_tfqd10h",
              "paper_id": "2503.08827",
              "source_id": "arxiv",
              "start_time": "2025-03-16T21:50:02.008Z",
              "end_time": "2025-03-16T21:50:03.229Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 1,
              "total_elapsed_seconds": 1
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T21:52:31.505Z",
            "data": {
              "session_id": "session_1742161943220_euhsoww",
              "paper_id": "2503.08827",
              "source_id": "arxiv",
              "start_time": "2025-03-16T21:52:23.220Z",
              "end_time": "2025-03-16T21:52:31.505Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T22:10:28.250Z",
            "data": {
              "session_id": "session_1742163014742_buhs4ax",
              "paper_id": "2503.08827",
              "source_id": "arxiv",
              "start_time": "2025-03-16T22:10:14.743Z",
              "end_time": "2025-03-16T22:10:28.250Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T22:11:51.101Z",
            "data": {
              "session_id": "session_1742163107000_jme7d65",
              "paper_id": "2503.08827",
              "source_id": "arxiv",
              "start_time": "2025-03-16T22:11:47.000Z",
              "end_time": "2025-03-16T22:11:51.100Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 4,
              "total_elapsed_seconds": 4
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T22:22:46.011Z",
            "data": {
              "session_id": "session_1742163765042_8k9c2om",
              "paper_id": "2503.08827",
              "source_id": "arxiv",
              "start_time": "2025-03-16T22:22:45.042Z",
              "end_time": "2025-03-16T22:22:46.011Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 1,
              "total_elapsed_seconds": 1
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T22:24:03.704Z",
            "data": {
              "session_id": "session_1742163842539_se3h50y",
              "paper_id": "2503.08827",
              "source_id": "arxiv",
              "start_time": "2025-03-16T22:24:02.539Z",
              "end_time": "2025-03-16T22:24:03.703Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 1,
              "total_elapsed_seconds": 1
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T23:56:36.970Z",
            "data": {
              "session_id": "session_1742169393326_u2d3bep",
              "paper_id": "2503.08827",
              "source_id": "arxiv",
              "start_time": "2025-03-16T23:56:33.326Z",
              "end_time": "2025-03-16T23:56:36.970Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 4,
              "total_elapsed_seconds": 4
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T23:58:59.542Z",
            "data": {
              "session_id": "session_1742169538525_7gwzkeg",
              "paper_id": "2503.08827",
              "source_id": "arxiv",
              "start_time": "2025-03-16T23:58:58.525Z",
              "end_time": "2025-03-16T23:58:59.542Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 1,
              "total_elapsed_seconds": 1
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-17T00:02:55.462Z",
            "data": {
              "session_id": "session_1742169774321_iayvfg3",
              "paper_id": "2503.08827",
              "source_id": "arxiv",
              "start_time": "2025-03-17T00:02:54.321Z",
              "end_time": "2025-03-17T00:02:55.462Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 1,
              "total_elapsed_seconds": 1
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-17T02:03:54.213Z",
            "data": {
              "session_id": "session_1742177033542_5xkh63e",
              "paper_id": "2503.08827",
              "source_id": "arxiv",
              "start_time": "2025-03-17T02:03:53.542Z",
              "end_time": "2025-03-17T02:03:54.213Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 1,
              "total_elapsed_seconds": 1
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-22T17:39:23.567Z",
            "data": {
              "session_id": "session_1742665163004_cyijis4",
              "source_id": "arxiv",
              "paper_id": "2503.08827",
              "start_time": "2025-03-22T17:39:00.005Z",
              "end_time": "2025-03-22T17:39:23.004Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 3,
              "total_elapsed_seconds": 23
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-22T21:48:47.697Z",
            "data": {
              "session_id": "session_1742680127132_5jdz2j6",
              "source_id": "arxiv",
              "paper_id": "2503.08827",
              "start_time": "2025-03-22T21:48:04.160Z",
              "end_time": "2025-03-22T21:48:47.132Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 13,
              "total_elapsed_seconds": 43
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-16T21:48:24+00:00",
        "updated_at": "2025-03-29T22:43:04+00:00",
        "version": 25
      }
    },
    "paper:arxiv.2501.18812": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.18812",
        "url": "https://arxiv.org/abs/2501.18812",
        "title": "Estimating the Probability of Sampling a Trained Neural Network at Random",
        "authors": "Adam Scherlis, Nora Belrose",
        "abstract": "We present an algorithm for estimating the probability mass, under a Gaussian or uniform prior, of a region in neural network parameter space corresponding to a particular behavior, such as achieving test loss below some threshold. When the prior is uniform, this problem is equivalent to measuring the volume of a region. We show empirically and theoretically that existing algorithms for estimating volumes in parameter space underestimate the true volume by millions of orders of magnitude. We find that this error can be dramatically reduced, but not entirely eliminated, with an importance sampling method using gradient information that is already provided by popular optimizers. The negative logarithm of this probability can be interpreted as a measure of a network's information content, in accordance with minimum description length (MDL) principles and rate-distortion theory. As expected, this quantity increases during language model training. We also find that badly-generalizing behavioral regions are smaller, and therefore less likely to be sampled at random, demonstrating an inductive bias towards well-generalizing functions.",
        "timestamp": "2025-03-16T22:19:29.057Z",
        "rating": "novote",
        "publishedDate": "[Submitted on 31 Jan 2025]",
        "tags": [
          "Machine Learning (cs.LG)"
        ]
      },
      "meta": {
        "created_at": "2025-03-16T22:19:29+00:00",
        "updated_at": "2025-03-29T22:43:03+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2501.18812": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.18812",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T22:19:36.664Z",
            "data": {
              "session_id": "session_1742163568615_my4rs4m",
              "paper_id": "2501.18812",
              "source_id": "arxiv",
              "start_time": "2025-03-16T22:19:28.615Z",
              "end_time": "2025-03-16T22:19:36.664Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T22:21:05.654Z",
            "data": {
              "session_id": "session_1742163632658_5jpt539",
              "paper_id": "2501.18812",
              "source_id": "arxiv",
              "start_time": "2025-03-16T22:20:32.658Z",
              "end_time": "2025-03-16T22:21:05.654Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 3,
              "total_elapsed_seconds": 33
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T23:39:48.001Z",
            "data": {
              "session_id": "session_1742168379533_cufi5ye",
              "paper_id": "2501.18812",
              "source_id": "arxiv",
              "start_time": "2025-03-16T23:39:39.534Z",
              "end_time": "2025-03-16T23:39:48.001Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T23:55:06.235Z",
            "data": {
              "session_id": "session_1742169297589_l3mtykq",
              "paper_id": "2501.18812",
              "source_id": "arxiv",
              "start_time": "2025-03-16T23:54:57.589Z",
              "end_time": "2025-03-16T23:55:06.235Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T23:57:19.193Z",
            "data": {
              "session_id": "session_1742169396971_rj4h1jq",
              "paper_id": "2501.18812",
              "source_id": "arxiv",
              "start_time": "2025-03-16T23:56:36.971Z",
              "end_time": "2025-03-16T23:57:19.193Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 2,
              "total_elapsed_seconds": 42
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T23:59:37.051Z",
            "data": {
              "session_id": "session_1742169552093_f5az0is",
              "paper_id": "2501.18812",
              "source_id": "arxiv",
              "start_time": "2025-03-16T23:59:12.093Z",
              "end_time": "2025-03-16T23:59:37.051Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 5,
              "total_elapsed_seconds": 25
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-17T00:03:18.007Z",
            "data": {
              "session_id": "session_1742169776681_cp74i63",
              "paper_id": "2501.18812",
              "source_id": "arxiv",
              "start_time": "2025-03-17T00:02:56.681Z",
              "end_time": "2025-03-17T00:03:18.007Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 1,
              "total_elapsed_seconds": 21
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-17T00:04:14.429Z",
            "data": {
              "session_id": "session_1742169852699_p3anyvg",
              "paper_id": "2501.18812",
              "source_id": "arxiv",
              "start_time": "2025-03-17T00:04:12.700Z",
              "end_time": "2025-03-17T00:04:14.428Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 2,
              "total_elapsed_seconds": 2
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-17T02:03:52.239Z",
            "data": {
              "session_id": "session_1742177030114_spcqzuc",
              "paper_id": "2501.18812",
              "source_id": "arxiv",
              "start_time": "2025-03-17T02:03:50.114Z",
              "end_time": "2025-03-17T02:03:52.239Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 2,
              "total_elapsed_seconds": 2
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-30T21:45:07.436Z",
            "data": {
              "session_id": "session_1743371106887_ul8jblg",
              "source_id": "arxiv",
              "paper_id": "2501.18812",
              "start_time": "2025-03-30T21:44:56.372Z",
              "end_time": "2025-03-30T21:45:06.887Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-16T22:19:37+00:00",
        "updated_at": "2025-03-30T21:46:11+00:00",
        "version": 14
      }
    },
    "interactions:arxiv.2405.07987": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2405.07987",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T23:39:31.165Z",
            "data": {
              "session_id": "session_1742168370216_pt80k0d",
              "paper_id": "2405.07987",
              "source_id": "arxiv",
              "start_time": "2025-03-16T23:39:30.216Z",
              "end_time": "2025-03-16T23:39:31.164Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 1,
              "total_elapsed_seconds": 1
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T23:46:02.133Z",
            "data": {
              "session_id": "session_1742168726539_yt8l1cd",
              "paper_id": "2405.07987",
              "source_id": "arxiv",
              "start_time": "2025-03-16T23:45:26.539Z",
              "end_time": "2025-03-16T23:46:02.133Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 1,
              "total_elapsed_seconds": 36
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-16T23:54:57.567Z",
            "data": {
              "session_id": "session_1742169287333_nni79pk",
              "paper_id": "2405.07987",
              "source_id": "arxiv",
              "start_time": "2025-03-16T23:54:47.333Z",
              "end_time": "2025-03-16T23:54:57.566Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 0,
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-17T00:02:53.124Z",
            "data": {
              "session_id": "session_1742169771803_nmbcxy1",
              "paper_id": "2405.07987",
              "source_id": "arxiv",
              "start_time": "2025-03-17T00:02:51.803Z",
              "end_time": "2025-03-17T00:02:53.124Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 1,
              "total_elapsed_seconds": 1
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-17T02:04:13.208Z",
            "data": {
              "session_id": "session_1742177034236_rv98vsz",
              "paper_id": "2405.07987",
              "source_id": "arxiv",
              "start_time": "2025-03-17T02:03:54.236Z",
              "end_time": "2025-03-17T02:04:13.208Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 4,
              "total_elapsed_seconds": 19
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-04T20:36:25.665Z",
            "data": {
              "session_id": "session_1743798985275_lbc1446",
              "source_id": "arxiv",
              "paper_id": "2405.07987",
              "start_time": "2025-04-04T20:36:16.366Z",
              "end_time": "2025-04-04T20:36:25.275Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-04T20:48:07.088Z",
            "data": {
              "session_id": "session_1743799686410_s28ujzo",
              "source_id": "arxiv",
              "paper_id": "2405.07987",
              "start_time": "2025-04-04T20:46:31.641Z",
              "end_time": "2025-04-04T20:48:06.410Z",
              "heartbeat_count": 18,
              "duration_seconds": 90,
              "idle_seconds": 5,
              "total_elapsed_seconds": 95
            }
          }
        ]
      },
      "meta": {
        "issue_number": 1928,
        "object_id": "interactions:arxiv.2405.07987",
        "created_at": "2025-03-16T23:39:32+00:00",
        "updated_at": "2025-04-04T20:49:08+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2405.07987": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2405.07987",
        "url": "https://arxiv.org/abs/2405.07987",
        "title": "The Platonic Representation Hypothesis",
        "authors": "Minyoung Huh, Brian Cheung, Tongzhou Wang, Phillip Isola",
        "abstract": "We argue that representations in AI models, particularly deep networks, are converging. First, we survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned. Next, we demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way. We hypothesize that this convergence is driving toward a shared statistical model of reality, akin to Plato's concept of an ideal reality. We term such a representation the platonic representation and discuss several possible selective pressures toward it. Finally, we discuss the implications of these trends, their limitations, and counterexamples to our analysis.",
        "timestamp": "2025-03-16T23:39:31.033Z",
        "rating": "novote",
        "publishedDate": "[Submitted on 13 May 2024 (v1\ud83d\udcdd), last revised 25 Jul 2024 (this version, v5)]",
        "tags": [
          "Machine Learning (cs.LG)",
          "Artificial Intelligence (cs.AI)",
          "Computer Vision and Pattern Recognition (cs.CV)",
          "Neural and Evolutionary Computing (cs.NE)"
        ]
      },
      "meta": {
        "created_at": "2025-03-16T23:39:31+00:00",
        "updated_at": "2025-03-29T22:43:02+00:00",
        "version": 2
      }
    },
    "paper:arxiv.1912.07242": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1912.07242",
        "url": "https://arxiv.org/abs/1912.07242",
        "title": "More Data Can Hurt for Linear Regression: Sample-wise Double Descent",
        "authors": "Preetum Nakkiran",
        "abstract": "In this expository note we describe a surprising phenomenon in\noverparameterized linear regression, where the dimension exceeds the number of\nsamples: there is a regime where the test risk of the estimator found by\ngradient descent increases with additional samples. In other words, more data\nactually hurts the estimator. This behavior is implicit in a recent line of\ntheoretical works analyzing \"double-descent\" phenomenon in linear models. In\nthis note, we isolate and understand this behavior in an extremely simple\nsetting: linear regression with isotropic Gaussian covariates. In particular,\nthis occurs due to an unconventional type of bias-variance tradeoff in the\noverparameterized regime: the bias decreases with more samples, but variance\nincreases.",
        "timestamp": "2025-03-17T01:53:44.901Z",
        "rating": "novote",
        "publishedDate": "2019-12-16T08:28:26Z",
        "tags": [
          "stat.ML",
          "cs.LG",
          "cs.NE",
          "math.ST",
          "stat.TH"
        ]
      },
      "meta": {
        "created_at": "2025-03-17T01:53:45+00:00",
        "updated_at": "2025-03-29T22:43:01+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.1912.07242": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1912.07242",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-17T01:54:31.851Z",
            "data": {
              "session_id": "session_1742176471137_jz2d051",
              "paper_id": "1912.07242",
              "source_id": "arxiv",
              "start_time": "2025-03-17T01:54:31.137Z",
              "end_time": "2025-03-17T01:54:31.851Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 1,
              "total_elapsed_seconds": 1
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-17T01:53:40+00:00",
        "updated_at": "2025-03-29T22:43:01+00:00",
        "version": 7
      }
    },
    "interactions:url.7B57F8D5": {
      "data": {
        "sourceId": "url",
        "paperId": "7B57F8D5",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-17T04:11:28.068Z",
            "data": {
              "session_id": "session_1742184673062_qic1uvl",
              "paper_id": "7B57F8D5",
              "source_id": "url",
              "start_time": "2025-03-17T04:11:13.062Z",
              "end_time": "2025-03-17T04:11:28.067Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-17T04:11:28+00:00",
        "updated_at": "2025-03-29T22:42:59+00:00",
        "version": 3
      }
    },
    "paper:url.7B57F8D5": {
      "data": {
        "sourceId": "url",
        "paperId": "7B57F8D5",
        "url": "https://openreview.net/pdf?id=pSk5qyt1ob",
        "title": "7B57F8D5",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-17T04:11:09.760Z",
        "publishedDate": "",
        "tags": [],
        "rating": "novote"
      },
      "meta": {
        "created_at": "2025-03-17T04:11:10+00:00",
        "updated_at": "2025-03-29T22:42:59+00:00",
        "version": 2
      }
    },
    "interactions:url.1E3A4DA": {
      "data": {
        "sourceId": "url",
        "paperId": "1E3A4DA",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-17T04:11:04.736Z",
            "data": {
              "session_id": "session_1742184649727_e5d0ink",
              "paper_id": "1E3A4DA",
              "source_id": "url",
              "start_time": "2025-03-17T04:10:49.727Z",
              "end_time": "2025-03-17T04:11:04.734Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-17T05:29:41.143Z",
            "data": {
              "session_id": "session_1742189366136_noc21ru",
              "paper_id": "1E3A4DA",
              "source_id": "url",
              "start_time": "2025-03-17T05:29:26.136Z",
              "end_time": "2025-03-17T05:29:41.143Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-17T04:11:05+00:00",
        "updated_at": "2025-03-29T22:43:00+00:00",
        "version": 4
      }
    },
    "paper:url.1E3A4DA": {
      "data": {
        "sourceId": "url",
        "paperId": "1E3A4DA",
        "url": "https://openreview.net/forum?id=pSk5qyt1ob",
        "title": "On Training-Conditional Conformal Prediction and Binomial Proportion Confidence Intervals",
        "authors": "Rudi Coppola, Manuel Mazo Espinosa",
        "abstract": "Estimating the expectation of a Bernoulli random variable based on $N$ independent trials is a classical problem in statistics, typically addressed using Binomial Proportion Confidence Intervals (BPCI). In the control systems community, many critical tasks\u2014such as certifying the statistical safety of dynamical systems\u2014can be formulated as BPCI problems.\n\nConformal Prediction (CP), a distribution-free technique for uncertainty quantification, has gained significant attention in recent years and has been applied to various control systems problems, particularly to address uncertainties in learned dynamics or controllers. A variant known as training-conditional CP was recently employed to tackle the problem of safety certification.\n\nIn this note, we highlight that the use of training-conditional CP in this context does not provide valid safety guarantees. We demonstrate why CP is unsuitable for BPCI problems and argue that traditional BPCI methods are better suited for statistical safety certification.",
        "timestamp": "2025-03-17T04:10:46.125Z",
        "publishedDate": "",
        "tags": [],
        "rating": "novote"
      },
      "meta": {
        "created_at": "2025-03-17T04:10:46+00:00",
        "updated_at": "2025-03-29T22:43:00+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2501.11433": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.11433",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-17T04:32:12.834Z",
            "data": {
              "session_id": "session_1742185932833_m8zzpos",
              "paper_id": "2501.11433",
              "source_id": "arxiv",
              "start_time": "2025-03-17T04:32:12.833Z",
              "end_time": "2025-03-17T04:32:12.834Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 0,
              "total_elapsed_seconds": 0
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-17T04:35:12.843Z",
            "data": {
              "session_id": "session_1742186013621_nv1idvp",
              "paper_id": "2501.11433",
              "source_id": "arxiv",
              "start_time": "2025-03-17T04:33:33.621Z",
              "end_time": "2025-03-17T04:35:12.843Z",
              "heartbeat_count": 19,
              "duration_seconds": 95,
              "idle_seconds": 4,
              "total_elapsed_seconds": 99
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-17T04:49:02.128Z",
            "data": {
              "session_id": "session_1742186941106_gse55wi",
              "paper_id": "2501.11433",
              "source_id": "arxiv",
              "start_time": "2025-03-17T04:49:01.106Z",
              "end_time": "2025-03-17T04:49:02.128Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 1,
              "total_elapsed_seconds": 1
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-17T05:11:01.231Z",
            "data": {
              "session_id": "session_1742188253931_1hpco09",
              "paper_id": "2501.11433",
              "source_id": "arxiv",
              "start_time": "2025-03-17T05:10:53.931Z",
              "end_time": "2025-03-17T05:11:01.230Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-17T04:32:00+00:00",
        "updated_at": "2025-03-29T22:42:58+00:00",
        "version": 8
      }
    },
    "paper:arxiv.2501.11433": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.11433",
        "url": "https://arxiv.org/abs/2501.11433v2",
        "title": "One Does Not Simply Meme Alone: Evaluating Co-Creativity Between LLMs and Humans in the Generation of Humor",
        "authors": "Zhikun Wu (KTH Royal Institute of Technology), Thomas Weber (LMU Munich), Florian M\u00fcller (TU Darmstadt)",
        "abstract": "Collaboration has been shown to enhance creativity, leading to more innovative and effective outcomes. While previous research has explored the abilities of Large Language Models (LLMs) to serve as co-creative partners in tasks like writing poetry or creating narratives, the collaborative potential of LLMs in humor-rich and culturally nuanced domains remains an open question. To address this gap, we conducted a user study to explore the potential of LLMs in co-creating memes - a humor-driven and culturally specific form of creative expression. We conducted a user study with three groups of 50 participants each: a human-only group creating memes without AI assistance, a human-AI collaboration group interacting with a state-of-the-art LLM model, and an AI-only group where the LLM autonomously generated memes. We assessed the quality of the generated memes through crowdsourcing, with each meme rated on creativity, humor, and shareability. Our results showed that LLM assistance increased the number of ideas generated and reduced the effort participants felt. However, it did not improve the quality of the memes when humans collaborated with LLM. Interestingly, memes created entirely by AI performed better than both human-only and human-AI collaborative memes in all areas on average. However, when looking at the top-performing memes, human-created ones were better in humor, while human-AI collaborations stood out in creativity and shareability. These findings highlight the complexities of human-AI collaboration in creative tasks. While AI can boost productivity and create content that appeals to a broad audience, human creativity remains crucial for content that connects on a deeper level.",
        "timestamp": "2025-03-17T04:31:57.365Z",
        "rating": "novote",
        "publishedDate": "[Submitted on 20 Jan 2025 (v1\ud83d\udcdd), last revised 23 Jan 2025 (this version, v2)]",
        "tags": [
          "Human-Computer Interaction (cs.HC)"
        ],
        "doi": "10.1145/3708359.3712094",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-17T04:31:57+00:00",
        "updated_at": "2025-03-29T22:42:58+00:00",
        "version": 2
      }
    },
    "interactions:url.65F9475E": {
      "data": {
        "sourceId": "url",
        "paperId": "65F9475E",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-17T05:30:17.003Z",
            "data": {
              "session_id": "session_1742189401997_zptsq84",
              "paper_id": "65F9475E",
              "source_id": "url",
              "start_time": "2025-03-17T05:30:01.997Z",
              "end_time": "2025-03-17T05:30:17.003Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-17T05:30:17+00:00",
        "updated_at": "2025-03-29T22:42:56+00:00",
        "version": 3
      }
    },
    "paper:url.65F9475E": {
      "data": {
        "sourceId": "url",
        "paperId": "65F9475E",
        "url": "https://openreview.net/forum?id=r9p9CV52MV",
        "title": "ModuLoRA: Finetuning 2-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers",
        "authors": "Junjie Yin, Jiahao Dong, Yingheng Wang, Christopher De Sa, Volodymyr Kuleshov",
        "abstract": "We propose a memory-efficient finetuning algorithm for large language models (LLMs) that supports finetuning LLMs with 65B parameters in 2/3/4-bit precision on as little as one 24GB GPU. Our method, modular low-rank adaptation (ModuLoRA), integrates any user-specified weight quantizer with finetuning via low-rank adapters (LoRAs). Our approach relies on a simple quantization-agnostic backward pass that adaptively materializes low-precision LLM weights from a custom black-box quantization module. This approach enables finetuning 2-bit and 3-bit LLMs for the first time---leveraging state-of-the-art 2-bit QuIP# quantization and 3-bit OPTQ quantization---outperforming finetuning that relies on less sophisticated 4-bit and 8-bit methods. In our experiments, ModuLoRA attains competitive performance on text classification, natural language infernece, and instruction following tasks using significantly less memory than existing approaches, and we also surpass the state-of-the-art ROUGE score on a popular summarization task. We release ModuLoRA together with a series of low-precision models as part of LLMTOOLS, a user-friendly library for quantizing, running, and finetuning LLMs on consumer GPUs.",
        "timestamp": "2025-03-17T05:29:58.441Z",
        "publishedDate": "",
        "tags": [],
        "rating": "novote"
      },
      "meta": {
        "created_at": "2025-03-17T05:29:58+00:00",
        "updated_at": "2025-03-29T22:42:57+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.10880": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.10880",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-17T05:28:52.071Z",
            "data": {
              "session_id": "session_1742189324648_nt3ivgt",
              "paper_id": "2503.10880",
              "source_id": "arxiv",
              "start_time": "2025-03-17T05:28:44.648Z",
              "end_time": "2025-03-17T05:28:52.071Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-17T05:28:53+00:00",
        "updated_at": "2025-03-29T22:42:57+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.10880": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.10880",
        "url": "https://arxiv.org/abs/2503.10880",
        "title": "Spontaneous Optimal Mixing via Defect-Vortex Coupling in Confined Active Nematics",
        "authors": "Brandon Klein, Alejandro J. Soto Franco, Md Mainul Hasan Sabbir, Matthew J. Deutsch, Robin L. B. Selinger, Kevin A. Mitchell, Daniel A. Beller",
        "abstract": "Active nematic flows in two dimensions, largely driven by motile $+1/2$ disclinations, mix themselves efficiently and exhibit chaos in the bulk steady state. Motivated by recent experimental findings for three-defect braiding in cardioid-shaped domains, we investigate how this tendency toward chaotic fluid mixing can, counterintuitively, produce certain ordered, periodic flows in confinement with a controllable net topological charge. We study two-dimensional active nematics in systems with boundary conditions requiring a prescribed number of excess $+1/2$ disclinations, using Beris-Edwards nematohydrodynamics simulations, alongside an agent-based, hydrodynamic simulation approach. We find ordered flows for systems of three and four defects, and we use tools from braid theory to show that spontaneously occurring periodic defect motions produce maximal topological entropy. Our theory correctly predicts the generic absence of stable periodic orbits of more than four defects in strong confinement in simulation. Our results identify the parameter regime outside of which periodicity is lost, and allow us to probe the limits of topological entropy production.",
        "timestamp": "2025-03-17T05:28:45.216Z",
        "rating": "novote",
        "publishedDate": "[Submitted on 13 Mar 2025]",
        "tags": [
          "Soft Condensed Matter (cond-mat.soft)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-17T05:28:45+00:00",
        "updated_at": "2025-03-29T22:42:57+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.11272": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.11272",
        "url": "https://arxiv.org/abs/2503.11272",
        "title": "When Do Transformers Outperform Feedforward and Recurrent Networks? A\n  Statistical Perspective",
        "authors": "Alireza Mousavi-Hosseini, Clayton Sanford, Denny Wu, Murat A. Erdogdu",
        "abstract": "Theoretical efforts to prove advantages of Transformers in comparison with\nclassical architectures such as feedforward and recurrent neural networks have\nmostly focused on representational power. In this work, we take an alternative\nperspective and prove that even with infinite compute, feedforward and\nrecurrent networks may suffer from larger sample complexity compared to\nTransformers, as the latter can adapt to a form of dynamic sparsity.\nSpecifically, we consider a sequence-to-sequence data generating model on\nsequences of length $N$, in which the output at each position depends only on\n$q$ relevant tokens with $q \\ll N$, and the positions of these tokens are\ndescribed in the input prompt. We prove that a single-layer Transformer can\nlearn this model if and only if its number of attention heads is at least $q$,\nin which case it achieves a sample complexity almost independent of $N$, while\nrecurrent networks require $N^{\\Omega(1)}$ samples on the same problem. If we\nsimplify this model, recurrent networks may achieve a complexity almost\nindependent of $N$, while feedforward networks still require $N$ samples.\nConsequently, our proposed sparse retrieval model illustrates a natural\nhierarchy in sample complexity across these architectures.",
        "timestamp": "2025-03-17T07:02:38.311Z",
        "rating": "novote",
        "publishedDate": "2025-03-14T10:30:42Z",
        "tags": [
          "stat.ML",
          "cs.LG"
        ],
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-17T07:02:38+00:00",
        "updated_at": "2025-03-29T22:42:55+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.11272": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.11272",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-17T07:02:37.188Z",
            "data": {
              "session_id": "session_1742194740595_de0gue4",
              "paper_id": "2503.11272",
              "source_id": "arxiv",
              "start_time": "2025-03-17T06:59:00.595Z",
              "end_time": "2025-03-17T07:02:37.187Z",
              "heartbeat_count": 43,
              "duration_seconds": 215,
              "idle_seconds": 2,
              "total_elapsed_seconds": 217
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-17T07:07:45.976Z",
            "data": {
              "session_id": "session_1742194957874_9ik9560",
              "paper_id": "2503.11272",
              "source_id": "arxiv",
              "start_time": "2025-03-17T07:02:37.874Z",
              "end_time": "2025-03-17T07:07:45.976Z",
              "heartbeat_count": 61,
              "duration_seconds": 305,
              "idle_seconds": 3,
              "total_elapsed_seconds": 308
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-17T07:11:54.887Z",
            "data": {
              "session_id": "session_1742195383683_5kp80se",
              "paper_id": "2503.11272",
              "source_id": "arxiv",
              "start_time": "2025-03-17T07:09:43.683Z",
              "end_time": "2025-03-17T07:11:54.886Z",
              "heartbeat_count": 26,
              "duration_seconds": 130,
              "idle_seconds": 1,
              "total_elapsed_seconds": 131
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-17T07:02:38+00:00",
        "updated_at": "2025-03-29T22:42:56+00:00",
        "version": 5
      }
    },
    "interactions:url.421D7430": {
      "data": {
        "sourceId": "url",
        "paperId": "421D7430",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-17T07:18:36.391Z",
            "data": {
              "session_id": "session_1742195901384_nu9o5zd",
              "paper_id": "421D7430",
              "source_id": "url",
              "start_time": "2025-03-17T07:18:21.384Z",
              "end_time": "2025-03-17T07:18:36.391Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-17T07:18:37+00:00",
        "updated_at": "2025-03-29T22:42:54+00:00",
        "version": 3
      }
    },
    "paper:url.421D7430": {
      "data": {
        "sourceId": "url",
        "paperId": "421D7430",
        "url": "https://aphyr.com/posts/342-typing-the-technical-interview",
        "title": "Typing the technical interview",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-17T07:18:17.469Z",
        "publishedDate": "",
        "tags": [],
        "rating": "novote"
      },
      "meta": {
        "created_at": "2025-03-17T07:18:17+00:00",
        "updated_at": "2025-03-29T22:42:55+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.11651": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.11651",
        "url": "https://arxiv.org/abs/2503.11651",
        "title": "VGGT: Visual Geometry Grounded Transformer",
        "authors": "Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, David Novotny",
        "abstract": "We present VGGT, a feed-forward neural network that directly infers all key\n3D attributes of a scene, including camera parameters, point maps, depth maps,\nand 3D point tracks, from one, a few, or hundreds of its views. This approach\nis a step forward in 3D computer vision, where models have typically been\nconstrained to and specialized for single tasks. It is also simple and\nefficient, reconstructing images in under one second, and still outperforming\nalternatives that require post-processing with visual geometry optimization\ntechniques. The network achieves state-of-the-art results in multiple 3D tasks,\nincluding camera parameter estimation, multi-view depth estimation, dense point\ncloud reconstruction, and 3D point tracking. We also show that using pretrained\nVGGT as a feature backbone significantly enhances downstream tasks, such as\nnon-rigid point tracking and feed-forward novel view synthesis. Code and models\nare publicly available at https://github.com/facebookresearch/vggt.",
        "timestamp": "2025-03-17T07:24:23.471Z",
        "rating": "novote",
        "publishedDate": "2025-03-14T17:59:47Z",
        "tags": [
          "cs.CV"
        ],
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-17T07:24:23+00:00",
        "updated_at": "2025-03-29T22:42:54+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.11651": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.11651",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-17T07:24:31.868Z",
            "data": {
              "session_id": "session_1742196262821_rbjyswo",
              "paper_id": "2503.11651",
              "source_id": "arxiv",
              "start_time": "2025-03-17T07:24:22.821Z",
              "end_time": "2025-03-17T07:24:31.868Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-17T07:24:23+00:00",
        "updated_at": "2025-03-29T22:42:53+00:00",
        "version": 4
      }
    },
    "interactions:url.4C264A22": {
      "data": {
        "sourceId": "url",
        "paperId": "4C264A22",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-17T07:32:21.334Z",
            "data": {
              "session_id": "session_1742196726327_vx8z7e2",
              "paper_id": "4C264A22",
              "source_id": "url",
              "start_time": "2025-03-17T07:32:06.327Z",
              "end_time": "2025-03-17T07:32:21.333Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-17T07:32:22+00:00",
        "updated_at": "2025-03-29T22:42:52+00:00",
        "version": 3
      }
    },
    "paper:url.4C264A22": {
      "data": {
        "sourceId": "url",
        "paperId": "4C264A22",
        "url": "https://royalsocietypublishing.org/doi/10.1098/rsos.171085",
        "title": "The reproducibility of research and the misinterpretation of p-values | Royal Society Open Science",
        "authors": "",
        "abstract": " We wish to answer this question: If you observe a \u2018significant\u2019 p-value after doing a single unbiased experiment, what is the probability that your\nresult is a false positive? The weak evidence provided by p-values between 0.01 and 0.05 is explored by ...",
        "timestamp": "2025-03-17T07:32:00.397Z",
        "publishedDate": "",
        "tags": [],
        "rating": "novote"
      },
      "meta": {
        "created_at": "2025-03-17T07:32:00+00:00",
        "updated_at": "2025-03-29T22:42:53+00:00",
        "version": 2
      }
    },
    "interactions:url.4092DA01": {
      "data": {
        "sourceId": "url",
        "paperId": "4092DA01",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-17T08:29:53.068Z",
            "data": {
              "session_id": "session_1742200178062_i92aqrk",
              "paper_id": "4092DA01",
              "source_id": "url",
              "start_time": "2025-03-17T08:29:38.062Z",
              "end_time": "2025-03-17T08:29:53.068Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-17T08:29:54+00:00",
        "updated_at": "2025-03-29T22:42:51+00:00",
        "version": 3
      }
    },
    "paper:url.4092DA01": {
      "data": {
        "sourceId": "url",
        "paperId": "4092DA01",
        "url": "https://diffusion.csail.mit.edu/",
        "title": "Generative AI with Stochastic Differential Equations - IAP 2025",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-17T08:29:34.172Z",
        "publishedDate": "",
        "tags": [],
        "rating": "novote"
      },
      "meta": {
        "created_at": "2025-03-17T08:29:34+00:00",
        "updated_at": "2025-03-29T22:42:52+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.02113": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.02113",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-17T22:29:54.133Z",
            "data": {
              "session_id": "session_1742250591348_sqhhivx",
              "paper_id": "2503.02113",
              "source_id": "arxiv",
              "start_time": "2025-03-17T22:29:51.348Z",
              "end_time": "2025-03-17T22:29:54.132Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 3,
              "total_elapsed_seconds": 3
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-17T22:29:47+00:00",
        "updated_at": "2025-03-29T22:42:51+00:00",
        "version": 4
      }
    },
    "paper:url.6A701FBE": {
      "data": {
        "sourceId": "url",
        "paperId": "6A701FBE",
        "url": "https://openreview.net/forum?id=QTsJXSvAI2",
        "title": "Where Do We Stand with Implicit Neural Representations? A Technical and Performance Survey",
        "authors": "Amer Essakine, Yanqi Cheng, Chun-Wun Cheng, Lipei Zhang, Zhongying Deng, Lei Zhu, Carola-Bibiane Sch\u00f6nlieb, Angelica I Aviles-Rivero",
        "abstract": "Implicit Neural Representations (INRs) have emerged as a paradigm in knowledge representation, offering exceptional flexibility and performance across a diverse range of applications. INRs leverage multilayer perceptrons (MLPs) to model data as continuous implicit functions, providing critical advantages such as resolution independence, memory efficiency, and generalisation beyond discretised data structures. Their ability to solve complex inverse problems makes them particularly effective for tasks including audio reconstruction, image representation, 3D object reconstruction, and high-dimensional data synthesis. This survey provides a comprehensive review of state-of-the-art INR methods, introducing a clear taxonomy that categorises them into four key areas: activation functions, position encoding, combined strategies, and network structure optimisation. We rigorously analyse their critical properties\u2014such as full differentiability, smoothness, compactness, and adaptability to varying resolutions\u2014while also examining their strengths and limitations in addressing locality biases and capturing fine details. Our experimental comparison offers new insights into the trade-offs between different approaches, showcasing the capabilities and challenges of the latest INR techniques across various tasks. In addition to identifying areas where current methods excel, we highlight key limitations and potential avenues for improvement, such as developing more expressive activation functions, enhancing positional encoding mechanisms, and improving scalability for complex, high-dimensional data. This survey serves as a roadmap for researchers, offering practical guidance for future exploration in the field of INRs. We aim to foster new methodologies by outlining promising research directions for INRs and applications.",
        "timestamp": "2025-03-18T01:40:01.675Z",
        "publishedDate": "",
        "tags": [],
        "rating": "novote"
      },
      "meta": {
        "created_at": "2025-03-18T01:40:02+00:00",
        "updated_at": "2025-03-29T22:42:49+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.08241": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.08241",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T01:38:25.256Z",
            "data": {
              "session_id": "session_1742261883253_fzofpiy",
              "paper_id": "2503.08241",
              "source_id": "arxiv",
              "start_time": "2025-03-18T01:38:03.254Z",
              "end_time": "2025-03-18T01:38:25.256Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 2,
              "total_elapsed_seconds": 22
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T02:08:40.665Z",
            "data": {
              "session_id": "session_1742263719213_838ck20",
              "paper_id": "2503.08241",
              "source_id": "arxiv",
              "start_time": "2025-03-18T02:08:39.213Z",
              "end_time": "2025-03-18T02:08:40.665Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 1,
              "total_elapsed_seconds": 1
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T02:09:40.042Z",
            "data": {
              "session_id": "session_1742263762037_swxok74",
              "paper_id": "2503.08241",
              "source_id": "arxiv",
              "start_time": "2025-03-18T02:09:22.037Z",
              "end_time": "2025-03-18T02:09:40.041Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 3,
              "total_elapsed_seconds": 18
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T02:12:47.820Z",
            "data": {
              "session_id": "session_1742263950968_pjxr5ep",
              "paper_id": "2503.08241",
              "source_id": "arxiv",
              "start_time": "2025-03-18T02:12:30.968Z",
              "end_time": "2025-03-18T02:12:47.819Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 2,
              "total_elapsed_seconds": 17
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-18T01:38:26+00:00",
        "updated_at": "2025-03-29T22:42:49+00:00",
        "version": 6
      }
    },
    "paper:arxiv.2503.08241": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.08241",
        "url": "https://arxiv.org/abs/2503.08241",
        "title": "HASARD: A Benchmark for Vision-Based Safe Reinforcement Learning in\n  Embodied Agents",
        "authors": "Tristan Tomilin, Meng Fang, Mykola Pechenizkiy",
        "abstract": "Advancing safe autonomous systems through reinforcement learning (RL)\nrequires robust benchmarks to evaluate performance, analyze methods, and assess\nagent competencies. Humans primarily rely on embodied visual perception to\nsafely navigate and interact with their surroundings, making it a valuable\ncapability for RL agents. However, existing vision-based 3D benchmarks only\nconsider simple navigation tasks. To address this shortcoming, we introduce\n\\textbf{HASARD}, a suite of diverse and complex tasks to $\\textbf{HA}$rness\n$\\textbf{SA}$fe $\\textbf{R}$L with $\\textbf{D}$oom, requiring strategic\ndecision-making, comprehending spatial relationships, and predicting the\nshort-term future. HASARD features three difficulty levels and two action\nspaces. An empirical evaluation of popular baseline methods demonstrates the\nbenchmark's complexity, unique challenges, and reward-cost trade-offs.\nVisualizing agent navigation during training with top-down heatmaps provides\ninsight into a method's learning process. Incrementally training across\ndifficulty levels offers an implicit learning curriculum. HASARD is the first\nsafe RL benchmark to exclusively target egocentric vision-based learning,\noffering a cost-effective and insightful way to explore the potential and\nboundaries of current and future safe RL methods. The environments and baseline\nimplementations are open-sourced at\nhttps://sites.google.com/view/hasard-bench/.",
        "timestamp": "2025-03-18T01:38:04.296Z",
        "rating": "novote",
        "publishedDate": "2025-03-11T10:05:01Z",
        "tags": [
          "cs.AI",
          "cs.CV",
          "cs.LG",
          "cs.RO"
        ],
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-18T01:38:04+00:00",
        "updated_at": "2025-03-29T22:42:50+00:00",
        "version": 2
      }
    },
    "interactions:url.6A701FBE": {
      "data": {
        "sourceId": "url",
        "paperId": "6A701FBE",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T01:40:20.906Z",
            "data": {
              "session_id": "session_1742262005903_225k651",
              "paper_id": "6A701FBE",
              "source_id": "url",
              "start_time": "2025-03-18T01:40:05.903Z",
              "end_time": "2025-03-18T01:40:20.905Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T06:03:20.633Z",
            "data": {
              "session_id": "session_1742277780630_5r5k6xm",
              "paper_id": "6A701FBE",
              "source_id": "url",
              "start_time": "2025-03-18T06:03:00.630Z",
              "end_time": "2025-03-18T06:03:20.632Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 15,
              "total_elapsed_seconds": 20
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-18T01:40:21+00:00",
        "updated_at": "2025-03-29T22:42:48+00:00",
        "version": 4
      }
    },
    "interactions:arxiv.2503.12593": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.12593",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T03:38:11.666Z",
            "data": {
              "session_id": "session_1742269045097_e968xym",
              "paper_id": "2503.12593",
              "source_id": "arxiv",
              "start_time": "2025-03-18T03:37:25.097Z",
              "end_time": "2025-03-18T03:38:11.665Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 2,
              "total_elapsed_seconds": 47
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T03:40:30.253Z",
            "data": {
              "session_id": "session_1742269101932_ooasjwd",
              "paper_id": "2503.12593",
              "source_id": "arxiv",
              "start_time": "2025-03-18T03:38:21.932Z",
              "end_time": "2025-03-18T03:40:30.252Z",
              "heartbeat_count": 25,
              "duration_seconds": 125,
              "idle_seconds": 3,
              "total_elapsed_seconds": 128
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T04:22:48.221Z",
            "data": {
              "session_id": "session_1742271762153_emh4mhx",
              "paper_id": "2503.12593",
              "source_id": "arxiv",
              "start_time": "2025-03-18T04:22:42.153Z",
              "end_time": "2025-03-18T04:22:48.220Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-18T03:38:12+00:00",
        "updated_at": "2025-03-29T22:42:47+00:00",
        "version": 6
      }
    },
    "paper:arxiv.2503.12593": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.12593",
        "url": "https://arxiv.org/abs/2503.12593",
        "title": "Fourier-Based 3D Multistage Transformer for Aberration Correction in Multicellular Specimens",
        "authors": "Thayer Alshaabi, Daniel E. Milkie, Gaoxiang Liu, Cyna Shirazinejad, Jason L. Hong, Kemal Achour, Frederik G\u00f6rlitz, Ana Milunovic-Jevtic, Cat Simmons, Ibrahim S. Abuzahriyeh, Erin Hong, Samara Erin Williams, Nathanael Harrison, Evan Huang, Eun Seok Bae, Alison N. Killilea, David G. Drubin, Ian A. Swinburne, Srigokul Upadhyayula, Eric Betzig",
        "abstract": "High-resolution tissue imaging is often compromised by sample-induced optical aberrations that degrade resolution and contrast. While wavefront sensor-based adaptive optics (AO) can measure these aberrations, such hardware solutions are typically complex, expensive to implement, and slow when serially mapping spatially varying aberrations across large fields of view. Here, we introduce AOViFT (Adaptive Optical Vision Fourier Transformer) -- a machine learning-based aberration sensing framework built around a 3D multistage Vision Transformer that operates on Fourier domain embeddings. AOViFT infers aberrations and restores diffraction-limited performance in puncta-labeled specimens with substantially reduced computational cost, training time, and memory footprint compared to conventional architectures or real-space networks. We validated AOViFT on live gene-edited zebrafish embryos, demonstrating its ability to correct spatially varying aberrations using either a deformable mirror or post-acquisition deconvolution. By eliminating the need for the guide star and wavefront sensing hardware and simplifying the experimental workflow, AOViFT lowers technical barriers for high-resolution volumetric microscopy across diverse biological samples.",
        "timestamp": "2025-03-18T03:37:22.687Z",
        "rating": "novote",
        "publishedDate": "[Submitted on 16 Mar 2025]",
        "tags": [
          "Image and Video Processing (eess.IV)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)",
          "Biological Physics (physics.bio-ph)",
          "Quantitative Methods (q-bio.QM)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-18T03:37:23+00:00",
        "updated_at": "2025-03-29T22:42:48+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.13298": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.13298",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T04:41:15.151Z",
            "data": {
              "session_id": "session_1742272864955_aj6f515",
              "paper_id": "2503.13298",
              "source_id": "arxiv",
              "start_time": "2025-03-18T04:41:04.955Z",
              "end_time": "2025-03-18T04:41:15.150Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 0,
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T04:46:18.846Z",
            "data": {
              "session_id": "session_1742272879848_uh7u0me",
              "paper_id": "2503.13298",
              "source_id": "arxiv",
              "start_time": "2025-03-18T04:41:19.848Z",
              "end_time": "2025-03-18T04:46:18.846Z",
              "heartbeat_count": 59,
              "duration_seconds": 295,
              "idle_seconds": 4,
              "total_elapsed_seconds": 299
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T04:52:50.276Z",
            "data": {
              "session_id": "session_1742273563178_n08yq0b",
              "paper_id": "2503.13298",
              "source_id": "arxiv",
              "start_time": "2025-03-18T04:52:43.178Z",
              "end_time": "2025-03-18T04:52:50.276Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T16:34:37.743Z",
            "data": {
              "session_id": "session_1742315673116_xqdnjsh",
              "paper_id": "2503.13298",
              "source_id": "arxiv",
              "start_time": "2025-03-18T16:34:33.116Z",
              "end_time": "2025-03-18T16:34:37.742Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 5,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-18T04:41:16+00:00",
        "updated_at": "2025-03-29T22:42:47+00:00",
        "version": 9
      }
    },
    "paper:arxiv.2503.13298": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.13298",
        "url": "https://arxiv.org/abs/2503.13298",
        "title": "From Few-Shot Optimal Control to Few-Shot Learning",
        "authors": "Roman Chertovskih, Nikolay Pogodaev, Maxim Staritsyn, A. Pedro Aguiar",
        "abstract": "We present an approach to solving unconstrained nonlinear optimal control problems for a broad class of dynamical systems. This approach involves lifting the nonlinear problem to a linear ``super-problem'' on a dual Banach space, followed by a non-standard ``exact'' variational analysis, -- culminating in a descent method that achieves rapid convergence with minimal iterations. We investigate the applicability of this framework to mean-field control and discuss its perspectives for the analysis of information propagation in self-interacting neural networks.",
        "timestamp": "2025-03-18T04:41:05.582Z",
        "rating": "novote",
        "publishedDate": "[Submitted on 17 Mar 2025]",
        "tags": [
          "Optimization and Control (math.OC)",
          "Numerical Analysis (math.NA)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-18T04:41:06+00:00",
        "updated_at": "2025-03-29T22:42:47+00:00",
        "version": 2
      }
    },
    "paper:url.13CAADEE": {
      "data": {
        "sourceId": "url",
        "paperId": "13CAADEE",
        "url": "https://openreview.net/forum?id=rWSiBknwQa",
        "title": "Are Large Language Models Really Robust to Word-Level Perturbations?",
        "authors": "Haoyu Wang, Guozheng Ma, Cong Yu, Ning Gui, Linrui Zhang, Zhiqi Huang, Suwei Ma, Yongzhe Chang, Sen Zhang, Li Shen, Xueqian Wang, Peilin Zhao, Dacheng Tao",
        "abstract": "The swift advancement in the scales and capabilities of Large Language Models (LLMs) positions them as promising tools for a variety of downstream tasks. In addition to the pursuit of better performance and the avoidance of violent feedback on a certain prompt, to ensure the responsibility of the LLMs, much attention is drawn to the robustness of LLMs. However, existing evaluation methods mostly rely on traditional question answering datasets with predefined supervised labels, potentially  ignoring the superior generation capabilities of contemporary LLMs. To investigate the robustness of LLMs while using their generation ability, we propose a novel rational evaluation pipeline that leverages reward models as diagnostic tools to evaluate the long conversation generated from more challenging open questions by LLMs, which we refer to as the Reward Model for Reasonable Robustness Evaluation (TREvaL). Longer conversations manifest the comprehensive grasp of language models in terms of their proficiency in understanding questions, a capability not entirely encompassed by individual words or letters.Our extensive empirical experiments demonstrate that TREvaL provides an identification for the lack of robustness of nowadays LLMs.Notably, we are surprised to discover that robustness tends to decrease as fine-tuning (SFT and RLHF) is conducted, calling for more attention on the robustness during alignment process.",
        "timestamp": "2025-03-18T05:20:27.693Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "Transactions on Machine Learning Research",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-18T05:20:28+00:00",
        "updated_at": "2025-03-29T22:42:45+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.13433": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.13433",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T05:19:42.124Z",
            "data": {
              "session_id": "session_1742275165903_fkkblom",
              "paper_id": "2503.13433",
              "source_id": "arxiv",
              "start_time": "2025-03-18T05:19:25.903Z",
              "end_time": "2025-03-18T05:19:42.124Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 1,
              "total_elapsed_seconds": 16
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-18T05:19:26+00:00",
        "updated_at": "2025-03-29T22:42:46+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2503.13433": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.13433",
        "url": "https://arxiv.org/abs/2503.13433",
        "title": "Less Biased Noise Scale Estimation for Threshold-Robust RANSAC",
        "authors": "Johan Edstedt",
        "abstract": "The gold-standard for robustly estimating relative pose through image matching is RANSAC. While RANSAC is powerful, it requires setting the inlier threshold that determines whether the error of a correspondence under an estimated model is sufficiently small to be included in its consensus set. Setting this threshold is typically done by hand, and is difficult to tune without a access to ground truth data. Thus, a method capable of automatically determining the optimal threshold would be desirable. In this paper we revisit inlier noise scale estimation, which is an attractive approach as the inlier noise scale is linear to the optimal threshold. We revisit the noise scale estimation method SIMFIT and find bias in the estimate of the noise scale. In particular, we fix underestimates from using the same data for fitting the model as estimating the inlier noise, and from not taking the threshold itself into account. Secondly, since the optimal threshold within a scene is approximately constant we propose a multi-pair extension of SIMFIT++, by filtering of estimates, which improves results. Our approach yields robust performance across a range of thresholds, shown in Figure 1.",
        "timestamp": "2025-03-18T05:18:58.414Z",
        "rating": "novote",
        "publishedDate": "[Submitted on 17 Mar 2025]",
        "tags": [
          "Computer Vision and Pattern Recognition (cs.CV)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-18T05:18:58+00:00",
        "updated_at": "2025-03-29T22:42:46+00:00",
        "version": 2
      }
    },
    "interactions:url.13CAADEE": {
      "data": {
        "sourceId": "url",
        "paperId": "13CAADEE",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T05:23:01.171Z",
            "data": {
              "session_id": "session_1742275366164_lfhizhi",
              "paper_id": "13CAADEE",
              "source_id": "url",
              "start_time": "2025-03-18T05:22:46.164Z",
              "end_time": "2025-03-18T05:23:01.170Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T05:24:01.167Z",
            "data": {
              "session_id": "session_1742275426162_d3t14he",
              "paper_id": "13CAADEE",
              "source_id": "url",
              "start_time": "2025-03-18T05:23:46.162Z",
              "end_time": "2025-03-18T05:24:01.166Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T05:27:01.172Z",
            "data": {
              "session_id": "session_1742275606165_mahg70k",
              "paper_id": "13CAADEE",
              "source_id": "url",
              "start_time": "2025-03-18T05:26:46.165Z",
              "end_time": "2025-03-18T05:27:01.171Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T05:28:01.169Z",
            "data": {
              "session_id": "session_1742275666161_v8wb4tn",
              "paper_id": "13CAADEE",
              "source_id": "url",
              "start_time": "2025-03-18T05:27:46.161Z",
              "end_time": "2025-03-18T05:28:01.169Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T05:31:01.170Z",
            "data": {
              "session_id": "session_1742275846162_enis4li",
              "paper_id": "13CAADEE",
              "source_id": "url",
              "start_time": "2025-03-18T05:30:46.162Z",
              "end_time": "2025-03-18T05:31:01.169Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T05:32:01.165Z",
            "data": {
              "session_id": "session_1742275906161_clutck4",
              "paper_id": "13CAADEE",
              "source_id": "url",
              "start_time": "2025-03-18T05:31:46.161Z",
              "end_time": "2025-03-18T05:32:01.164Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T05:34:01.168Z",
            "data": {
              "session_id": "session_1742276026162_jgvhkjv",
              "paper_id": "13CAADEE",
              "source_id": "url",
              "start_time": "2025-03-18T05:33:46.162Z",
              "end_time": "2025-03-18T05:34:01.167Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T05:37:01.168Z",
            "data": {
              "session_id": "session_1742276206161_h8rk9m6",
              "paper_id": "13CAADEE",
              "source_id": "url",
              "start_time": "2025-03-18T05:36:46.161Z",
              "end_time": "2025-03-18T05:37:01.167Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T05:38:01.169Z",
            "data": {
              "session_id": "session_1742276266161_jtj522e",
              "paper_id": "13CAADEE",
              "source_id": "url",
              "start_time": "2025-03-18T05:37:46.161Z",
              "end_time": "2025-03-18T05:38:01.167Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T05:41:01.170Z",
            "data": {
              "session_id": "session_1742276446162_xjzvxcq",
              "paper_id": "13CAADEE",
              "source_id": "url",
              "start_time": "2025-03-18T05:40:46.162Z",
              "end_time": "2025-03-18T05:41:01.169Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T05:44:01.169Z",
            "data": {
              "session_id": "session_1742276626162_r7abqgb",
              "paper_id": "13CAADEE",
              "source_id": "url",
              "start_time": "2025-03-18T05:43:46.162Z",
              "end_time": "2025-03-18T05:44:01.168Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T05:45:01.167Z",
            "data": {
              "session_id": "session_1742276686159_q3oigyz",
              "paper_id": "13CAADEE",
              "source_id": "url",
              "start_time": "2025-03-18T05:44:46.159Z",
              "end_time": "2025-03-18T05:45:01.166Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T05:47:01.166Z",
            "data": {
              "session_id": "session_1742276806159_6n9p05j",
              "paper_id": "13CAADEE",
              "source_id": "url",
              "start_time": "2025-03-18T05:46:46.159Z",
              "end_time": "2025-03-18T05:47:01.166Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T05:49:01.165Z",
            "data": {
              "session_id": "session_1742276926158_uazl7v0",
              "paper_id": "13CAADEE",
              "source_id": "url",
              "start_time": "2025-03-18T05:48:46.158Z",
              "end_time": "2025-03-18T05:49:01.164Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T05:52:01.190Z",
            "data": {
              "session_id": "session_1742277106183_fub3ojn",
              "paper_id": "13CAADEE",
              "source_id": "url",
              "start_time": "2025-03-18T05:51:46.183Z",
              "end_time": "2025-03-18T05:52:01.189Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T05:53:01.190Z",
            "data": {
              "session_id": "session_1742277166182_lpxepsy",
              "paper_id": "13CAADEE",
              "source_id": "url",
              "start_time": "2025-03-18T05:52:46.182Z",
              "end_time": "2025-03-18T05:53:01.188Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-18T05:22:02+00:00",
        "updated_at": "2025-03-29T22:42:45+00:00",
        "version": 34
      }
    },
    "paper:arxiv.2503.11989": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.11989",
        "url": "https://arxiv.org/abs/2503.11989",
        "title": "Applications of Large Language Model Reasoning in Feature Generation",
        "authors": "Dharani Chandra",
        "abstract": "Large Language Models (LLMs) have revolutionized natural language processing through their state of art reasoning capabilities. This paper explores the convergence of LLM reasoning techniques and feature generation for machine learning tasks. We examine four key reasoning approaches: Chain of Thought, Tree of Thoughts, Retrieval-Augmented Generation, and Thought Space Exploration. Our analysis reveals how these approaches can be used to identify effective feature generation rules without having to manually specify search spaces. The paper categorizes LLM-based feature generation methods across various domains including finance, healthcare, and text analytics. LLMs can extract key information from clinical notes and radiology reports in healthcare, by enabling more efficient data utilization. In finance, LLMs facilitate text generation, summarization, and entity extraction from complex documents. We analyze evaluation methodologies for assessing feature quality and downstream performance, with particular attention to OCTree's decision tree reasoning approach that provides language-based feedback for iterative improvements. Current challenges include hallucination, computational efficiency, and domain adaptation. As of March 2025, emerging approaches include inference-time compute scaling, reinforcement learning, and supervised fine-tuning with model distillation. Future directions point toward multimodal feature generation, self-improving systems, and neuro-symbolic approaches. This paper provides a detailed overview of an emerging field that promises to automate and enhance feature engineering through language model reasoning.",
        "timestamp": "2025-03-18T06:05:02.021Z",
        "rating": "novote",
        "publishedDate": "[Submitted on 15 Mar 2025]",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-18T06:05:02+00:00",
        "updated_at": "2025-03-29T22:42:44+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.11989": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.11989",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T06:05:59.151Z",
            "data": {
              "session_id": "session_1742277943158_un1wc5g",
              "paper_id": "2503.11989",
              "source_id": "arxiv",
              "start_time": "2025-03-18T06:05:43.158Z",
              "end_time": "2025-03-18T06:05:59.151Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 1,
              "total_elapsed_seconds": 16
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-18T06:05:43+00:00",
        "updated_at": "2025-03-29T22:42:44+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2503.12266": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.12266",
        "url": "https://arxiv.org/abs/2503.12266",
        "title": "Support Collapse of Deep Gaussian Processes with Polynomial Kernels for\n  a Wide Regime of Hyperparameters",
        "authors": "Daryna Chernobrovkina, Steffen Gr\u00fcnew\u00e4lder",
        "abstract": "We analyze the prior that a Deep Gaussian Process with polynomial kernels\ninduces. We observe that, even for relatively small depths, averaging effects\noccur within such a Deep Gaussian Process and that the prior can be analyzed\nand approximated effectively by means of the Berry-Esseen Theorem. One of the\nkey findings of this analysis is that, in the absence of careful\nhyper-parameter tuning, the prior of a Deep Gaussian Process either collapses\nrapidly towards zero as the depth increases or places negligible mass on low\nnorm functions. This aligns well with experimental findings and mirrors known\nresults for convolution based Deep Gaussian Processes.",
        "timestamp": "2025-03-18T06:31:34.829Z",
        "rating": "novote",
        "publishedDate": "2025-03-15T21:29:45Z",
        "tags": [
          "stat.ML",
          "cs.LG"
        ],
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-18T06:31:35+00:00",
        "updated_at": "2025-03-29T22:42:42+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.12266": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.12266",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T06:32:07.343Z",
            "data": {
              "session_id": "session_1742279509249_ngwxxye",
              "paper_id": "2503.12266",
              "source_id": "arxiv",
              "start_time": "2025-03-18T06:31:49.249Z",
              "end_time": "2025-03-18T06:32:07.343Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 3,
              "total_elapsed_seconds": 18
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-18T06:31:34+00:00",
        "updated_at": "2025-03-29T22:42:43+00:00",
        "version": 5
      }
    },
    "paper:arxiv.arxiv.2503.12266": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "arxiv.2503.12266",
        "url": "https://arxiv.org/abs/2503.12266",
        "title": "Support Collapse of Deep Gaussian Processes with Polynomial Kernels for a Wide Regime of Hyperparameters",
        "authors": "Daryna Chernobrovkina, Steffen Gr\u00fcnew\u00e4lder",
        "abstract": "We analyze the prior that a Deep Gaussian Process with polynomial kernels induces. We observe that, even for relatively small depths, averaging effects occur within such a Deep Gaussian Process and that the prior can be analyzed and approximated effectively by means of the Berry-Esseen Theorem. One of the key findings of this analysis is that, in the absence of careful hyper-parameter tuning, the prior of a Deep Gaussian Process either collapses rapidly towards zero as the depth increases or places negligible mass on low norm functions. This aligns well with experimental findings and mirrors known results for convolution based Deep Gaussian Processes.",
        "timestamp": "2025-03-18T06:30:18.669Z",
        "rating": "novote",
        "publishedDate": "[Submitted on 15 Mar 2025]",
        "tags": [
          "Machine Learning (stat.ML)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-18T06:30:19+00:00",
        "updated_at": "2025-03-29T22:42:43+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.11990": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.11990",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T06:44:10.128Z",
            "data": {
              "session_id": "session_1742280238864_05injeg",
              "paper_id": "2503.11990",
              "source_id": "arxiv",
              "start_time": "2025-03-18T06:43:58.864Z",
              "end_time": "2025-03-18T06:44:10.127Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-18T06:44:11+00:00",
        "updated_at": "2025-03-29T22:42:42+00:00",
        "version": 3
      }
    },
    "paper:arxiv.arxiv.2503.11990": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "arxiv.2503.11990",
        "url": "https://arxiv.org/abs/2503.11990",
        "title": "Testing Stochastic Block Models Based on Maximum Sampling Entry-Wise Deviations",
        "authors": "Wu, Yujia, Lan, Wei, Feng, Long, Tsai, Chih-Ling",
        "abstract": "The stochastic block model (SBM) has been widely used to analyze network data. Various goodness-of-fit tests have been proposed to assess the adequacy of model structures. To the best of our knowledge, however, none of the existing approaches are applicable for sparse networks in which the connection probability of any two communities is of order log n/n, and the number of communities is divergent. To fill this gap, we propose a novel goodness-of-fit test for the stochastic block model. The key idea is to construct statistics by sampling the maximum entry-deviations of the adjacency matrix that the negative impacts of network sparsity are alleviated by the sampling process. We demonstrate theoretically that the proposed test statistic converges to the Type-I extreme value distribution under the null hypothesis regardless of the network structure. Accordingly, it can be applied to both dense and sparse networks. In addition, we obtain the asymptotic power against alternatives. Moreover, we introduce a bootstrap-corrected test statistic to improve the finite sample performance, recommend an augmented test statistic to increase the power, and extend the proposed test to the degree-corrected SBM. Simulation studies and two empirical examples with both dense and sparse networks indicate that the proposed method performs well.",
        "timestamp": "2025-03-18T06:43:58.489Z",
        "rating": "novote",
        "publishedDate": "2025/03/15",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-18T06:43:59+00:00",
        "updated_at": "2025-03-29T22:42:42+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2502.08497": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.08497",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T07:06:40.623Z",
            "data": {
              "session_id": "session_1742281563647_lz8ru34",
              "paper_id": "2502.08497",
              "source_id": "arxiv",
              "start_time": "2025-03-18T07:06:03.647Z",
              "end_time": "2025-03-18T07:06:40.623Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 2,
              "total_elapsed_seconds": 37
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-18T07:06:41+00:00",
        "updated_at": "2025-03-29T22:42:39+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2502.08497": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.08497",
        "url": "https://arxiv.org/abs/2502.08497",
        "title": "Foundations of Digital Circuits: Denotation, Operational, and Algebraic Semantics",
        "authors": "Kaye, George",
        "abstract": "This thesis details a project to define a fully compositional theory of synchronous sequential circuits built from primitive components, motivated by applying techniques successfully used in programming languages to hardware. The first part of the thesis defines the syntactic foundations of sequential circuit morphisms, and then builds three different semantic theories: denotational, operational and algebraic. We characterise the denotational semantics of sequential circuits as certain causal stream functions, as well as providing a link to existing circuit methodologies by mapping between circuit morphisms, stream functions and Mealy machines. The operational semantics is defined as a strategy for applying some global transformations followed by local reductions to demonstrate how a circuit processes a value, leading to a notion of observational equivalence. The algebraic semantics consists of equations for bringing circuits into a pseudo-normal form, and then encoding between different state sets. This part of the thesis concludes with a discussion of some novel applications, such as those for using partial evaluation for digital circuits. While mathematically rigorous, the categorical string diagram formalism is not suited for reasoning computationally. The second part of this thesis details an extension of string diagram rewriting with hypergraphs so that it is compatible with the traced comonoid structure present in the category of digital circuits. We identify the properties that characterise cospans of hypergraphs corresponding to traced comonoid terms, and demonstrate how to identify rewriting contexts valid for rewriting modulo traced comonoid structure. We apply the graph rewriting framework to fixed point operators as well as the operational semantics from the first part, and present a new hardware description language based on these theoretical developments.",
        "timestamp": "2025-03-18T07:06:04.236Z",
        "rating": "novote",
        "publishedDate": "2025/02/12",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-18T07:06:04+00:00",
        "updated_at": "2025-03-29T22:42:40+00:00",
        "version": 2
      }
    },
    "paper:url.url.1DA35195": {
      "data": {
        "sourceId": "url",
        "paperId": "url.1DA35195",
        "url": "https://tiger-ai-lab.github.io/Vamba/",
        "title": "Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers",
        "authors": "",
        "abstract": "We propose Vamba, a hybrid Mamba-Transformer model that leverages cross-attention layers and Mamba-2 blocks for efficient hour-long video understanding.",
        "timestamp": "2025-03-18T06:52:42.223Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "vamba"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-18T06:52:42+00:00",
        "updated_at": "2025-03-29T22:42:41+00:00",
        "version": 2
      }
    },
    "paper:arxiv.arxiv.2502.08497": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "arxiv.2502.08497",
        "url": "https://arxiv.org/abs/2502.08497",
        "title": "Foundations of Digital Circuits: Denotation, Operational, and Algebraic Semantics",
        "authors": "Kaye, George",
        "abstract": "This thesis details a project to define a fully compositional theory of synchronous sequential circuits built from primitive components, motivated by applying techniques successfully used in programming languages to hardware. The first part of the thesis defines the syntactic foundations of sequential circuit morphisms, and then builds three different semantic theories: denotational, operational and algebraic. We characterise the denotational semantics of sequential circuits as certain causal stream functions, as well as providing a link to existing circuit methodologies by mapping between circuit morphisms, stream functions and Mealy machines. The operational semantics is defined as a strategy for applying some global transformations followed by local reductions to demonstrate how a circuit processes a value, leading to a notion of observational equivalence. The algebraic semantics consists of equations for bringing circuits into a pseudo-normal form, and then encoding between different state sets. This part of the thesis concludes with a discussion of some novel applications, such as those for using partial evaluation for digital circuits. While mathematically rigorous, the categorical string diagram formalism is not suited for reasoning computationally. The second part of this thesis details an extension of string diagram rewriting with hypergraphs so that it is compatible with the traced comonoid structure present in the category of digital circuits. We identify the properties that characterise cospans of hypergraphs corresponding to traced comonoid terms, and demonstrate how to identify rewriting contexts valid for rewriting modulo traced comonoid structure. We apply the graph rewriting framework to fixed point operators as well as the operational semantics from the first part, and present a new hardware description language based on these theoretical developments.",
        "timestamp": "2025-03-18T06:50:55.071Z",
        "rating": "novote",
        "publishedDate": "2025/02/12",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-18T06:50:55+00:00",
        "updated_at": "2025-03-29T22:42:41+00:00",
        "version": 2
      }
    },
    "interactions:url.19046E18": {
      "data": {
        "sourceId": "url",
        "paperId": "19046E18",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T07:13:38.155Z",
            "data": {
              "session_id": "session_1742281747946_h93ymgc",
              "paper_id": "19046E18",
              "source_id": "url",
              "start_time": "2025-03-18T07:09:07.946Z",
              "end_time": "2025-03-18T07:13:38.154Z",
              "heartbeat_count": 51,
              "duration_seconds": 255,
              "idle_seconds": 15,
              "total_elapsed_seconds": 270
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-18T07:13:39+00:00",
        "updated_at": "2025-03-29T22:42:38+00:00",
        "version": 3
      }
    },
    "paper:url.19046E18": {
      "data": {
        "sourceId": "url",
        "paperId": "19046E18",
        "url": "https://dapo-sia.github.io/static/pdf/dapo_paper.pdf",
        "title": "19046E18",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-18T07:09:08.556Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "created_at": "2025-03-18T07:09:09+00:00",
        "updated_at": "2025-03-29T22:42:39+00:00",
        "version": 2
      }
    },
    "interactions:url.4516794": {
      "data": {
        "sourceId": "url",
        "paperId": "4516794",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T14:56:57.547Z",
            "data": {
              "session_id": "session_1742309802539_nm257ad",
              "paper_id": "4516794",
              "source_id": "url",
              "start_time": "2025-03-18T14:56:42.539Z",
              "end_time": "2025-03-18T14:56:57.546Z",
              "heartbeat_count": 0,
              "duration_seconds": 0,
              "idle_seconds": 15,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-18T14:56:58+00:00",
        "updated_at": "2025-03-29T22:42:37+00:00",
        "version": 3
      }
    },
    "paper:pdf.728CBF81": {
      "data": {
        "sourceId": "pdf",
        "paperId": "728CBF81",
        "url": "https://papers.nips.cc/paper_files/paper/2014/file/b78666971ceae55a8e87efb7cbfd9ad4-Paper.pdf",
        "title": "728CBF81",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-18T14:56:53.952Z",
        "publishedDate": "",
        "tags": [],
        "rating": "novote"
      },
      "meta": {
        "created_at": "2025-03-18T14:56:54+00:00",
        "updated_at": "2025-03-29T22:42:37+00:00",
        "version": 2
      }
    },
    "paper:url.4516794": {
      "data": {
        "sourceId": "url",
        "paperId": "4516794",
        "url": "https://papers.nips.cc/paper_files/paper/2014/hash/b78666971ceae55a8e87efb7cbfd9ad4-Abstract.html",
        "title": "Neural Word Embedding as Implicit Matrix Factorization",
        "authors": "Levy, Omer, Goldberg, Yoav",
        "abstract": "",
        "timestamp": "2025-03-18T14:56:38.611Z",
        "publishedDate": "",
        "tags": [],
        "rating": "novote"
      },
      "meta": {
        "created_at": "2025-03-18T14:56:39+00:00",
        "updated_at": "2025-03-29T22:42:38+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2501.04896": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.04896",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-03-18T23:14:21+00:00",
        "updated_at": "2025-03-29T22:42:36+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2501.04896": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.04896",
        "url": "https://arxiv.org/pdf/2501.04896",
        "title": "2501.04896",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-18T23:13:55.508Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-18T23:13:56+00:00",
        "updated_at": "2025-03-29T22:42:36+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.11718": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.11718",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T23:18:13.959Z",
            "data": {
              "session_id": "session_1742339841311_ep2xuu6",
              "paper_id": "2503.11718",
              "source_id": "arxiv",
              "start_time": "2025-03-18T23:17:21.311Z",
              "end_time": "2025-03-18T23:18:13.959Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 3,
              "total_elapsed_seconds": 53
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-18T23:18:14+00:00",
        "updated_at": "2025-03-29T22:42:34+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.11718": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.11718",
        "url": "https://arxiv.org/abs/2503.11718",
        "title": "The Relativity of Causal Knowledge",
        "authors": "D'Acunto, Gabriele, Battiloro, Claudio",
        "abstract": "Recent advances in artificial intelligence reveal the limits of purely predictive systems and call for a shift toward causal and collaborative reasoning. Drawing inspiration from the revolution of Grothendieck in mathematics, we introduce the relativity of causal knowledge, which posits structural causal models (SCMs) are inherently imperfect, subjective representations embedded within networks of relationships. By leveraging category theory, we arrange SCMs into a functor category and show that their observational and interventional probability measures naturally form convex structures. This result allows us to encode non-intervened SCMs with convex spaces of probability measures. Next, using sheaf theory, we construct the network sheaf and cosheaf of causal knowledge. These structures enable the transfer of causal knowledge across the network while incorporating interventional consistency and the perspective of the subjects, ultimately leading to the formal, mathematical definition of relative causal knowledge.",
        "timestamp": "2025-03-18T23:17:21.783Z",
        "rating": "novote",
        "publishedDate": "2025/03/13",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-18T23:17:22+00:00",
        "updated_at": "2025-03-29T22:42:34+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.13045": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.13045",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-03-18T23:32:03+00:00",
        "updated_at": "2025-03-29T22:42:32+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.13045": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.13045",
        "url": "https://arxiv.org/abs/2503.13045",
        "title": "All You Need to Know About Training Image Retrieval Models",
        "authors": "Berton, Gabriele, Musgrave, Kevin, Masone, Carlo",
        "abstract": "Image retrieval is the task of finding images in a database that are most similar to a given query image. The performance of an image retrieval pipeline depends on many training-time factors, including the embedding model architecture, loss function, data sampler, mining function, learning rate(s), and batch size. In this work, we run tens of thousands of training runs to understand the effect each of these factors has on retrieval accuracy. We also discover best practices that hold across multiple datasets. The code is available at https://github.com/gmberton/image-retrieval",
        "timestamp": "2025-03-18T23:31:55.427Z",
        "rating": "novote",
        "publishedDate": "2025/03/17",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-18T23:31:55+00:00",
        "updated_at": "2025-03-29T22:42:33+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.09573": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.09573",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T23:55:50.615Z",
            "data": {
              "session_id": "session_1742342149825_gcfove2",
              "source_id": "arxiv",
              "paper_id": "2503.09573",
              "start_time": "2025-03-18T23:55:43.809Z",
              "end_time": "2025-03-18T23:55:49.825Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-18T23:55:51+00:00",
        "updated_at": "2025-03-29T22:42:31+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.09573": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.09573",
        "url": "https://arxiv.org/abs/2503.09573",
        "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models",
        "authors": "Arriola, Marianne, Gokaslan, Aaron, Chiu, Justin T, Yang, Zhihan, Qi, Zhixuan, Han, Jiaqi, Sahoo, Subham Sekhar, Kuleshov, Volodymyr",
        "abstract": "Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences. We provide the code, along with the model weights and blog post on the project page: https://m-arriola.com/bd3lms/",
        "timestamp": "2025-03-18T23:55:44.187Z",
        "rating": "novote",
        "publishedDate": "2025/03/12",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-18T23:55:44+00:00",
        "updated_at": "2025-03-29T22:42:31+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2409.17088": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2409.17088",
        "url": "https://arxiv.org/abs/2409.17088",
        "title": "Textoshop: Interactions Inspired by Drawing Software to Facilitate Text Editing",
        "authors": "Masson, Damien, Kim, Young-Ho, Chevalier, Fanny",
        "abstract": "We explore how interactions inspired by drawing software can help edit text. Making an analogy between visual and text editing, we consider words as pixels, sentences as regions, and tones as colours. For instance, direct manipulations move, shorten, expand, and reorder text; tools change number, tense, and grammar; colours map to tones explored along three dimensions in a tone picker; and layers help organize and version text. This analogy also leads to new workflows, such as boolean operations on text fragments to construct more elaborated text. A study shows participants were more successful at editing text and preferred using the proposed interface over existing solutions. Broadly, our work highlights the potential of interaction analogies to rethink existing workflows, while capitalizing on familiar features.",
        "timestamp": "2025-03-18T23:55:20.986Z",
        "rating": "novote",
        "publishedDate": "2024/09/25",
        "tags": [],
        "doi": "10.1145/3706598.3713862",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-18T23:55:21+00:00",
        "updated_at": "2025-03-29T22:42:32+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2409.17088": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2409.17088",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-18T23:56:31.762Z",
            "data": {
              "session_id": "session_1742342191030_hhr54p8",
              "source_id": "arxiv",
              "paper_id": "2409.17088",
              "start_time": "2025-03-18T23:56:21.741Z",
              "end_time": "2025-03-18T23:56:31.030Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-19T00:12:25.109Z",
            "data": {
              "session_id": "session_1742343144373_q00nicb",
              "source_id": "arxiv",
              "paper_id": "2409.17088",
              "start_time": "2025-03-19T00:12:07.905Z",
              "end_time": "2025-03-19T00:12:24.373Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 1,
              "total_elapsed_seconds": 16
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-19T00:13:39.093Z",
            "data": {
              "session_id": "session_1742343218000_xhgcnex",
              "source_id": "arxiv",
              "paper_id": "2409.17088",
              "start_time": "2025-03-19T00:12:44.455Z",
              "end_time": "2025-03-19T00:13:38.000Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 4,
              "total_elapsed_seconds": 54
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-19T21:37:04.528Z",
            "data": {
              "session_id": "session_1742420223730_t954awc",
              "source_id": "arxiv",
              "paper_id": "2409.17088",
              "start_time": "2025-03-19T21:36:56.184Z",
              "end_time": "2025-03-19T21:37:03.730Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-18T23:56:32+00:00",
        "updated_at": "2025-03-29T22:42:30+00:00",
        "version": 7
      }
    },
    "interactions:arxiv.2410.05669": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.05669",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-19T00:27:38.941Z",
            "data": {
              "session_id": "session_1742344058134_nyygjsx",
              "source_id": "arxiv",
              "paper_id": "2410.05669",
              "start_time": "2025-03-19T00:27:26.322Z",
              "end_time": "2025-03-19T00:27:38.134Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-19T00:27:39+00:00",
        "updated_at": "2025-03-29T22:42:29+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2410.05669": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.05669",
        "url": "https://arxiv.org/abs/2410.05669",
        "title": "ACPBench: Reasoning about Action, Change, and Planning",
        "authors": "Kokel, Harsha, Katz, Michael, Srinivas, Kavitha, Sohrabi, Shirin",
        "abstract": "There is an increasing body of work using Large Language Models (LLMs) as agents for orchestrating workflows and making decisions in domains that require planning and multi-step reasoning. As a result, it is imperative to evaluate LLMs on core skills required for planning. In this work, we present ACPBench, a benchmark for evaluating the reasoning tasks in the field of planning. The benchmark consists of 7 reasoning tasks over 13 planning domains. The collection is constructed from planning domains described in a formal language. This allows us to synthesize problems with provably correct solutions across many tasks and domains. Further, it allows us the luxury of scale without additional human effort, i.e., many additional problems can be created automatically. Our extensive evaluation of 22 LLMs and OpenAI o1 reasoning models highlights the significant gap in the reasoning capability of the LLMs. Our findings with OpenAI o1, a multi-turn reasoning model, reveal significant gains in performance on multiple-choice questions, yet surprisingly, no notable progress is made on boolean questions. The ACPBench collection is available at https://ibm.github.io/ACPBench.",
        "timestamp": "2025-03-19T00:27:25.482Z",
        "rating": "novote",
        "publishedDate": "2024/10/08",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-19T00:27:26+00:00",
        "updated_at": "2025-03-29T22:42:30+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.13630": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.13630",
        "url": "https://arxiv.org/abs/2503.13630",
        "title": "The Birth of Quantum Mechanics: A Historical Study Through the Canonical Papers",
        "authors": "K\u00fc\u00e7\u00fck, Eren Volkan",
        "abstract": "This paper explores the historical development of the theory of quantum mechanics between 1900 and 1927 by chronological examination of the foundational papers and ideas. Beginning with Planck's introduction of energy quantisation in blackbody radiation, we follow the emergence of Einstein's light quanta hypothesis, Bohr's atomic model, and the statistical implications of indistinguishable particles. Special emphasis is placed on the transition from the Old Quantum Theory to modern quantum mechanics, particularly through Heisenberg's matrix mechanics and Schr\\\"{o}dinger's wave mechanics. This study aims to provide a structured historical account, offering insights into the conceptual transformations that led to quantum mechanics while making the development accessible to physicists, historians of science, and advanced students interested in the origins of modern quantum theory.",
        "timestamp": "2025-03-19T03:42:17.894Z",
        "rating": "novote",
        "publishedDate": "2025/03/17",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-19T03:42:18+00:00",
        "updated_at": "2025-03-29T22:42:28+00:00",
        "version": 2
      }
    },
    "interactions:url.2278B45": {
      "data": {
        "sourceId": "url",
        "paperId": "2278B45",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-19T00:43:49.012Z",
            "data": {
              "session_id": "session_1742345028264_t5bp8qp",
              "source_id": "url",
              "paper_id": "2278B45",
              "start_time": "2025-03-19T00:43:41.558Z",
              "end_time": "2025-03-19T00:43:48.264Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-19T00:43:43+00:00",
        "updated_at": "2025-03-29T22:42:28+00:00",
        "version": 4
      }
    },
    "paper:url.2278B45": {
      "data": {
        "sourceId": "url",
        "paperId": "2278B45",
        "url": "https://openreview.net/forum?id=8EtSBX41mt",
        "title": "Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?",
        "authors": "Egor Zverev, Sahar Abdelnabi, Soroush Tabesh, Mario Fritz, Christoph H. Lampert",
        "abstract": "Large Language Models (LLMs) show impressive results in numerous practical applications, but they lack essential safety features that are common in other areas of computer science, particularly an explicit separation of instructions and data. This makes them vulnerable to manipulations such as indirect prompt injections and generally unsuitable for safety-critical tasks. Surprisingly, there is currently no established definition or benchmark to quantify this phenomenon. In this work, we close this gap by introducing a formal measure for instruction-data separation for single-turn language models and an empirical variant that is calculable from a model\u2019s outputs. We also present a new dataset, SEP, that allows estimating the measure for real-world models. Our results on various LLMs show that the problem of instruction-data separation is real: all models fail to achieve high separation, and canonical mitigation techniques, such as prompt engineering and fine-tuning, either fail to substantially improve separation or reduce model utility.",
        "timestamp": "2025-03-19T00:43:35.754Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-19T00:43:36+00:00",
        "updated_at": "2025-03-29T22:42:28+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.13630": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.13630",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-19T03:42:51.561Z",
            "data": {
              "session_id": "session_1742355770858_tie3xb4",
              "source_id": "arxiv",
              "paper_id": "2503.13630",
              "start_time": "2025-03-19T03:42:20.702Z",
              "end_time": "2025-03-19T03:42:50.858Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 0,
              "total_elapsed_seconds": 30
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-19T03:44:17.958Z",
            "data": {
              "session_id": "session_1742355857065_zdbi62o",
              "source_id": "arxiv",
              "paper_id": "2503.13630",
              "start_time": "2025-03-19T03:42:50.859Z",
              "end_time": "2025-03-19T03:44:17.065Z",
              "heartbeat_count": 17,
              "duration_seconds": 85,
              "idle_seconds": 1,
              "total_elapsed_seconds": 86
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-19T03:50:55.472Z",
            "data": {
              "session_id": "session_1742356254742_wyi7afr",
              "source_id": "arxiv",
              "paper_id": "2503.13630",
              "start_time": "2025-03-19T03:50:22.311Z",
              "end_time": "2025-03-19T03:50:54.742Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 2,
              "total_elapsed_seconds": 32
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-19T03:52:08.418Z",
            "data": {
              "session_id": "session_1742356327535_zagw53n",
              "source_id": "arxiv",
              "paper_id": "2503.13630",
              "start_time": "2025-03-19T03:51:16.120Z",
              "end_time": "2025-03-19T03:52:07.535Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 1,
              "total_elapsed_seconds": 51
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-19T03:42:52+00:00",
        "updated_at": "2025-03-29T22:42:27+00:00",
        "version": 7
      }
    },
    "interactions:arxiv.2502.15680": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.15680",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-19T04:39:12.750Z",
            "data": {
              "session_id": "session_1742359152016_v5p7z4h",
              "source_id": "arxiv",
              "paper_id": "2502.15680",
              "start_time": "2025-03-19T04:38:12.465Z",
              "end_time": "2025-03-19T04:39:12.016Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 5,
              "total_elapsed_seconds": 60
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-19T04:41:11.802Z",
            "data": {
              "session_id": "session_1742359270977_n7aub71",
              "source_id": "arxiv",
              "paper_id": "2502.15680",
              "start_time": "2025-03-19T04:40:30.442Z",
              "end_time": "2025-03-19T04:41:10.977Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 1,
              "total_elapsed_seconds": 41
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-19T04:45:44.204Z",
            "data": {
              "session_id": "session_1742359543210_svizwws",
              "source_id": "arxiv",
              "paper_id": "2502.15680",
              "start_time": "2025-03-19T04:44:14.200Z",
              "end_time": "2025-03-19T04:45:43.210Z",
              "heartbeat_count": 17,
              "duration_seconds": 85,
              "idle_seconds": 4,
              "total_elapsed_seconds": 89
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-19T04:38:11+00:00",
        "updated_at": "2025-03-29T22:42:26+00:00",
        "version": 6
      }
    },
    "paper:arxiv.2502.15680": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.15680",
        "url": "https://arxiv.org/abs/2502.15680",
        "title": "Privacy Ripple Effects from Adding or Removing Personal Information in Language Model Training",
        "authors": "Borkar, Jaydeep, Jagielski, Matthew, Lee, Katherine, Mireshghallah, Niloofar, Smith, David A., Choquette-Choo, Christopher A.",
        "abstract": "Due to the sensitive nature of personally identifiable information (PII), its owners may have the authority to control its inclusion or request its removal from large-language model (LLM) training. Beyond this, PII may be added or removed from training datasets due to evolving dataset curation techniques, because they were newly scraped for retraining, or because they were included in a new downstream fine-tuning stage. We find that the amount and ease of PII memorization is a dynamic property of a model that evolves throughout training pipelines and depends on commonly altered design choices. We characterize three such novel phenomena: (1) similar-appearing PII seen later in training can elicit memorization of earlier-seen sequences in what we call assisted memorization, and this is a significant factor (in our settings, up to 1/3); (2) adding PII can increase memorization of other PII significantly (in our settings, as much as $\\approx\\!7.5\\times$); and (3) removing PII can lead to other PII being memorized. Model creators should consider these first- and second-order privacy risks when training models to avoid the risk of new PII regurgitation.",
        "timestamp": "2025-03-19T04:37:20.155Z",
        "rating": "novote",
        "publishedDate": "2025/02/21",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-19T04:37:20+00:00",
        "updated_at": "2025-03-29T22:42:27+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.12743": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.12743",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-19T05:47:24.639Z",
            "data": {
              "session_id": "session_1742363243775_qlzojxc",
              "source_id": "arxiv",
              "paper_id": "2503.12743",
              "start_time": "2025-03-19T05:46:45.154Z",
              "end_time": "2025-03-19T05:47:23.775Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 4,
              "total_elapsed_seconds": 39
            }
          },
          {
            "type": "rating",
            "timestamp": "2025-03-19T07:42:24.821Z",
            "data": {
              "rating": "thumbsdown"
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-19T05:47:25+00:00",
        "updated_at": "2025-03-29T22:42:25+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2503.12743": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.12743",
        "url": "https://arxiv.org/abs/2503.12743",
        "title": "Cancermorphic Computing Toward Multilevel Machine Intelligence",
        "authors": "Moreddu, Rosalia, Levin, Michael",
        "abstract": "Despite their potential to address crucial bottlenecks in computing architectures and contribute to the pool of biological inspiration for engineering, pathological biological mechanisms remain absent from computational theory. We hereby introduce the concept of cancer-inspired computing as a paradigm drawing from the adaptive, resilient, and evolutionary strategies of cancer, for designing computational systems capable of thriving in dynamic, adversarial or resource-constrained environments. Unlike known bioinspired approaches (e.g., evolutionary and neuromorphic architectures), cancer-inspired computing looks at emulating the uniqueness of cancer cells survival tactics, such as somatic mutation, metastasis, angiogenesis and immune evasion, as parallels to desirable features in computing architectures, for example decentralized propagation and resource optimization, to impact areas like fault tolerance and cybersecurity. While the chaotic growth of cancer is currently viewed as uncontrollable in biology, randomness-based algorithms are already being successfully demonstrated in enhancing the capabilities of other computing architectures, for example chaos computing integration. This vision focuses on the concepts of multilevel intelligence and context-driven mutation, and their potential to simultaneously overcome plasticity-limited neuromorphic approaches and the randomness of chaotic approaches. The introduction of this concept aims to generate interdisciplinary discussion to explore the potential of cancer-inspired mechanisms toward powerful and resilient artificial systems.",
        "timestamp": "2025-03-19T05:46:45.736Z",
        "rating": "thumbsdown",
        "publishedDate": "2025/03/17",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-19T05:46:46+00:00",
        "updated_at": "2025-03-29T22:42:25+00:00",
        "version": 3
      }
    },
    "interactions:arxiv.2503.14456": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.14456",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-19T06:00:53.342Z",
            "data": {
              "session_id": "session_1742364052530_ocl1h3n",
              "source_id": "arxiv",
              "paper_id": "2503.14456",
              "start_time": "2025-03-19T05:59:33.097Z",
              "end_time": "2025-03-19T06:00:52.530Z",
              "heartbeat_count": 15,
              "duration_seconds": 75,
              "idle_seconds": 4,
              "total_elapsed_seconds": 79
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-19T06:16:39.357Z",
            "data": {
              "session_id": "session_1742364998634_d3u092s",
              "source_id": "arxiv",
              "paper_id": "2503.14456",
              "start_time": "2025-03-19T06:16:08.622Z",
              "end_time": "2025-03-19T06:16:38.634Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 15,
              "total_elapsed_seconds": 30
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-19T06:00:54+00:00",
        "updated_at": "2025-03-29T22:42:24+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2503.14456": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.14456",
        "url": "https://arxiv.org/abs/2503.14456",
        "title": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution",
        "authors": "Peng, Bo, Zhang, Ruichong, Goldstein, Daniel, Alcaide, Eric, Hou, Haowen, Lu, Janna, Merrill, William, Song, Guangyu, Tan, Kaifeng, Utpala, Saiteja, Wilce, Nathan, Wind, Johan S., Wu, Tianyi, Wuttke, Daniel, Zhou-Zheng, Christian",
        "abstract": "We present RWKV-7 \"Goose\", a new sequence modeling architecture, along with pre-trained language models that establish a new state-of-the-art in downstream performance at the 3 billion parameter scale on multilingual tasks, and match current SoTA English language performance despite being trained on dramatically fewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only constant memory usage and constant inference time per token. RWKV-7 introduces a newly generalized formulation of the delta rule with vector-valued gating and in-context learning rates, as well as a relaxed value replacement rule. We show that RWKV-7 can perform state tracking and recognize all regular languages, while retaining parallelizability of training. This exceeds the capabilities of Transformers under standard complexity conjectures, which are limited to $\\mathsf{TC}^0$. To demonstrate RWKV-7's language modeling capability, we also present an extended open source 3.1 trillion token multilingual corpus, and train four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on this dataset. To foster openness, reproduction, and adoption, we release our models and dataset component listing at https://huggingface.co/RWKV, and our training and inference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0 License.",
        "timestamp": "2025-03-19T05:59:22.018Z",
        "rating": "novote",
        "publishedDate": "2025/03/18",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-19T05:59:22+00:00",
        "updated_at": "2025-03-29T22:42:24+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.14378": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.14378",
        "url": "https://arxiv.org/abs/2503.14378",
        "title": "Impossible Videos",
        "authors": "Bai, Zechen, Ci, Hai, Shou, Mike Zheng",
        "abstract": "Synthetic videos nowadays is widely used to complement data scarcity and diversity of real-world videos. Current synthetic datasets primarily replicate real-world scenarios, leaving impossible, counterfactual and anti-reality video concepts underexplored. This work aims to answer two questions: 1) Can today's video generation models effectively follow prompts to create impossible video content? 2) Are today's video understanding models good enough for understanding impossible videos? To this end, we introduce IPV-Bench, a novel benchmark designed to evaluate and foster progress in video understanding and generation. IPV-Bench is underpinned by a comprehensive taxonomy, encompassing 4 domains, 14 categories. It features diverse scenes that defy physical, biological, geographical, or social laws. Based on the taxonomy, a prompt suite is constructed to evaluate video generation models, challenging their prompt following and creativity capabilities. In addition, a video benchmark is curated to assess Video-LLMs on their ability of understanding impossible videos, which particularly requires reasoning on temporal dynamics and world knowledge. Comprehensive evaluations reveal limitations and insights for future directions of video models, paving the way for next-generation video models.",
        "timestamp": "2025-03-19T14:11:55.751Z",
        "rating": "novote",
        "publishedDate": "2025/03/18",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-19T14:11:56+00:00",
        "updated_at": "2025-03-29T22:42:23+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.07395": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.07395",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-20T15:17:08.340Z",
            "data": {
              "session_id": "session_1742483827296_o0t2wvk",
              "source_id": "arxiv",
              "paper_id": "2503.07395",
              "start_time": "2025-03-20T15:16:17.140Z",
              "end_time": "2025-03-20T15:17:07.296Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 0,
              "total_elapsed_seconds": 50
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-20T15:17:09+00:00",
        "updated_at": "2025-03-29T22:42:22+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.07395": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.07395",
        "url": "https://arxiv.org/abs/2503.07395",
        "title": "Revisiting Noise in Natural Language Processing for Computational Social Science",
        "authors": "Borenstein, Nadav",
        "abstract": "Computational Social Science (CSS) is an emerging field driven by the unprecedented availability of human-generated content for researchers. This field, however, presents a unique set of challenges due to the nature of the theories and datasets it explores, including highly subjective tasks and complex, unstructured textual corpora. Among these challenges, one of the less well-studied topics is the pervasive presence of noise. This thesis aims to address this gap in the literature by presenting a series of interconnected case studies that examine different manifestations of noise in CSS. These include character-level errors following the OCR processing of historical records, archaic language, inconsistencies in annotations for subjective and ambiguous tasks, and even noise and biases introduced by large language models during content generation. This thesis challenges the conventional notion that noise in CSS is inherently harmful or useless. Rather, it argues that certain forms of noise can encode meaningful information that is invaluable for advancing CSS research, such as the unique communication styles of individuals or the culture-dependent nature of datasets and tasks. Further, this thesis highlights the importance of nuance in dealing with noise and the considerations CSS researchers must address when encountering it, demonstrating that different types of noise require distinct strategies.",
        "timestamp": "2025-03-20T15:16:17.949Z",
        "rating": "novote",
        "publishedDate": "2025/03/10",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-20T15:16:18+00:00",
        "updated_at": "2025-03-29T22:42:22+00:00",
        "version": 2
      }
    },
    "interactions:url.2C817B24": {
      "data": {
        "sourceId": "url",
        "paperId": "2C817B24",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-20T23:36:34.466Z",
            "data": {
              "session_id": "session_1742513794421_268inl5",
              "source_id": "url",
              "paper_id": "2C817B24",
              "start_time": "2025-03-20T23:36:26.972Z",
              "end_time": "2025-03-20T23:36:34.420Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-20T23:36:28+00:00",
        "updated_at": "2025-03-29T22:42:21+00:00",
        "version": 4
      }
    },
    "paper:url.2C817B24": {
      "data": {
        "sourceId": "url",
        "paperId": "2C817B24",
        "url": "https://www.science.org/doi/10.1126/science.adq3278",
        "title": "Macroecological rules predict how biomass scales with species richness in nature",
        "authors": "",
        "abstract": "Despite advances in theory and experiments, how biodiversity influences the structure and functioning of natural ecosystems remains debated. By applying new theory to data on 84,695 plant, animal, and protist assemblages, we show that the general ...",
        "timestamp": "2025-03-20T23:36:21.915Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "Science",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-20T23:36:22+00:00",
        "updated_at": "2025-03-29T22:42:22+00:00",
        "version": 2
      }
    },
    "interactions:url.6EDD8BE4": {
      "data": {
        "sourceId": "url",
        "paperId": "6EDD8BE4",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-21T06:05:43.477Z",
            "data": {
              "session_id": "session_1742537142752_b84ohlf",
              "source_id": "url",
              "paper_id": "6EDD8BE4",
              "start_time": "2025-03-21T06:05:26.185Z",
              "end_time": "2025-03-21T06:05:42.752Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 2,
              "total_elapsed_seconds": 17
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-21T06:26:53.195Z",
            "data": {
              "session_id": "session_1742538412986_gucjmid",
              "source_id": "url",
              "paper_id": "6EDD8BE4",
              "start_time": "2025-03-21T06:26:32.975Z",
              "end_time": "2025-03-21T06:26:52.986Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 15,
              "total_elapsed_seconds": 20
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-21T06:05:44+00:00",
        "updated_at": "2025-03-29T22:42:19+00:00",
        "version": 4
      }
    },
    "paper:url.6EDD8BE4": {
      "data": {
        "sourceId": "url",
        "paperId": "6EDD8BE4",
        "url": "https://download.ssrn.com/2025/3/3/5162111.pdf?response-content-disposition=inline&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEEUaCXVzLWVhc3QtMSJHMEUCIDzLwmb6PVen3MKcnBlnghedH%2FzUmeTwvu%2BhdmomgjrbAiEAzVwG0DpJmeKmnjZxwwygpUDlFUSHufuJ4ztZzh%2BKylEqxgUInv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAEGgwzMDg0NzUzMDEyNTciDEKYOUZcL6oC9BzZvSqaBT1mDfQ0txIRVaf8Q0PSlLwdaECT3gc88JPCNCxdAFlF4LWKl7EyLpDLjhvNMtOekrNpaNUitQ05xESZLGgyMG4bck2kmzDKdM%2FFXkrdUbWng2ptbJGZZGBOUMoat2cGUrkws8ePgwHhDzXgrwnfmtbSrEuOD8RgBzmDC%2BiOmSVQn5XW9nuz6cSJiruTSCghARbcuzCjK9u7A4L318vl9hx3nMQOEdh80junCNTiIEPTPItF6rdgOtEM0pXxiJdAo%2Fb4tF0F%2B5uJFp8zOzY0D5lurYhZ58rx9AB7dLe%2BxkekujsqapMDnXyY5cDydVu4QEy0%2Bg4iIMYxFp6Ho5p3h3Ec%2BHSMDfPVowUFFCytAn7Nxvt4wbUQXWKmB5IWZ5tuxGgvNA%2BIrkC0RG6QzR48rQFGoK6IcuwSlb1btuzn6TvJQn%2Fcb17ACZizP5UBGkdffMnkygD9KDn%2FIR9HXxizJTTQShB0YIY5oRYgK1osJ76RfCTPidJcXL9Ke%2FzWkXiodeXsH9lOpcpQ6l2y4yOv0lCCrnuwCgy2YzxfbE%2FqKJW%2FrWLkiAR3Xsx%2FVBnQaGJBRun1VO3MCvgZCojE0Z6tKUBHJV5hWW7EEic8xasue%2FbqiPR%2F0zWBv1%2BGvtAyzkrLVzsb%2FGeT5VC4r9bY87bL74BKfKsfRrcUa4mL%2FPF7qUWNTK3Ex3hS%2FcP0MVJL2UKClkYWI1Eaz4D76U8BBlnlgAWXeuFkpffH4Ev5U4tV3jTc2lIw06k1lPqfFocEiztmD%2BNiboh8CVflLX5sJ%2Fap22aNBecqkFTZ%2BFa3%2FIZEKtMH5MoHmh6qRyv5m9ueWINUj4JKk9CW%2FoDWeDUc%2BvBX1AckRkyR95a9D8XUXMZTMOanlqAKg3zzN7vTTjCM4vO%2BBjqxAQjbN0bbw78RS4Ck58u3wR3b3NDo8Y9Eu0RWzPDpjT0Drq6%2FN6Bl8SAoI%2FEJSJYiXzniAg1k%2ByUVshR7QFq6L2OCNApT9mUFEftr6DI%2B2nCajpawrdDiiqzXreJcpTlQYCWD2YT667yy7z2G5JAmhVT3YTYel%2BU%2BgFgj5I8%2FPuCzlMpGanH8ROuwj1nLfEp7cc5tcSkcR1hLNrYjwA%2F0PotJr4YkPg24lWmrH2ppoazJAw%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250321T060514Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAUPUUPRWEWS6ACKIH%2F20250321%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=334b3a1e11cd3f53bdbb446872fdb37e63dce13866ca62e5fcc6b76e1d9836df&abstractId=5162111",
        "title": "6EDD8BE4",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-21T06:05:26.592Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-21T06:05:27+00:00",
        "updated_at": "2025-03-29T22:42:19+00:00",
        "version": 2
      }
    },
    "interactions:url.18723684": {
      "data": {
        "sourceId": "url",
        "paperId": "18723684",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-21T06:05:12.693Z",
            "data": {
              "session_id": "session_1742537112680_ucpmtkl",
              "source_id": "url",
              "paper_id": "18723684",
              "start_time": "2025-03-21T06:05:03.363Z",
              "end_time": "2025-03-21T06:05:12.680Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-21T06:05:05+00:00",
        "updated_at": "2025-03-29T22:42:20+00:00",
        "version": 4
      }
    },
    "paper:url.18723684": {
      "data": {
        "sourceId": "url",
        "paperId": "18723684",
        "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5162111",
        "title": "AI-Powered Lawyering: AI Reasoning Models, Retrieval Augmented Generation, and the Future of Legal Practice",
        "authors": "Schwarcz, Daniel, Manning, Sam, Barry, Patrick, Cleveland, David R., Prescott, J.J., Rich, Beverly",
        "abstract": "Generative AI is set to transform the legal profession, but its full impact remains uncertain. While AI models like GPT-4 improve the efficiency with which lega",
        "timestamp": "2025-03-21T06:03:32.034Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "SSRN",
          "AI-Powered Lawyering: AI Reasoning Models",
          "Retrieval Augmented Generation",
          "and the Future of Legal Practice",
          "Daniel Schwarcz",
          "Sam Manning",
          "Patrick Barry",
          "David R. Cleveland",
          "J.J. Prescott",
          "Beverly Rich"
        ],
        "doi": "10.2139/ssrn.5162111",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-21T06:03:32+00:00",
        "updated_at": "2025-03-29T22:42:20+00:00",
        "version": 2
      }
    },
    "paper:url.3D6293D0": {
      "data": {
        "sourceId": "url",
        "paperId": "3D6293D0",
        "url": "https://www.jstor.org/stable/2203138",
        "title": "The Emerging Right to Democratic Governance on JSTOR",
        "authors": "",
        "abstract": "Thomas M. Franck, The Emerging Right to Democratic Governance, The American Journal of International Law, Vol. 86, No. 1 (Jan., 1992), pp. 46-91",
        "timestamp": "2025-03-21T00:53:38.157Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-21T00:53:38+00:00",
        "updated_at": "2025-03-29T22:42:21+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.16413": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.16413",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-21T06:53:26.246Z",
            "data": {
              "session_id": "session_1742540004687_tabmdae",
              "source_id": "arxiv",
              "paper_id": "2503.16413",
              "start_time": "2025-03-21T06:51:41.735Z",
              "end_time": "2025-03-21T06:53:24.687Z",
              "heartbeat_count": 20,
              "duration_seconds": 100,
              "idle_seconds": 3,
              "total_elapsed_seconds": 103
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-21T06:53:27+00:00",
        "updated_at": "2025-03-29T22:42:18+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.16413": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.16413",
        "url": "https://arxiv.org/abs/2503.16413",
        "title": "M3: 3D-Spatial MultiModal Memory",
        "authors": "Zou, Xueyan, Song, Yuchen, Qiu, Ri-Zhao, Peng, Xuanbin, Ye, Jianglong, Liu, Sifei, Wang, Xiaolong",
        "abstract": "We present 3D Spatial MultiModal Memory (M3), a multimodal memory system designed to retain information about medium-sized static scenes through video sources for visual perception. By integrating 3D Gaussian Splatting techniques with foundation models, M3 builds a multimodal memory capable of rendering feature representations across granularities, encompassing a wide range of knowledge. In our exploration, we identify two key challenges in previous works on feature splatting: (1) computational constraints in storing high-dimensional features for each Gaussian primitive, and (2) misalignment or information loss between distilled features and foundation model features. To address these challenges, we propose M3 with key components of principal scene components and Gaussian memory attention, enabling efficient training and inference. To validate M3, we conduct comprehensive quantitative evaluations of feature similarity and downstream tasks, as well as qualitative visualizations to highlight the pixel trace of Gaussian memory attention. Our approach encompasses a diverse range of foundation models, including vision-language models (VLMs), perception models, and large multimodal and language models (LMMs/LLMs). Furthermore, to demonstrate real-world applicability, we deploy M3's feature field in indoor scenes on a quadruped robot. Notably, we claim that M3 is the first work to address the core compression challenges in 3D feature distillation.",
        "timestamp": "2025-03-21T06:51:42.311Z",
        "rating": "novote",
        "publishedDate": "2025/03/20",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-21T06:51:42+00:00",
        "updated_at": "2025-03-29T22:42:18+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2502.15540": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.15540",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-21T07:39:16.017Z",
            "data": {
              "session_id": "session_1742542755297_xfztozs",
              "source_id": "arxiv",
              "paper_id": "2502.15540",
              "start_time": "2025-03-21T07:39:07.579Z",
              "end_time": "2025-03-21T07:39:15.297Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-21T07:39:17+00:00",
        "updated_at": "2025-03-29T22:42:17+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2502.15540": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.15540",
        "url": "https://arxiv.org/abs/2502.15540",
        "title": "Generalization Guarantees for Representation Learning via Data-Dependent Gaussian Mixture Priors",
        "authors": "Sefidgaran, Milad, Zaidi, Abdellatif, Krasnowski, Piotr",
        "abstract": "We establish in-expectation and tail bounds on the generalization error of representation learning type algorithms. The bounds are in terms of the relative entropy between the distribution of the representations extracted from the training and \"test'' datasets and a data-dependent symmetric prior, i.e., the Minimum Description Length (MDL) of the latent variables for the training and test datasets. Our bounds are shown to reflect the \"structure\" and \"simplicity'' of the encoder and significantly improve upon the few existing ones for the studied model. We then use our in-expectation bound to devise a suitable data-dependent regularizer; and we investigate thoroughly the important question of the selection of the prior. We propose a systematic approach to simultaneously learning a data-dependent Gaussian mixture prior and using it as a regularizer. Interestingly, we show that a weighted attention mechanism emerges naturally in this procedure. Our experiments show that our approach outperforms the now popular Variational Information Bottleneck (VIB) method as well as the recent Category-Dependent VIB (CDVIB).",
        "timestamp": "2025-03-21T07:39:08.161Z",
        "rating": "novote",
        "publishedDate": "2025/02/21",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-21T07:39:08+00:00",
        "updated_at": "2025-03-29T22:42:17+00:00",
        "version": 2
      }
    },
    "interactions:url.3C90A8B3": {
      "data": {
        "sourceId": "url",
        "paperId": "3C90A8B3",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-21T14:59:15.399Z",
            "data": {
              "session_id": "session_1742569154565_jtxp63u",
              "source_id": "url",
              "paper_id": "3C90A8B3",
              "start_time": "2025-03-21T14:59:05.623Z",
              "end_time": "2025-03-21T14:59:14.565Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-21T14:59:16+00:00",
        "updated_at": "2025-03-29T22:42:16+00:00",
        "version": 3
      }
    },
    "paper:url.3C90A8B3": {
      "data": {
        "sourceId": "url",
        "paperId": "3C90A8B3",
        "url": "https://openreview.net/forum?id=0RJvZY0h6O",
        "title": "Lognormal Mutations and their Use in Detecting Surreptitious Fake Images",
        "authors": "Olivier Teytaud, Mariia Zameshina, Tom Sander, Pierre Fernandez, Furong Ye, Laurent Najman, Thomas B\u00e4ck, Ismail Labiad",
        "abstract": "In many cases, adversarial attacks against fake detectors employ algorithms specifically crafted for automatic image classifiers.\nThese algorithms perform well, thanks to an excellent ad hoc distribution of initial attacks. \nHowever, these attacks are easily detected due to their specific initial distribution. Consequently, we explore alternative black-box attacks inspired by generic black-box optimization tools, particularly focusing on the \\lognormal{} algorithm that we successfully extend to attack fake detectors. \nMoreover, we demonstrate that this attack evades detection by neural networks trained to flag classical adversarial examples. \nTherefore, we train more general models capable of identifying a broader spectrum of attacks, including classical black-box attacks designed for images, black-box attacks driven by classical optimization, and no-box attacks.\nBy integrating these attack detection capabilities with fake detectors, we develop more robust and effective fake detection systems.",
        "timestamp": "2025-03-21T14:59:05.073Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "Transactions on Machine Learning Research",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-21T14:59:05+00:00",
        "updated_at": "2025-03-29T22:42:17+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.12211": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.12211",
        "url": "https://arxiv.org/html/2503.12211v1",
        "title": "Changing Base Without Losing Pace: A GPU-Efficient Alternative to MatMuls in DNNs",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-21T15:03:52.099Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-21T15:03:52+00:00",
        "updated_at": "2025-03-29T22:42:16+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.12211": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.12211",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-21T15:04:05.467Z",
            "data": {
              "session_id": "session_1742569444653_n4cgszm",
              "source_id": "arxiv",
              "paper_id": "2503.12211",
              "start_time": "2025-03-21T15:03:51.487Z",
              "end_time": "2025-03-21T15:04:04.653Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 3,
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-21T15:06:26.932Z",
            "data": {
              "session_id": "session_1742569586474_65legx8",
              "source_id": "arxiv",
              "paper_id": "2503.12211",
              "start_time": "2025-03-21T15:04:06.526Z",
              "end_time": "2025-03-21T15:06:26.474Z",
              "heartbeat_count": 27,
              "duration_seconds": 135,
              "idle_seconds": 5,
              "total_elapsed_seconds": 140
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-21T15:04:06+00:00",
        "updated_at": "2025-03-29T22:42:15+00:00",
        "version": 4
      }
    },
    "interactions:arxiv.2503.15612": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.15612",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-21T15:13:59.938Z",
            "data": {
              "session_id": "session_1742570038994_rmrgxzc",
              "source_id": "arxiv",
              "paper_id": "2503.15612",
              "start_time": "2025-03-21T15:13:20.730Z",
              "end_time": "2025-03-21T15:13:58.994Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 3,
              "total_elapsed_seconds": 38
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-21T15:18:37.801Z",
            "data": {
              "session_id": "session_1742570317328_j6vtf8a",
              "source_id": "arxiv",
              "paper_id": "2503.15612",
              "start_time": "2025-03-21T15:15:18.388Z",
              "end_time": "2025-03-21T15:18:37.328Z",
              "heartbeat_count": 39,
              "duration_seconds": 195,
              "idle_seconds": 4,
              "total_elapsed_seconds": 199
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-21T19:14:36.160Z",
            "data": {
              "session_id": "session_1742584475614_6zpdke4",
              "source_id": "arxiv",
              "paper_id": "2503.15612",
              "start_time": "2025-03-21T19:12:17.588Z",
              "end_time": "2025-03-21T19:14:35.614Z",
              "heartbeat_count": 27,
              "duration_seconds": 135,
              "idle_seconds": 3,
              "total_elapsed_seconds": 138
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-21T15:14:00+00:00",
        "updated_at": "2025-03-29T22:42:12+00:00",
        "version": 5
      }
    },
    "paper:arxiv.2503.15612": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.15612",
        "url": "https://arxiv.org/abs/2503.15612",
        "title": "Unification of observational entropy with maximum entropy principles",
        "authors": "Schindler, Joseph, Strasberg, Philipp, Galke, Niklas, Winter, Andreas, Jabbour, Michael G.",
        "abstract": "We introduce a definition of coarse-grained entropy that unifies measurement-based (observational entropy) and max-entropy-based (Jaynes) approaches to coarse-graining, by identifying physical constraints with information theoretic priors. The definition is shown to include as special cases most other entropies of interest in physics. We then consider second laws, showing that the definition admits new entropy increase theorems and connections to thermodynamics. We survey mathematical properties of the definition, and show it resolves some pathologies of the traditional observational entropy in infinite dimensions. Finally, we study the dynamics of this entropy in a quantum random matrix model and a classical hard sphere gas. Together the results suggest that this generalized observational entropy can form the basis of a highly general approach to statistical mechanics.",
        "timestamp": "2025-03-21T15:13:19.818Z",
        "rating": "novote",
        "publishedDate": "2025/03/19",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-21T15:13:20+00:00",
        "updated_at": "2025-03-29T22:42:13+00:00",
        "version": 2
      }
    },
    "interactions:url.4A11B091": {
      "data": {
        "sourceId": "url",
        "paperId": "4A11B091",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-03-21T15:12:36+00:00",
        "updated_at": "2025-03-29T22:42:13+00:00",
        "version": 2
      }
    },
    "paper:url.4A11B091": {
      "data": {
        "sourceId": "url",
        "paperId": "4A11B091",
        "url": "https://openreview.net/forum?id=MF7ljU8xcf",
        "title": "Compute-Optimal LLMs Provably Generalize Better with Scale",
        "authors": "Marc Anton Finzi, Sanyam Kapoor, Diego Granziol, Anming Gu, Christopher De Sa, J Zico Kolter, Andrew Gordon Wilson",
        "abstract": "Why do larger language models generalize better? To explore this question, we develop generalization bounds on the pretraining objective of large language models (LLMs) in the compute-optimal regime, as described by the Chinchilla scaling laws. We introduce a novel, fully empirical Freedman-type martingale concentration inequality that tightens existing bounds by accounting for the variance of the loss function. The generalization bound can be broken into three contributions: the number of parameters per token, the loss variance, and the quantization error at a fixed bitrate. As language models are scaled up, the number of parameters per data point stays constant; however, both the loss variance and the quantization error decrease, implying that larger models should have \\emph{smaller} generalization gaps. We examine why larger models tend to be more quantizable from an information theoretic perspective, showing that the rate at which they can integrate new information grows slower than their capacity on the compute optimal frontier. From these findings we produce a scaling law for the generalization gap, showing that our bounds decrease in a predictable way.",
        "timestamp": "2025-03-21T15:12:28.347Z",
        "rating": "thumbsup",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-21T15:12:28+00:00",
        "updated_at": "2025-03-29T22:42:14+00:00",
        "version": 3
      }
    },
    "paper:url.3329ED1E": {
      "data": {
        "sourceId": "url",
        "paperId": "3329ED1E",
        "url": "https://openreview.net/pdf?id=MF7ljU8xcf",
        "title": "3329ED1E",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-21T15:11:57.243Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-21T15:11:57+00:00",
        "updated_at": "2025-03-29T22:42:15+00:00",
        "version": 2
      }
    },
    "paper:url.287DF847": {
      "data": {
        "sourceId": "url",
        "paperId": "287DF847",
        "url": "https://journals.sagepub.com/doi/10.1111/1467-9280.00282",
        "title": "Birds of a Feather Flock Conjointly (?): Rhyme as Reason in Aphorisms - Matthew S. McGlone, Jessica Tofighbakhsh, 2000",
        "authors": "",
        "abstract": "We explored the role that poetic form can play in people's perceptions of the accuracy of aphorisms as descriptions of human behavior. Participants judged ...",
        "timestamp": "2025-03-21T15:26:28.631Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "Psychological Science",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-21T15:26:29+00:00",
        "updated_at": "2025-03-29T22:42:10+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.15633": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.15633",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-21T15:24:23.301Z",
            "data": {
              "session_id": "session_1742570662207_rb385cz",
              "source_id": "arxiv",
              "paper_id": "2503.15633",
              "start_time": "2025-03-21T15:24:09.944Z",
              "end_time": "2025-03-21T15:24:22.207Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-21T15:24:24+00:00",
        "updated_at": "2025-03-29T22:42:10+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.15633": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.15633",
        "url": "https://arxiv.org/abs/2503.15633",
        "title": "Vision-Speech Models: Teaching Speech Models to Converse about Images",
        "authors": "Royer, Am\u00e9lie, B\u00f6hle, Moritz, de Marmiesse, Gabriel, Mazar\u00e9, Laurent, Zeghidour, Neil, D\u00e9fossez, Alexandre, P\u00e9rez, Patrick",
        "abstract": "The recent successes of Vision-Language models raise the question of how to equivalently imbue a pretrained speech model with vision understanding, an important milestone towards building a multimodal speech model able to freely converse about images. Building such a conversational Vision-Speech model brings its unique challenges: (i) paired image-speech datasets are much scarcer than their image-text counterparts, (ii) ensuring real-time latency at inference is crucial thus bringing compute and memory constraints, and (iii) the model should preserve prosodic features (e.g., speaker tone) which cannot be inferred from text alone. In this work, we introduce MoshiVis, augmenting a recent dialogue speech LLM, Moshi, with visual inputs through lightweight adaptation modules. An additional dynamic gating mechanism enables the model to more easily switch between the visual inputs and unrelated conversation topics. To reduce training costs, we design a simple one-stage, parameter-efficient fine-tuning pipeline in which we leverage a mixture of image-text (i.e., \"speechless\") and image-speech samples. We evaluate the model on downstream visual understanding tasks with both audio and text prompts, and report qualitative samples of interactions with MoshiVis. Our inference code will be made available, as well as the image-speech data used for audio evaluation.",
        "timestamp": "2025-03-21T15:24:10.580Z",
        "rating": "novote",
        "publishedDate": "2025/03/19",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-21T15:24:11+00:00",
        "updated_at": "2025-03-29T22:42:11+00:00",
        "version": 2
      }
    },
    "paper:url.2038831F": {
      "data": {
        "sourceId": "url",
        "paperId": "2038831F",
        "url": "https://openreview.net/forum?id=PJNhZoCjLh",
        "title": "Generalization and Distributed Learning of GFlowNets",
        "authors": "Tiago Silva, Amauri H Souza, Omar Rivasplata, Vikas Garg, Samuel Kaski, Diego Mesquita",
        "abstract": "Conventional wisdom attributes the success of Generative Flow Networks (GFlowNets) to their ability to exploit the compositional structure of the sample space for learning generalizable flow functions (Bengio et al., 2021). Despite the abundance of empirical evidence, formalizing this belief with verifiable non-vacuous statistical guarantees has remained elusive. We address this issue with the first data-dependent generalization bounds for GFlowNets. We also elucidate the negative impact of the state space size on the generalization performance of these models via Azuma-Hoeffding-type oracle PAC-Bayesian inequalities. We leverage our theoretical insights to design a novel distributed learning algorithm for GFlowNets, which we call *Subgraph Asynchronous Learning* (SAL). In a nutshell, SAL utilizes a divide-and-conquer strategy: multiple GFlowNets are trained in parallel on smaller subnetworks of the flow network, and then aggregated with an additional GFlowNet that allocates appropriate flow to each subnetwork.  Our experiments with synthetic and real-world problems demonstrate the benefits of SAL over centralized training in terms of mode coverage and distribution matching.",
        "timestamp": "2025-03-21T15:22:31.835Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-21T15:22:32+00:00",
        "updated_at": "2025-03-29T22:42:11+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2106.04399": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2106.04399",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-21T15:53:45.032Z",
            "data": {
              "session_id": "session_1742572424191_1tjpyxr",
              "source_id": "arxiv",
              "paper_id": "2106.04399",
              "start_time": "2025-03-21T15:43:26.582Z",
              "end_time": "2025-03-21T15:53:44.191Z",
              "heartbeat_count": 123,
              "duration_seconds": 615,
              "idle_seconds": 3,
              "total_elapsed_seconds": 618
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-21T15:53:46+00:00",
        "updated_at": "2025-03-29T22:42:09+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2106.04399": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2106.04399",
        "url": "https://arxiv.org/abs/2106.04399",
        "title": "Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation",
        "authors": "Bengio, Emmanuel, Jain, Moksh, Korablyov, Maksym, Precup, Doina, Bengio, Yoshua",
        "abstract": "This paper is about the problem of learning a stochastic policy for generating an object (like a molecular graph) from a sequence of actions, such that the probability of generating an object is proportional to a given positive reward for that object. Whereas standard return maximization tends to converge to a single return-maximizing sequence, there are cases where we would like to sample a diverse set of high-return solutions. These arise, for example, in black-box function optimization when few rounds are possible, each with large batches of queries, where the batches should be diverse, e.g., in the design of new molecules. One can also see this as a problem of approximately converting an energy function to a generative distribution. While MCMC methods can achieve that, they are expensive and generally only perform local exploration. Instead, training a generative policy amortizes the cost of search during training and yields to fast generation. Using insights from Temporal Difference learning, we propose GFlowNet, based on a view of the generative process as a flow network, making it possible to handle the tricky case where different trajectories can yield the same final state, e.g., there are many ways to sequentially add atoms to generate some molecular graph. We cast the set of trajectories as a flow and convert the flow consistency equations into a learning objective, akin to the casting of the Bellman equations into Temporal Difference methods. We prove that any global minimum of the proposed objectives yields a policy which samples from the desired distribution, and demonstrate the improved performance and diversity of GFlowNet on a simple domain where there are many modes to the reward function, and on a molecule synthesis task.",
        "timestamp": "2025-03-21T15:43:27.176Z",
        "rating": "novote",
        "publishedDate": "2021/06/08",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-21T15:43:27+00:00",
        "updated_at": "2025-03-29T22:42:09+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2502.18802": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.18802",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-21T19:14:40.700Z",
            "data": {
              "session_id": "session_1742584480690_p1f5p6i",
              "source_id": "arxiv",
              "paper_id": "2502.18802",
              "start_time": "2025-03-21T19:14:35.617Z",
              "end_time": "2025-03-21T19:14:40.690Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-21T19:14:41+00:00",
        "updated_at": "2025-03-29T22:42:07+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2502.18802": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.18802",
        "url": "https://arxiv.org/abs/2502.18802",
        "title": "Language Models Grow Less Humanlike beyond Phase Transition",
        "authors": "Aoyama, Tatsuya, Wilcox, Ethan",
        "abstract": "LMs' alignment with human reading behavior (i.e. psychometric predictive power; PPP) is known to improve during pretraining up to a tipping point, beyond which it either plateaus or degrades. Various factors, such as word frequency, recency bias in attention, and context size, have been theorized to affect PPP, yet there is no current account that explains why such a tipping point exists, and how it interacts with LMs' pretraining dynamics more generally. We hypothesize that the underlying factor is a pretraining phase transition, characterized by the rapid emergence of specialized attention heads. We conduct a series of correlational and causal experiments to show that such a phase transition is responsible for the tipping point in PPP. We then show that, rather than producing attention patterns that contribute to the degradation in PPP, phase transitions alter the subsequent learning dynamics of the model, such that further training keeps damaging PPP.",
        "timestamp": "2025-03-21T19:12:04.123Z",
        "rating": "novote",
        "publishedDate": "2025/02/26",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-21T19:12:04+00:00",
        "updated_at": "2025-03-29T22:42:08+00:00",
        "version": 2
      }
    },
    "paper:url.4EAD45C2": {
      "data": {
        "sourceId": "url",
        "paperId": "4EAD45C2",
        "url": "https://scottaaronson.blog/?p=1799",
        "title": "Why I Am Not An Integrated Information Theorist (or, The Unconscious Expander)",
        "authors": "",
        "abstract": "Happy birthday to me! Recently, lots of people\u00a0have been asking\u00a0me what I think\u00a0about\u00a0IIT\u2014no, not the Indian Institutes of Technology, but Integrated Information Theory, a\u00a0widely-discussed &#\u2026",
        "timestamp": "2025-03-21T17:18:57.435Z",
        "rating": "novote",
        "publishedDate": "2014-05-21T21:41:05+00:00",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-21T17:18:58+00:00",
        "updated_at": "2025-03-29T22:42:08+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.13522": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.13522",
        "url": "https://arxiv.org/abs/2503.13522",
        "title": "Advanced Deep Learning Methods for Protein Structure Prediction and Design",
        "authors": "Wu, Weikun, Wang, Tianyang, Zhang, Yichao, Deng, Ningyuan, Song, Xinyuan, Bi, Ziqian, Yao, Zheyu, Chen, Keyu, Li, Ming, Niu, Qian, Liu, Junyu, Peng, Benji, Zhang, Sen, Liu, Ming, Zhang, Li, Pan, Xuanhe, Wang, Jinlang, Feng, Pohsun, Wen, Yizhu, Yan, Lawrence KQ, Tseng, Hongming, Zhong, Yan, Wang, Yunze, Qin, Ziyuan, Jing, Bowen, Yang, Junjie, Zhou, Jun, Liang, Chia Xin, Song, Junhao",
        "abstract": "After AlphaFold won the Nobel Prize, protein prediction with deep learning once again became a hot topic. We comprehensively explore advanced deep learning methods applied to protein structure prediction and design. It begins by examining recent innovations in prediction architectures, with detailed discussions on improvements such as diffusion based frameworks and novel pairwise attention modules. The text analyses key components including structure generation, evaluation metrics, multiple sequence alignment processing, and network architecture, thereby illustrating the current state of the art in computational protein modelling. Subsequent chapters focus on practical applications, presenting case studies that range from individual protein predictions to complex biomolecular interactions. Strategies for enhancing prediction accuracy and integrating deep learning techniques with experimental validation are thoroughly explored. The later sections review the industry landscape of protein design, highlighting the transformative role of artificial intelligence in biotechnology and discussing emerging market trends and future challenges. Supplementary appendices provide essential resources such as databases and open source tools, making this volume a valuable reference for researchers and students.",
        "timestamp": "2025-03-21T23:24:31.920Z",
        "rating": "thumbsdown",
        "publishedDate": "2025/03/14",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-21T23:24:32+00:00",
        "updated_at": "2025-03-30T08:22:05+00:00",
        "version": 4
      }
    },
    "interactions:arxiv.2312.11556": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2312.11556",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-21T23:23:09.615Z",
            "data": {
              "session_id": "session_1742599388394_wy9mov9",
              "source_id": "arxiv",
              "paper_id": "2312.11556",
              "start_time": "2025-03-21T23:22:55.887Z",
              "end_time": "2025-03-21T23:23:08.394Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 3,
              "total_elapsed_seconds": 13
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-21T23:23:10+00:00",
        "updated_at": "2025-03-29T22:42:06+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2312.11556": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2312.11556",
        "url": "https://arxiv.org/abs/2312.11556",
        "title": "StarVector: Generating Scalable Vector Graphics Code from Images and Text",
        "authors": "Rodriguez, Juan A., Puri, Abhay, Agarwal, Shubham, Laradji, Issam H., Rodriguez, Pau, Rajeswar, Sai, Vazquez, David, Pal, Christopher, Pedersoli, Marco",
        "abstract": "Scalable Vector Graphics (SVGs) are vital for modern image rendering due to their scalability and versatility. Previous SVG generation methods have focused on curve-based vectorization, lacking semantic understanding, often producing artifacts, and struggling with SVG primitives beyond path curves. To address these issues, we introduce StarVector, a multimodal large language model for SVG generation. It performs image vectorization by understanding image semantics and using SVG primitives for compact, precise outputs. Unlike traditional methods, StarVector works directly in the SVG code space, leveraging visual understanding to apply accurate SVG primitives. To train StarVector, we create SVG-Stack, a diverse dataset of 2M samples that enables generalization across vectorization tasks and precise use of primitives like ellipses, polygons, and text. We address challenges in SVG evaluation, showing that pixel-based metrics like MSE fail to capture the unique qualities of vector graphics. We introduce SVG-Bench, a benchmark across 10 datasets, and 3 tasks: Image-to-SVG, Text-to-SVG generation, and diagram generation. Using this setup, StarVector achieves state-of-the-art performance, producing more compact and semantically rich SVGs.",
        "timestamp": "2025-03-21T23:22:56.348Z",
        "rating": "novote",
        "publishedDate": "2023/12/17",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-21T23:22:56+00:00",
        "updated_at": "2025-03-29T22:42:07+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.10061": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.10061",
        "url": "https://arxiv.org/abs/2503.10061",
        "title": "Compute Optimal Scaling of Skills: Knowledge vs Reasoning",
        "authors": "Roberts, Nicholas, Chatterji, Niladri, Narang, Sharan, Lewis, Mike, Hupkes, Dieuwke",
        "abstract": "Scaling laws are a critical component of the LLM development pipeline, most famously as a way to forecast training decisions such as 'compute-optimally' trading-off parameter count and dataset size, alongside a more recent growing list of other crucial decisions. In this work, we ask whether compute-optimal scaling behaviour can be skill-dependent. In particular, we examine knowledge and reasoning-based skills such as knowledge-based QA and code generation, and we answer this question in the affirmative: scaling laws are skill-dependent. Next, to understand whether skill-dependent scaling is an artefact of the pretraining datamix, we conduct an extensive ablation of different datamixes and find that, also when correcting for datamix differences, knowledge and code exhibit fundamental differences in scaling behaviour. We conclude with an analysis of how our findings relate to standard compute-optimal scaling using a validation set, and find that a misspecified validation set can impact compute-optimal parameter count by nearly 50%, depending on its skill composition.",
        "timestamp": "2025-03-21T23:26:03.128Z",
        "rating": "novote",
        "publishedDate": "2025/03/13",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-21T23:26:03+00:00",
        "updated_at": "2025-03-29T22:42:04+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.13522": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.13522",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-21T23:24:44.903Z",
            "data": {
              "session_id": "session_1742599484197_2crhkqy",
              "source_id": "arxiv",
              "paper_id": "2503.13522",
              "start_time": "2025-03-21T23:24:31.908Z",
              "end_time": "2025-03-21T23:24:44.197Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "rating",
            "timestamp": "2025-03-21T23:27:22.593Z",
            "data": {
              "rating": "thumbsdown"
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-21T23:24:45+00:00",
        "updated_at": "2025-03-29T22:42:05+00:00",
        "version": 6
      }
    },
    "interactions:url.722805DC": {
      "data": {
        "sourceId": "url",
        "paperId": "722805DC",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-22T02:22:41.916Z",
            "data": {
              "session_id": "session_1742610161117_gbs96yu",
              "source_id": "url",
              "paper_id": "722805DC",
              "start_time": "2025-03-22T02:22:35.823Z",
              "end_time": "2025-03-22T02:22:41.117Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-22T02:22:42+00:00",
        "updated_at": "2025-03-29T22:42:04+00:00",
        "version": 3
      }
    },
    "paper:url.722805DC": {
      "data": {
        "sourceId": "url",
        "paperId": "722805DC",
        "url": "https://www.scientificamerican.com/article/see-how-measles-outbreaks-flourish-where-vaccination-rates-fall/",
        "title": "Measles Cases Are Exploding in These Areas\u2014The Data Reveals Why",
        "authors": "Ripley Cleghorn",
        "abstract": "Measles continues to spread in Texas and other states and has caused the first reported U.S. death from the virus in a decade. Vaccination data over time reveal vulnerabilities in protection",
        "timestamp": "2025-03-22T02:22:34.538Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-22T02:22:35+00:00",
        "updated_at": "2025-03-29T22:42:04+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2404.13208": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.13208",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-22T05:47:12.480Z",
            "data": {
              "session_id": "session_1742622432467_31hozj4",
              "source_id": "arxiv",
              "paper_id": "2404.13208",
              "start_time": "2025-03-22T05:46:36.047Z",
              "end_time": "2025-03-22T05:47:12.467Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 1,
              "total_elapsed_seconds": 36
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-22T05:46:36+00:00",
        "updated_at": "2025-03-29T22:42:03+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2404.13208": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.13208",
        "url": "https://arxiv.org/html/2404.13208v1",
        "title": "The Instruction Hierarchy:Training LLMs to Prioritize Privileged Instructions",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-22T05:46:17.110Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-22T05:46:17+00:00",
        "updated_at": "2025-03-29T22:42:03+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2312.10794": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2312.10794",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-22T05:59:34.001Z",
            "data": {
              "session_id": "session_1742623173275_67rvp5n",
              "source_id": "arxiv",
              "paper_id": "2312.10794",
              "start_time": "2025-03-22T05:59:17.485Z",
              "end_time": "2025-03-22T05:59:33.275Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 1,
              "total_elapsed_seconds": 16
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-22T06:15:09.727Z",
            "data": {
              "session_id": "session_1742624109486_yg9g0ul",
              "source_id": "arxiv",
              "paper_id": "2312.10794",
              "start_time": "2025-03-22T06:14:36.802Z",
              "end_time": "2025-03-22T06:15:09.485Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 3,
              "total_elapsed_seconds": 33
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-22T06:25:06.840Z",
            "data": {
              "session_id": "session_1742624706590_lfpjnnx",
              "source_id": "arxiv",
              "paper_id": "2312.10794",
              "start_time": "2025-03-22T06:24:55.648Z",
              "end_time": "2025-03-22T06:25:06.590Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-22T05:59:34+00:00",
        "updated_at": "2025-03-29T22:42:02+00:00",
        "version": 5
      }
    },
    "paper:arxiv.2312.10794": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2312.10794",
        "url": "https://arxiv.org/abs/2312.10794",
        "title": "A mathematical perspective on Transformers",
        "authors": "Geshkovski, Borjan, Letrouit, Cyril, Polyanskiy, Yury, Rigollet, Philippe",
        "abstract": "Transformers play a central role in the inner workings of large language models. We develop a mathematical framework for analyzing Transformers based on their interpretation as interacting particle systems, which reveals that clusters emerge in long time. Our study explores the underlying theory and offers new perspectives for mathematicians as well as computer scientists.",
        "timestamp": "2025-03-22T05:59:17.915Z",
        "rating": "novote",
        "publishedDate": "2023/12/17",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-22T05:59:18+00:00",
        "updated_at": "2025-03-29T22:42:02+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2410.01131": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.01131",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-22T06:16:05.162Z",
            "data": {
              "session_id": "session_1742624164789_ueydmye",
              "source_id": "arxiv",
              "paper_id": "2410.01131",
              "start_time": "2025-03-22T06:15:39.579Z",
              "end_time": "2025-03-22T06:16:04.789Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 0,
              "total_elapsed_seconds": 25
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T18:26:44.754Z",
            "data": {
              "session_id": "session_1743964004743_16hj87p",
              "source_id": "arxiv",
              "paper_id": "2410.01131",
              "start_time": "2025-04-06T18:26:33.324Z",
              "end_time": "2025-04-06T18:26:44.743Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2063,
        "object_id": "interactions:arxiv.2410.01131",
        "created_at": "2025-03-22T06:16:06+00:00",
        "updated_at": "2025-04-06T18:27:43+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.13025": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.13025",
        "url": "https://arxiv.org/abs/2502.13025",
        "title": "Agentic Deep Graph Reasoning Yields Self-Organizing Knowledge Networks",
        "authors": "Buehler, Markus J.",
        "abstract": "We present an agentic, autonomous graph expansion framework that iteratively structures and refines knowledge in situ. Unlike conventional knowledge graph construction methods relying on static extraction or single-pass learning, our approach couples a reasoning-native large language model with a continually updated graph representation. At each step, the system actively generates new concepts and relationships, merges them into a global graph, and formulates subsequent prompts based on its evolving structure. Through this feedback-driven loop, the model organizes information into a scale-free network characterized by hub formation, stable modularity, and bridging nodes that link disparate knowledge clusters. Over hundreds of iterations, new nodes and edges continue to appear without saturating, while centrality measures and shortest path distributions evolve to yield increasingly distributed connectivity. Our analysis reveals emergent patterns, such as the rise of highly connected 'hub' concepts and the shifting influence of 'bridge' nodes, indicating that agentic, self-reinforcing graph construction can yield open-ended, coherent knowledge structures. Applied to materials design problems, we present compositional reasoning experiments by extracting node-specific and synergy-level principles to foster genuinely novel knowledge synthesis, yielding cross-domain ideas that transcend rote summarization and strengthen the framework's potential for open-ended scientific discovery. We discuss other applications in scientific discovery and outline future directions for enhancing scalability and interpretability.",
        "timestamp": "2025-03-22T06:15:11.702Z",
        "rating": "novote",
        "publishedDate": "2025/02/18",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-22T06:15:12+00:00",
        "updated_at": "2025-03-29T22:42:01+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2502.13025": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.13025",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-22T06:24:28.288Z",
            "data": {
              "session_id": "session_1742624667970_wrghott",
              "source_id": "arxiv",
              "paper_id": "2502.13025",
              "start_time": "2025-03-22T06:24:03.299Z",
              "end_time": "2025-03-22T06:24:27.970Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 5,
              "total_elapsed_seconds": 25
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-22T06:24:29+00:00",
        "updated_at": "2025-03-29T22:41:59+00:00",
        "version": 3
      }
    },
    "interactions:arxiv.2503.10663": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.10663",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-22T06:24:02.616Z",
            "data": {
              "session_id": "session_1742624641866_mjv1e73",
              "source_id": "arxiv",
              "paper_id": "2503.10663",
              "start_time": "2025-03-22T06:23:37.482Z",
              "end_time": "2025-03-22T06:24:01.866Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 4,
              "total_elapsed_seconds": 24
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-22T06:24:03+00:00",
        "updated_at": "2025-03-29T22:41:59+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.10663": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.10663",
        "url": "https://arxiv.org/html/2503.10663v1",
        "title": "Optimal Transport for Brain-Image Alignment: Unveiling Redundancy and Synergy in Neural Information Processing",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-22T06:23:37.358Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-22T06:23:37+00:00",
        "updated_at": "2025-03-29T22:42:00+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2306.02572": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2306.02572",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-22T16:29:37.593Z",
            "data": {
              "session_id": "session_1742660976835_56vt5fz",
              "source_id": "arxiv",
              "paper_id": "2306.02572",
              "start_time": "2025-03-22T16:29:15.970Z",
              "end_time": "2025-03-22T16:29:36.835Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 1,
              "total_elapsed_seconds": 21
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-22T16:29:38+00:00",
        "updated_at": "2025-03-29T22:41:58+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2306.02572": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2306.02572",
        "url": "https://arxiv.org/abs/2306.02572",
        "title": "Introduction to Latent Variable Energy-Based Models: A Path Towards Autonomous Machine Intelligence",
        "authors": "Dawid, Anna, LeCun, Yann",
        "abstract": "Current automated systems have crucial limitations that need to be addressed before artificial intelligence can reach human-like levels and bring new technological revolutions. Among others, our societies still lack Level 5 self-driving cars, domestic robots, and virtual assistants that learn reliable world models, reason, and plan complex action sequences. In these notes, we summarize the main ideas behind the architecture of autonomous intelligence of the future proposed by Yann LeCun. In particular, we introduce energy-based and latent variable models and combine their advantages in the building block of LeCun's proposal, that is, in the hierarchical joint embedding predictive architecture (H-JEPA).",
        "timestamp": "2025-03-22T16:29:16.486Z",
        "rating": "novote",
        "publishedDate": "2023/06/05",
        "tags": [],
        "doi": "10.1088/1742-5468/ad292b",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-22T16:29:16+00:00",
        "updated_at": "2025-03-29T22:41:58+00:00",
        "version": 2
      }
    },
    "paper:url.726FFC71": {
      "data": {
        "sourceId": "url",
        "paperId": "726FFC71",
        "url": "https://openreview.net/forum?id=BZ5a1r-kVsf",
        "title": "A Path Towards Autonomous Machine Intelligence",
        "authors": "",
        "abstract": "How could machines learn as efficiently as humans and animals?  How could machines learn to reason and plan?  How could machines learn representations of percepts and action plans at multiple...",
        "timestamp": "2025-03-22T16:35:01.858Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-22T16:35:02+00:00",
        "updated_at": "2025-03-29T22:41:56+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2301.08243": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2301.08243",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-22T16:33:55.276Z",
            "data": {
              "session_id": "session_1742661234408_z8wf0c2",
              "source_id": "arxiv",
              "paper_id": "2301.08243",
              "start_time": "2025-03-22T16:33:47.165Z",
              "end_time": "2025-03-22T16:33:54.408Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-22T16:33:56+00:00",
        "updated_at": "2025-03-29T22:41:57+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2301.08243": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2301.08243",
        "url": "https://arxiv.org/abs/2301.08243",
        "title": "Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
        "authors": "Assran, Mahmoud, Duval, Quentin, Misra, Ishan, Bojanowski, Piotr, Vincent, Pascal, Rabbat, Michael, LeCun, Yann, Ballas, Nicolas",
        "abstract": "This paper demonstrates an approach for learning highly semantic image representations without relying on hand-crafted data-augmentations. We introduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a non-generative approach for self-supervised learning from images. The idea behind I-JEPA is simple: from a single context block, predict the representations of various target blocks in the same image. A core design choice to guide I-JEPA towards producing semantic representations is the masking strategy; specifically, it is crucial to (a) sample target blocks with sufficiently large scale (semantic), and to (b) use a sufficiently informative (spatially distributed) context block. Empirically, when combined with Vision Transformers, we find I-JEPA to be highly scalable. For instance, we train a ViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strong downstream performance across a wide range of tasks, from linear classification to object counting and depth prediction.",
        "timestamp": "2025-03-22T16:33:47.835Z",
        "rating": "novote",
        "publishedDate": "2023/01/19",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-22T16:33:48+00:00",
        "updated_at": "2025-03-29T22:41:57+00:00",
        "version": 2
      }
    },
    "interactions:url.726FFC71": {
      "data": {
        "sourceId": "url",
        "paperId": "726FFC71",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-22T16:36:01.323Z",
            "data": {
              "session_id": "session_1742661360365_wpzpyhn",
              "source_id": "url",
              "paper_id": "726FFC71",
              "start_time": "2025-03-22T16:35:04.403Z",
              "end_time": "2025-03-22T16:36:00.365Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 1,
              "total_elapsed_seconds": 56
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-22T16:36:02+00:00",
        "updated_at": "2025-03-29T22:41:56+00:00",
        "version": 3
      }
    },
    "interactions:arxiv.2503.13456": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.13456",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-22T16:44:57.088Z",
            "data": {
              "session_id": "session_1742661896140_b6xcpik",
              "source_id": "arxiv",
              "paper_id": "2503.13456",
              "start_time": "2025-03-22T16:42:10.068Z",
              "end_time": "2025-03-22T16:44:56.140Z",
              "heartbeat_count": 33,
              "duration_seconds": 165,
              "idle_seconds": 1,
              "total_elapsed_seconds": 166
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-22T16:44:58+00:00",
        "updated_at": "2025-03-29T22:41:55+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.13456": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.13456",
        "url": "https://arxiv.org/abs/2503.13456",
        "title": "How good is the h-index?",
        "authors": "Borji, Ali",
        "abstract": "The h-index has become a widely used metric for evaluating the productivity and citation impact of researchers. Introduced by physicist Jorge E. Hirsch in 2005, the h-index measures both the quantity (number of publications) and quality (citations) of a researcher's output. While it has gained popularity for its simplicity and practicality, the h-index is not without its limitations. We examine the strengths and weaknesses of this metric, presenting preliminary experimental results that demonstrate the limitations of the h-index. We also propose a potential solution. The primary aim of this work is to shed light on the shortcomings of the h-index and its implications for ranking scientists, motivating them, allocating funding, and advancing science.",
        "timestamp": "2025-03-22T16:42:10.789Z",
        "rating": "novote",
        "publishedDate": "2025/01/19",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-22T16:42:11+00:00",
        "updated_at": "2025-03-29T22:41:55+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.14445": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.14445",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-22T16:53:16.674Z",
            "data": {
              "session_id": "session_1742662396030_g2qepxf",
              "source_id": "arxiv",
              "paper_id": "2503.14445",
              "start_time": "2025-03-22T16:53:10.704Z",
              "end_time": "2025-03-22T16:53:16.030Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-22T16:53:18+00:00",
        "updated_at": "2025-03-29T22:41:54+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.14445": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.14445",
        "url": "https://arxiv.org/pdf/2503.14445v1",
        "title": "2503.14445",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-22T16:51:27.991Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-22T16:51:28+00:00",
        "updated_at": "2025-03-29T22:41:54+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2106.10165": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2106.10165",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-22T17:28:04.622Z",
            "data": {
              "session_id": "session_1742664483803_enex10c",
              "source_id": "arxiv",
              "paper_id": "2106.10165",
              "start_time": "2025-03-22T17:27:24.634Z",
              "end_time": "2025-03-22T17:28:03.803Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 4,
              "total_elapsed_seconds": 39
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-30T20:49:58.933Z",
            "data": {
              "session_id": "session_1743367798151_yp792y6",
              "source_id": "arxiv",
              "paper_id": "2106.10165",
              "start_time": "2025-03-30T20:48:55.311Z",
              "end_time": "2025-03-30T20:49:58.151Z",
              "heartbeat_count": 12,
              "duration_seconds": 60,
              "idle_seconds": 3,
              "total_elapsed_seconds": 63
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-22T17:28:05+00:00",
        "updated_at": "2025-03-30T20:51:06+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2106.10165": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2106.10165",
        "url": "https://arxiv.org/abs/2106.10165",
        "title": "The Principles of Deep Learning Theory",
        "authors": "Roberts, Daniel A., Yaida, Sho, Hanin, Boris",
        "abstract": "This book develops an effective theory approach to understanding deep neural networks of practical relevance. Beginning from a first-principles component-level picture of networks, we explain how to determine an accurate description of the output of trained networks by solving layer-to-layer iteration equations and nonlinear learning dynamics. A main result is that the predictions of networks are described by nearly-Gaussian distributions, with the depth-to-width aspect ratio of the network controlling the deviations from the infinite-width Gaussian description. We explain how these effectively-deep networks learn nontrivial representations from training and more broadly analyze the mechanism of representation learning for nonlinear models. From a nearly-kernel-methods perspective, we find that the dependence of such models' predictions on the underlying learning algorithm can be expressed in a simple and universal way. To obtain these results, we develop the notion of representation group flow (RG flow) to characterize the propagation of signals through the network. By tuning networks to criticality, we give a practical solution to the exploding and vanishing gradient problem. We further explain how RG flow leads to near-universal behavior and lets us categorize networks built from different activation functions into universality classes. Altogether, we show that the depth-to-width ratio governs the effective model complexity of the ensemble of trained networks. By using information-theoretic techniques, we estimate the optimal aspect ratio at which we expect the network to be practically most useful and show how residual connections can be used to push this scale to arbitrary depths. With these tools, we can learn in detail about the inductive bias of architectures, hyperparameters, and optimizers.",
        "timestamp": "2025-03-22T17:26:36.367Z",
        "rating": "novote",
        "publishedDate": "2021/06/18",
        "tags": [],
        "doi": "10.1017/9781009023405",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-22T17:26:36+00:00",
        "updated_at": "2025-03-29T22:41:52+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2407.04491": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.04491",
        "url": "https://arxiv.org/abs/2407.04491",
        "title": "Better by Default: Strong Pre-Tuned MLPs and Boosted Trees on Tabular Data",
        "authors": "Holzm\u00fcller, David, Grinsztajn, L\u00e9o, Steinwart, Ingo",
        "abstract": "For classification and regression on tabular data, the dominance of gradient-boosted decision trees (GBDTs) has recently been challenged by often much slower deep learning methods with extensive hyperparameter tuning. We address this discrepancy by introducing (a) RealMLP, an improved multilayer perceptron (MLP), and (b) strong meta-tuned default parameters for GBDTs and RealMLP. We tune RealMLP and the default parameters on a meta-train benchmark with 118 datasets and compare them to hyperparameter-optimized versions on a disjoint meta-test benchmark with 90 datasets, as well as the GBDT-friendly benchmark by Grinsztajn et al. (2022). Our benchmark results on medium-to-large tabular datasets (1K--500K samples) show that RealMLP offers a favorable time-accuracy tradeoff compared to other neural baselines and is competitive with GBDTs in terms of benchmark scores. Moreover, a combination of RealMLP and GBDTs with improved default parameters can achieve excellent results without hyperparameter tuning. Finally, we demonstrate that some of RealMLP's improvements can also considerably improve the performance of TabR with default parameters.",
        "timestamp": "2025-03-22T17:24:45.884Z",
        "rating": "novote",
        "publishedDate": "2024/07/05",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-22T17:24:46+00:00",
        "updated_at": "2025-03-29T22:41:52+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.14481": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.14481",
        "url": "https://arxiv.org/abs/2503.14481",
        "title": "Don't lie to your friends: Learning what you know from collaborative self-play",
        "authors": "Eisenstein, Jacob, Aghajani, Reza, Fisch, Adam, Dua, Dheeru, Huot, Fantine, Lapata, Mirella, Zayats, Vicky, Berant, Jonathan",
        "abstract": "To be helpful assistants, AI agents must be aware of their own capabilities and limitations. This includes knowing when to answer from parametric knowledge versus using tools, when to trust tool outputs, and when to abstain or hedge. Such capabilities are hard to teach through supervised fine-tuning because they require constructing examples that reflect the agent's specific capabilities. We therefore propose a radically new approach to teaching agents what they know: \\emph{collaborative self-play}. We construct multi-agent collaborations in which the group is rewarded for collectively arriving at correct answers. The desired meta-knowledge emerges from the incentives built into the structure of the interaction. We focus on small societies of agents that have access to heterogeneous tools (corpus-specific retrieval), and therefore must collaborate to maximize their success while minimizing their effort. Experiments show that group-level rewards for multi-agent communities can induce policies that \\emph{transfer} to improve tool use and selective prediction in settings where individual agents are deployed in isolation.",
        "timestamp": "2025-03-22T17:01:09.932Z",
        "rating": "novote",
        "publishedDate": "2025/03/18",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-22T17:01:10+00:00",
        "updated_at": "2025-03-29T22:41:53+00:00",
        "version": 2
      }
    },
    "paper:url.78C03D64": {
      "data": {
        "sourceId": "url",
        "paperId": "78C03D64",
        "url": "https://openreview.net/forum?id=EWT4GxjGDS",
        "title": "Producers Equilibria and Dynamics in Engagement-Driven Recommender Systems",
        "authors": "Krishna Acharya, Juba Ziani, Jingyan Wang, Varun Vangala",
        "abstract": "Online platforms such as YouTube, Instagram heavily rely on recommender systems to decide what content to present to users. Producers, in turn, often create content that is likely to be recommended to users and have users engage with it. To do so, producers try to align their content with the preferences of their targeted user base. In this work, we explore the equilibrium behavior of producers who are interested in maximizing user engagement. We study two variants of the content-serving rule for the platform's recommender system, and provide a structural characterization of producer behavior at equilibrium: namely, each producer chooses to focus on a single embedded feature. We further show that specialization, defined as different producers optimizing for distinct types of content, naturally emerges from the competition among producers trying to maximize user engagement. We provide a heuristic for computing equilibria of our engagement game, and evaluate it experimentally. We highlight i) the performance and convergence of our heuristic, ii) the degree of producer specialization, and iii) the impact of the content-serving rule on producer and user utilities at equilibrium and provide guidance on how to set the content-serving rule.",
        "timestamp": "2025-03-22T16:57:15.114Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "Transactions on Machine Learning Research",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-22T16:57:15+00:00",
        "updated_at": "2025-03-29T22:41:53+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2112.08272": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2112.08272",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-22T18:05:54.485Z",
            "data": {
              "session_id": "session_1742666753827_kyzr0md",
              "source_id": "arxiv",
              "paper_id": "2112.08272",
              "start_time": "2025-03-22T18:05:40.180Z",
              "end_time": "2025-03-22T18:05:53.827Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-25T04:57:21.053Z",
            "data": {
              "session_id": "session_1742878641040_f98roh9",
              "source_id": "arxiv",
              "paper_id": "2112.08272",
              "start_time": "2025-03-25T04:57:15.875Z",
              "end_time": "2025-03-25T04:57:21.040Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-22T18:05:55+00:00",
        "updated_at": "2025-03-29T22:41:49+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2112.08272": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2112.08272",
        "url": "https://arxiv.org/pdf/2112.08272",
        "title": "2112.08272",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-22T18:05:39.010Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-22T18:05:39+00:00",
        "updated_at": "2025-03-29T22:41:50+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2305.17218": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2305.17218",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-22T18:05:34.094Z",
            "data": {
              "session_id": "session_1742666733354_6j3y131",
              "source_id": "arxiv",
              "paper_id": "2305.17218",
              "start_time": "2025-03-22T18:05:21.929Z",
              "end_time": "2025-03-22T18:05:33.354Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-25T04:57:38.902Z",
            "data": {
              "session_id": "session_1742878658886_dwfftuu",
              "source_id": "arxiv",
              "paper_id": "2305.17218",
              "start_time": "2025-03-25T04:57:24.251Z",
              "end_time": "2025-03-25T04:57:38.886Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 5,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-22T18:05:34+00:00",
        "updated_at": "2025-03-29T22:41:50+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2305.17218": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2305.17218",
        "url": "https://arxiv.org/pdf/2305.17218",
        "title": "2305.17218",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-22T18:05:22.438Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-22T18:05:22+00:00",
        "updated_at": "2025-03-29T22:41:51+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2501.18593": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.18593",
        "url": "https://arxiv.org/pdf/2501.18593",
        "title": "2501.18593",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-22T21:47:46.689Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-22T21:47:47+00:00",
        "updated_at": "2025-03-29T22:41:48+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.11056": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.11056",
        "url": "https://arxiv.org/abs/2503.11056",
        "title": "Flow to the Mode: Mode-Seeking Diffusion Autoencoders for State-of-the-Art Image Tokenization",
        "authors": "Sargent, Kyle, Hsu, Kyle, Johnson, Justin, Fei-Fei, Li, Wu, Jiajun",
        "abstract": "Since the advent of popular visual generation frameworks like VQGAN and latent diffusion models, state-of-the-art image generation systems have generally been two-stage systems that first tokenize or compress visual data into a lower-dimensional latent space before learning a generative model. Tokenizer training typically follows a standard recipe in which images are compressed and reconstructed subject to a combination of MSE, perceptual, and adversarial losses. Diffusion autoencoders have been proposed in prior work as a way to learn end-to-end perceptually-oriented image compression, but have not yet shown state-of-the-art performance on the competitive task of ImageNet-1K reconstruction. We propose FlowMo, a transformer-based diffusion autoencoder that achieves a new state-of-the-art for image tokenization at multiple compression rates without using convolutions, adversarial losses, spatially-aligned two-dimensional latent codes, or distilling from other tokenizers. Our key insight is that FlowMo training should be broken into a mode-matching pre-training stage and a mode-seeking post-training stage. In addition, we conduct extensive analyses and explore the training of generative models atop the FlowMo tokenizer. Our code and models will be available at http://kylesargent.github.io/flowmo .",
        "timestamp": "2025-03-22T21:31:59.562Z",
        "rating": "novote",
        "publishedDate": "2025/03/14",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-22T21:32:00+00:00",
        "updated_at": "2025-03-29T22:41:48+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2412.06264": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2412.06264",
        "url": "https://arxiv.org/abs/2412.06264",
        "title": "Flow Matching Guide and Code",
        "authors": "Lipman, Yaron, Havasi, Marton, Holderrieth, Peter, Shaul, Neta, Le, Matt, Karrer, Brian, Chen, Ricky T. Q., Lopez-Paz, David, Ben-Hamu, Heli, Gat, Itai",
        "abstract": "Flow Matching (FM) is a recent framework for generative modeling that has achieved state-of-the-art performance across various domains, including image, video, audio, speech, and biological structures. This guide offers a comprehensive and self-contained review of FM, covering its mathematical foundations, design choices, and extensions. By also providing a PyTorch package featuring relevant examples (e.g., image and text generation), this work aims to serve as a resource for both novice and experienced researchers interested in understanding, applying and further developing FM.",
        "timestamp": "2025-03-22T21:01:04.472Z",
        "rating": "novote",
        "publishedDate": "2024/12/09",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-22T21:01:04+00:00",
        "updated_at": "2025-03-29T22:41:49+00:00",
        "version": 2
      }
    },
    "interactions:url.2D6B792C": {
      "data": {
        "sourceId": "url",
        "paperId": "2D6B792C",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-23T04:19:41.089Z",
            "data": {
              "session_id": "session_1742703580307_ohiatus",
              "source_id": "url",
              "paper_id": "2D6B792C",
              "start_time": "2025-03-23T04:19:33.783Z",
              "end_time": "2025-03-23T04:19:40.307Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-23T04:19:42+00:00",
        "updated_at": "2025-03-29T22:41:47+00:00",
        "version": 3
      }
    },
    "paper:url.2D6B792C": {
      "data": {
        "sourceId": "url",
        "paperId": "2D6B792C",
        "url": "https://journals.sagepub.com/doi/full/10.1177/10597123211066155",
        "title": "Self-organisation, (M, R)\u2013systems and enactive cognitive science - Tomasz Korbak, 2023",
        "authors": "",
        "abstract": "The notion of self-organisation plays a major role in enactive cognitive science. In this paper, I review several formal models of self-organisation that variou...",
        "timestamp": "2025-03-23T04:19:33.445Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "Self-organisation",
          "relational biology",
          "closure to efficient cause",
          "enactivism",
          "Markov blankets"
        ],
        "doi": "",
        "journalName": "Adaptive Behavior",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-23T04:19:33+00:00",
        "updated_at": "2025-03-29T22:41:47+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.16351": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.16351",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-23T09:28:08.433Z",
            "data": {
              "session_id": "session_1742722087711_zo5fqoi",
              "source_id": "arxiv",
              "paper_id": "2503.16351",
              "start_time": "2025-03-23T09:28:00.502Z",
              "end_time": "2025-03-23T09:28:07.711Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-23T09:28:09+00:00",
        "updated_at": "2025-03-29T22:41:46+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.16351": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.16351",
        "url": "https://arxiv.org/abs/2503.16351",
        "title": "Lyra: An Efficient and Expressive Subquadratic Architecture for Modeling Biological Sequences",
        "authors": "Ramesh, Krithik, Siddiqui, Sameed M., Gu, Albert, Mitzenmacher, Michael D., Sabeti, Pardis C.",
        "abstract": "Deep learning architectures such as convolutional neural networks and Transformers have revolutionized biological sequence modeling, with recent advances driven by scaling up foundation and task-specific models. The computational resources and large datasets required, however, limit their applicability in biological contexts. We introduce Lyra, a subquadratic architecture for sequence modeling, grounded in the biological framework of epistasis for understanding sequence-to-function relationships. Mathematically, we demonstrate that state space models efficiently capture global epistatic interactions and combine them with projected gated convolutions for modeling local relationships. We demonstrate that Lyra is performant across over 100 wide-ranging biological tasks, achieving state-of-the-art (SOTA) performance in many key areas, including protein fitness landscape prediction, biophysical property prediction (e.g. disordered protein region functions) peptide engineering applications (e.g. antibody binding, cell-penetrating peptide prediction), RNA structure analysis, RNA function prediction, and CRISPR guide design. It achieves this with orders-of-magnitude improvements in inference speed and reduction in parameters (up to 120,000-fold in our tests) compared to recent biology foundation models. Using Lyra, we were able to train and run every task in this study on two or fewer GPUs in under two hours, democratizing access to biological sequence modeling at SOTA performance, with potential applications to many fields.",
        "timestamp": "2025-03-23T09:28:01.139Z",
        "rating": "novote",
        "publishedDate": "2025/03/20",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-23T09:28:01+00:00",
        "updated_at": "2025-03-29T22:41:47+00:00",
        "version": 2
      }
    },
    "interactions:url.6711017F": {
      "data": {
        "sourceId": "url",
        "paperId": "6711017F",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-23T15:19:42.312Z",
            "data": {
              "session_id": "session_1742743181608_p993j3f",
              "source_id": "url",
              "paper_id": "6711017F",
              "start_time": "2025-03-23T15:19:36.331Z",
              "end_time": "2025-03-23T15:19:41.608Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-23T15:19:43+00:00",
        "updated_at": "2025-03-29T22:41:44+00:00",
        "version": 3
      }
    },
    "paper:url.6711017F": {
      "data": {
        "sourceId": "url",
        "paperId": "6711017F",
        "url": "https://philpapers.org/rec/EKRDLL",
        "title": "Do Large Language Models Hallucinate Electric Fata Morganas?",
        "authors": "Kristina \u0160ekrst",
        "abstract": "This paper explores the intersection of AI hallucinations and the question of AI consciousness, examining whether the erroneous outputs generated by large language models (LLMs) could be mistaken for signs of ...",
        "timestamp": "2025-03-23T15:19:36.737Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "AI hallucinations;large language models;artificial general intelligence;understanding;intentionality;consciousness;artificial consciousness;strong AI;weak AI;AI consciousness"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-23T15:19:37+00:00",
        "updated_at": "2025-03-29T22:41:45+00:00",
        "version": 2
      }
    },
    "paper:url.6AD48491": {
      "data": {
        "sourceId": "url",
        "paperId": "6AD48491",
        "url": "https://philpapers.org/archive/EKRDLL.pdf",
        "title": "6AD48491",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-23T15:19:28.460Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "created_at": "2025-03-23T15:19:28+00:00",
        "updated_at": "2025-03-29T22:41:46+00:00",
        "version": 2
      }
    },
    "interactions:url.4004DCAF": {
      "data": {
        "sourceId": "url",
        "paperId": "4004DCAF",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-23T23:04:22.004Z",
            "data": {
              "session_id": "session_1742771061209_2albp0e",
              "source_id": "url",
              "paper_id": "4004DCAF",
              "start_time": "2025-03-23T23:04:15.245Z",
              "end_time": "2025-03-23T23:04:21.209Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-23T23:04:23+00:00",
        "updated_at": "2025-03-29T22:41:43+00:00",
        "version": 3
      }
    },
    "paper:url.4004DCAF": {
      "data": {
        "sourceId": "url",
        "paperId": "4004DCAF",
        "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4905118",
        "title": "The AI-Copyright Trap",
        "authors": "Craig, Carys J.",
        "abstract": "As AI tools proliferate, policy makers are increasingly being called upon to protect creators and the cultural industries from the extractive, exploitative, and",
        "timestamp": "2025-03-23T23:04:13.650Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "SSRN",
          "The AI-Copyright Trap",
          "Carys J. Craig"
        ],
        "doi": "10.2139/ssrn.4905118",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-23T23:04:14+00:00",
        "updated_at": "2025-03-29T22:41:43+00:00",
        "version": 2
      }
    },
    "paper:url.104FF02E": {
      "data": {
        "sourceId": "url",
        "paperId": "104FF02E",
        "url": "https://www.researchgate.net/publication/371442901_Paradoxes_Within_the_Management_of_Volunteers",
        "title": "(PDF) Paradoxes Within the Management of Volunteers",
        "authors": "",
        "abstract": "PDF | While scholars of management have extensively discussed paradoxes, scholars of volunteer management have given them little systematic attention.... | Find, read and cite all the research you need on ResearchGate",
        "timestamp": "2025-03-23T22:58:47.054Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-23T22:58:47+00:00",
        "updated_at": "2025-03-29T22:41:44+00:00",
        "version": 2
      }
    },
    "interactions:url.16C098CB": {
      "data": {
        "sourceId": "url",
        "paperId": "16C098CB",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-24T01:25:48.516Z",
            "data": {
              "session_id": "session_1742779547401_4u9vqmz",
              "source_id": "url",
              "paper_id": "16C098CB",
              "start_time": "2025-03-24T01:25:39.237Z",
              "end_time": "2025-03-24T01:25:47.401Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-24T01:25:49+00:00",
        "updated_at": "2025-03-29T22:41:41+00:00",
        "version": 3
      }
    },
    "paper:url.16C098CB": {
      "data": {
        "sourceId": "url",
        "paperId": "16C098CB",
        "url": "https://peerj.com/articles/16349/",
        "title": "Interactive bioacoustic playback as a tool for detecting and exploring nonhuman intelligence: \u201cconversing\u201d with an Alaskan humpback whale",
        "authors": "Brenda McCowan, Josephine Hubbard, Lisa Walker, Fred Sharpe, Jodi Frediani, Laurance Doyle",
        "abstract": "Here we report on a rare and opportunistic acoustic turn-taking with an adult female humpback whale, known as Twain, in Southeast Alaska. Post hoc acoustic and statistical analyses of a 20-min acoustic exchange between the broadcast of a recorded contact call, known as a \u2018whup/throp\u2019, with call responses by Twain revealed an intentional human-whale acoustic (and behavioral) interaction. Our results show that Twain participated both physically and acoustically in three phases of interaction (Phase 1: Engagement, Phase 2: Agitation, Phase 3: Disengagement), independently determined by blind observers reporting on surface behavior and respiratory activity of the interacting whale. A close examination of both changes to the latency between Twain\u2019s calls and the temporal matching to the latency of the exemplar across phases indicated that Twain was actively engaged in the exchange during Phase 1 (Engagement), less so during Phase 2 (Agitation), and disengaged during Phase 3 (Disengagement). These results, while preliminary, point to several key considerations for effective playback design, namely the importance of salient, dynamic and adaptive playbacks, that should be utilized in experimentation with whales and other interactive nonhuman species.",
        "timestamp": "2025-03-24T01:25:38.507Z",
        "rating": "novote",
        "publishedDate": "2023-11-29",
        "tags": [],
        "doi": "10.7717/peerj.16349",
        "journalName": "PeerJ",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-24T01:25:38+00:00",
        "updated_at": "2025-03-29T22:41:42+00:00",
        "version": 2
      }
    },
    "interactions:url.2C7FB16B": {
      "data": {
        "sourceId": "url",
        "paperId": "2C7FB16B",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-24T01:24:39.725Z",
            "data": {
              "session_id": "session_1742779478903_p6hv511",
              "source_id": "url",
              "paper_id": "2C7FB16B",
              "start_time": "2025-03-24T01:24:18.281Z",
              "end_time": "2025-03-24T01:24:38.903Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 1,
              "total_elapsed_seconds": 21
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-24T01:24:40+00:00",
        "updated_at": "2025-03-29T22:41:42+00:00",
        "version": 3
      }
    },
    "paper:url.2C7FB16B": {
      "data": {
        "sourceId": "url",
        "paperId": "2C7FB16B",
        "url": "https://www.science.org/doi/10.1126/science.adq7055",
        "title": "Whale song shows language-like statistical structure",
        "authors": "",
        "abstract": "Humpback whale song is a culturally transmitted behavior. Human language, which is also culturally transmitted, has statistically coherent parts whose frequency distribution follows a power law. These properties facilitate learning and may therefore ...",
        "timestamp": "2025-03-24T01:24:18.876Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "Science",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-24T01:24:19+00:00",
        "updated_at": "2025-03-29T22:41:42+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2211.11081": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2211.11081",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-24T01:36:20.930Z",
            "data": {
              "session_id": "session_1742780180915_sasyexw",
              "source_id": "arxiv",
              "paper_id": "2211.11081",
              "start_time": "2025-03-24T01:35:47.610Z",
              "end_time": "2025-03-24T01:36:20.915Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 3,
              "total_elapsed_seconds": 33
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-24T01:36:21+00:00",
        "updated_at": "2025-03-29T22:41:39+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2303.10931": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2303.10931",
        "url": "https://arxiv.org/abs/2303.10931",
        "title": "Approaching an unknown communication system by latent space exploration and causal inference",
        "authors": "Begu\u0161, Ga\u0161per, Leban, Andrej, Gero, Shane",
        "abstract": "This paper proposes a methodology for discovering meaningful properties in data by exploring the latent space of unsupervised deep generative models. We combine manipulation of individual latent variables to extreme values with methods inspired by causal inference into an approach we call causal disentanglement with extreme values (CDEV) and show that this method yields insights for model interpretability. With this, we can test for what properties of unknown data the model encodes as meaningful, using it to glean insight into the communication system of sperm whales (Physeter macrocephalus), one of the most intriguing and understudied animal communication systems. The network architecture used has been shown to learn meaningful representations of speech; here, it is used as a learning mechanism to decipher the properties of another vocal communication system in which case we have no ground truth. The proposed methodology suggests that sperm whales encode information using the number of clicks in a sequence, the regularity of their timing, and audio properties such as the spectral mean and the acoustic regularity of the sequences. Some of these findings are consistent with existing hypotheses, while others are proposed for the first time. We also argue that our models uncover rules that govern the structure of units in the communication system and apply them while generating innovative data not shown during training. This paper suggests that an interpretation of the outputs of deep neural networks with causal inference methodology can be a viable strategy for approaching data about which little is known and presents another case of how deep learning can limit the hypothesis space. Finally, the proposed approach can be extended to other architectures and datasets.",
        "timestamp": "2025-03-24T01:35:47.316Z",
        "rating": "novote",
        "publishedDate": "2023/03/20",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-24T01:35:47+00:00",
        "updated_at": "2025-03-29T22:41:39+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2211.11081": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2211.11081",
        "url": "https://arxiv.org/abs/2211.11081",
        "title": "A Theory of Unsupervised Translation Motivated by Understanding Animal Communication",
        "authors": "Goldwasser, Shafi, Gruber, David F., Kalai, Adam Tauman, Paradise, Orr",
        "abstract": "Neural networks are capable of translating between languages -- in some cases even between two languages where there is little or no access to parallel translations, in what is known as Unsupervised Machine Translation (UMT). Given this progress, it is intriguing to ask whether machine learning tools can ultimately enable understanding animal communication, particularly that of highly intelligent animals. We propose a theoretical framework for analyzing UMT when no parallel translations are available and when it cannot be assumed that the source and target corpora address related subject domains or posses similar linguistic structure. We exemplify this theory with two stylized models of language, for which our framework provides bounds on necessary sample complexity; the bounds are formally proven and experimentally verified on synthetic data. These bounds show that the error rates are inversely related to the language complexity and amount of common ground. This suggests that unsupervised translation of animal communication may be feasible if the communication system is sufficiently complex.",
        "timestamp": "2025-03-24T01:35:43.298Z",
        "rating": "novote",
        "publishedDate": "2022/11/20",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-24T01:35:43+00:00",
        "updated_at": "2025-03-29T22:41:40+00:00",
        "version": 2
      }
    },
    "interactions:url.4E020D77": {
      "data": {
        "sourceId": "url",
        "paperId": "4E020D77",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-24T01:35:23.950Z",
            "data": {
              "session_id": "session_1742780123197_ctfeq7p",
              "source_id": "url",
              "paper_id": "4E020D77",
              "start_time": "2025-03-24T01:35:06.822Z",
              "end_time": "2025-03-24T01:35:23.197Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 1,
              "total_elapsed_seconds": 16
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-24T01:35:24+00:00",
        "updated_at": "2025-03-29T22:41:40+00:00",
        "version": 3
      }
    },
    "paper:url.4E020D77": {
      "data": {
        "sourceId": "url",
        "paperId": "4E020D77",
        "url": "https://drive.google.com/file/d/1O8Y1OM3YUl_0EgjIl4IK3og14-LCACfp/view",
        "title": "01. Contextual and Combinatorial Structure in Sperm Whale Vocalisations.pdf",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-24T01:35:07.398Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-24T01:35:07+00:00",
        "updated_at": "2025-03-29T22:41:41+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2303.10931": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2303.10931",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-24T01:44:52.822Z",
            "data": {
              "session_id": "session_1742780692401_lp9mo6z",
              "source_id": "arxiv",
              "paper_id": "2303.10931",
              "start_time": "2025-03-24T01:44:42.028Z",
              "end_time": "2025-03-24T01:44:52.401Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 0,
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-24T01:47:31.693Z",
            "data": {
              "session_id": "session_1742780851031_tcik5lr",
              "source_id": "arxiv",
              "paper_id": "2303.10931",
              "start_time": "2025-03-24T01:46:31.015Z",
              "end_time": "2025-03-24T01:47:31.031Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 15,
              "total_elapsed_seconds": 60
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-24T01:44:53+00:00",
        "updated_at": "2025-03-29T22:41:38+00:00",
        "version": 4
      }
    },
    "interactions:arxiv.2503.02574": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.02574",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-24T02:25:08.774Z",
            "data": {
              "session_id": "session_1742783108012_c5kkvv5",
              "source_id": "arxiv",
              "paper_id": "2503.02574",
              "start_time": "2025-03-24T02:24:54.462Z",
              "end_time": "2025-03-24T02:25:08.012Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-24T02:25:09+00:00",
        "updated_at": "2025-03-29T22:41:37+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.02574": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.02574",
        "url": "https://arxiv.org/abs/2503.02574",
        "title": "LLM-Safety Evaluations Lack Robustness",
        "authors": "Beyer, Tim, Xhonneux, Sophie, Geisler, Simon, Gidel, Gauthier, Schwinn, Leo, G\u00fcnnemann, Stephan",
        "abstract": "In this paper, we argue that current safety alignment research efforts for large language models are hindered by many intertwined sources of noise, such as small datasets, methodological inconsistencies, and unreliable evaluation setups. This can, at times, make it impossible to evaluate and compare attacks and defenses fairly, thereby slowing progress. We systematically analyze the LLM safety evaluation pipeline, covering dataset curation, optimization strategies for automated red-teaming, response generation, and response evaluation using LLM judges. At each stage, we identify key issues and highlight their practical impact. We also propose a set of guidelines for reducing noise and bias in evaluations of future attack and defense papers. Lastly, we offer an opposing perspective, highlighting practical reasons for existing limitations. We believe that addressing the outlined problems in future research will improve the field's ability to generate easily comparable results and make measurable progress.",
        "timestamp": "2025-03-24T02:24:55.031Z",
        "rating": "novote",
        "publishedDate": "2025/03/04",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-24T02:24:55+00:00",
        "updated_at": "2025-03-29T22:41:38+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.17208": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.17208",
        "url": "https://arxiv.org/abs/2503.17208",
        "title": "Hamiltonian Chaos: From Galactic Dynamics to Plasma Physics",
        "authors": "Moges, Henok Tenaw",
        "abstract": "The primary focus of this thesis is the numerical investigation of chaos in Hamiltonian models describing charged particle orbits in plasma, star motions in barred galaxies, and orbits' diffusion in multidimensional maps. We systematically explore the interplay between magnetic and kinetic chaos in toroidal fusion plasmas, where non-axisymmetric perturbations disrupt smooth magnetic flux surfaces, generating complex particle trajectories. Using the Generalized Alignment Index (GALI) method, we efficiently quantify chaos, compare the behavior of magnetic field lines and particle orbits, visualize the radial distribution of chaotic regions, and offer GALI as a valuable tool for studying plasma physics dynamics. We also study the evolution of phase space structures in a 3D barred galactic potential, following successive 2D and 3D pitchfork and period-doubling bifurcations of periodic orbits. By employing the `color and rotation' technique to visualize the system's 4D Poincar\\'e surface of sections, we reveal distinct structural patterns. We further investigate the long-term diffusion transport and chaos properties of single and coupled standard maps, focusing on parameters inducing anomalous diffusion through accelerator modes exhibiting ballistic transport. Using different ensembles of initial conditions in chaotic regions influenced by these modes, we examine asymptotic diffusion rates and time scales, identifying conditions suppressing anomalous transport and leading to long-term convergence to normal diffusion across coupled maps. Lastly, we perform the first comprehensive investigation into the GALI indices for various attractors in continuous and discrete-time dissipative systems, extending the method's application to non-Hamiltonian systems. A key aspect of our work involves analyzing and comparing GALIs' with Lyapunov Exponents for systems exhibiting hyperchaotic motion.",
        "timestamp": "2025-03-24T07:15:21.147Z",
        "rating": "novote",
        "publishedDate": "2025/03/21",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-24T07:15:21+00:00",
        "updated_at": "2025-03-29T22:41:36+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.16941": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.16941",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-24T07:14:06.362Z",
            "data": {
              "session_id": "session_1742800445582_5pe4soj",
              "source_id": "arxiv",
              "paper_id": "2503.16941",
              "start_time": "2025-03-24T07:13:53.543Z",
              "end_time": "2025-03-24T07:14:05.582Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-24T07:14:07+00:00",
        "updated_at": "2025-03-29T22:41:36+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.16941": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.16941",
        "url": "https://arxiv.org/abs/2503.16941",
        "title": "Sparse Additive Contextual Bandits: A Nonparametric Approach for Online Decision-making with High-dimensional Covariates",
        "authors": "Wang, Wenjia, Zhang, Qingwen, Zhang, Xiaowei",
        "abstract": "Personalized services are central to today's digital landscape, where online decision-making is commonly formulated as contextual bandit problems. Two key challenges emerge in modern applications: high-dimensional covariates and the need for nonparametric models to capture complex reward-covariate relationships. We address these challenges by developing a contextual bandit algorithm based on sparse additive reward models in reproducing kernel Hilbert spaces. We establish statistical properties of the doubly penalized method applied to random regions, introducing novel analyses under bandit feedback. Our algorithm achieves sublinear cumulative regret over the time horizon $T$ while scaling logarithmically with covariate dimensionality $d$. Notably, we provide the first regret upper bound with logarithmic growth in $d$ for nonparametric contextual bandits with high-dimensional covariates. We also establish a lower bound, with the gap to the upper bound vanishing as smoothness increases. Extensive numerical experiments demonstrate our algorithm's superior performance in high-dimensional settings compared to existing approaches.",
        "timestamp": "2025-03-24T07:13:54.004Z",
        "rating": "novote",
        "publishedDate": "2025/03/21",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-24T07:13:54+00:00",
        "updated_at": "2025-03-29T22:41:37+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.17157": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.17157",
        "url": "https://arxiv.org/abs/2503.17157v1",
        "title": "Ubiquitous order known as chaos",
        "authors": "Ovchinnikov, Igor V.",
        "abstract": "A close relation has recently emerged between two of the most fundamental concepts in physics and mathematics: chaos and supersymmetry. In striking contrast to the semantics of the word 'chaos,' the true physical essence of this phenomenon now appears to be a spontaneous order associated with the breakdown of the topological supersymmetry (TS) hidden in all stochastic (partial) differential equations, i.e., in all systems from a broad domain ranging from cosmology to nanoscience. Among the low-hanging fruits of this new perspective, which can be called the supersymmetric theory of stochastic dynamics (STS), are theoretical explanations of 1/f noise and self-organized criticality. Central to STS is the physical meaning of TS breaking order parameter (OP). In this paper, we discuss that the OP is a field-theoretic embodiment of the 'butterfly effect' (BE) -- the infinitely long dynamical memory that is definitive of chaos. We stress that the formulation of the corresponding effective theory for the OP would mark the inception of the first consistent physical theory of the BE. Such a theory, potentially a valuable tool in solving chaos-related problems, would parallel the well-established and successful field theoretic descriptions of superconductivity, ferromagentism and other known orders arising from the spontaneous breakdown of various symmetries of nature.",
        "timestamp": "2025-03-24T07:17:15.635Z",
        "rating": "novote",
        "publishedDate": "2025/03/21",
        "tags": [],
        "doi": "10.1016/j.chaos.2024.114611",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-24T07:17:16+00:00",
        "updated_at": "2025-03-29T22:41:35+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.17208": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.17208",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-24T07:15:52.166Z",
            "data": {
              "session_id": "session_1742800552153_nm2pm29",
              "source_id": "arxiv",
              "paper_id": "2503.17208",
              "start_time": "2025-03-24T07:15:31.186Z",
              "end_time": "2025-03-24T07:15:52.153Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 1,
              "total_elapsed_seconds": 21
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-24T07:15:53+00:00",
        "updated_at": "2025-03-29T22:41:35+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.14956": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.14956",
        "url": "https://arxiv.org/abs/2503.14956v1",
        "title": "Machine learning predictions from unpredictable chaos",
        "authors": "Jiang, Jian, Chen, Long, ke, Lu, Dou, Bozheng, Zhu, Yueying, Shi, Yazhou, Qiu, Huahai, Zhang, Bengong, Zhou, Tianshou, Wei, Guo-Wei",
        "abstract": "Chaos is omnipresent in nature, and its understanding provides enormous social and economic benefits. However, the unpredictability of chaotic systems is a textbook concept due to their sensitivity to initial conditions, aperiodic behavior, fractal dimensions, nonlinearity, and strange attractors. In this work, we introduce, for the first time, chaotic learning, a novel multiscale topological paradigm that enables accurate predictions from chaotic systems. We show that seemingly random and unpredictable chaotic dynamics counterintuitively offer unprecedented quantitative predictions. Specifically, we devise multiscale topological Laplacians to embed real-world data into a family of interactive chaotic dynamical systems, modulate their dynamical behaviors, and enable the accurate prediction of the input data. As a proof of concept, we consider 28 datasets from four categories of realistic problems: 10 brain waves, four benchmark protein datasets, 13 single-cell RNA sequencing datasets, and an image dataset, as well as two distinct chaotic dynamical systems, namely the Lorenz and Rossler attractors. We demonstrate chaotic learning predictions of the physical properties from chaos. Our new chaotic learning paradigm profoundly changes the textbook perception of chaos and bridges topology, chaos, and learning for the first time.",
        "timestamp": "2025-03-24T07:22:39.290Z",
        "rating": "novote",
        "publishedDate": "2025/03/19",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-24T07:22:39+00:00",
        "updated_at": "2025-03-29T22:41:33+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.17045": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.17045",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-24T07:20:15.386Z",
            "data": {
              "session_id": "session_1742800814614_ixrul4q",
              "source_id": "arxiv",
              "paper_id": "2503.17045",
              "start_time": "2025-03-24T07:19:42.868Z",
              "end_time": "2025-03-24T07:20:14.614Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 2,
              "total_elapsed_seconds": 32
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-24T07:20:16+00:00",
        "updated_at": "2025-03-29T22:41:33+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.17045": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.17045",
        "url": "https://arxiv.org/abs/2503.17045",
        "title": "Multifractal formalism of Lyapunov exponents for fiber-bunched linear cocycles",
        "authors": "Mohammadpour, Reza, Varandas, Paulo",
        "abstract": "We develop a higher-dimensional extension of multifractal analysis for typical fiber-bunched linear cocycles. Our main result is a relative variational principle, which shows that the topological entropy of the level sets of Lyapunov exponents can be approximated by the metric entropy of ergodic measures fully concentrated on those level sets, addressing a question posed by Breuillard and Sert. We also establish a variational principle for the generalized singular value function. As an application to dynamically defined linear cocycles, we obtain a multifractal formalism for open sets of $C^{1+\\alpha}$ repellers and Anosov diffeomorphisms.",
        "timestamp": "2025-03-24T07:19:42.308Z",
        "rating": "novote",
        "publishedDate": "2025/03/21",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-24T07:19:42+00:00",
        "updated_at": "2025-03-29T22:41:34+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.14702": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.14702",
        "url": "https://arxiv.org/abs/2503.14702v1",
        "title": "Learning Choas In A Linear Way",
        "authors": "Cheng, Xiaoyuan, He, Yi, Yang, Yiming, Xue, Xiao, Chen, Sibo, Giles, Daniel, Tang, Xiaohang, Hu, Yukun",
        "abstract": "Learning long-term behaviors in chaotic dynamical systems, such as turbulent flows and climate modelling, is challenging due to their inherent instability and unpredictability. These systems exhibit positive Lyapunov exponents, which significantly hinder accurate long-term forecasting. As a result, understanding long-term statistical behavior is far more valuable than focusing on short-term accuracy. While autoregressive deep sequence models have been applied to capture long-term behavior, they often lead to exponentially increasing errors in learned dynamics. To address this, we shift the focus from simple prediction errors to preserving an invariant measure in dissipative chaotic systems. These systems have attractors, where trajectories settle, and the invariant measure is the probability distribution on attractors that remains unchanged under dynamics. Existing methods generate long trajectories of dissipative chaotic systems by aligning invariant measures, but it is not always possible to obtain invariant measures for arbitrary datasets. We propose the Poincare Flow Neural Network (PFNN), a novel operator learning framework designed to capture behaviors of chaotic systems without any explicit knowledge of the invariant measure. PFNN employs an auto-encoder to map the chaotic system to a finite-dimensional feature space, effectively linearizing the chaotic evolution. It then learns the linear evolution operators to match the physical dynamics by addressing two critical properties in dissipative chaotic systems: (1) contraction, the system's convergence toward its attractors, and (2) measure invariance, trajectories on the attractors following a probability distribution invariant to the dynamics. Our experiments on a variety of chaotic systems demonstrate that PFNN has more accurate predictions and physical statistics compared to competitive baselines.",
        "timestamp": "2025-03-24T07:24:58.859Z",
        "rating": "novote",
        "publishedDate": "2025/03/18",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-24T07:24:59+00:00",
        "updated_at": "2025-03-29T22:41:32+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.14956": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.14956",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-24T07:22:57.657Z",
            "data": {
              "session_id": "session_1742800976677_l2ni6kt",
              "source_id": "arxiv",
              "paper_id": "2503.14956",
              "start_time": "2025-03-24T07:22:38.740Z",
              "end_time": "2025-03-24T07:22:56.677Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 3,
              "total_elapsed_seconds": 18
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-24T07:22:58+00:00",
        "updated_at": "2025-03-29T22:41:33+00:00",
        "version": 3
      }
    },
    "interactions:arxiv.2503.13586": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.13586",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-24T07:29:01.561Z",
            "data": {
              "session_id": "session_1742801340696_v22ujyo",
              "source_id": "arxiv",
              "paper_id": "2503.13586",
              "start_time": "2025-03-24T07:28:18.098Z",
              "end_time": "2025-03-24T07:29:00.695Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 3,
              "total_elapsed_seconds": 43
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-24T07:29:02+00:00",
        "updated_at": "2025-03-29T22:41:31+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.13586": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.13586",
        "url": "https://arxiv.org/abs/2503.13586v1",
        "title": "Experimental verification of Generalised Synchronization",
        "authors": "Ghosh, Tania, Banerjee, Soumitro",
        "abstract": "Generalized synchronization (GS) describes a state in which two coupled dynamical systems exhibit a functional relationship between their variables. GS can be achieved by appropriately designing the coupling to constrain the dynamics onto an invariant submanifold, a concept well established in theory. However, experimental validation remains crucial. In this work, we experimentally demonstrate GS using coupled chaotic Lorenz oscillators, implementing unidirectional coupling where the state variables of the slave oscillator are scaled according to a predefined relationship with those of the master oscillator.",
        "timestamp": "2025-03-24T07:28:18.537Z",
        "rating": "novote",
        "publishedDate": "2025/03/17",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-24T07:28:19+00:00",
        "updated_at": "2025-03-29T22:41:32+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.08833": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.08833",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-24T07:39:45.869Z",
            "data": {
              "session_id": "session_1742801985104_y7vouyo",
              "source_id": "arxiv",
              "paper_id": "2503.08833",
              "start_time": "2025-03-24T07:39:35.598Z",
              "end_time": "2025-03-24T07:39:45.104Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 5,
              "total_elapsed_seconds": 10
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-24T07:39:46+00:00",
        "updated_at": "2025-03-29T22:41:29+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.08833": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.08833",
        "url": "https://arxiv.org/abs/2503.08833",
        "title": "Randomization in Optimal Execution Games",
        "authors": "Campbell, Steven, Nutz, Marcel",
        "abstract": "We study optimal execution in markets with transient price impact in a competitive setting with $N$ traders. Motivated by prior negative results on the existence of pure Nash equilibria, we consider randomized strategies for the traders and whether allowing such strategies can restore the existence of equilibria. We show that given a randomized strategy, there is a non-randomized strategy with strictly lower expected execution cost, and moreover this de-randomization can be achieved by a simple averaging procedure. As a consequence, Nash equilibria cannot contain randomized strategies, and non-existence of pure equilibria implies non-existence of randomized equilibria. Separately, we also establish uniqueness of equilibria. Both results hold in a general transaction cost model given by a strictly positive definite impact decay kernel and a convex trading cost.",
        "timestamp": "2025-03-24T07:39:35.940Z",
        "rating": "novote",
        "publishedDate": "2025/03/11",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-24T07:39:36+00:00",
        "updated_at": "2025-03-29T22:41:29+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.01686": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.01686",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-24T07:39:20.008Z",
            "data": {
              "session_id": "session_1742801959068_33a4x4j",
              "source_id": "arxiv",
              "paper_id": "2503.01686",
              "start_time": "2025-03-24T07:38:54.492Z",
              "end_time": "2025-03-24T07:39:19.068Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 5,
              "total_elapsed_seconds": 25
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-24T07:39:21+00:00",
        "updated_at": "2025-03-29T22:41:30+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.01686": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.01686",
        "url": "https://arxiv.org/abs/2503.01686",
        "title": "\\textsc{Perseus}: Tracing the Masterminds Behind Cryptocurrency Pump-and-Dump Schemes",
        "authors": "Fu, Honglin, Feng, Yebo, Wu, Cong, Xu, Jiahua",
        "abstract": "Masterminds are entities organizing, coordinating, and orchestrating cryptocurrency pump-and-dump schemes, a form of trade-based manipulation undermining market integrity and causing financial losses for unwitting investors. Previous research detects pump-and-dump activities in the market, predicts the target cryptocurrency, and examines investors and \\ac{osn} entities. However, these solutions do not address the root cause of the problem. There is a critical gap in identifying and tracing the masterminds involved in these schemes. In this research, we develop a detection system \\textsc{Perseus}, which collects real-time data from the \\acs{osn} and cryptocurrency markets. \\textsc{Perseus} then constructs temporal attributed graphs that preserve the direction of information diffusion and the structure of the community while leveraging \\ac{gnn} to identify the masterminds behind pump-and-dump activities. Our design of \\textsc{Perseus} leads to higher F1 scores and precision than the \\ac{sota} fraud detection method, achieving fast training and inferring speeds. Deployed in the real world from February 16 to October 9 2024, \\textsc{Perseus} successfully detects $438$ masterminds who are efficient in the pump-and-dump information diffusion networks. \\textsc{Perseus} provides regulators with an explanation of the risks of masterminds and oversight capabilities to mitigate the pump-and-dump schemes of cryptocurrency.",
        "timestamp": "2025-03-24T07:38:55.046Z",
        "rating": "novote",
        "publishedDate": "2025/03/03",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-24T07:38:55+00:00",
        "updated_at": "2025-03-29T22:41:30+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.08692": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.08692",
        "url": "https://arxiv.org/abs/2503.08692",
        "title": "Detecting Crypto Pump-and-Dump Schemes: A Thresholding-Based Approach to Handling Market Noise",
        "authors": "Karbalaii, Mahya",
        "abstract": "We propose a simple yet robust unsupervised model to detect pump-and-dump events on tokens listed on the Poloniex Exchange platform. By combining threshold-based criteria with exponentially weighted moving averages (EWMA) and volatility measures, our approach effectively distinguishes genuine anomalies from minor trading fluctuations, even for tokens with low liquidity and prolonged inactivity. These characteristics present a unique challenge, as standard anomaly-detection methods often over-flag negligible volume spikes. Our framework overcomes this issue by tailoring both price and volume thresholds to the specific trading patterns observed, resulting in a model that balances high true-positive detection with minimal noise.",
        "timestamp": "2025-03-24T07:32:44.982Z",
        "rating": "novote",
        "publishedDate": "2025/02/27",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-24T07:32:45+00:00",
        "updated_at": "2025-03-29T22:41:31+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2402.09809": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.09809",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-24T21:30:28.550Z",
            "data": {
              "session_id": "session_1742851828533_d1axuf9",
              "source_id": "arxiv",
              "paper_id": "2402.09809",
              "start_time": "2025-03-24T21:30:20.687Z",
              "end_time": "2025-03-24T21:30:28.533Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-24T21:30:29+00:00",
        "updated_at": "2025-03-29T22:41:28+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2402.09809": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.09809",
        "url": "https://arxiv.org/abs/2402.09809",
        "title": "Effective and Scalable Math Support: Evidence on the Impact of an AI- Tutor on Math Achievement in Ghana",
        "authors": "Henkel, Owen, Horne-Robinson, Hannah, Kozhakhmetova, Nessie, Lee, Amanda",
        "abstract": "This study evaluates the impact of Rori, an AI powered conversational math tutor accessible via WhatsApp, on the math performance of approximately 1,000 students in grades 3-9 across 11 schools in Ghana. Each school was assigned to a treatment group or control group; the students in the control group continued their regular math instruction, while students in the treatment group engaged with Rori, for two 30-minute sessions per week over 8 months in addition to regular math instruction. We find that the math growth scores were substantially higher for the treatment group with an effect size of 0.37, and that the results were statistically significant (p < 0.001). The fact that Rori works with basic mobile devices on low-bandwidth data networks gives the intervention strong potential to support personalized learning on other low-and-middle-income countries (LMICs), where laptop ownership and high-speed internet - prerequisite for many video-centered learning platforms - remain extremely limited. While the results should be interpreted judiciously, as they only report on year 1 of the intervention, and future research is necessary to better understand which conditions are necessary for successful implementation, they do suggest that chat-based tutoring solutions leveraging artificial intelligence could offer a costeffective approach to enhancing learning outcomes for millions of students globally.",
        "timestamp": "2025-03-24T21:30:11.325Z",
        "rating": "novote",
        "publishedDate": "2024/02/15",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-24T21:30:11+00:00",
        "updated_at": "2025-03-29T22:41:28+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2502.12447": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.12447",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-24T21:34:28.927Z",
            "data": {
              "session_id": "session_1742852068078_kua92kr",
              "source_id": "arxiv",
              "paper_id": "2502.12447",
              "start_time": "2025-03-24T21:33:32.722Z",
              "end_time": "2025-03-24T21:34:28.078Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 0,
              "total_elapsed_seconds": 55
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-24T21:34:29+00:00",
        "updated_at": "2025-03-29T22:41:26+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2502.12447": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.12447",
        "url": "https://arxiv.org/html/2502.12447v1",
        "title": "Protecting Human Cognition in the Age of AI",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-24T21:33:32.535Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-24T21:33:32+00:00",
        "updated_at": "2025-03-29T22:41:27+00:00",
        "version": 2
      }
    },
    "paper:url.413AD5AB": {
      "data": {
        "sourceId": "url",
        "paperId": "413AD5AB",
        "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4895486",
        "title": "Generative AI Can Harm Learning",
        "authors": "Bastani, Hamsa, Bastani, Osbert, Sungu, Alp, Ge, Haosen, Kabakc\u0131, \u00d6zge, Mariman, Rei",
        "abstract": "Generative artificial intelligence (AI) is poised to revolutionize how humans work, and has already demonstrated promise in significantly improving human produc",
        "timestamp": "2025-03-24T21:33:16.745Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "SSRN",
          "Generative AI Can Harm Learning",
          "Hamsa Bastani",
          "Osbert Bastani",
          "Alp Sungu",
          "Haosen Ge",
          "\u00d6zge Kabakc\u0131",
          "Rei Mariman"
        ],
        "doi": "10.2139/ssrn.4895486",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-24T21:33:17+00:00",
        "updated_at": "2025-03-29T22:41:28+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2401.10196": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2401.10196",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-24T22:40:04.109Z",
            "data": {
              "session_id": "session_1742856003324_pmpe50a",
              "source_id": "arxiv",
              "paper_id": "2401.10196",
              "start_time": "2025-03-24T22:39:51.512Z",
              "end_time": "2025-03-24T22:40:03.324Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-24T22:40:05+00:00",
        "updated_at": "2025-03-29T22:41:22+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2401.10196": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2401.10196",
        "url": "https://arxiv.org/abs/2401.10196",
        "title": "Functional Gaussian Graphical Regression Models For Air Quality Data",
        "authors": "Fici, Rita, Sottile, Gianluca, Augugliaro, Luigi, Wit, Ernst-Jan Camiel",
        "abstract": "Functional data describe a wide range of processes, such as growth curves and spectral absorption. In this study, we analyze air pollution data from the In-service Aircraft for a Global Observing System, focusing on the spatial interactions among chemicals in the atmosphere and their dependence on meteorological conditions. This requires functional regression, where both response and covariates are functional objects evolving over the troposphere. Evaluating both the functional relatedness between the response and covariates and the relatedness of a multivariate response function can be challenging. We propose a solution to these challenges by introducing a functional Gaussian graphical regression model, extending conditional Gaussian graphical models to partially separable functions. To estimate the model, we propose a doubly-penalized estimator. Additionally, we present a novel adaptation of Kullback-Leibler cross-validation tailored for graph estimators which accounts for precision and regression matrices when the population presents one or more sub-groups, named joint Kullback-Leibler cross-validation. Evaluation of model performance is done in terms of Kullback-Leibler divergence and graph recovery power.",
        "timestamp": "2025-03-24T22:39:52.119Z",
        "rating": "novote",
        "publishedDate": "2024/01/18",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-24T22:39:52+00:00",
        "updated_at": "2025-03-29T22:41:23+00:00",
        "version": 2
      }
    },
    "paper:arxiv.1108.5083": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1108.5083",
        "url": "https://arxiv.org/abs/1108.5083",
        "title": "A colour scheme for the display of astronomical intensity images",
        "authors": "Green, D. A.",
        "abstract": "I describe a colour scheme that is appropriate for the screen display of intensity images. This -- unlike many currently available schemes -- is designed to be monotonically increasing in terms of its perceived brightness. Also, when printed on a black and white postscript printer, the scheme results in a greyscale with monotonically increasing brightness. This scheme has recently been incorporated into the radio astronomical analysis packages CASA and AIPS.",
        "timestamp": "2025-03-24T22:38:01.582Z",
        "rating": "novote",
        "publishedDate": "2011/08/25",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-24T22:38:02+00:00",
        "updated_at": "2025-03-29T22:41:23+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2501.02950": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.02950",
        "url": "https://arxiv.org/abs/2501.02950",
        "title": "Key-value memory in the brain",
        "authors": "Gershman, Samuel J., Fiete, Ila, Irie, Kazuki",
        "abstract": "Classical models of memory in psychology and neuroscience rely on similarity-based retrieval of stored patterns, where similarity is a function of retrieval cues and the stored patterns. While parsimonious, these models do not allow distinct representations for storage and retrieval, despite their distinct computational demands. Key-value memory systems, in contrast, distinguish representations used for storage (values) and those used for retrieval (keys). This allows key-value memory systems to optimize simultaneously for fidelity in storage and discriminability in retrieval. We review the computational foundations of key-value memory, its role in modern machine learning systems, related ideas from psychology and neuroscience, applications to a number of empirical puzzles, and possible biological implementations.",
        "timestamp": "2025-03-24T21:55:09.844Z",
        "rating": "novote",
        "publishedDate": "2025/01/06",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-24T21:55:10+00:00",
        "updated_at": "2025-03-29T22:41:25+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2502.07709": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.07709",
        "url": "https://arxiv.org/abs/2502.07709",
        "title": "MAGELLAN: Metacognitive predictions of learning progress guide autotelic LLM agents in large goal spaces",
        "authors": "Gaven, Loris, Carta, Thomas, Romac, Cl\u00e9ment, Colas, C\u00e9dric, Lamprier, Sylvain, Sigaud, Olivier, Oudeyer, Pierre-Yves",
        "abstract": "Open-ended learning agents must efficiently prioritize goals in vast possibility spaces, focusing on those that maximize learning progress (LP). When such autotelic exploration is achieved by LLM agents trained with online RL in high-dimensional and evolving goal spaces, a key challenge for LP prediction is modeling one's own competence, a form of metacognitive monitoring. Traditional approaches either require extensive sampling or rely on brittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive framework that lets LLM agents learn to predict their competence and LP online. By capturing semantic relationships between goals, MAGELLAN enables sample-efficient LP estimation and dynamic adaptation to evolving goal spaces through generalization. In an interactive learning environment, we show that MAGELLAN improves LP prediction efficiency and goal prioritization, being the only method allowing the agent to fully master a large and evolving goal space. These results demonstrate how augmenting LLM agents with a metacognitive ability for LP predictions can effectively scale curriculum learning to open-ended goal spaces.",
        "timestamp": "2025-03-24T21:52:17.235Z",
        "rating": "novote",
        "publishedDate": "2025/02/11",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-24T21:52:17+00:00",
        "updated_at": "2025-03-29T22:41:25+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2502.14258": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.14258",
        "url": "https://arxiv.org/abs/2502.14258",
        "title": "Does Time Have Its Place? Temporal Heads: Where Language Models Recall Time-specific Information",
        "authors": "Park, Yein, Yoon, Chanwoong, Park, Jungwoo, Jeong, Minbyul, Kang, Jaewoo",
        "abstract": "While the ability of language models to elicit facts has been widely investigated, how they handle temporally changing facts remains underexplored. We discover Temporal Heads, specific attention heads primarily responsible for processing temporal knowledge through circuit analysis. We confirm that these heads are present across multiple models, though their specific locations may vary, and their responses differ depending on the type of knowledge and its corresponding years. Disabling these heads degrades the model's ability to recall time-specific knowledge while maintaining its general capabilities without compromising time-invariant and question-answering performances. Moreover, the heads are activated not only numeric conditions (\"In 2004\") but also textual aliases (\"In the year ...\"), indicating that they encode a temporal dimension beyond simple numerical representation. Furthermore, we expand the potential of our findings by demonstrating how temporal knowledge can be edited by adjusting the values of these heads.",
        "timestamp": "2025-03-24T21:51:58.246Z",
        "rating": "novote",
        "publishedDate": "2025/02/20",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-24T21:51:58+00:00",
        "updated_at": "2025-03-29T22:41:26+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.05139": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.05139",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-24T22:46:21.223Z",
            "data": {
              "session_id": "session_1742856380523_ccglds1",
              "source_id": "arxiv",
              "paper_id": "2503.05139",
              "start_time": "2025-03-24T22:46:03.434Z",
              "end_time": "2025-03-24T22:46:20.523Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 2,
              "total_elapsed_seconds": 17
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-24T22:46:22+00:00",
        "updated_at": "2025-03-29T22:41:21+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.05139": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.05139",
        "url": "https://arxiv.org/abs/2503.05139",
        "title": "Every FLOP Counts: Scaling a 300B Mixture-of-Experts LING LLM without Premium GPUs",
        "authors": "Ling Team, Zeng, Binwei, Huang, Chao, Zhang, Chao, Tian, Changxin, Chen, Cong, Jin, Dingnan, Yu, Feng, Zhu, Feng, Yuan, Feng, Wang, Fakang, Wang, Gangshan, Zhai, Guangyao, Zhang, Haitao, Li, Huizhong, Zhou, Jun, Liu, Jia, Fang, Junpeng, Ou, Junjie, Hu, Jun, Luo, Ji, Zhang, Ji, Liu, Jian, Sha, Jian, Qian, Jianxue, Wu, Jiewei, Zhao, Junping, Li, Jianguo, Feng, Jubao, Di, Jingchao, Xu, Junming, Yao, Jinghua, Xu, Kuan, Du, Kewei, Li, Longfei, Liang, Lei, Yu, Lu, Tang, Li, Ju, Lin, Xu, Peng, Cui, Qing, Liu, Song, Li, Shicheng, Song, Shun, Yan, Song, Cai, Tengwei, Chen, Tianyi, Guo, Ting, Huang, Ting, Feng, Tao, Wu, Tao, Wu, Wei, Zhang, Xiaolu, Yang, Xueming, Zhao, Xin, Hu, Xiaobo, Lin, Xin, Zhao, Yao, Wang, Yilong, Guo, Yongzhen, Wang, Yuanyuan, Yang, Yue, Cao, Yang, Fu, Yuhao, Xiong, Yi, Li, Yanzhe, Li, Zhe, Zhang, Zhiqiang, Liu, Ziqi, Huan, Zhaoxin, Wen, Zujie, Sun, Zhenhang, Du, Zhuoxuan, He, Zhengyu",
        "abstract": "In this technical report, we tackle the challenges of training large-scale Mixture of Experts (MoE) models, focusing on overcoming cost inefficiency and resource limitations prevalent in such systems. To address these issues, we present two differently sized MoE large language models (LLMs), namely Ling-Lite and Ling-Plus (referred to as \"Bailing\" in Chinese, spelled B\\v{a}il\\'ing in Pinyin). Ling-Lite contains 16.8 billion parameters with 2.75 billion activated parameters, while Ling-Plus boasts 290 billion parameters with 28.8 billion activated parameters. Both models exhibit comparable performance to leading industry benchmarks. This report offers actionable insights to improve the efficiency and accessibility of AI development in resource-constrained settings, promoting more scalable and sustainable technologies. Specifically, to reduce training costs for large-scale MoE models, we propose innovative methods for (1) optimization of model architecture and training processes, (2) refinement of training anomaly handling, and (3) enhancement of model evaluation efficiency. Additionally, leveraging high-quality data generated from knowledge graphs, our models demonstrate superior capabilities in tool use compared to other models. Ultimately, our experimental findings demonstrate that a 300B MoE LLM can be effectively trained on lower-performance devices while achieving comparable performance to models of a similar scale, including dense and MoE models. Compared to high-performance devices, utilizing a lower-specification hardware system during the pre-training phase demonstrates significant cost savings, reducing computing costs by approximately 20%. The models can be accessed at https://huggingface.co/inclusionAI.",
        "timestamp": "2025-03-24T22:46:03.244Z",
        "rating": "novote",
        "publishedDate": "2025/03/07",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-24T22:46:03+00:00",
        "updated_at": "2025-03-29T22:41:22+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2310.06816": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2310.06816",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-25T01:04:32.238Z",
            "data": {
              "session_id": "session_1742864671330_u9bcksn",
              "source_id": "arxiv",
              "paper_id": "2310.06816",
              "start_time": "2025-03-25T01:03:39.004Z",
              "end_time": "2025-03-25T01:04:31.330Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 2,
              "total_elapsed_seconds": 52
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-25T01:08:05.457Z",
            "data": {
              "session_id": "session_1742864885218_moqcsa0",
              "source_id": "arxiv",
              "paper_id": "2310.06816",
              "start_time": "2025-03-25T01:07:59.990Z",
              "end_time": "2025-03-25T01:08:05.218Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-25T01:04:33+00:00",
        "updated_at": "2025-03-29T22:41:20+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2310.06816": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2310.06816",
        "url": "https://arxiv.org/abs/2310.06816",
        "title": "Text Embeddings Reveal (Almost) As Much As Text",
        "authors": "Morris, John X., Kuleshov, Volodymyr, Shmatikov, Vitaly, Rush, Alexander M.",
        "abstract": "How much private information do text embeddings reveal about the original text? We investigate the problem of embedding \\textit{inversion}, reconstructing the full text represented in dense text embeddings. We frame the problem as controlled generation: generating text that, when reembedded, is close to a fixed point in latent space. We find that although a na\\\"ive model conditioned on the embedding performs poorly, a multi-step method that iteratively corrects and re-embeds text is able to recover $92\\%$ of $32\\text{-token}$ text inputs exactly. We train our model to decode text embeddings from two state-of-the-art embedding models, and also show that our model can recover important personal information (full names) from a dataset of clinical notes. Our code is available on Github: \\href{https://github.com/jxmorris12/vec2text}{github.com/jxmorris12/vec2text}.",
        "timestamp": "2025-03-25T01:03:39.016Z",
        "rating": "novote",
        "publishedDate": "2023/10/10",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-25T01:03:39+00:00",
        "updated_at": "2025-03-29T22:41:21+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.17299": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.17299",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-25T01:31:32.392Z",
            "data": {
              "session_id": "session_1742866291650_h6n62vm",
              "source_id": "arxiv",
              "paper_id": "2503.17299",
              "start_time": "2025-03-25T01:31:23.889Z",
              "end_time": "2025-03-25T01:31:31.650Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-25T01:31:33+00:00",
        "updated_at": "2025-03-29T22:41:19+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.17299": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.17299",
        "url": "https://arxiv.org/abs/2503.17299",
        "title": "Preference-Guided Diffusion for Multi-Objective Offline Optimization",
        "authors": "Annadani, Yashas, Belakaria, Syrine, Ermon, Stefano, Bauer, Stefan, Engelhardt, Barbara E",
        "abstract": "Offline multi-objective optimization aims to identify Pareto-optimal solutions given a dataset of designs and their objective values. In this work, we propose a preference-guided diffusion model that generates Pareto-optimal designs by leveraging a classifier-based guidance mechanism. Our guidance classifier is a preference model trained to predict the probability that one design dominates another, directing the diffusion model toward optimal regions of the design space. Crucially, this preference model generalizes beyond the training distribution, enabling the discovery of Pareto-optimal solutions outside the observed dataset. We introduce a novel diversity-aware preference guidance, augmenting Pareto dominance preference with diversity criteria. This ensures that generated solutions are optimal and well-distributed across the objective space, a capability absent in prior generative methods for offline multi-objective optimization. We evaluate our approach on various continuous offline multi-objective optimization tasks and find that it consistently outperforms other inverse/generative approaches while remaining competitive with forward/surrogate-based optimization methods. Our results highlight the effectiveness of classifier-guided diffusion models in generating diverse and high-quality solutions that approximate the Pareto front well.",
        "timestamp": "2025-03-25T01:31:24.285Z",
        "rating": "novote",
        "publishedDate": "2025/03/21",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-25T01:31:24+00:00",
        "updated_at": "2025-03-29T22:41:20+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2110.00675": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2110.00675",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-25T03:36:48.339Z",
            "data": {
              "session_id": "session_1742873807450_lgqmd4m",
              "source_id": "arxiv",
              "paper_id": "2110.00675",
              "start_time": "2025-03-25T03:36:36.341Z",
              "end_time": "2025-03-25T03:36:47.450Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-25T03:36:49+00:00",
        "updated_at": "2025-03-29T22:41:18+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2110.00675": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2110.00675",
        "url": "https://arxiv.org/abs/2110.00675",
        "title": "Contraction Theory for Nonlinear Stability Analysis and Learning-based Control: A Tutorial Overview",
        "authors": "Tsukamoto, Hiroyasu, Chung, Soon-Jo, Slotine, Jean-Jacques E.",
        "abstract": "Contraction theory is an analytical tool to study differential dynamics of a non-autonomous (i.e., time-varying) nonlinear system under a contraction metric defined with a uniformly positive definite matrix, the existence of which results in a necessary and sufficient characterization of incremental exponential stability of multiple solution trajectories with respect to each other. By using a squared differential length as a Lyapunov-like function, its nonlinear stability analysis boils down to finding a suitable contraction metric that satisfies a stability condition expressed as a linear matrix inequality, indicating that many parallels can be drawn between well-known linear systems theory and contraction theory for nonlinear systems. Furthermore, contraction theory takes advantage of a superior robustness property of exponential stability used in conjunction with the comparison lemma. This yields much-needed safety and stability guarantees for neural network-based control and estimation schemes, without resorting to a more involved method of using uniform asymptotic stability for input-to-state stability. Such distinctive features permit the systematic construction of a contraction metric via convex optimization, thereby obtaining an explicit exponential bound on the distance between a time-varying target trajectory and solution trajectories perturbed externally due to disturbances and learning errors. The objective of this paper is, therefore, to present a tutorial overview of contraction theory and its advantages in nonlinear stability analysis of deterministic and stochastic systems, with an emphasis on deriving formal robustness and stability guarantees for various learning-based and data-driven automatic control methods. In particular, we provide a detailed review of techniques for finding contraction metrics and associated control and estimation laws using deep neural networks.",
        "timestamp": "2025-03-25T03:36:36.830Z",
        "rating": "novote",
        "publishedDate": "2021/10/01",
        "tags": [],
        "doi": "10.1016/j.arcontrol.2021.10.001",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-25T03:36:37+00:00",
        "updated_at": "2025-03-29T22:41:19+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2502.00212": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.00212",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-25T04:30:45.124Z",
            "data": {
              "session_id": "session_1742877044556_g7lqv2a",
              "source_id": "arxiv",
              "paper_id": "2502.00212",
              "start_time": "2025-03-25T04:29:34.005Z",
              "end_time": "2025-03-25T04:30:44.556Z",
              "heartbeat_count": 14,
              "duration_seconds": 70,
              "idle_seconds": 1,
              "total_elapsed_seconds": 71
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-25T07:48:57.844Z",
            "data": {
              "session_id": "session_1742888937607_xivkiw1",
              "source_id": "arxiv",
              "paper_id": "2502.00212",
              "start_time": "2025-03-25T07:48:49.664Z",
              "end_time": "2025-03-25T07:48:57.607Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-25T04:30:46+00:00",
        "updated_at": "2025-03-29T22:41:17+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2502.00212": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.00212",
        "url": "https://arxiv.org/abs/2502.00212",
        "title": "STP: Self-play LLM Theorem Provers with Iterative Conjecturing and Proving",
        "authors": "Dong, Kefan, Ma, Tengyu",
        "abstract": "A fundamental challenge in formal theorem proving by LLMs is the lack of high-quality training data. Although reinforcement learning or expert iteration partially mitigates this issue by alternating between LLM generating proofs and finetuning them on correctly generated ones, performance quickly plateaus due to the scarcity of correct proofs (sparse rewards). To keep improving the models with limited data, we draw inspiration from mathematicians, who continuously develop new results, partly by proposing novel conjectures or exercises (which are often variants of known results) and attempting to solve them. We design the Self-play Theorem Prover (STP) that simultaneously takes on two roles, conjecturer and prover, each providing training signals to the other. The conjecturer is trained iteratively on previously generated conjectures that are barely provable by the current prover, which incentivizes it to generate increasingly challenging conjectures over time. The prover attempts to prove the conjectures with standard expert iteration. We evaluate STP with both Lean and Isabelle formal versifiers. With 51.3 billion tokens generated during the training in Lean, STP proves 28.5% of the statements in the LeanWorkbook dataset, doubling the previous best result of 13.2% achieved through expert iteration. The final model achieves state-of-the-art performance among whole-proof generation methods on miniF2F-test (65.0%, pass@3200), Proofnet-test (23.9%, pass@3200) and PutnamBench (8/644, pass@3200). We release our code, model, and dataset in this URL: https://github.com/kfdong/STP.",
        "timestamp": "2025-03-25T04:29:26.517Z",
        "rating": "novote",
        "publishedDate": "2025/01/31",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-25T04:29:27+00:00",
        "updated_at": "2025-03-29T22:41:18+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2404.16698": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.16698",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-25T04:52:33.944Z",
            "data": {
              "session_id": "session_1742878353935_a0tnruj",
              "source_id": "arxiv",
              "paper_id": "2404.16698",
              "start_time": "2025-03-25T04:52:13.916Z",
              "end_time": "2025-03-25T04:52:33.935Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 15,
              "total_elapsed_seconds": 20
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-25T04:52:35+00:00",
        "updated_at": "2025-03-29T22:41:16+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2404.16698": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.16698",
        "url": "https://arxiv.org/abs/2404.16698",
        "title": "Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents",
        "authors": "Piatti, Giorgio, Jin, Zhijing, Kleiman-Weiner, Max, Sch\u00f6lkopf, Bernhard, Sachan, Mrinmaya, Mihalcea, Rada",
        "abstract": "As AI systems pervade human life, ensuring that large language models (LLMs) make safe decisions remains a significant challenge. We introduce the Governance of the Commons Simulation (GovSim), a generative simulation platform designed to study strategic interactions and cooperative decision-making in LLMs. In GovSim, a society of AI agents must collectively balance exploiting a common resource with sustaining it for future use. This environment enables the study of how ethical considerations, strategic planning, and negotiation skills impact cooperative outcomes. We develop an LLM-based agent architecture and test it with the leading open and closed LLMs. We find that all but the most powerful LLM agents fail to achieve a sustainable equilibrium in GovSim, with the highest survival rate below 54%. Ablations reveal that successful multi-agent communication between agents is critical for achieving cooperation in these cases. Furthermore, our analyses show that the failure to achieve sustainable cooperation in most LLMs stems from their inability to formulate and analyze hypotheses about the long-term effects of their actions on the equilibrium of the group. Finally, we show that agents that leverage \"Universalization\"-based reasoning, a theory of moral thinking, are able to achieve significantly better sustainability. Taken together, GovSim enables us to study the mechanisms that underlie sustainable self-government with specificity and scale. We open source the full suite of our research results, including the simulation environment, agent prompts, and a comprehensive web interface.",
        "timestamp": "2025-03-25T04:52:09.974Z",
        "rating": "novote",
        "publishedDate": "2024/04/25",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-25T04:52:10+00:00",
        "updated_at": "2025-03-29T22:41:16+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2412.09723": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2412.09723",
        "url": "https://arxiv.org/abs/2412.09723",
        "title": "MAC-Ego3D: Multi-Agent Gaussian Consensus for Real-Time Collaborative Ego-Motion and Photorealistic 3D Reconstruction",
        "authors": "Xu, Xiaohao, Xue, Feng, Zhao, Shibo, Pan, Yike, Scherer, Sebastian, Huang, Xiaonan",
        "abstract": "Real-time multi-agent collaboration for ego-motion estimation and high-fidelity 3D reconstruction is vital for scalable spatial intelligence. However, traditional methods produce sparse, low-detail maps, while recent dense mapping approaches struggle with high latency. To overcome these challenges, we present MAC-Ego3D, a novel framework for real-time collaborative photorealistic 3D reconstruction via Multi-Agent Gaussian Consensus. MAC-Ego3D enables agents to independently construct, align, and iteratively refine local maps using a unified Gaussian splat representation. Through Intra-Agent Gaussian Consensus, it enforces spatial coherence among neighboring Gaussian splats within an agent. For global alignment, parallelized Inter-Agent Gaussian Consensus, which asynchronously aligns and optimizes local maps by regularizing multi-agent Gaussian splats, seamlessly integrates them into a high-fidelity 3D model. Leveraging Gaussian primitives, MAC-Ego3D supports efficient RGB-D rendering, enabling rapid inter-agent Gaussian association and alignment. MAC-Ego3D bridges local precision and global coherence, delivering higher efficiency, largely reducing localization error, and improving mapping fidelity. It establishes a new SOTA on synthetic and real-world benchmarks, achieving a 15x increase in inference speed, order-of-magnitude reductions in ego-motion estimation error for partial cases, and RGB PSNR gains of 4 to 10 dB. Our code will be made publicly available at https://github.com/Xiaohao-Xu/MAC-Ego3D .",
        "timestamp": "2025-03-25T04:51:58.387Z",
        "rating": "novote",
        "publishedDate": "2024/12/12",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-25T04:51:58+00:00",
        "updated_at": "2025-03-29T22:41:17+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2501.18593": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.18593",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-25T04:57:07.635Z",
            "data": {
              "session_id": "session_1742878627239_qrj1i0q",
              "source_id": "arxiv",
              "paper_id": "2501.18593",
              "start_time": "2025-03-25T04:56:47.403Z",
              "end_time": "2025-03-25T04:57:07.239Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 5,
              "total_elapsed_seconds": 20
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-25T04:57:08+00:00",
        "updated_at": "2025-03-29T22:41:15+00:00",
        "version": 3
      }
    },
    "interactions:arxiv.2503.18255": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.18255",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-25T05:47:33.451Z",
            "data": {
              "session_id": "session_1742881652713_lsh2quo",
              "source_id": "arxiv",
              "paper_id": "2503.18255",
              "start_time": "2025-03-25T05:47:27.451Z",
              "end_time": "2025-03-25T05:47:32.713Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-25T05:47:34+00:00",
        "updated_at": "2025-03-29T22:41:15+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.18255": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.18255",
        "url": "https://arxiv.org/abs/2503.18255",
        "title": "The Human-Machine Identity Blur: A Unified Framework for Cybersecurity Risk Management in 2025",
        "authors": "Janani, Kush",
        "abstract": "The modern enterprise is facing an unprecedented surge in digital identities, with machine identities now significantly outnumbering human identities. This paper examines the cybersecurity risks emerging from what we define as the \"human-machine identity blur\" - the point at which human and machine identities intersect, delegate authority, and create new attack surfaces. Drawing from industry data, expert insights, and real-world incident analysis, we identify key governance gaps in current identity management models that treat human and machine entities as separate domains. To address these challenges, we propose a Unified Identity Governance Framework based on four core principles: treating identity as a continuum rather than a binary distinction, applying consistent risk evaluation across all identity types, implementing continuous verification guided by zero trust principles, and maintaining governance throughout the entire identity lifecycle. Our research shows that organizations adopting this unified approach experience a 47 percent reduction in identity-related security incidents and a 62 percent improvement in incident response time. We conclude by offering a practical implementation roadmap and outlining future research directions as AI-driven systems become increasingly autonomous.",
        "timestamp": "2025-03-25T05:47:28.042Z",
        "rating": "novote",
        "publishedDate": "2025/03/24",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-25T05:47:28+00:00",
        "updated_at": "2025-03-29T22:41:15+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2412.09723": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2412.09723",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-25T06:00:52.313Z",
            "data": {
              "session_id": "session_1742882451916_f1nidhk",
              "source_id": "arxiv",
              "paper_id": "2412.09723",
              "start_time": "2025-03-25T06:00:43.397Z",
              "end_time": "2025-03-25T06:00:51.916Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-25T06:00:53+00:00",
        "updated_at": "2025-03-29T22:41:13+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.18189": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.18189",
        "url": "https://arxiv.org/abs/2503.18189",
        "title": "Ordering and refining path-complete Lyapunov functions through composition lifts",
        "authors": "Jongeneel, Wouter, Jungers, Rapha\u00ebl M.",
        "abstract": "A fruitful approach to study stability of switched systems is to look for multiple Lyapunov functions. However, in general, we do not yet understand the interplay between the desired stability certificate, the template of the Lyapunov functions and their mutual relationships to accommodate switching. In this work we elaborate on path-complete Lyapunov functions: a graphical framework that aims to elucidate this interplay. In particular, previously, several preorders were introduced to compare multiple Lyapunov functions. These preorders are initially algorithmically intractable due to the algebraic nature of Lyapunov inequalities, yet, lifting techniques were proposed to turn some preorders purely combinatorial and thereby eventually tractable. In this note we show that a conjecture in this area regarding the so-called composition lift, that was believed to be true, is false. This refutal, however, points us to a beneficial structural feature of the composition lift that we exploit to iteratively refine path-complete graphs, plus, it points us to a favourable adaptation of the composition lift.",
        "timestamp": "2025-03-25T05:53:36.213Z",
        "rating": "novote",
        "publishedDate": "2025/03/23",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-25T05:53:36+00:00",
        "updated_at": "2025-03-29T22:41:14+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.18443": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.18443",
        "url": "https://arxiv.org/abs/2503.18443",
        "title": "Optimal consumption under adjustment costs with respect to multiple reference levels",
        "authors": "Huang, Yijie, Yan, Kaixin, Zhang, Qinyi",
        "abstract": "This paper studies a type of consumption preference where some adjustment costs are incured whenever the past spending maximum and the past spending minimum records are updated. This preference can capture the adverse effects of the historical consumption high and low values on the agent's consumption performance, thereby matching with some empirically observed smooth consumption patterns. By employing the dual transform, the smooth-fit conditions and the super-contact conditions, we obtain the closed-form solution of the dual PDE problem, and can characterize the optimal investment and consumption controls in the piecewise feedback form. We provide the rigorous proof of the verification theorem and compensate the theoretical findings with some numerical examples and financial implications.",
        "timestamp": "2025-03-25T05:53:29.810Z",
        "rating": "novote",
        "publishedDate": "2025/03/24",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-25T05:53:30+00:00",
        "updated_at": "2025-03-29T22:41:14+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2402.01613": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.01613",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-25T06:33:36.045Z",
            "data": {
              "session_id": "session_1742884415295_0unyv3z",
              "source_id": "arxiv",
              "paper_id": "2402.01613",
              "start_time": "2025-03-25T06:33:27.885Z",
              "end_time": "2025-03-25T06:33:35.295Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-25T06:33:37+00:00",
        "updated_at": "2025-03-29T22:41:12+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2402.01613": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.01613",
        "url": "https://arxiv.org/abs/2402.01613",
        "title": "Nomic Embed: Training a Reproducible Long Context Text Embedder",
        "authors": "Nussbaum, Zach, Morris, John X., Duderstadt, Brandon, Mulyar, Andriy",
        "abstract": "This technical report describes the training of nomic-embed-text-v1, the first fully reproducible, open-source, open-weights, open-data, 8192 context length English text embedding model that outperforms both OpenAI Ada-002 and OpenAI text-embedding-3-small on the short-context MTEB benchmark and the long context LoCo benchmark. We release the training code and model weights under an Apache 2.0 license. In contrast with other open-source models, we release the full curated training data and code that allows for full replication of nomic-embed-text-v1. You can find code and data to replicate the model at https://github.com/nomic-ai/contrastors.",
        "timestamp": "2025-03-25T06:33:28.520Z",
        "rating": "novote",
        "publishedDate": "2024/02/02",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-25T06:33:29+00:00",
        "updated_at": "2025-03-29T22:41:13+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.18614": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.18614",
        "url": "https://arxiv.org/abs/2503.18614v1",
        "title": "Path degeneracy and applications",
        "authors": "Lin, Y., de Mendez, P. Ossona",
        "abstract": "In this work, we relate girth and path-degeneracy in classes with sub-exponential expansion, with explicit bounds for classes with polynomial expansion and proper minor-closed classes that are tight up to a constant factor (and tight up to second order terms if a classical conjecture on existence of $g$-cages is verified). As an application, we derive bounds on the generalized acyclic indices, on the generalized arboricities, and on the weak coloring numbers of high-girth graphs in such classes. Along the way, we prove a conjecture proposed in [T.~Bartnicki et al., Generalized arboricity of graphs with large girth, Discrete Mathematics 342 (2019), no.~5, 1343--1350.], which asserts that, for every integer $k$, there is an integer $g(p,k)$ such that every $K_k$ minor-free graph with girth at least $g(p,k)$ has $p$-arboricity at most $p+1$.",
        "timestamp": "2025-03-25T07:45:11.973Z",
        "rating": "novote",
        "publishedDate": "2025/03/24",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-25T07:45:12+00:00",
        "updated_at": "2025-03-29T22:41:10+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.12334": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.12334",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-25T07:40:38.165Z",
            "data": {
              "session_id": "session_1742888437379_yexbos4",
              "source_id": "arxiv",
              "paper_id": "2503.12334",
              "start_time": "2025-03-25T07:39:52.710Z",
              "end_time": "2025-03-25T07:40:37.379Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 5,
              "total_elapsed_seconds": 45
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-25T07:43:02.311Z",
            "data": {
              "session_id": "session_1742888582035_1ajip2t",
              "source_id": "arxiv",
              "paper_id": "2503.12334",
              "start_time": "2025-03-25T07:42:55.340Z",
              "end_time": "2025-03-25T07:43:02.035Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-25T07:40:39+00:00",
        "updated_at": "2025-03-29T22:41:11+00:00",
        "version": 5
      }
    },
    "paper:arxiv.2503.12334": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.12334",
        "url": "https://arxiv.org/abs/2503.12334",
        "title": "When neural implant meets multimodal LLM: A dual-loop system for neuromodulation and naturalistic neuralbehavioral research",
        "authors": "Wang, Edward Hong, Wen, Cynthia Xin",
        "abstract": "We propose a novel dual-loop system that synergistically combines responsive neurostimulation (RNS) implants with artificial intelligence-driven wearable devices for treating post-traumatic stress disorder (PTSD) and enabling naturalistic brain research. In PTSD Therapy Mode, an implanted closed-loop neural device monitors amygdala activity and provides on-demand stimulation upon detecting pathological theta oscillations, while an ensemble of wearables (smart glasses, smartwatches, smartphones) uses multimodal large language model (LLM) analysis of sensory data to detect environmental or physiological PTSD triggers and deliver timely audiovisual interventions. Logged events from both the neural and wearable loops are analyzed to personalize trigger detection and progressively transition patients to non-invasive interventions. In Neuroscience Research Mode, the same platform is adapted for real-world brain activity capture. Wearable-LLM systems recognize naturalistic events (social interactions, emotional situations, compulsive behaviors, decision making) and signal implanted RNS devices (via wireless triggers) to record synchronized intracranial data during these moments. This approach builds on recent advances in mobile intracranial EEG recording and closed-loop neuromodulation in humans (BRAIN Initiative, 2023) (Mobbs et al., 2021). We discuss how our interdisciplinary system could revolutionize PTSD therapy and cognitive neuroscience by enabling 24/7 monitoring, context-aware intervention, and rich data collection outside traditional labs. The vision is a future where AI-enhanced devices continuously collaborate with the human brain, offering therapeutic support and deep insights into neural function, with the resulting real-world context rich neural data, in turn, accelerating the development of more biologically-grounded and human-centric AI.",
        "timestamp": "2025-03-25T07:39:53.223Z",
        "rating": "thumbsdown",
        "publishedDate": "2025/03/16",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-25T07:39:53+00:00",
        "updated_at": "2025-03-29T22:41:11+00:00",
        "version": 3
      }
    },
    "interactions:arxiv.2503.18443": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.18443",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-25T07:47:56.618Z",
            "data": {
              "session_id": "session_1742888876205_bmsmmrs",
              "source_id": "arxiv",
              "paper_id": "2503.18443",
              "start_time": "2025-03-25T07:47:13.719Z",
              "end_time": "2025-03-25T07:47:56.205Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 2,
              "total_elapsed_seconds": 42
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-25T07:47:57+00:00",
        "updated_at": "2025-03-29T22:41:09+00:00",
        "version": 3
      }
    },
    "interactions:arxiv.2503.18614": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.18614",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-25T07:45:24.572Z",
            "data": {
              "session_id": "session_1742888723819_dqos46e",
              "source_id": "arxiv",
              "paper_id": "2503.18614",
              "start_time": "2025-03-25T07:45:12.091Z",
              "end_time": "2025-03-25T07:45:23.819Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-25T07:45:25+00:00",
        "updated_at": "2025-03-29T22:41:09+00:00",
        "version": 3
      }
    },
    "interactions:arxiv.2503.18189": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.18189",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-25T07:48:50.062Z",
            "data": {
              "session_id": "session_1742888929636_mkbc3ey",
              "source_id": "arxiv",
              "paper_id": "2503.18189",
              "start_time": "2025-03-25T07:48:25.888Z",
              "end_time": "2025-03-25T07:48:49.636Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 4,
              "total_elapsed_seconds": 24
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-25T07:48:51+00:00",
        "updated_at": "2025-03-29T22:41:08+00:00",
        "version": 3
      }
    },
    "interactions:arxiv.2502.00716": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.00716",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-25T14:13:26.115Z",
            "data": {
              "session_id": "session_1742912005159_4esvhvj",
              "source_id": "arxiv",
              "paper_id": "2502.00716",
              "start_time": "2025-03-25T14:12:11.760Z",
              "end_time": "2025-03-25T14:13:25.158Z",
              "heartbeat_count": 14,
              "duration_seconds": 70,
              "idle_seconds": 3,
              "total_elapsed_seconds": 73
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-25T14:13:27+00:00",
        "updated_at": "2025-03-29T22:41:08+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2502.00716": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.00716",
        "url": "https://arxiv.org/pdf/2502.00716",
        "title": "2502.00716",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-25T14:12:12.413Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-25T14:12:13+00:00",
        "updated_at": "2025-03-29T22:41:08+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.16237": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.16237",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-25T14:27:22.972Z",
            "data": {
              "session_id": "session_1742912842070_sh2cmjf",
              "source_id": "arxiv",
              "paper_id": "2503.16237",
              "start_time": "2025-03-25T14:27:09.945Z",
              "end_time": "2025-03-25T14:27:22.070Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-25T14:27:24+00:00",
        "updated_at": "2025-03-29T22:41:06+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.16237": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.16237",
        "url": "https://arxiv.org/abs/2503.16237",
        "title": "3D radio data visualisation in open science platforms for next-generation observatories",
        "authors": "Labadie-Garc\u00eda, I., Garrido, J., Verdes-Montenegro, L., Mendoza, M. \u00c1., Parra-Roy\u00f3n, M., S\u00e1nchez-Exp\u00f3sito, S., Ianjamasimanana, R.",
        "abstract": "Next-generation telescopes will bring groundbreaking discoveries but they will also present new technological challenges. The Square Kilometre Array Observatory (SKAO) will be one of the most demanding scientific infrastructures, with a projected data output of 700 PB per year to be distributed to a network of SKA Regional Centres. Current tools are not fully suited to manage such massive data volumes, therefore, new research is required to transform science archives from data providers into service providers. In this paper we examine how a science archive can deliver advanced visualisation capabilities for the SKA science archive. In particular, we have conducted a thorough exploration of existing visualisation software for astronomy and other fields to identify tools capable of addressing Big Data requirements. Using selected technologies, we have developed a prototype archive that provides access to interactive visualisations of 3D radio data through web-based interfaces, adhering to International Virtual Observatory Alliance (IVOA) recommendations to favour interoperability and Open Science practices. In addition, we discuss how current IVOA recommendations support these visualisation capabilities and how they could be expanded. Our prototype archive includes a service to generate 3D models on the fly as a server operation, enabling remote visualisations in a flexible manner; for instance, a set of parameters can be used to customise the models and their visualisation. We have used SKA precursor and pathfinder data to test its usability and scalability, concluding that remote visualisation is a viable solution for handling high-volume data. However, our prototype is constrained by memory limitations, requiring techniques to reduce memory usage.",
        "timestamp": "2025-03-25T14:27:10.514Z",
        "rating": "novote",
        "publishedDate": "2025/03/20",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-25T14:27:11+00:00",
        "updated_at": "2025-03-29T22:41:07+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2404.04374": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.04374",
        "url": "https://arxiv.org/pdf/2404.04374",
        "title": "2404.04374",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-25T14:25:25.836Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-25T14:25:26+00:00",
        "updated_at": "2025-03-29T22:41:07+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2502.21228": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.21228",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-25T18:01:12.124Z",
            "data": {
              "session_id": "session_1742925672082_33yp92w",
              "source_id": "arxiv",
              "paper_id": "2502.21228",
              "start_time": "2025-03-25T18:00:42.814Z",
              "end_time": "2025-03-25T18:01:12.082Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 4,
              "total_elapsed_seconds": 29
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-25T18:04:59.537Z",
            "data": {
              "session_id": "session_1742925898926_uhmqsnb",
              "source_id": "arxiv",
              "paper_id": "2502.21228",
              "start_time": "2025-03-25T18:03:30.132Z",
              "end_time": "2025-03-25T18:04:58.926Z",
              "heartbeat_count": 17,
              "duration_seconds": 85,
              "idle_seconds": 4,
              "total_elapsed_seconds": 89
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-25T18:09:49.584Z",
            "data": {
              "session_id": "session_1742926189372_a0l7mi3",
              "source_id": "arxiv",
              "paper_id": "2502.21228",
              "start_time": "2025-03-25T18:09:35.430Z",
              "end_time": "2025-03-25T18:09:49.372Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-25T18:01:13+00:00",
        "updated_at": "2025-03-29T22:41:05+00:00",
        "version": 5
      }
    },
    "paper:arxiv.2502.21228": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.21228",
        "url": "https://www.arxiv.org/pdf/2502.21228",
        "title": "2502.21228",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-25T18:00:27.894Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-25T18:00:28+00:00",
        "updated_at": "2025-03-29T22:41:06+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.17860": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.17860",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-25T22:14:36.890Z",
            "data": {
              "session_id": "session_1742940876142_0e45nj2",
              "source_id": "arxiv",
              "paper_id": "2503.17860",
              "start_time": "2025-03-25T22:14:16.090Z",
              "end_time": "2025-03-25T22:14:36.142Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 0,
              "total_elapsed_seconds": 20
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-26T14:28:18.065Z",
            "data": {
              "session_id": "session_1742999298052_6fl69i7",
              "source_id": "arxiv",
              "paper_id": "2503.17860",
              "start_time": "2025-03-26T14:28:10.058Z",
              "end_time": "2025-03-26T14:28:18.052Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-26T14:29:23.899Z",
            "data": {
              "session_id": "session_1742999363675_m23tnbl",
              "source_id": "arxiv",
              "paper_id": "2503.17860",
              "start_time": "2025-03-26T14:29:14.861Z",
              "end_time": "2025-03-26T14:29:23.675Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-25T22:14:37+00:00",
        "updated_at": "2025-03-29T22:41:04+00:00",
        "version": 6
      }
    },
    "paper:arxiv.2503.17860": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.17860",
        "url": "https://arxiv.org/abs/2503.17860",
        "title": "Enhancing Retrieval Systems with Inference-Time Logical Reasoning",
        "authors": "Faltings, Felix, Wei, Wei, Bao, Yujia",
        "abstract": "Traditional retrieval methods rely on transforming user queries into vector representations and retrieving documents based on cosine similarity within an embedding space. While efficient and scalable, this approach often fails to handle complex queries involving logical constructs such as negations, conjunctions, and disjunctions. In this paper, we propose a novel inference-time logical reasoning framework that explicitly incorporates logical reasoning into the retrieval process. Our method extracts logical reasoning structures from natural language queries and then composes the individual cosine similarity scores to formulate the final document scores. This approach enables the retrieval process to handle complex logical reasoning without compromising computational efficiency. Our results on both synthetic and real-world benchmarks demonstrate that the proposed method consistently outperforms traditional retrieval methods across different models and datasets, significantly improving retrieval performance for complex queries.",
        "timestamp": "2025-03-25T22:14:16.445Z",
        "rating": "novote",
        "publishedDate": "2025/03/22",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-25T22:14:17+00:00",
        "updated_at": "2025-03-29T22:41:04+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2108.10816": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2108.10816",
        "url": "https://ar5iv.labs.arxiv.org/html/2108.10816",
        "title": "A large millikelvin platform at Fermilab for quantum computing applications",
        "authors": "",
        "abstract": "The need for larger mK cooling platforms is being driven by the desire to host ever growing numbers of cryogenic qubits in quantum computing platforms. As part of the Superconducting Quantum Materials and Systems Cente\u2026",
        "timestamp": "2025-03-25T22:07:48.699Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-25T22:07:49+00:00",
        "updated_at": "2025-03-29T22:41:05+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.05628": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.05628",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-26T00:11:30.365Z",
            "data": {
              "session_id": "session_1742947889376_2kqsmef",
              "source_id": "arxiv",
              "paper_id": "2503.05628",
              "start_time": "2025-03-26T00:10:38.401Z",
              "end_time": "2025-03-26T00:11:29.376Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 1,
              "total_elapsed_seconds": 51
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-26T00:11:31+00:00",
        "updated_at": "2025-03-29T22:41:02+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.05628": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.05628",
        "url": "https://arxiv.org/abs/2503.05628",
        "title": "Superintelligence Strategy: Expert Version",
        "authors": "Hendrycks, Dan, Schmidt, Eric, Wang, Alexandr",
        "abstract": "Rapid advances in AI are beginning to reshape national security. Destabilizing AI developments could rupture the balance of power and raise the odds of great-power conflict, while widespread proliferation of capable AI hackers and virologists would lower barriers for rogue actors to cause catastrophe. Superintelligence -- AI vastly better than humans at nearly all cognitive tasks -- is now anticipated by AI researchers. Just as nations once developed nuclear strategies to secure their survival, we now need a coherent superintelligence strategy to navigate a new period of transformative change. We introduce the concept of Mutual Assured AI Malfunction (MAIM): a deterrence regime resembling nuclear mutual assured destruction (MAD) where any state's aggressive bid for unilateral AI dominance is met with preventive sabotage by rivals. Given the relative ease of sabotaging a destabilizing AI project -- through interventions ranging from covert cyberattacks to potential kinetic strikes on datacenters -- MAIM already describes the strategic picture AI superpowers find themselves in. Alongside this, states can increase their competitiveness by bolstering their economies and militaries through AI, and they can engage in nonproliferation to rogue actors to keep weaponizable AI capabilities out of their hands. Taken together, the three-part framework of deterrence, nonproliferation, and competitiveness outlines a robust strategy to superintelligence in the years ahead.",
        "timestamp": "2025-03-26T00:10:37.645Z",
        "rating": "novote",
        "publishedDate": "2025/03/07",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-26T00:10:38+00:00",
        "updated_at": "2025-03-29T22:41:03+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.15450": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.15450",
        "url": "https://arxiv.org/abs/2503.15450",
        "title": "SkyLadder: Better and Faster Pretraining via Context Window Scheduling",
        "authors": "Zhu, Tongyao, Liu, Qian, Wang, Haonan, Chen, Shiqi, Gu, Xiangming, Pang, Tianyu, Kan, Min-Yen",
        "abstract": "Recent advancements in LLM pretraining have featured ever-expanding context windows to process longer sequences. However, our pilot study reveals that models pretrained with shorter context windows consistently outperform their long-context counterparts under a fixed token budget. This finding motivates us to explore an optimal context window scheduling strategy to better balance long-context capability with pretraining efficiency. To this end, we propose SkyLadder, a simple yet effective approach that implements a short-to-long context window transition. SkyLadder preserves strong standard benchmark performance, while matching or exceeding baseline results on long context tasks. Through extensive experiments, we pre-train 1B-parameter models (up to 32K context) and 3B-parameter models (8K context) on 100B tokens, demonstrating that SkyLadder yields consistent gains of up to 3.7% on common benchmarks, while achieving up to 22% faster training speeds compared to baselines. The code is at https://github.com/sail-sg/SkyLadder.",
        "timestamp": "2025-03-26T00:09:38.972Z",
        "rating": "novote",
        "publishedDate": "2025/03/19",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-26T00:09:39+00:00",
        "updated_at": "2025-03-29T22:41:04+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2501.13947": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.13947",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-26T00:18:10.884Z",
            "data": {
              "session_id": "session_1742948290129_v0trveh",
              "source_id": "arxiv",
              "paper_id": "2501.13947",
              "start_time": "2025-03-26T00:17:58.585Z",
              "end_time": "2025-03-26T00:18:10.129Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-26T00:18:11+00:00",
        "updated_at": "2025-03-29T22:41:01+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2501.13947": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.13947",
        "url": "https://arxiv.org/abs/2501.13947",
        "title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
        "authors": "Yang, Wenli, Some, Lilian, Bain, Michael, Kang, Byeong",
        "abstract": "The rapid development of artificial intelligence has led to marked progress in the field. One interesting direction for research is whether Large Language Models (LLMs) can be integrated with structured knowledge-based systems. This approach aims to combine the generative language understanding of LLMs and the precise knowledge representation systems by which they are integrated. This article surveys the relationship between LLMs and knowledge bases, looks at how they can be applied in practice, and discusses related technical, operational, and ethical challenges. Utilizing a comprehensive examination of the literature, the study both identifies important issues and assesses existing solutions. It demonstrates the merits of incorporating generative AI into structured knowledge-base systems concerning data contextualization, model accuracy, and utilization of knowledge resources. The findings give a full list of the current situation of research, point out the main gaps, and propose helpful paths to take. These insights contribute to advancing AI technologies and support their practical deployment across various sectors.",
        "timestamp": "2025-03-26T00:17:59.109Z",
        "rating": "novote",
        "publishedDate": "2025/01/19",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-26T00:17:59+00:00",
        "updated_at": "2025-03-29T22:41:02+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.13751": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.13751",
        "url": "https://arxiv.org/abs/2503.13751",
        "title": "Optimizing ML Training with Metagradient Descent",
        "authors": "Engstrom, Logan, Ilyas, Andrew, Chen, Benjamin, Feldmann, Axel, Moses, William, Madry, Aleksander",
        "abstract": "A major challenge in training large-scale machine learning models is configuring the training process to maximize model performance, i.e., finding the best training setup from a vast design space. In this work, we unlock a gradient-based approach to this problem. We first introduce an algorithm for efficiently calculating metagradients -- gradients through model training -- at scale. We then introduce a \"smooth model training\" framework that enables effective optimization using metagradients. With metagradient descent (MGD), we greatly improve on existing dataset selection methods, outperform accuracy-degrading data poisoning attacks by an order of magnitude, and automatically find competitive learning rate schedules.",
        "timestamp": "2025-03-26T00:22:38.959Z",
        "rating": "novote",
        "publishedDate": "2025/03/17",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-26T00:22:39+00:00",
        "updated_at": "2025-03-29T22:41:00+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2404.04374": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.04374",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-26T00:21:42.492Z",
            "data": {
              "session_id": "session_1742948502070_p72jd81",
              "source_id": "arxiv",
              "paper_id": "2404.04374",
              "start_time": "2025-03-26T00:21:31.367Z",
              "end_time": "2025-03-26T00:21:42.070Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-26T00:21:43+00:00",
        "updated_at": "2025-03-29T22:41:01+00:00",
        "version": 3
      }
    },
    "interactions:arxiv.2503.13751": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.13751",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-26T00:23:20.276Z",
            "data": {
              "session_id": "session_1742948600264_36zztw7",
              "source_id": "arxiv",
              "paper_id": "2503.13751",
              "start_time": "2025-03-26T00:22:45.566Z",
              "end_time": "2025-03-26T00:23:20.264Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 5,
              "total_elapsed_seconds": 35
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-26T00:26:49.031Z",
            "data": {
              "session_id": "session_1742948808377_7jcjumz",
              "source_id": "arxiv",
              "paper_id": "2503.13751",
              "start_time": "2025-03-26T00:23:20.266Z",
              "end_time": "2025-03-26T00:26:48.377Z",
              "heartbeat_count": 41,
              "duration_seconds": 205,
              "idle_seconds": 3,
              "total_elapsed_seconds": 208
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-26T00:31:54.408Z",
            "data": {
              "session_id": "session_1742949113802_mcdeq06",
              "source_id": "arxiv",
              "paper_id": "2503.13751",
              "start_time": "2025-03-26T00:27:45.547Z",
              "end_time": "2025-03-26T00:31:53.802Z",
              "heartbeat_count": 49,
              "duration_seconds": 245,
              "idle_seconds": 3,
              "total_elapsed_seconds": 248
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-30T00:59:48.963Z",
            "data": {
              "session_id": "session_1743296388708_mp8np67",
              "source_id": "arxiv",
              "paper_id": "2503.13751",
              "start_time": "2025-03-30T00:59:38.504Z",
              "end_time": "2025-03-30T00:59:48.708Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 0,
              "total_elapsed_seconds": 10
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-26T00:23:21+00:00",
        "updated_at": "2025-03-30T01:00:54+00:00",
        "version": 6
      }
    },
    "paper:arxiv.2503.19358": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.19358",
        "url": "https://arxiv.org/abs/2503.19358",
        "title": "From Sparse to Dense: Camera Relocalization with Scene-Specific Detector from Feature Gaussian Splatting",
        "authors": "Huang, Zhiwei, Yu, Hailin, Shentu, Yichun, Yuan, Jin, Zhang, Guofeng",
        "abstract": "This paper presents a novel camera relocalization method, STDLoc, which leverages Feature Gaussian as scene representation. STDLoc is a full relocalization pipeline that can achieve accurate relocalization without relying on any pose prior. Unlike previous coarse-to-fine localization methods that require image retrieval first and then feature matching, we propose a novel sparse-to-dense localization paradigm. Based on this scene representation, we introduce a novel matching-oriented Gaussian sampling strategy and a scene-specific detector to achieve efficient and robust initial pose estimation. Furthermore, based on the initial localization results, we align the query feature map to the Gaussian feature field by dense feature matching to enable accurate localization. The experiments on indoor and outdoor datasets show that STDLoc outperforms current state-of-the-art localization methods in terms of localization accuracy and recall.",
        "timestamp": "2025-03-26T14:29:13.720Z",
        "rating": "novote",
        "publishedDate": "2025/03/25",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-26T14:29:14+00:00",
        "updated_at": "2025-03-29T22:41:00+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.19358": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.19358",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-26T19:48:06.780Z",
            "data": {
              "session_id": "session_1743018486002_gpd0o7j",
              "source_id": "arxiv",
              "paper_id": "2503.19358",
              "start_time": "2025-03-26T19:47:57.689Z",
              "end_time": "2025-03-26T19:48:06.002Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-26T20:02:07.032Z",
            "data": {
              "session_id": "session_1743019326785_ez5n8hd",
              "source_id": "arxiv",
              "paper_id": "2503.19358",
              "start_time": "2025-03-26T20:01:59.863Z",
              "end_time": "2025-03-26T20:02:06.785Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-26T19:48:08+00:00",
        "updated_at": "2025-03-29T22:40:58+00:00",
        "version": 4
      }
    },
    "interactions:arxiv.2409.18124": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2409.18124",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-26T19:47:37.411Z",
            "data": {
              "session_id": "session_1743018457399_q2vgvj4",
              "source_id": "arxiv",
              "paper_id": "2409.18124",
              "start_time": "2025-03-26T19:47:30.906Z",
              "end_time": "2025-03-26T19:47:37.399Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-26T19:47:38+00:00",
        "updated_at": "2025-03-29T22:40:58+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2409.18124": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2409.18124",
        "url": "https://arxiv.org/abs/2409.18124",
        "title": "Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction",
        "authors": "He, Jing, Li, Haodong, Yin, Wei, Liang, Yixun, Li, Leheng, Zhou, Kaiqiang, Zhang, Hongbo, Liu, Bingbing, Chen, Ying-Cong",
        "abstract": "Leveraging the visual priors of pre-trained text-to-image diffusion models offers a promising solution to enhance zero-shot generalization in dense prediction tasks. However, existing methods often uncritically use the original diffusion formulation, which may not be optimal due to the fundamental differences between dense prediction and image generation. In this paper, we provide a systemic analysis of the diffusion formulation for the dense prediction, focusing on both quality and efficiency. And we find that the original parameterization type for image generation, which learns to predict noise, is harmful for dense prediction; the multi-step noising/denoising diffusion process is also unnecessary and challenging to optimize. Based on these insights, we introduce Lotus, a diffusion-based visual foundation model with a simple yet effective adaptation protocol for dense prediction. Specifically, Lotus is trained to directly predict annotations instead of noise, thereby avoiding harmful variance. We also reformulate the diffusion process into a single-step procedure, simplifying optimization and significantly boosting inference speed. Additionally, we introduce a novel tuning strategy called detail preserver, which achieves more accurate and fine-grained predictions. Without scaling up the training data or model capacity, Lotus achieves SoTA performance in zero-shot depth and normal estimation across various datasets. It also enhances efficiency, being significantly faster than most existing diffusion-based methods. Lotus' superior quality and efficiency also enable a wide range of practical applications, such as joint estimation, single/multi-view 3D reconstruction, etc. Project page: https://lotus3d.github.io/.",
        "timestamp": "2025-03-26T19:47:05.374Z",
        "rating": "novote",
        "publishedDate": "2024/09/26",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-26T19:47:05+00:00",
        "updated_at": "2025-03-29T22:40:59+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2408.04811": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2408.04811",
        "url": "https://arxiv.org/abs/2408.04811",
        "title": "h4rm3l: A language for Composable Jailbreak Attack Synthesis",
        "authors": "Doumbouya, Moussa Koulako Bala, Nandi, Ananjan, Poesia, Gabriel, Ghilardi, Davide, Goldie, Anna, Bianchi, Federico, Jurafsky, Dan, Manning, Christopher D.",
        "abstract": "Despite their demonstrated valuable capabilities, state-of-the-art (SOTA) widely deployed large language models (LLMs) still have the potential to cause harm to society due to the ineffectiveness of their safety filters, which can be bypassed by prompt transformations called jailbreak attacks. Current approaches to LLM safety assessment, which employ datasets of templated prompts and benchmarking pipelines, fail to cover sufficiently large and diverse sets of jailbreak attacks, leading to the widespread deployment of unsafe LLMs. Recent research showed that novel jailbreak attacks could be derived by composition; however, a formal composable representation for jailbreak attacks, which, among other benefits, could enable the exploration of a large compositional space of jailbreak attacks through program synthesis methods, has not been previously proposed. We introduce h4rm3l, a novel approach that addresses this gap with a human-readable domain-specific language (DSL). Our framework comprises: (1) The h4rm3l DSL, which formally expresses jailbreak attacks as compositions of parameterized string transformation primitives. (2) A synthesizer with bandit algorithms that efficiently generates jailbreak attacks optimized for a target black box LLM. (3) The h4rm3l red-teaming software toolkit that employs the previous two components and an automated harmful LLM behavior classifier that is strongly aligned with human judgment. We demonstrate h4rm3l's efficacy by synthesizing a dataset of 2656 successful novel jailbreak attacks targeting 6 SOTA open-source and proprietary LLMs, and by benchmarking those models against a subset of these synthesized attacks. Our results show that h4rm3l's synthesized attacks are diverse and more successful than existing jailbreak attacks in literature, with success rates exceeding 90% on SOTA LLMs.",
        "timestamp": "2025-03-26T19:46:42.524Z",
        "rating": "novote",
        "publishedDate": "2024/08/09",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-26T19:46:43+00:00",
        "updated_at": "2025-03-29T22:40:59+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2303.08231": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2303.08231",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-26T19:58:21.380Z",
            "data": {
              "session_id": "session_1743019100523_wd7illp",
              "source_id": "arxiv",
              "paper_id": "2303.08231",
              "start_time": "2025-03-26T19:57:38.416Z",
              "end_time": "2025-03-26T19:58:20.523Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 2,
              "total_elapsed_seconds": 42
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-26T19:58:22+00:00",
        "updated_at": "2025-03-29T22:40:57+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2303.08231": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2303.08231",
        "url": "https://arxiv.org/abs/2303.08231",
        "title": "Rotation-Invariant Transformer for Point Cloud Matching",
        "authors": "Yu, Hao, Qin, Zheng, Hou, Ji, Saleh, Mahdi, Li, Dongsheng, Busam, Benjamin, Ilic, Slobodan",
        "abstract": "The intrinsic rotation invariance lies at the core of matching point clouds with handcrafted descriptors. However, it is widely despised by recent deep matchers that obtain the rotation invariance extrinsically via data augmentation. As the finite number of augmented rotations can never span the continuous SO(3) space, these methods usually show instability when facing rotations that are rarely seen. To this end, we introduce RoITr, a Rotation-Invariant Transformer to cope with the pose variations in the point cloud matching task. We contribute both on the local and global levels. Starting from the local level, we introduce an attention mechanism embedded with Point Pair Feature (PPF)-based coordinates to describe the pose-invariant geometry, upon which a novel attention-based encoder-decoder architecture is constructed. We further propose a global transformer with rotation-invariant cross-frame spatial awareness learned by the self-attention mechanism, which significantly improves the feature distinctiveness and makes the model robust with respect to the low overlap. Experiments are conducted on both the rigid and non-rigid public benchmarks, where RoITr outperforms all the state-of-the-art models by a considerable margin in the low-overlapping scenarios. Especially when the rotations are enlarged on the challenging 3DLoMatch benchmark, RoITr surpasses the existing methods by at least 13 and 5 percentage points in terms of Inlier Ratio and Registration Recall, respectively.",
        "timestamp": "2025-03-26T19:57:38.955Z",
        "rating": "novote",
        "publishedDate": "2023/03/14",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-26T19:57:39+00:00",
        "updated_at": "2025-03-29T22:40:57+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.19206": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.19206",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-26T20:06:54.148Z",
            "data": {
              "session_id": "session_1743019614136_6os4z58",
              "source_id": "arxiv",
              "paper_id": "2503.19206",
              "start_time": "2025-03-26T20:06:40.074Z",
              "end_time": "2025-03-26T20:06:54.136Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-27T01:53:56.905Z",
            "data": {
              "session_id": "session_1743040436678_e2bclyb",
              "source_id": "arxiv",
              "paper_id": "2503.19206",
              "start_time": "2025-03-27T01:53:48.486Z",
              "end_time": "2025-03-27T01:53:56.678Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-26T20:06:40+00:00",
        "updated_at": "2025-03-29T22:40:56+00:00",
        "version": 5
      }
    },
    "paper:arxiv.2503.19206": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.19206",
        "url": "https://arxiv.org/abs/2503.19206",
        "title": "Overtrained Language Models Are Harder to Fine-Tune",
        "authors": "Springer, Jacob Mitchell, Goyal, Sachin, Wen, Kaiyue, Kumar, Tanishq, Yue, Xiang, Malladi, Sadhika, Neubig, Graham, Raghunathan, Aditi",
        "abstract": "Large language models are pre-trained on ever-growing token budgets under the assumption that better pre-training performance translates to improved downstream models. In this work, we challenge this assumption and show that extended pre-training can make models harder to fine-tune, leading to degraded final performance. We term this phenomenon catastrophic overtraining. For example, the instruction-tuned OLMo-1B model pre-trained on 3T tokens leads to over 2% worse performance on multiple standard LLM benchmarks than its 2.3T token counterpart. Through controlled experiments and theoretical analysis, we show that catastrophic overtraining arises from a systematic increase in the broad sensitivity of pre-trained parameters to modifications, including but not limited to fine-tuning. Our findings call for a critical reassessment of pre-training design that considers the downstream adaptability of the model.",
        "timestamp": "2025-03-26T20:06:28.751Z",
        "rating": "novote",
        "publishedDate": "2025/03/24",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-26T20:06:29+00:00",
        "updated_at": "2025-03-29T22:40:56+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.20124": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.20124",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-27T01:27:58.526Z",
            "data": {
              "session_id": "session_1743038877590_b43xk29",
              "source_id": "arxiv",
              "paper_id": "2503.20124",
              "start_time": "2025-03-27T01:27:44.274Z",
              "end_time": "2025-03-27T01:27:57.590Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 3,
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-27T03:20:37.010Z",
            "data": {
              "session_id": "session_1743045636791_vpiviic",
              "source_id": "arxiv",
              "paper_id": "2503.20124",
              "start_time": "2025-03-27T03:20:30.876Z",
              "end_time": "2025-03-27T03:20:36.791Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-27T01:27:59+00:00",
        "updated_at": "2025-03-29T22:40:54+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2503.20124": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.20124",
        "url": "https://arxiv.org/abs/2503.20124",
        "title": "Synthesizing world models for bilevel planning",
        "authors": "Ahmed, Zergham, Tenenbaum, Joshua B., Bates, Christopher J., Gershman, Samuel J.",
        "abstract": "Modern reinforcement learning (RL) systems have demonstrated remarkable capabilities in complex environments, such as video games. However, they still fall short of achieving human-like sample efficiency and adaptability when learning new domains. Theory-based reinforcement learning (TBRL) is an algorithmic framework specifically designed to address this gap. Modeled on cognitive theories, TBRL leverages structured, causal world models - \"theories\" - as forward simulators for use in planning, generalization and exploration. Although current TBRL systems provide compelling explanations of how humans learn to play video games, they face several technical limitations: their theory languages are restrictive, and their planning algorithms are not scalable. To address these challenges, we introduce TheoryCoder, an instantiation of TBRL that exploits hierarchical representations of theories and efficient program synthesis methods for more powerful learning and planning. TheoryCoder equips agents with general-purpose abstractions (e.g., \"move to\"), which are then grounded in a particular environment by learning a low-level transition model (a Python program synthesized from observations by a large language model). A bilevel planning algorithm can exploit this hierarchical structure to solve large domains. We demonstrate that this approach can be successfully applied to diverse and challenging grid-world games, where approaches based on directly synthesizing a policy perform poorly. Ablation studies demonstrate the benefits of using hierarchical abstractions.",
        "timestamp": "2025-03-27T01:27:43.931Z",
        "rating": "novote",
        "publishedDate": "2025/03/26",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-27T01:27:44+00:00",
        "updated_at": "2025-03-29T22:40:54+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2104.09864": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2104.09864",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-27T01:27:18.956Z",
            "data": {
              "session_id": "session_1743038838198_07w1kko",
              "source_id": "arxiv",
              "paper_id": "2104.09864",
              "start_time": "2025-03-27T01:27:07.453Z",
              "end_time": "2025-03-27T01:27:18.198Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-27T01:27:19+00:00",
        "updated_at": "2025-03-29T22:40:55+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2104.09864": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2104.09864",
        "url": "https://arxiv.org/pdf/2104.09864",
        "title": "2104.09864",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-27T01:27:08.086Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-27T01:27:08+00:00",
        "updated_at": "2025-03-29T22:40:55+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2410.14340": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.14340",
        "url": "https://arxiv.org/abs/2410.14340",
        "title": "Zero-shot Action Localization via the Confidence of Large Vision-Language Models",
        "authors": "Aklilu, Josiah, Wang, Xiaohan, Yeung-Levy, Serena",
        "abstract": "Precise action localization in untrimmed video is vital for fields such as professional sports and minimally invasive surgery, where the delineation of particular motions in recordings can dramatically enhance analysis. But in many cases, large scale datasets with video-label pairs for localization are unavailable, limiting the opportunity to fine-tune video-understanding models. Recent developments in large vision-language models (LVLM) address this need with impressive zero-shot capabilities in a variety of video understanding tasks. However, the adaptation of LVLMs, with their powerful visual question answering capabilities, to zero-shot localization in long-form video is still relatively unexplored. To this end, we introduce a true Zero-shot Action Localization method (ZEAL). Specifically, we leverage the built-in action knowledge of a large language model (LLM) to inflate actions into detailed descriptions of the archetypal start and end of the action. These descriptions serve as queries to LVLM for generating frame-level confidence scores which can be aggregated to produce localization outputs. The simplicity and flexibility of our method lends it amenable to more capable LVLMs as they are developed, and we demonstrate remarkable results in zero-shot action localization on a challenging benchmark, without any training. Our code is publicly available at $\\href{https://github.com/josaklil-ai/zeal}{github.com/josaklil-ai/zeal}$.",
        "timestamp": "2025-03-27T01:46:13.380Z",
        "rating": "novote",
        "publishedDate": "2024/10/18",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-27T01:46:13+00:00",
        "updated_at": "2025-03-29T22:40:52+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2502.10190": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.10190",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-27T01:44:17.671Z",
            "data": {
              "session_id": "session_1743039856033_614ofh3",
              "source_id": "arxiv",
              "paper_id": "2502.10190",
              "start_time": "2025-03-27T01:43:55.409Z",
              "end_time": "2025-03-27T01:44:16.033Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 1,
              "total_elapsed_seconds": 21
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-27T01:44:18+00:00",
        "updated_at": "2025-03-29T22:40:53+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2502.10190": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.10190",
        "url": "https://arxiv.org/abs/2502.10190",
        "title": "VideoDiff: Human-AI Video Co-Creation with Alternatives",
        "authors": "Huh, Mina, Li, Dingzeyu, Pimmel, Kim, Shin, Hijung Valentina, Pavel, Amy, Dontcheva, Mira",
        "abstract": "To make an engaging video, people sequence interesting moments and add visuals such as B-rolls or text. While video editing requires time and effort, AI has recently shown strong potential to make editing easier through suggestions and automation. A key strength of generative models is their ability to quickly generate multiple variations, but when provided with many alternatives, creators struggle to compare them to find the best fit. We propose VideoDiff, an AI video editing tool designed for editing with alternatives. With VideoDiff, creators can generate and review multiple AI recommendations for each editing process: creating a rough cut, inserting B-rolls, and adding text effects. VideoDiff simplifies comparisons by aligning videos and highlighting differences through timelines, transcripts, and video previews. Creators have the flexibility to regenerate and refine AI suggestions as they compare alternatives. Our study participants (N=12) could easily compare and customize alternatives, creating more satisfying results.",
        "timestamp": "2025-03-27T01:43:55.911Z",
        "rating": "novote",
        "publishedDate": "2025/02/14",
        "tags": [],
        "doi": "10.1145/3706598.3713417",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-27T01:43:56+00:00",
        "updated_at": "2025-03-29T22:40:53+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2410.14340": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.14340",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-27T01:46:54.825Z",
            "data": {
              "session_id": "session_1743040013888_puh23xd",
              "source_id": "arxiv",
              "paper_id": "2410.14340",
              "start_time": "2025-03-27T01:46:12.686Z",
              "end_time": "2025-03-27T01:46:53.888Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 1,
              "total_elapsed_seconds": 41
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-27T01:48:04.128Z",
            "data": {
              "session_id": "session_1743040083553_0oli619",
              "source_id": "arxiv",
              "paper_id": "2410.14340",
              "start_time": "2025-03-27T01:47:21.139Z",
              "end_time": "2025-03-27T01:48:03.553Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 2,
              "total_elapsed_seconds": 42
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-27T01:46:56+00:00",
        "updated_at": "2025-03-29T22:40:52+00:00",
        "version": 4
      }
    },
    "interactions:url.528FC6F1": {
      "data": {
        "sourceId": "url",
        "paperId": "528FC6F1",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-27T01:54:18.909Z",
            "data": {
              "session_id": "session_1743040458106_7c3itkz",
              "source_id": "url",
              "paper_id": "528FC6F1",
              "start_time": "2025-03-27T01:54:06.647Z",
              "end_time": "2025-03-27T01:54:18.105Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-27T01:54:19+00:00",
        "updated_at": "2025-03-29T22:40:51+00:00",
        "version": 3
      }
    },
    "paper:url.528FC6F1": {
      "data": {
        "sourceId": "url",
        "paperId": "528FC6F1",
        "url": "https://burningman.org/about/10-principles/",
        "title": "The 10 Principles of Burning Man",
        "authors": "",
        "abstract": "Burning Man co-founder Larry Harvey wrote the 10 Principles in 2004 as guidelines for the newly-formed Regional Network. They were crafted not as a dictate of how people should be and act, but as a reflection of the community's ethos...",
        "timestamp": "2025-03-27T01:54:06.992Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-27T01:54:07+00:00",
        "updated_at": "2025-03-29T22:40:51+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.20152": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.20152",
        "url": "https://arxiv.org/abs/2503.20152",
        "title": "Tuning the Delicate Topology of Topological Phases",
        "authors": "Sabharwal, Snigdh",
        "abstract": "We present a unified framework to systematically embed complex knotted and linked structures, beyond the torus family, into diverse topological phases, including Hopf insulators, classical spin liquids, topological semimetals, and non-Hermitian metals. Using rational maps and level sets of complex polynomials, we explicitly construct new topological models exhibiting rich and previously inaccessible textures. These topological features manifest distinctly across physical systems: emergent magnetic field lines in Hopf insulators directly reflect the rational-map topology, paralleling topological electromagnetism, while in classical spin liquids the topology is experimentally accessible via the equal-time structure factor. Our approach thus provides both a conceptual unification of previously disconnected systems and a practical toolset for realizing and detecting intricate topological textures in experiments.",
        "timestamp": "2025-03-27T03:21:52.897Z",
        "rating": "novote",
        "publishedDate": "2025/03/26",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-27T03:21:53+00:00",
        "updated_at": "2025-03-29T22:40:50+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2407.20797": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.20797",
        "url": "https://arxiv.org/abs/2407.20797",
        "title": "Witnessing Disorder in Quantum Magnets",
        "authors": "Sabharwal, Snigdh, Shimokawa, Tokuro, Shannon, Nic",
        "abstract": "There are no clean samples in nature. Therefore, when we come to discuss the entanglement properties of quantum materials, the effects of disorder must be taken into account. This question is of particular interest for high-entangled states, such as quantum spin liquids, which lie outside the Landau paradigm for classifying phases of matter. In this work, we explore what experimentally-accessible measures, in the form of concurrence, residual tangle and quantum Fisher information, can teach us about the entanglement in the presence of disorder. As a representative example, we consider the Tomonaga-Luttinger liquids (TLL) and disorder-driven random singlet (RS) states found in antiferromagnetic quantum spin chains. Using quantum Fisher information and residual tangle, we demonstrate that both TLL and RS states exhibit multi-partite entanglement. In the case of the RS state, we attribute this to entanglement localized below a crossover length scale. We further show that the order of disorder average matters in calculating measures like concurrence, and that this can lead to false inferences when interpreting experiment. None the less, correctly interpreted, these witnesses provide useful information about the effects of disorder. We explore how information about the central charge of the TLL can be extracted from the low-temperature behavior of concurrence, and conjecture that this analysis can be extended to the effective central charge of the RS state. Finally, we establish how RS and TLL states can be distinguished through the growth of multi-partite entanglement, as witnessed by the equal-time structure factor. These results establish that, used carefully, experiments based on entanglement witnesses can provide important information about quantum spin systems in the presence of disorder.",
        "timestamp": "2025-03-27T03:27:56.912Z",
        "rating": "novote",
        "publishedDate": "2024/07/30",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-27T03:27:57+00:00",
        "updated_at": "2025-03-29T22:40:49+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.20152": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.20152",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-27T03:23:19.329Z",
            "data": {
              "session_id": "session_1743045797949_yt26xvf",
              "source_id": "arxiv",
              "paper_id": "2503.20152",
              "start_time": "2025-03-27T03:21:52.259Z",
              "end_time": "2025-03-27T03:23:17.949Z",
              "heartbeat_count": 17,
              "duration_seconds": 85,
              "idle_seconds": 1,
              "total_elapsed_seconds": 86
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-27T03:23:20+00:00",
        "updated_at": "2025-03-29T22:40:50+00:00",
        "version": 3
      }
    },
    "interactions:url.874CB6": {
      "data": {
        "sourceId": "url",
        "paperId": "874CB6",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-27T03:30:13.542Z",
            "data": {
              "session_id": "session_1743046212688_8w8td9x",
              "source_id": "url",
              "paper_id": "874CB6",
              "start_time": "2025-03-27T03:29:52.573Z",
              "end_time": "2025-03-27T03:30:12.688Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 0,
              "total_elapsed_seconds": 20
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-27T03:30:14+00:00",
        "updated_at": "2025-03-29T22:40:48+00:00",
        "version": 3
      }
    },
    "paper:url.874CB6": {
      "data": {
        "sourceId": "url",
        "paperId": "874CB6",
        "url": "https://www.nature.com/articles/s41562-024-02001-8",
        "title": "A new sociology of humans and machines",
        "authors": "Tsvetkova, Milena, Yasseri, Taha, Pescetelli, Niccolo, Werner, Tobias",
        "abstract": "This Perspective calls for a new sociology of humans and machines to study groups and networks comprising multiple interacting humans and algorithms, bots or robots. A deeper understanding of human\u2013machine social systems can contribute new and valued insights for AI research, design and policy.",
        "timestamp": "2025-03-27T03:29:51.894Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1038/s41562-024-02001-8",
        "journalName": "Nature Human Behaviour",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-27T03:29:52+00:00",
        "updated_at": "2025-03-29T22:40:49+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.20035": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.20035",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-27T05:03:28.384Z",
            "data": {
              "session_id": "session_1743051807942_1rfy3gx",
              "source_id": "arxiv",
              "paper_id": "2503.20035",
              "start_time": "2025-03-27T05:02:40.033Z",
              "end_time": "2025-03-27T05:03:27.942Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 3,
              "total_elapsed_seconds": 48
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-27T05:03:29+00:00",
        "updated_at": "2025-03-29T22:40:46+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.20035": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.20035",
        "url": "https://arxiv.org/abs/2503.20035",
        "title": "The problem of infinite information flow",
        "authors": "Bian, Zheng, Bollt, Erik M.",
        "abstract": "We study conditional mutual information (cMI) between a pair of variables $X,Y$ given a third one $Z$ and derived quantities including transfer entropy (TE) and causation entropy (CE) in the dynamically relevant context where $X=T(Y,Z)$ is determined by $Y,Z$ via a deterministic transformation $T$. Under mild continuity assumptions on their distributions, we prove a zero-infinity dichotomy for cMI for a wide class of $T$, which gives a yes-or-no answer to the question of information flow as quantified by TE or CE. Such an answer fails to distinguish between the relative amounts of information flow. To resolve this problem, we propose a discretization strategy and a conjectured formula to discern the \\textit{relative ambiguities} of the system, which can serve as a reliable proxy for the relative amounts of information flow. We illustrate and validate this approach with numerical evidence.",
        "timestamp": "2025-03-27T05:02:38.415Z",
        "rating": "novote",
        "publishedDate": "2025/03/25",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-27T05:02:38+00:00",
        "updated_at": "2025-03-29T22:40:47+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.18866": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.18866",
        "url": "https://arxiv.org/abs/2503.18866",
        "title": "Reasoning to Learn from Latent Thoughts",
        "authors": "Ruan, Yangjun, Band, Neil, Maddison, Chris J., Hashimoto, Tatsunori",
        "abstract": "Compute scaling for language model (LM) pretraining has outpaced the growth of human-written texts, leading to concerns that data will become the bottleneck to LM scaling. To continue scaling pretraining in this data-constrained regime, we propose that explicitly modeling and inferring the latent thoughts that underlie the text generation process can significantly improve pretraining data efficiency. Intuitively, our approach views web text as the compressed final outcome of a verbose human thought process and that the latent thoughts contain important contextual knowledge and reasoning steps that are critical to data-efficient learning. We empirically demonstrate the effectiveness of our approach through data-constrained continued pretraining for math. We first show that synthetic data approaches to inferring latent thoughts significantly improve data efficiency, outperforming training on the same amount of raw data (5.7\\% $\\rightarrow$ 25.4\\% on MATH). Furthermore, we demonstrate latent thought inference without a strong teacher, where an LM bootstraps its own performance by using an EM algorithm to iteratively improve the capability of the trained LM and the quality of thought-augmented pretraining data. We show that a 1B LM can bootstrap its performance across at least three iterations and significantly outperform baselines trained on raw data, with increasing gains from additional inference compute when performing the E-step. The gains from inference scaling and EM iterations suggest new opportunities for scaling data-constrained pretraining.",
        "timestamp": "2025-03-27T04:25:02.134Z",
        "rating": "novote",
        "publishedDate": "2025/03/24",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-27T04:25:02+00:00",
        "updated_at": "2025-03-29T22:40:47+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.20757": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.20757",
        "url": "https://arxiv.org/abs/2503.20757",
        "title": "MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree Search",
        "authors": "Hu, Yunhai, Zhao, Yilun, Zhao, Chen, Cohan, Arman",
        "abstract": "We introduce MCTS-RAG, a novel approach that enhances the reasoning capabilities of small language models on knowledge-intensive tasks by leveraging retrieval-augmented generation (RAG) to provide relevant context and Monte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically integrates retrieval and reasoning through an iterative decision-making process. Unlike standard RAG methods, which typically retrieve information independently from reasoning and thus integrate knowledge suboptimally, or conventional MCTS reasoning, which depends solely on internal model knowledge without external facts, MCTS-RAG combines structured reasoning with adaptive retrieval. This integrated approach enhances decision-making, reduces hallucinations, and ensures improved factual accuracy and response consistency. The experimental results on multiple reasoning and knowledge-intensive datasets datasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method enables small-scale LMs to achieve performance comparable to frontier LLMs like GPT-4o by effectively scaling inference-time compute, setting a new standard for reasoning in small-scale models.",
        "timestamp": "2025-03-27T03:58:35.164Z",
        "rating": "novote",
        "publishedDate": "2025/03/26",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-27T03:58:35+00:00",
        "updated_at": "2025-03-29T22:40:48+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.20168": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.20168",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-27T06:51:46.529Z",
            "data": {
              "session_id": "session_1743058305263_bwdcces",
              "source_id": "arxiv",
              "paper_id": "2503.20168",
              "start_time": "2025-03-27T06:51:18.803Z",
              "end_time": "2025-03-27T06:51:45.263Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 1,
              "total_elapsed_seconds": 26
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-27T06:51:48+00:00",
        "updated_at": "2025-03-29T22:40:45+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.20168": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.20168",
        "url": "https://arxiv.org/abs/2503.20168",
        "title": "EVolSplat: Efficient Volume-based Gaussian Splatting for Urban View Synthesis",
        "authors": "Miao, Sheng, Huang, Jiaxin, Bai, Dongfeng, Yan, Xu, Zhou, Hongyu, Wang, Yue, Liu, Bingbing, Geiger, Andreas, Liao, Yiyi",
        "abstract": "Novel view synthesis of urban scenes is essential for autonomous driving-related applications.Existing NeRF and 3DGS-based methods show promising results in achieving photorealistic renderings but require slow, per-scene optimization. We introduce EVolSplat, an efficient 3D Gaussian Splatting model for urban scenes that works in a feed-forward manner. Unlike existing feed-forward, pixel-aligned 3DGS methods, which often suffer from issues like multi-view inconsistencies and duplicated content, our approach predicts 3D Gaussians across multiple frames within a unified volume using a 3D convolutional network. This is achieved by initializing 3D Gaussians with noisy depth predictions, and then refining their geometric properties in 3D space and predicting color based on 2D textures. Our model also handles distant views and the sky with a flexible hemisphere background model. This enables us to perform fast, feed-forward reconstruction while achieving real-time rendering. Experimental evaluations on the KITTI-360 and Waymo datasets show that our method achieves state-of-the-art quality compared to existing feed-forward 3DGS- and NeRF-based methods.",
        "timestamp": "2025-03-27T06:51:19.325Z",
        "rating": "novote",
        "publishedDate": "2025/03/26",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-27T06:51:19+00:00",
        "updated_at": "2025-03-29T22:40:46+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.18352": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.18352",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-27T07:21:38.112Z",
            "data": {
              "session_id": "session_1743060096410_xkwua4z",
              "source_id": "arxiv",
              "paper_id": "2503.18352",
              "start_time": "2025-03-27T07:21:06.659Z",
              "end_time": "2025-03-27T07:21:36.410Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 5,
              "total_elapsed_seconds": 30
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-27T07:21:39+00:00",
        "updated_at": "2025-03-29T22:40:44+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.18352": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.18352",
        "url": "https://arxiv.org/abs/2503.18352",
        "title": "Diffusion-4K: Ultra-High-Resolution Image Synthesis with Latent Diffusion Models",
        "authors": "Zhang, Jinjin, Huang, Qiuyu, Liu, Junjie, Guo, Xiefan, Huang, Di",
        "abstract": "In this paper, we present Diffusion-4K, a novel framework for direct ultra-high-resolution image synthesis using text-to-image diffusion models. The core advancements include: (1) Aesthetic-4K Benchmark: addressing the absence of a publicly available 4K image synthesis dataset, we construct Aesthetic-4K, a comprehensive benchmark for ultra-high-resolution image generation. We curated a high-quality 4K dataset with carefully selected images and captions generated by GPT-4o. Additionally, we introduce GLCM Score and Compression Ratio metrics to evaluate fine details, combined with holistic measures such as FID, Aesthetics and CLIPScore for a comprehensive assessment of ultra-high-resolution images. (2) Wavelet-based Fine-tuning: we propose a wavelet-based fine-tuning approach for direct training with photorealistic 4K images, applicable to various latent diffusion models, demonstrating its effectiveness in synthesizing highly detailed 4K images. Consequently, Diffusion-4K achieves impressive performance in high-quality image synthesis and text prompt adherence, especially when powered by modern large-scale diffusion models (e.g., SD3-2B and Flux-12B). Extensive experimental results from our benchmark demonstrate the superiority of Diffusion-4K in ultra-high-resolution image synthesis.",
        "timestamp": "2025-03-27T07:21:07.296Z",
        "rating": "novote",
        "publishedDate": "2025/03/24",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-27T07:21:07+00:00",
        "updated_at": "2025-03-29T22:40:44+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.17973": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.17973",
        "url": "https://arxiv.org/abs/2503.17973",
        "title": "PhysTwin: Physics-Informed Reconstruction and Simulation of Deformable Objects from Videos",
        "authors": "Jiang, Hanxiao, Hsu, Hao-Yu, Zhang, Kaifeng, Yu, Hsin-Ni, Wang, Shenlong, Li, Yunzhu",
        "abstract": "Creating a physical digital twin of a real-world object has immense potential in robotics, content creation, and XR. In this paper, we present PhysTwin, a novel framework that uses sparse videos of dynamic objects under interaction to produce a photo- and physically realistic, real-time interactive virtual replica. Our approach centers on two key components: (1) a physics-informed representation that combines spring-mass models for realistic physical simulation, generative shape models for geometry, and Gaussian splats for rendering; and (2) a novel multi-stage, optimization-based inverse modeling framework that reconstructs complete geometry, infers dense physical properties, and replicates realistic appearance from videos. Our method integrates an inverse physics framework with visual perception cues, enabling high-fidelity reconstruction even from partial, occluded, and limited viewpoints. PhysTwin supports modeling various deformable objects, including ropes, stuffed animals, cloth, and delivery packages. Experiments show that PhysTwin outperforms competing methods in reconstruction, rendering, future prediction, and simulation under novel interactions. We further demonstrate its applications in interactive real-time simulation and model-based robotic motion planning.",
        "timestamp": "2025-03-27T07:19:59.401Z",
        "rating": "novote",
        "publishedDate": "2025/03/23",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-27T07:19:59+00:00",
        "updated_at": "2025-03-29T22:40:45+00:00",
        "version": 2
      }
    },
    "interactions:url.url.1DA35195": {
      "data": {
        "sourceId": "url",
        "paperId": "url.1DA35195",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-03-18T06:54:13+00:00",
        "updated_at": "2025-03-29T22:42:40+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.21450": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.21450",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-28T03:14:42.484Z",
            "data": {
              "session_id": "session_1743131682476_x3lf6pm",
              "source_id": "arxiv",
              "paper_id": "2503.21450",
              "start_time": "2025-03-28T03:14:30.906Z",
              "end_time": "2025-03-28T03:14:42.476Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-28T03:14:13+00:00",
        "updated_at": "2025-03-29T22:40:42+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2503.21450": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.21450",
        "url": "https://arxiv.org/abs/2503.21450v1",
        "title": "CMADiff: Cross-Modal Aligned Diffusion for Controllable Protein Generation",
        "authors": "Zhou, Changjian, Qiu, Yuexi, Ling, Tongtong, Li, Jiafeng, Liu, Shuanghe, Wang, Xiangjing, Song, Jia, Xiang, Wensheng",
        "abstract": "AI-assisted protein design has emerged as a critical tool for advancing biotechnology, as deep generative models have demonstrated their reliability in this domain. However, most existing models primarily utilize protein sequence or structural data for training, neglecting the physicochemical properties of proteins.Moreover, they are deficient to control the generation of proteins in intuitive conditions. To address these limitations,we propose CMADiff here, a novel framework that enables controllable protein generation by aligning the physicochemical properties of protein sequences with text-based descriptions through a latent diffusion process. Specifically, CMADiff employs a Conditional Variational Autoencoder (CVAE) to integrate physicochemical features as conditional input, forming a robust latent space that captures biological traits. In this latent space, we apply a conditional diffusion process, which is guided by BioAligner, a contrastive learning-based module that aligns text descriptions with protein features, enabling text-driven control over protein sequence generation. Validated by a series of evaluations including AlphaFold3, the experimental results indicate that CMADiff outperforms protein sequence generation benchmarks and holds strong potential for future applications. The implementation and code are available at https://github.com/HPC-NEAU/PhysChemDiff.",
        "timestamp": "2025-03-28T03:13:50.157Z",
        "rating": "novote",
        "publishedDate": "2025/03/27",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-28T03:13:50+00:00",
        "updated_at": "2025-03-29T22:40:42+00:00",
        "version": 2
      }
    },
    "paper:url.D0FBC77": {
      "data": {
        "sourceId": "url",
        "paperId": "D0FBC77",
        "url": "https://www.sciencedirect.com/science/article/abs/pii/S1532046424000686",
        "title": "Automated annotation of disease subtypes",
        "authors": "",
        "abstract": "Distinguishing diseases into distinct subtypes is crucial for study and effective treatment strategies. The Open Targets Platform (OT) integrates biom\u2026",
        "timestamp": "2025-03-28T02:26:01.036Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1016/j.jbi.2024.104650",
        "journalName": "Journal of Biomedical Informatics",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-28T02:26:01+00:00",
        "updated_at": "2025-03-29T22:40:43+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2502.04382": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.04382",
        "url": "https://arxiv.org/abs/2502.04382",
        "title": "Sparse Autoencoders for Hypothesis Generation",
        "authors": "Movva, Rajiv, Peng, Kenny, Garg, Nikhil, Kleinberg, Jon, Pierson, Emma",
        "abstract": "We describe HypotheSAEs, a general method to hypothesize interpretable relationships between text data (e.g., headlines) and a target variable (e.g., clicks). HypotheSAEs has three steps: (1) train a sparse autoencoder on text embeddings to produce interpretable features describing the data distribution, (2) select features that predict the target variable, and (3) generate a natural language interpretation of each feature (e.g., \"mentions being surprised or shocked\") using an LLM. Each interpretation serves as a hypothesis about what predicts the target variable. Compared to baselines, our method better identifies reference hypotheses on synthetic datasets (at least +0.06 in F1) and produces more predictive hypotheses on real datasets (~twice as many significant findings), despite requiring 1-2 orders of magnitude less compute than recent LLM-based methods. HypotheSAEs also produces novel discoveries on two well-studied tasks: explaining partisan differences in Congressional speeches and identifying drivers of engagement with online headlines.",
        "timestamp": "2025-03-28T02:23:28.737Z",
        "rating": "novote",
        "publishedDate": "2025/02/05",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-28T02:23:29+00:00",
        "updated_at": "2025-03-29T22:40:43+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.19786": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.19786",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-28T03:53:38.688Z",
            "data": {
              "session_id": "session_1743134017886_08ldzjl",
              "source_id": "arxiv",
              "paper_id": "2503.19786",
              "start_time": "2025-03-28T03:53:31.027Z",
              "end_time": "2025-03-28T03:53:37.886Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-28T03:54:52.286Z",
            "data": {
              "session_id": "session_1743134091824_8qqheqo",
              "source_id": "arxiv",
              "paper_id": "2503.19786",
              "start_time": "2025-03-28T03:53:51.468Z",
              "end_time": "2025-03-28T03:54:51.824Z",
              "heartbeat_count": 12,
              "duration_seconds": 60,
              "idle_seconds": 0,
              "total_elapsed_seconds": 60
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-28T03:53:39+00:00",
        "updated_at": "2025-03-29T22:40:41+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2503.19786": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.19786",
        "url": "https://arxiv.org/abs/2503.19786",
        "title": "Gemma 3 Technical Report",
        "authors": "Gemma Team, Kamath, Aishwarya, Ferret, Johan, Pathak, Shreya, Vieillard, Nino, Merhej, Ramona, Perrin, Sarah, Matejovicova, Tatiana, Ram\u00e9, Alexandre, Rivi\u00e8re, Morgane, Rouillard, Louis, Mesnard, Thomas, Cideron, Geoffrey, Grill, Jean-bastien, Ramos, Sabela, Yvinec, Edouard, Casbon, Michelle, Pot, Etienne, Penchev, Ivo, Liu, Ga\u00ebl, Visin, Francesco, Kenealy, Kathleen, Beyer, Lucas, Zhai, Xiaohai, Tsitsulin, Anton, Busa-Fekete, Robert, Feng, Alex, Sachdeva, Noveen, Coleman, Benjamin, Gao, Yi, Mustafa, Basil, Barr, Iain, Parisotto, Emilio, Tian, David, Eyal, Matan, Cherry, Colin, Peter, Jan-Thorsten, Sinopalnikov, Danila, Bhupatiraju, Surya, Agarwal, Rishabh, Kazemi, Mehran, Malkin, Dan, Kumar, Ravin, Vilar, David, Brusilovsky, Idan, Luo, Jiaming, Steiner, Andreas, Friesen, Abe, Sharma, Abhanshu, Sharma, Abheesht, Gilady, Adi Mayrav, Goedeckemeyer, Adrian, Saade, Alaa, Feng, Alex, Kolesnikov, Alexander, Bendebury, Alexei, Abdagic, Alvin, Vadi, Amit, Gy\u00f6rgy, Andr\u00e1s, Pinto, Andr\u00e9 Susano, Das, Anil, Bapna, Ankur, Miech, Antoine, Yang, Antoine, Paterson, Antonia, Shenoy, Ashish, Chakrabarti, Ayan, Piot, Bilal, Wu, Bo, Shahriari, Bobak, Petrini, Bryce, Chen, Charlie, Lan, Charline Le, Choquette-Choo, Christopher A., Carey, CJ, Brick, Cormac, Deutsch, Daniel, Eisenbud, Danielle, Cattle, Dee, Cheng, Derek, Paparas, Dimitris, Sreepathihalli, Divyashree Shivakumar, Reid, Doug, Tran, Dustin, Zelle, Dustin, Noland, Eric, Huizenga, Erwin, Kharitonov, Eugene, Liu, Frederick, Amirkhanyan, Gagik, Cameron, Glenn, Hashemi, Hadi, Klimczak-Pluci\u0144ska, Hanna, Singh, Harman, Mehta, Harsh, Lehri, Harshal Tushar, Hazimeh, Hussein, Ballantyne, Ian, Szpektor, Idan, Nardini, Ivan, Pouget-Abadie, Jean, Chan, Jetha, Stanton, Joe, Wieting, John, Lai, Jonathan, Orbay, Jordi, Fernandez, Joseph, Newlan, Josh, Ji, Ju-yeong, Singh, Jyotinder, Black, Kat, Yu, Kathy, Hui, Kevin, Vodrahalli, Kiran, Greff, Klaus, Qiu, Linhai, Valentine, Marcella, Coelho, Marina, Ritter, Marvin, Hoffman, Matt, Watson, Matthew, Chaturvedi, Mayank, Moynihan, Michael, Ma, Min, Babar, Nabila, Noy, Natasha, Byrd, Nathan, Roy, Nick, Momchev, Nikola, Chauhan, Nilay, Sachdeva, Noveen, Bunyan, Oskar, Botarda, Pankil, Caron, Paul, Rubenstein, Paul Kishan, Culliton, Phil, Schmid, Philipp, Sessa, Pier Giuseppe, Xu, Pingmei, Stanczyk, Piotr, Tafti, Pouya, Shivanna, Rakesh, Wu, Renjie, Pan, Renke, Rokni, Reza, Willoughby, Rob, Vallu, Rohith, Mullins, Ryan, Jerome, Sammy, Smoot, Sara, Girgin, Sertan, Iqbal, Shariq, Reddy, Shashir, Sheth, Shruti, P\u00f5der, Siim, Bhatnagar, Sijal, Panyam, Sindhu Raghuram, Eiger, Sivan, Zhang, Susan, Liu, Tianqi, Yacovone, Trevor, Liechty, Tyler, Kalra, Uday, Evci, Utku, Misra, Vedant, Roseberry, Vincent, Feinberg, Vlad, Kolesnikov, Vlad, Han, Woohyun, Kwon, Woosuk, Chen, Xi, Chow, Yinlam, Zhu, Yuvein, Wei, Zichuan, Egyed, Zoltan, Cotruta, Victor, Giang, Minh, Kirk, Phoebe, Rao, Anand, Black, Kat, Babar, Nabila, Lo, Jessica, Moreira, Erica, Martins, Luiz Gustavo, Sanseviero, Omar, Gonzalez, Lucas, Gleicher, Zach, Warkentin, Tris, Mirrokni, Vahab, Senter, Evan, Collins, Eli, Barral, Joelle, Ghahramani, Zoubin, Hadsell, Raia, Matias, Yossi, Sculley, D., Petrov, Slav, Fiedel, Noah, Shazeer, Noam, Vinyals, Oriol, Dean, Jeff, Hassabis, Demis, Kavukcuoglu, Koray, Farabet, Clement, Buchatskaya, Elena, Alayrac, Jean-Baptiste, Anil, Rohan, Dmitry, Lepikhin, Borgeaud, Sebastian, Bachem, Olivier, Joulin, Armand, Andreev, Alek, Hardin, Cassidy, Dadashi, Robert, Hussenot, L\u00e9onard",
        "abstract": "We introduce Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging in scale from 1 to 27 billion parameters. This version introduces vision understanding abilities, a wider coverage of languages and longer context - at least 128K tokens. We also change the architecture of the model to reduce the KV-cache memory that tends to explode with long context. This is achieved by increasing the ratio of local to global attention layers, and keeping the span on local attention short. The Gemma 3 models are trained with distillation and achieve superior performance to Gemma 2 for both pre-trained and instruction finetuned versions. In particular, our novel post-training recipe significantly improves the math, chat, instruction-following and multilingual abilities, making Gemma3-4B-IT competitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro across benchmarks. We release all our models to the community.",
        "timestamp": "2025-03-28T03:53:30.532Z",
        "rating": "novote",
        "publishedDate": "2025/03/25",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-28T03:53:31+00:00",
        "updated_at": "2025-03-29T22:40:41+00:00",
        "version": 2
      }
    },
    "paper:url.A844FFF": {
      "data": {
        "sourceId": "url",
        "paperId": "A844FFF",
        "url": "https://link.springer.com/article/10.3758/s13420-025-00668-1",
        "title": "Memory encoded in the interactions of ants",
        "authors": "Czaczkes, Tomer J.",
        "abstract": "Dreyer et al., Proceedings of the National Academy of Sciences 122, e2414274121, (2025) challenged ant and human groups to carry an oddly shaped load through a series of narrow rooms, and found that both succeed remarkably well, but used very different tactics. While the fact that humans dumb themselves down in some groups is interesting, the discovery of a collective memory built into the interaction patterns of the ants is extremely exciting.",
        "timestamp": "2025-03-28T04:53:55.220Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.3758/s13420-025-00668-1",
        "journalName": "Learning & Behavior",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-28T04:53:55+00:00",
        "updated_at": "2025-03-29T22:40:40+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2501.02393": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.02393",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-28T04:51:34.716Z",
            "data": {
              "session_id": "session_1743137494704_10b7nkc",
              "source_id": "arxiv",
              "paper_id": "2501.02393",
              "start_time": "2025-03-28T04:51:20.886Z",
              "end_time": "2025-03-28T04:51:34.704Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-28T04:51:36+00:00",
        "updated_at": "2025-03-29T22:40:40+00:00",
        "version": 3
      }
    },
    "paper:url.13CF44F5": {
      "data": {
        "sourceId": "url",
        "paperId": "13CF44F5",
        "url": "https://www.pnas.org/doi/10.1073/pnas.2414274121",
        "title": "Comparing cooperative geometric puzzle solving in ants versus humans",
        "authors": "Dreyer, Tabea, Haluts, Amir, Korman, Amos, Gov, Nir, Fonio, Ehud, Feinerman, Ofer",
        "abstract": "Biological ensembles use collective intelligence to tackle challenges together, but\nsuboptimal coordination can undermine the effectiveness of grou...",
        "timestamp": "2025-03-28T04:55:14.160Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "collective intelligence",
          "social insects",
          "human behavior",
          "cooperative transport",
          "consensus decisions"
        ],
        "doi": "10.1073/pnas.2414274121",
        "journalName": "Proceedings of the National Academy of Sciences",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-28T04:55:14+00:00",
        "updated_at": "2025-03-29T22:40:38+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2501.08120": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.08120",
        "url": "https://arxiv.org/abs/2501.08120",
        "title": "In-situ graph reasoning and knowledge expansion using Graph-PReFLexOR",
        "authors": "Buehler, Markus J.",
        "abstract": "The pursuit of automated scientific discovery has fueled progress from symbolic logic to modern AI, forging new frontiers in reasoning and pattern recognition. Transformers function as potential systems, where every possible relationship remains latent potentiality until tasks impose constraints, akin to measurement. Yet, refining their sampling requires more than probabilistic selection: solutions must conform to specific structures or rules, ensuring consistency and the invocation of general principles. We present Graph-PReFLexOR (Graph-based Preference-based Recursive Language Modeling for Exploratory Optimization of Reasoning), a framework that combines graph reasoning with symbolic abstraction to dynamically expand domain knowledge. Inspired by reinforcement learning, Graph-PReFLexOR defines reasoning as a structured mapping, where tasks yield knowledge graphs, abstract patterns, and ultimately, final answers. Inspired by category theory, it encodes concepts as nodes and their relationships as edges, supporting hierarchical inference and adaptive learning through isomorphic representations. Demonstrations include hypothesis generation, materials design, and creative reasoning, such as discovering relationships between mythological concepts like 'thin places' with materials science. We propose a 'knowledge garden growth' strategy that integrates insights across domains, promoting interdisciplinary connections. Results with a 3-billion-parameter Graph-PReFLexOR model show superior reasoning depth and adaptability, underscoring the potential for transparent, multidisciplinary AI-driven discovery. It lays the groundwork for general autonomous reasoning solutions.",
        "timestamp": "2025-03-28T04:55:01.936Z",
        "rating": "novote",
        "publishedDate": "2025/01/14",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-28T04:55:02+00:00",
        "updated_at": "2025-03-29T22:40:39+00:00",
        "version": 2
      }
    },
    "interactions:url.A844FFF": {
      "data": {
        "sourceId": "url",
        "paperId": "A844FFF",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-28T04:54:38.303Z",
            "data": {
              "session_id": "session_1743137677279_36bz5xr",
              "source_id": "url",
              "paper_id": "A844FFF",
              "start_time": "2025-03-28T04:53:55.714Z",
              "end_time": "2025-03-28T04:54:37.279Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 2,
              "total_elapsed_seconds": 42
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-28T04:54:39+00:00",
        "updated_at": "2025-03-29T22:40:39+00:00",
        "version": 3
      }
    },
    "interactions:url.13CF44F5": {
      "data": {
        "sourceId": "url",
        "paperId": "13CF44F5",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-28T04:56:49.379Z",
            "data": {
              "session_id": "session_1743137808461_jnd4zz2",
              "source_id": "url",
              "paper_id": "13CF44F5",
              "start_time": "2025-03-28T04:55:14.953Z",
              "end_time": "2025-03-28T04:56:48.461Z",
              "heartbeat_count": 18,
              "duration_seconds": 90,
              "idle_seconds": 4,
              "total_elapsed_seconds": 94
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-28T04:56:50+00:00",
        "updated_at": "2025-03-29T22:40:38+00:00",
        "version": 3
      }
    },
    "interactions:arxiv.2109.05237": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2109.05237",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-28T09:07:48.478Z",
            "data": {
              "session_id": "session_1743152867591_d9xn4ka",
              "source_id": "arxiv",
              "paper_id": "2109.05237",
              "start_time": "2025-03-28T09:06:51.757Z",
              "end_time": "2025-03-28T09:07:47.591Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 1,
              "total_elapsed_seconds": 56
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-28T09:07:49+00:00",
        "updated_at": "2025-03-29T22:40:36+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2109.05237": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2109.05237",
        "url": "https://arxiv.org/pdf/2109.05237",
        "title": "2109.05237",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-28T09:06:52.570Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-28T09:06:53+00:00",
        "updated_at": "2025-03-29T22:40:37+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2501.06252": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.06252",
        "url": "https://arxiv.org/abs/2501.06252",
        "title": "Transformer-Squared: Self-adaptive LLMs",
        "authors": "Sun, Qi, Cetin, Edoardo, Tang, Yujin",
        "abstract": "Self-adaptive large language models (LLMs) aim to solve the challenges posed by traditional fine-tuning methods, which are often computationally intensive and static in their ability to handle diverse tasks. We introduce Transformer-Squared, a novel self-adaptation framework that adapts LLMs for unseen tasks in real-time by selectively adjusting only the singular components of their weight matrices. During inference, Transformer-Squared employs a two-pass mechanism: first, a dispatch system identifies the task properties, and then task-specific 'expert' vectors, trained using reinforcement learning, are dynamically mixed to obtain targeted behavior for the incoming prompt. Our method consistently outperforms ubiquitous approaches such as LoRA, with fewer parameters and greater efficiency. Furthermore, Transformer-Squared demonstrates versatility across different LLM architectures and modalities, including vision-language tasks. Transformer-Squared represents a significant leap forward, offering a scalable, efficient solution for enhancing the adaptability and task-specific performance of LLMs, paving the way for truly dynamic, self-organizing AI systems.",
        "timestamp": "2025-03-28T08:24:47.831Z",
        "rating": "novote",
        "publishedDate": "2025/01/09",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-28T08:24:48+00:00",
        "updated_at": "2025-03-29T22:40:37+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2406.19204": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.19204",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-28T09:18:11.184Z",
            "data": {
              "session_id": "session_1743153490268_iwyw5f5",
              "source_id": "arxiv",
              "paper_id": "2406.19204",
              "start_time": "2025-03-28T09:17:52.063Z",
              "end_time": "2025-03-28T09:18:10.268Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 3,
              "total_elapsed_seconds": 18
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-28T09:18:12+00:00",
        "updated_at": "2025-03-29T22:40:34+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2406.19204": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.19204",
        "url": "https://arxiv.org/abs/2406.19204",
        "title": "CoDiNG -- Naming Game with Continuous Latent Opinions of Individual Agents",
        "authors": "Nurek, Mateusz, Ko\u0142aczek, Joanna, Michalski, Rados\u0142aw, Szyma\u0144ski, Boles\u0142aw K., Lizardo, Omar",
        "abstract": "Understanding the mechanisms behind opinion formation is crucial for gaining insight into the processes that shape political beliefs, cultural attitudes, consumer choices, and social movements. This work aims to explore a nuanced model that captures the intricacies of real-world opinion dynamics by synthesizing principles from cognitive science and employing social network analysis. The proposed model is a hybrid continuous-discrete extension of the well-known Naming Game opinion model. The added latent continuous layer of opinion strength follows cognitive processes in the human brain, akin to memory imprints. The discrete layer allows for the conversion of intrinsic continuous opinion into discrete form, which often occurs when we publicly verbalize our opinions. We evaluated our model using real data as ground truth and demonstrated that the proposed mechanism outperforms the classic Naming Game model in many cases, reflecting that our model is closer to the real process of opinion formation.",
        "timestamp": "2025-03-28T09:17:51.878Z",
        "rating": "novote",
        "publishedDate": "2024/06/27",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-28T09:17:52+00:00",
        "updated_at": "2025-03-29T22:40:35+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.20648": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.20648",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-28T17:36:13.774Z",
            "data": {
              "session_id": "session_1743183373732_tkqdoo2",
              "source_id": "arxiv",
              "paper_id": "2503.20648",
              "start_time": "2025-03-28T17:35:56.010Z",
              "end_time": "2025-03-28T17:36:13.732Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 3,
              "total_elapsed_seconds": 18
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-28T17:36:14+00:00",
        "updated_at": "2025-03-29T22:40:33+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.20648": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.20648",
        "url": "https://arxiv.org/html/2503.20648v1",
        "title": "TN-Eval: Rubric and Evaluation Protocols for Measuring the Quality of Behavioral Therapy Notes",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-28T17:35:49.898Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-28T17:35:50+00:00",
        "updated_at": "2025-03-29T22:40:34+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2502.05264": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.05264",
        "url": "https://arxiv.org/abs/2502.05264",
        "title": "Quantum automated learning with provable and explainable trainability",
        "authors": "Ye, Qi, Geng, Shuangyue, Han, Zizhao, Li, Weikang, Duan, L. -M., Deng, Dong-Ling",
        "abstract": "Machine learning is widely believed to be one of the most promising practical applications of quantum computing. Existing quantum machine learning schemes typically employ a quantum-classical hybrid approach that relies crucially on gradients of model parameters. Such an approach lacks provable convergence to global minima and will become infeasible as quantum learning models scale up. Here, we introduce quantum automated learning, where no variational parameter is involved and the training process is converted to quantum state preparation. In particular, we encode training data into unitary operations and iteratively evolve a random initial state under these unitaries and their inverses, with a target-oriented perturbation towards higher prediction accuracy sandwiched in between. Under reasonable assumptions, we rigorously prove that the evolution converges exponentially to the desired state corresponding to the global minimum of the loss function. We show that such a training process can be understood from the perspective of preparing quantum states by imaginary time evolution, where the data-encoded unitaries together with target-oriented perturbations would train the quantum learning model in an automated fashion. We further prove that the quantum automated learning paradigm features good generalization ability with the generalization error upper bounded by the ratio between a logarithmic function of the Hilbert space dimension and the number of training samples. In addition, we carry out extensive numerical simulations on real-life images and quantum data to demonstrate the effectiveness of our approach and validate the assumptions. Our results establish an unconventional quantum learning strategy that is gradient-free with provable and explainable trainability, which would be crucial for large-scale practical applications of quantum computing in machine learning scenarios.",
        "timestamp": "2025-03-28T17:42:24.833Z",
        "rating": "novote",
        "publishedDate": "2025/02/07",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-28T17:42:25+00:00",
        "updated_at": "2025-03-29T22:40:32+00:00",
        "version": 2
      }
    },
    "paper:url.4F9993C2": {
      "data": {
        "sourceId": "url",
        "paperId": "4F9993C2",
        "url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/ae28c7bc9414ffd8ffd2b3d454e6ef3e-Paper-Conference.pdf",
        "title": "4F9993C2",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-28T17:42:23.106Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "created_at": "2025-03-28T17:42:23+00:00",
        "updated_at": "2025-03-29T22:40:32+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2307.13456": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2307.13456",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-28T17:40:16.608Z",
            "data": {
              "session_id": "session_1743183615744_7myuv1e",
              "source_id": "arxiv",
              "paper_id": "2307.13456",
              "start_time": "2025-03-28T17:39:45.561Z",
              "end_time": "2025-03-28T17:40:15.744Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 0,
              "total_elapsed_seconds": 30
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-28T17:40:17+00:00",
        "updated_at": "2025-03-29T22:40:33+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2307.13456": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2307.13456",
        "url": "https://arxiv.org/abs/2307.13456",
        "title": "Weak solutions to gradient flows of functionals with inhomogeneous growth in metric spaces",
        "authors": "G\u00f3rny, Wojciech",
        "abstract": "We use the framework of the first-order differential structure in metric measure spaces introduced by Gigli to define a notion of weak solutions to gradient flows of convex, lower semicontinuous and coercive functionals. We prove their existence and uniqueness and show that they are also variational solutions; in particular, this is an existence result for variational solutions. Then, we apply this technique in the case of a gradient flow of a functional with inhomogeneous growth.",
        "timestamp": "2025-03-28T17:39:46.168Z",
        "rating": "novote",
        "publishedDate": "2023/07/25",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-28T17:39:46+00:00",
        "updated_at": "2025-03-29T22:40:33+00:00",
        "version": 2
      }
    },
    "interactions:url.6B71649F": {
      "data": {
        "sourceId": "url",
        "paperId": "6B71649F",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-28T18:50:31.371Z",
            "data": {
              "session_id": "session_1743187830624_cfin3ab",
              "source_id": "url",
              "paper_id": "6B71649F",
              "start_time": "2025-03-28T18:50:21.456Z",
              "end_time": "2025-03-28T18:50:30.624Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-28T18:50:32+00:00",
        "updated_at": "2025-03-29T22:40:30+00:00",
        "version": 3
      }
    },
    "paper:url.6B71649F": {
      "data": {
        "sourceId": "url",
        "paperId": "6B71649F",
        "url": "https://scholars.org/contribution/twenty-lessons-fighting-tyranny-twentieth",
        "title": "Twenty Lessons on Fighting Tyranny from the Twentieth Century",
        "authors": "",
        "abstract": "Americans are no wiser than the Europeans who saw democracy yield to fascism, Nazism or communism.\u00a0Our one advantage is that we might learn from their experience.\u00a0Now is a good time to do so.\u00a0From across the fearful twentieth century, here are twenty lessons about what it takes to oppose tyranny, adapted to the circumstances of today.",
        "timestamp": "2025-03-28T18:50:20.860Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-28T18:50:21+00:00",
        "updated_at": "2025-03-29T22:40:31+00:00",
        "version": 2
      }
    },
    "paper:url.7D6B92D2": {
      "data": {
        "sourceId": "url",
        "paperId": "7D6B92D2",
        "url": "https://openreview.net/forum?id=nvb60szj5C",
        "title": "DeltaProduct: Improving State-Tracking in Linear RNNs via Householder Products",
        "authors": "Julien Siems, Timur Carstensen, Arber Zela, Frank Hutter, Massimiliano Pontil, Riccardo Grazzi",
        "abstract": "Linear Recurrent Neural Networks (linear RNNs) have emerged as competitive alternatives to Transformers for sequence modeling, offering efficient training and linear-time inference. However, existing architectures face a fundamental trade-off between expressivity and efficiency, dictated by the structure of their state-transition matrices. While diagonal matrices used in architectures like Mamba, GLA, or mLSTM yield fast runtime, they suffer from severely limited expressivity. To address this, recent architectures such as (Gated) DeltaNet and RWKV-7 adopted a diagonal plus rank-1 structure, allowing simultaneous token-channel mixing, which overcomes some expressivity limitations with only a slight decrease in training efficiency. Building on the interpretation of DeltaNet's recurrence as performing one step of online gradient descent per token on an associative recall loss, we introduce DeltaProduct, which instead takes multiple ($n_h$) steps per token. This naturally leads to diagonal plus rank-$n_h$ state-transition matrices, formed as products of $n_h$ generalized Householder transformations, providing a tunable mechanism to balance expressivity and efficiency and a stable recurrence. Through extensive experiments, we demonstrate that DeltaProduct achieves superior state-tracking and language modeling capabilities while exhibiting significantly improved length extrapolation compared to DeltaNet. Additionally, we also strengthen the theoretical foundation of DeltaNet by proving that it can solve dihedral group word problems in just two layers.",
        "timestamp": "2025-03-28T17:45:25.990Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-28T17:45:26+00:00",
        "updated_at": "2025-03-29T22:40:31+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2405.12964": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2405.12964",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-28T19:28:48.277Z",
            "data": {
              "session_id": "session_1743190127470_zw7t7na",
              "source_id": "arxiv",
              "paper_id": "2405.12964",
              "start_time": "2025-03-28T19:28:29.846Z",
              "end_time": "2025-03-28T19:28:47.470Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 3,
              "total_elapsed_seconds": 18
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-28T19:28:49+00:00",
        "updated_at": "2025-03-29T22:40:28+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2405.12964": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2405.12964",
        "url": "https://arxiv.org/abs/2405.12964",
        "title": "Differential Walk on Spheres",
        "authors": "Miller, Bailey, Sawhney, Rohan, Crane, Keenan, Gkioulekas, Ioannis",
        "abstract": "We introduce a Monte Carlo method for computing derivatives of the solution to a partial differential equation (PDE) with respect to problem parameters (such as domain geometry or boundary conditions). Derivatives can be evaluated at arbitrary points, without performing a global solve or constructing a volumetric grid or mesh. The method is hence well suited to inverse problems with complex geometry, such as PDE-constrained shape optimization. Like other walk on spheres (WoS) algorithms, our method is trivial to parallelize, and is agnostic to boundary representation (meshes, splines, implicit surfaces, etc.), supporting large topological changes. We focus in particular on screened Poisson equations, which model diverse problems from scientific and geometric computing. As in differentiable rendering, we jointly estimate derivatives with respect to all parameters -- hence, cost does not grow significantly with parameter count. In practice, even noisy derivative estimates exhibit fast, stable convergence for stochastic gradient-based optimization, as we show through examples from thermal design, shape from diffusion, and computer graphics.",
        "timestamp": "2025-03-28T19:28:30.482Z",
        "rating": "novote",
        "publishedDate": "2024/05/21",
        "tags": [],
        "doi": "10.1145/3687913",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-28T19:28:31+00:00",
        "updated_at": "2025-03-29T22:40:29+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2306.01923": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2306.01923",
        "url": "https://arxiv.org/abs/2306.01923",
        "title": "The Surprising Effectiveness of Diffusion Models for Optical Flow and Monocular Depth Estimation",
        "authors": "Saxena, Saurabh, Herrmann, Charles, Hur, Junhwa, Kar, Abhishek, Norouzi, Mohammad, Sun, Deqing, Fleet, David J.",
        "abstract": "Denoising diffusion probabilistic models have transformed image generation with their impressive fidelity and diversity. We show that they also excel in estimating optical flow and monocular depth, surprisingly, without task-specific architectures and loss functions that are predominant for these tasks. Compared to the point estimates of conventional regression-based methods, diffusion models also enable Monte Carlo inference, e.g., capturing uncertainty and ambiguity in flow and depth. With self-supervised pre-training, the combined use of synthetic and real data for supervised training, and technical innovations (infilling and step-unrolled denoising diffusion training) to handle noisy-incomplete training data, and a simple form of coarse-to-fine refinement, one can train state-of-the-art diffusion models for depth and optical flow estimation. Extensive experiments focus on quantitative performance against benchmarks, ablations, and the model's ability to capture uncertainty and multimodality, and impute missing values. Our model, DDVM (Denoising Diffusion Vision Model), obtains a state-of-the-art relative depth error of 0.074 on the indoor NYU benchmark and an Fl-all outlier rate of 3.26\\% on the KITTI optical flow benchmark, about 25\\% better than the best published method. For an overview see https://diffusion-vision.github.io.",
        "timestamp": "2025-03-28T18:55:56.416Z",
        "rating": "novote",
        "publishedDate": "2023/06/02",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-28T18:55:56+00:00",
        "updated_at": "2025-03-29T22:40:30+00:00",
        "version": 2
      }
    },
    "interactions:url.1F8E10": {
      "data": {
        "sourceId": "url",
        "paperId": "1F8E10",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-28T20:20:47.038Z",
            "data": {
              "session_id": "session_1743193246318_gixlsc7",
              "source_id": "url",
              "paper_id": "1F8E10",
              "start_time": "2025-03-28T20:20:34.714Z",
              "end_time": "2025-03-28T20:20:46.318Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-28T20:20:48+00:00",
        "updated_at": "2025-03-29T22:40:27+00:00",
        "version": 3
      }
    },
    "paper:url.1F8E10": {
      "data": {
        "sourceId": "url",
        "paperId": "1F8E10",
        "url": "https://minarcik.com/minkowski-penalties/",
        "title": "Minkowski Penalties: Robust Differentiable Constraint Enforcement for Vector Graphics",
        "authors": "Ji\u0159\u00ed Minar\u010d\u00edk, Sam Estep, Wode Ni, Keenan Crane",
        "abstract": "",
        "timestamp": "2025-03-28T20:20:33.948Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "Minkowski Penalties",
          "vector graphics",
          "differentiable constraints",
          "SIGGRAPH"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-28T20:20:34+00:00",
        "updated_at": "2025-03-29T22:40:28+00:00",
        "version": 2
      }
    },
    "paper:url.63ABCC60": {
      "data": {
        "sourceId": "url",
        "paperId": "63ABCC60",
        "url": "https://www.cs.cmu.edu/~kmcrane/Projects/WalkOnStars/index.html",
        "title": "Keenan Crane - Walk on Stars",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-28T19:57:59.273Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-28T19:57:59+00:00",
        "updated_at": "2025-03-29T22:40:28+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2402.17743": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.17743",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-28T20:24:08.740Z",
            "data": {
              "session_id": "session_1743193448727_xl48752",
              "source_id": "arxiv",
              "paper_id": "2402.17743",
              "start_time": "2025-03-28T20:24:03.090Z",
              "end_time": "2025-03-28T20:24:08.727Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-28T20:24:03+00:00",
        "updated_at": "2025-03-29T22:40:26+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2402.17743": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.17743",
        "url": "https://arxiv.org/abs/2402.17743",
        "title": "Rose: Composable Autodiff for the Interactive Web",
        "authors": "Estep, Sam, Ni, Wode, Rothkopf, Raven, Sunshine, Joshua",
        "abstract": "Reverse-mode automatic differentiation (autodiff) has been popularized by deep learning, but its ability to compute gradients is also valuable for interactive use cases such as bidirectional computer-aided design, embedded physics simulations, visualizing causal inference, and more. Unfortunately, the web is ill-served by existing autodiff frameworks, which use autodiff strategies that perform poorly on dynamic scalar programs, and pull in heavy dependencies that would result in unacceptable webpage sizes. This work introduces Rose, a lightweight autodiff framework for the web using a new hybrid approach to reverse-mode autodiff, blending conventional tracing and transformation techniques in a way that uses the host language for metaprogramming while also allowing the programmer to explicitly define reusable functions that comprise a larger differentiable computation. We demonstrate the value of the Rose design by porting two differentiable physics simulations, and evaluate its performance on an optimization-based diagramming application, showing Rose outperforming the state-of-the-art in web-based autodiff by multiple orders of magnitude.",
        "timestamp": "2025-03-28T20:23:31.869Z",
        "rating": "novote",
        "publishedDate": "2024/02/27",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-28T20:23:32+00:00",
        "updated_at": "2025-03-29T22:40:26+00:00",
        "version": 2
      }
    },
    "paper:url.35577C0B": {
      "data": {
        "sourceId": "url",
        "paperId": "35577C0B",
        "url": "https://minarcik.com/minkowski-penalties/MinkowskiPenalties.pdf",
        "title": "35577C0B",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-28T20:23:19.394Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "created_at": "2025-03-28T20:23:19+00:00",
        "updated_at": "2025-03-29T22:40:27+00:00",
        "version": 2
      }
    },
    "interactions:url.2D4E1D87": {
      "data": {
        "sourceId": "url",
        "paperId": "2D4E1D87",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-28T20:34:59.818Z",
            "data": {
              "session_id": "session_1743194099046_6kriqp2",
              "source_id": "url",
              "paper_id": "2D4E1D87",
              "start_time": "2025-03-28T20:34:49.764Z",
              "end_time": "2025-03-28T20:34:59.046Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-28T20:35:00+00:00",
        "updated_at": "2025-03-29T22:40:23+00:00",
        "version": 3
      }
    },
    "paper:url.2D4E1D87": {
      "data": {
        "sourceId": "url",
        "paperId": "2D4E1D87",
        "url": "https://gi1242.codeberg.page/cmu-math-cs-mcm/",
        "title": "Monte Carlo Methods and Applications",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-28T20:34:49.693Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-28T20:34:50+00:00",
        "updated_at": "2025-03-29T22:40:24+00:00",
        "version": 2
      }
    },
    "interactions:url.7935486C": {
      "data": {
        "sourceId": "url",
        "paperId": "7935486C",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-28T20:34:06.753Z",
            "data": {
              "session_id": "session_1743194046013_hxhkf2j",
              "source_id": "url",
              "paper_id": "7935486C",
              "start_time": "2025-03-28T20:33:52.009Z",
              "end_time": "2025-03-28T20:34:06.013Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-28T20:34:07+00:00",
        "updated_at": "2025-03-29T22:40:24+00:00",
        "version": 3
      }
    },
    "paper:url.7935486C": {
      "data": {
        "sourceId": "url",
        "paperId": "7935486C",
        "url": "https://brickisland.net/DDGSpring2024/",
        "title": "CS 15-458/858: Discrete Differential Geometry \u2013 CARNEGIE MELLON UNIVERSITY | SPRING 2024 | TUE/THU 12:30-1:50 | GHC 4303",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-28T20:33:51.624Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-28T20:33:52+00:00",
        "updated_at": "2025-03-29T22:40:25+00:00",
        "version": 2
      }
    },
    "interactions:url.2791CD05": {
      "data": {
        "sourceId": "url",
        "paperId": "2791CD05",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-28T20:33:46.992Z",
            "data": {
              "session_id": "session_1743194026175_oyeken5",
              "source_id": "url",
              "paper_id": "2791CD05",
              "start_time": "2025-03-28T20:33:35.318Z",
              "end_time": "2025-03-28T20:33:46.175Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-28T20:33:48+00:00",
        "updated_at": "2025-03-29T22:40:25+00:00",
        "version": 3
      }
    },
    "paper:url.2791CD05": {
      "data": {
        "sourceId": "url",
        "paperId": "2791CD05",
        "url": "https://www.cs.cmu.edu/~kmcrane/Projects/DDG/paper.pdf",
        "title": "2791CD05",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-28T20:33:35.882Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "created_at": "2025-03-28T20:33:36+00:00",
        "updated_at": "2025-03-29T22:40:25+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.1807.04799": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1807.04799",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-28T21:49:37.260Z",
            "data": {
              "session_id": "session_1743198576745_nf3jerf",
              "source_id": "arxiv",
              "paper_id": "1807.04799",
              "start_time": "2025-03-28T21:49:08.972Z",
              "end_time": "2025-03-28T21:49:36.745Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 3,
              "total_elapsed_seconds": 28
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-28T21:54:08.413Z",
            "data": {
              "session_id": "session_1743198848400_uw1x8n1",
              "source_id": "arxiv",
              "paper_id": "1807.04799",
              "start_time": "2025-03-28T21:53:23.950Z",
              "end_time": "2025-03-28T21:54:08.400Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 4,
              "total_elapsed_seconds": 44
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-28T21:58:09.321Z",
            "data": {
              "session_id": "session_1743199088787_1zbcdr2",
              "source_id": "arxiv",
              "paper_id": "1807.04799",
              "start_time": "2025-03-28T21:54:08.402Z",
              "end_time": "2025-03-28T21:58:08.787Z",
              "heartbeat_count": 48,
              "duration_seconds": 240,
              "idle_seconds": 0,
              "total_elapsed_seconds": 240
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-28T21:49:38+00:00",
        "updated_at": "2025-03-29T22:40:22+00:00",
        "version": 6
      }
    },
    "paper:arxiv.1807.04799": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1807.04799",
        "url": "https://arxiv.org/abs/1807.04799",
        "title": "The sound of an axon's growth",
        "authors": "Folz, Frederic, Wettmann, Lukas, Morigi, Giovanna, Kruse, Karsten",
        "abstract": "Axons are linear processes of nerve cells that can range from a few tens of micrometers up to meters in length. In addition to external cues, the length of an axon is also regulated by unknown internal mechanisms. Molecular motors have been suggested to generate oscillations with an axon length-dependent frequency that could be used to measure an axon's extension. Here, we present a mechanism that depends on the spectral decomposition of the oscillatory signal to determine the axon length.",
        "timestamp": "2025-03-28T21:43:40.873Z",
        "rating": "novote",
        "publishedDate": "2018/07/12",
        "tags": [],
        "doi": "10.1103/PhysRevE.99.050401",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-28T21:43:41+00:00",
        "updated_at": "2025-03-29T22:40:22+00:00",
        "version": 2
      }
    },
    "paper:url.7853D94B": {
      "data": {
        "sourceId": "url",
        "paperId": "7853D94B",
        "url": "https://babel.hathitrust.org/cgi/pt?id=wu.89045844305&seq=4",
        "title": "On some asymptotic properties of maximum likelihood estimates and related Bayes' estimates  ",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-29T00:20:48.785Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-29T00:20:49+00:00",
        "updated_at": "2025-03-29T22:40:20+00:00",
        "version": 2
      }
    },
    "interactions:url.360D8511": {
      "data": {
        "sourceId": "url",
        "paperId": "360D8511",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-29T00:18:46.834Z",
            "data": {
              "session_id": "session_1743207526067_2ovbkxn",
              "source_id": "url",
              "paper_id": "360D8511",
              "start_time": "2025-03-29T00:18:33.384Z",
              "end_time": "2025-03-29T00:18:46.067Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 3,
              "total_elapsed_seconds": 13
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-29T00:18:47+00:00",
        "updated_at": "2025-03-29T22:40:21+00:00",
        "version": 3
      }
    },
    "paper:url.360D8511": {
      "data": {
        "sourceId": "url",
        "paperId": "360D8511",
        "url": "https://www.jstor.org/stable/2699974",
        "title": "The Publications and Writings of Lucien Le Cam on JSTOR",
        "authors": "",
        "abstract": "The Publications and Writings of Lucien Le Cam, The Annals of Statistics, Vol. 30, No. 3 (Jun., 2002), pp. 683-687",
        "timestamp": "2025-03-29T00:18:31.579Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-29T00:18:32+00:00",
        "updated_at": "2025-03-29T22:40:21+00:00",
        "version": 2
      }
    },
    "paper:url.4FD9F15": {
      "data": {
        "sourceId": "url",
        "paperId": "4FD9F15",
        "url": "https://openreview.net/forum?id=58gPkcVbFL",
        "title": "Evolution of Discriminator and Generator Gradients in GAN Training: From Fitting to Collapse",
        "authors": "Weiguo Gao, Ming Li",
        "abstract": "Generative Adversarial Networks (GANs) are powerful generative models but often suffer from mode mixture and mode collapse. We propose a perspective that views GAN training as a two-phase progression from fitting to collapse, where mode mixture and mode collapse are treated as inter-connected. Inspired by the particle model interpretation of GANs, we leverage the discriminator gradient to analyze particle movement and the generator gradient, specifically \"steepness,\" to quantify the severity of mode mixture by measuring the generator's sensitivity to changes in the latent space. Using these theoretical insights into evolution of gradients, we design a specialized metric that integrates both gradients to detect the transition from fitting to collapse. This metric forms the basis of an early stopping algorithm, which stops training at a point that retains sample quality and diversity. Experiments on synthetic and real-world datasets, including MNIST, Fashion MNIST, and CIFAR-10, validate our theoretical findings and demonstrate the effectiveness of the proposed algorithm.",
        "timestamp": "2025-03-29T03:05:42.246Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "Transactions on Machine Learning Research",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-29T03:05:42+00:00",
        "updated_at": "2025-03-29T22:40:19+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2410.00301": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.00301",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-29T03:04:49.953Z",
            "data": {
              "session_id": "session_1743217489049_sgzmqc5",
              "source_id": "arxiv",
              "paper_id": "2410.00301",
              "start_time": "2025-03-29T03:04:27.066Z",
              "end_time": "2025-03-29T03:04:49.049Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 2,
              "total_elapsed_seconds": 22
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-29T03:04:50+00:00",
        "updated_at": "2025-03-29T22:40:20+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2410.00301": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.00301",
        "url": "https://arxiv.org/abs/2410.00301",
        "title": "Network Science in Psychology",
        "authors": "Sweet, Tracy, Wang, Selena",
        "abstract": "Social network analysis can answer research questions such as why or how individuals interact or form relationships and how those relationships impact other outcomes. Despite the breadth of methods available to address psychological research questions, social network analysis is not yet a standard practice in psychological research. To promote the use of social network analysis in psychological research, we present an overview of network methods, situating each method within the context of research studies and questions in psychology.",
        "timestamp": "2025-03-29T03:04:27.641Z",
        "rating": "novote",
        "publishedDate": "2024/10/01",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-29T03:04:28+00:00",
        "updated_at": "2025-03-29T22:40:20+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2409.17410": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2409.17410",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-29T03:10:03.985Z",
            "data": {
              "session_id": "session_1743217802125_s1wdaiv",
              "source_id": "arxiv",
              "paper_id": "2409.17410",
              "start_time": "2025-03-29T03:08:54.107Z",
              "end_time": "2025-03-29T03:10:02.125Z",
              "heartbeat_count": 13,
              "duration_seconds": 65,
              "idle_seconds": 3,
              "total_elapsed_seconds": 68
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-29T03:10:05+00:00",
        "updated_at": "2025-03-29T22:40:17+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2409.17410": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2409.17410",
        "url": "https://arxiv.org/abs/2409.17410",
        "title": "Copying style, Extracting value: Illustrators' Perception of AI Style Transfer and its Impact on Creative Labor",
        "authors": "Porquet, Julien, Wang, Sitong, Chilton, Lydia B.",
        "abstract": "Generative text-to-image models are disrupting the lives of creative professionals. Specifically, illustrators are threatened by models that claim to extract and reproduce their style. Yet, research on style transfer has rarely focused on their perspectives. We provided four illustrators with a model fine-tuned to their style and conducted semi-structured interviews about the model's successes, limitations, and potential uses. Evaluating their output, artists reported that style transfer successfully copies aesthetic fragments but is limited by content-style disentanglement and lacks the crucial emergent quality of their style. They also deemed the others' copies more successful. Understanding the results of style transfer as \"boundary objects,\" we analyze how they can simultaneously be considered unsuccessful by artists and poised to replace their work by others. We connect our findings to critical HCI frameworks, demonstrating that style transfer, rather than merely a Creativity Support Tool, should also be understood as a supply chain optimization one.",
        "timestamp": "2025-03-29T03:08:54.449Z",
        "rating": "novote",
        "publishedDate": "2024/09/25",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-29T03:08:54+00:00",
        "updated_at": "2025-03-29T22:40:18+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2404.02905": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.02905",
        "url": "https://arxiv.org/abs/2404.02905v2",
        "title": "Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction",
        "authors": "Tian, Keyu, Jiang, Yi, Yuan, Zehuan, Peng, Bingyue, Wang, Liwei",
        "abstract": "We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine \"next-scale prediction\" or \"next-resolution prediction\", diverging from the standard raster-scan \"next-token prediction\". This simple, intuitive methodology allows autoregressive (AR) transformers to learn visual distributions fast and generalize well: VAR, for the first time, makes GPT-like AR models surpass diffusion transformers in image generation. On ImageNet 256x256 benchmark, VAR significantly improve AR baseline by improving Frechet inception distance (FID) from 18.65 to 1.73, inception score (IS) from 80.4 to 350.2, with around 20x faster inference speed. It is also empirically verified that VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions including image quality, inference speed, data efficiency, and scalability. Scaling up VAR models exhibits clear power-law scaling laws similar to those observed in LLMs, with linear correlation coefficients near -0.998 as solid evidence. VAR further showcases zero-shot generalization ability in downstream tasks including image in-painting, out-painting, and editing. These results suggest VAR has initially emulated the two important properties of LLMs: Scaling Laws and zero-shot task generalization. We have released all models and codes to promote the exploration of AR/VAR models for visual generation and unified learning.",
        "timestamp": "2025-03-29T05:19:17.287Z",
        "rating": "novote",
        "publishedDate": "2024/04/03",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-29T05:19:17+00:00",
        "updated_at": "2025-03-29T22:40:16+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.19010": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.19010",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-29T05:17:24.413Z",
            "data": {
              "session_id": "session_1743225443230_lgf10n0",
              "source_id": "arxiv",
              "paper_id": "2503.19010",
              "start_time": "2025-03-29T05:16:17.376Z",
              "end_time": "2025-03-29T05:17:23.230Z",
              "heartbeat_count": 13,
              "duration_seconds": 65,
              "idle_seconds": 1,
              "total_elapsed_seconds": 66
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-29T05:17:25+00:00",
        "updated_at": "2025-03-29T22:40:16+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.19010": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.19010",
        "url": "https://arxiv.org/abs/2503.19010",
        "title": "The Forward Physics Facility at the Large Hadron Collider",
        "authors": "Anchordoqui, Luis A., Ariga, Akitaka, Ariga, Tomoko, Barr, Alan J., Batell, Brian, Bian, Jianming, Boyd, Jamie, Citron, Matthew, De Roeck, Albert, Diwan, Milind V., Feng, Jonathan L., Hill, Christopher S., Kling, Felix, Linden, Steven, M\u00e4kel\u00e4, Toni, Mavrokoridis, Kostas, McFayden, Josh, Otono, Hidetoshi, Rojo, Juan, Soldin, Dennis, Stasto, Anna, Trojanowski, Sebastian, Vicenzi, Matteo, Wu, Wenjie",
        "abstract": "The Forward Physics Facility (FPF) is a proposal developed to exploit the unique scientific potential made possible by the intense hadron beams produced in the far-forward direction at the high luminosity LHC (HL-LHC). Housed in a well-shielded cavern 627 m from the LHC interactions, the facility will enable a broad and deep scientific programme which will greatly extend the physics capability of the HL-LHC. Instrumented with a suite of four complementary detectors -- FLArE, FASER$\\nu$2, FASER2 and FORMOSA -- the FPF has unique potential to shed light on neutrino physics, QCD, astroparticle physics, and to search for dark matter and other new particles. This contribution describes some of the key scientific drivers for the facility, the engineering and technical studies that have been made in preparation for it, the design of its four complementary experiments, and the status of the project's partnerships and planning.",
        "timestamp": "2025-03-29T05:16:17.888Z",
        "rating": "novote",
        "publishedDate": "2025/03/24",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-29T05:16:18+00:00",
        "updated_at": "2025-03-29T22:40:17+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2502.21180": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.21180",
        "url": "https://arxiv.org/abs/2502.21180",
        "title": "$SO(10)$ theory on the plateau: the importance of being renormalizable",
        "authors": "Preda, Anca, Senjanovi\u0107, Goran, Zantedeschi, Michael",
        "abstract": "We revisit a minimal renormalisable $SO(10)$ grand unified theory, with the Higgs representation $45_{\\rm H}$, $126_{\\rm H}$ and complex $10_{\\rm H}$, responsible for the unification, intermediate and the weak scale symmetry breaking, respectively. We perform the study of unification constraints and find that it allows for the Left-Right symmetric scale to be accessible even at the LHC, and the Quark-Lepton unification scale as low as its phenomenological limit around $10^5\\,$GeV. Moreover, one can have neutron - anti neutron oscillations at the level of the present day sensibility in both of the above cases, while in the former case one can have simultaneously neutrinoless double beta decay induced by new light scalar states, reachable today - with both electrons emerging as left-handed, as in the neutrino exchange through its possible Majorana mass. We also discuss a recently raised issue of the fine-tuning of the light Higgs mass and its potential conflict with low intermediate mass scales.",
        "timestamp": "2025-03-29T05:13:26.336Z",
        "rating": "novote",
        "publishedDate": "2025/02/28",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-29T05:13:26+00:00",
        "updated_at": "2025-03-29T22:40:17+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2408.11039": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2408.11039",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-29T05:22:39.396Z",
            "data": {
              "session_id": "session_1743225759382_1mt9aw2",
              "source_id": "arxiv",
              "paper_id": "2408.11039",
              "start_time": "2025-03-29T05:22:30.156Z",
              "end_time": "2025-03-29T05:22:39.382Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T06:02:24.155Z",
            "data": {
              "session_id": "session_1743919343856_4g8617g",
              "source_id": "arxiv",
              "paper_id": "2408.11039",
              "start_time": "2025-04-06T06:02:16.777Z",
              "end_time": "2025-04-06T06:02:23.856Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2291,
        "object_id": "interactions:arxiv.2408.11039",
        "created_at": "2025-03-29T05:22:40+00:00",
        "updated_at": "2025-04-06T06:03:35+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2408.11039": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2408.11039",
        "url": "https://export.arxiv.org/abs/2408.11039",
        "title": "Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model",
        "authors": "Zhou, Chunting, Yu, Lili, Babu, Arun, Tirumala, Kushal, Yasunaga, Michihiro, Shamis, Leonid, Kahn, Jacob, Ma, Xuezhe, Zettlemoyer, Luke, Levy, Omer",
        "abstract": "",
        "timestamp": "2025-03-29T05:22:24.474Z",
        "rating": "novote",
        "publishedDate": "2024/08/20",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-29T05:22:24+00:00",
        "updated_at": "2025-03-29T22:40:15+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.21073": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.21073",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-29T05:30:26.216Z",
            "data": {
              "session_id": "session_1743226225323_vcmsvoz",
              "source_id": "arxiv",
              "paper_id": "2503.21073",
              "start_time": "2025-03-29T05:29:40.668Z",
              "end_time": "2025-03-29T05:30:25.323Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 5,
              "total_elapsed_seconds": 45
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-29T05:30:27+00:00",
        "updated_at": "2025-03-29T22:40:10+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.21073": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.21073",
        "url": "https://arxiv.org/abs/2503.21073",
        "title": "Shared Global and Local Geometry of Language Model Embeddings",
        "authors": "Lee, Andrew, Weber, Melanie, Vi\u00e9gas, Fernanda, Wattenberg, Martin",
        "abstract": "Researchers have recently suggested that models share common representations. In this work, we find that the token embeddings of language models exhibit common geometric structure. First, we find ``global'' similarities: token embeddings often share similar relative orientations. Next, we characterize local geometry in two ways: (1) by using Locally Linear Embeddings, and (2) by defining a simple measure for the intrinsic dimension of each token embedding. Our intrinsic dimension measure demonstrates that token embeddings lie on a lower dimensional manifold. We qualitatively show that tokens with lower intrinsic dimensions often have semantically coherent clusters, while those with higher intrinsic dimensions do not. Both characterizations allow us to find similarities in the local geometry of token embeddings. Perhaps most surprisingly, we find that alignment in token embeddings persists through the hidden states of language models, allowing us to develop an application for interpretability. Namely, we empirically demonstrate that steering vectors from one language model can be transferred to another, despite the two models having different dimensions.",
        "timestamp": "2025-03-29T05:29:40.305Z",
        "rating": "novote",
        "publishedDate": "2025/03/27",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-29T05:29:40+00:00",
        "updated_at": "2025-03-29T22:40:11+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2402.07717": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.07717",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-29T05:29:10.834Z",
            "data": {
              "session_id": "session_1743226150787_v7vjuyv",
              "source_id": "arxiv",
              "paper_id": "2402.07717",
              "start_time": "2025-03-29T05:29:03.957Z",
              "end_time": "2025-03-29T05:29:10.787Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-29T05:29:11+00:00",
        "updated_at": "2025-03-29T22:40:12+00:00",
        "version": 3
      }
    },
    "interactions:url.3EF6523C": {
      "data": {
        "sourceId": "url",
        "paperId": "3EF6523C",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-29T05:29:04.637Z",
            "data": {
              "session_id": "session_1743226143933_8e1r2hs",
              "source_id": "url",
              "paper_id": "3EF6523C",
              "start_time": "2025-03-29T05:28:37.378Z",
              "end_time": "2025-03-29T05:29:03.933Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 2,
              "total_elapsed_seconds": 27
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-29T05:29:05+00:00",
        "updated_at": "2025-03-29T22:40:12+00:00",
        "version": 3
      }
    },
    "paper:url.3EF6523C": {
      "data": {
        "sourceId": "url",
        "paperId": "3EF6523C",
        "url": "https://www.researchgate.net/publication/224260399_Shannon_meets_Blackwell_and_Le_Cam_Channels_codes_and_statistical_experiments",
        "title": "Shannon meets Blackwell and Le Cam: Channels, codes, and statistical experiments",
        "authors": "",
        "abstract": "Download Citation | Shannon meets Blackwell and Le Cam: Channels, codes, and statistical experiments | The Blackwell-Le Cam decision theory provides an approximation framework for statistical experiments in terms of expected risks of optimal... | Find, read and cite all the research you need on ResearchGate",
        "timestamp": "2025-03-29T05:28:35.695Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-29T05:28:36+00:00",
        "updated_at": "2025-03-29T22:40:13+00:00",
        "version": 2
      }
    },
    "interactions:url.18CB21B": {
      "data": {
        "sourceId": "url",
        "paperId": "18CB21B",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-29T05:27:53.694Z",
            "data": {
              "session_id": "session_1743226073007_qqwuwbu",
              "source_id": "url",
              "paper_id": "18CB21B",
              "start_time": "2025-03-29T05:27:42.484Z",
              "end_time": "2025-03-29T05:27:53.007Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-29T05:27:54+00:00",
        "updated_at": "2025-03-29T22:40:13+00:00",
        "version": 3
      }
    },
    "paper:url.18CB21B": {
      "data": {
        "sourceId": "url",
        "paperId": "18CB21B",
        "url": "https://maxim.ece.illinois.edu/pubs/raginsky_ISIT11.pdf",
        "title": "18CB21B",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-29T05:27:42.839Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "created_at": "2025-03-29T05:27:43+00:00",
        "updated_at": "2025-03-29T22:40:14+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2402.07717": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.07717",
        "url": "https://arxiv.org/abs/2402.07717",
        "title": "Computationally efficient reductions between some statistical models",
        "authors": "Lou, Mengqi, Bresler, Guy, Pananjady, Ashwin",
        "abstract": "We study the problem of approximately transforming a sample from a source statistical model to a sample from a target statistical model without knowing the parameters of the source model, and construct several computationally efficient such reductions between canonical statistical experiments. In particular, we provide computationally efficient procedures that approximately reduce uniform, Erlang, and Laplace location models to general target families. We illustrate our methodology by establishing nonasymptotic reductions between some canonical high-dimensional problems, spanning mixtures of experts, phase retrieval, and signal denoising. Notably, the reductions are structure-preserving and can accommodate missing data. We also point to a possible application in transforming one differentially private mechanism to another.",
        "timestamp": "2025-03-29T05:27:31.383Z",
        "rating": "novote",
        "publishedDate": "2024/02/12",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-29T05:27:31+00:00",
        "updated_at": "2025-03-29T22:40:14+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2203.13556": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2203.13556",
        "url": "https://arxiv.org/pdf/2203.13556",
        "title": "2203.13556",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-29T05:45:19.191Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-29T05:45:19+00:00",
        "updated_at": "2025-03-29T22:40:09+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2411.00734": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.00734",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-29T05:43:39.513Z",
            "data": {
              "session_id": "session_1743227019489_e3jgbix",
              "source_id": "arxiv",
              "paper_id": "2411.00734",
              "start_time": "2025-03-29T05:43:29.346Z",
              "end_time": "2025-03-29T05:43:39.489Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 0,
              "total_elapsed_seconds": 10
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-29T05:43:40+00:00",
        "updated_at": "2025-03-29T22:40:09+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2411.00734": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.00734",
        "url": "https://arxiv.org/pdf/2411.00734",
        "title": "2411.00734",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-29T05:43:05.649Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-29T05:43:06+00:00",
        "updated_at": "2025-03-29T22:40:10+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2104.09732": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2104.09732",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-29T05:57:22.150Z",
            "data": {
              "session_id": "session_1743227842142_3p8wlfj",
              "source_id": "arxiv",
              "paper_id": "2104.09732",
              "start_time": "2025-03-29T05:57:15.576Z",
              "end_time": "2025-03-29T05:57:22.142Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-29T05:57:23+00:00",
        "updated_at": "2025-03-29T22:40:07+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2104.09732": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2104.09732",
        "url": "https://arxiv.org/pdf/2104.09732",
        "title": "2104.09732",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-29T05:57:11.525Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-29T05:57:12+00:00",
        "updated_at": "2025-03-29T22:40:07+00:00",
        "version": 2
      }
    },
    "paper:url.52964906": {
      "data": {
        "sourceId": "url",
        "paperId": "52964906",
        "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/7ede97c3e082c6df10a8d6103a2eebd2-Abstract-Conference.html",
        "title": "FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision",
        "authors": "Shah, Jay, Bikshandi, Ganesh, Zhang, Ying, Thakkar, Vijay, Ramani, Pradeep, Dao, Tri",
        "abstract": "",
        "timestamp": "2025-03-29T05:48:50.566Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "Advances in Neural Information Processing Systems",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-29T05:48:51+00:00",
        "updated_at": "2025-03-29T22:40:08+00:00",
        "version": 2
      }
    },
    "interactions:url.47975D68": {
      "data": {
        "sourceId": "url",
        "paperId": "47975D68",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-29T13:47:48.506Z",
            "data": {
              "session_id": "session_1743256067576_hxihp7e",
              "source_id": "url",
              "paper_id": "47975D68",
              "start_time": "2025-03-29T13:46:26.805Z",
              "end_time": "2025-03-29T13:47:47.576Z",
              "heartbeat_count": 16,
              "duration_seconds": 80,
              "idle_seconds": 1,
              "total_elapsed_seconds": 81
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-29T13:47:49+00:00",
        "updated_at": "2025-03-29T22:40:04+00:00",
        "version": 3
      }
    },
    "paper:url.47975D68": {
      "data": {
        "sourceId": "url",
        "paperId": "47975D68",
        "url": "https://transformer-circuits.pub/2025/attribution-graphs/methods.html",
        "title": "Circuit Tracing: Revealing Computational Graphs in Language Models",
        "authors": "",
        "abstract": "We describe an approach to tracing the ",
        "timestamp": "2025-03-29T13:46:26.436Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-29T13:46:26+00:00",
        "updated_at": "2025-03-29T22:40:05+00:00",
        "version": 2
      }
    },
    "interactions:url.7B1A593D": {
      "data": {
        "sourceId": "url",
        "paperId": "7B1A593D",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-29T13:46:14.614Z",
            "data": {
              "session_id": "session_1743255973674_xq334kt",
              "source_id": "url",
              "paper_id": "7B1A593D",
              "start_time": "2025-03-29T13:44:49.840Z",
              "end_time": "2025-03-29T13:46:13.674Z",
              "heartbeat_count": 16,
              "duration_seconds": 80,
              "idle_seconds": 4,
              "total_elapsed_seconds": 84
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-29T13:46:15+00:00",
        "updated_at": "2025-03-29T22:40:05+00:00",
        "version": 3
      }
    },
    "paper:url.7B1A593D": {
      "data": {
        "sourceId": "url",
        "paperId": "7B1A593D",
        "url": "https://transformer-circuits.pub/2025/attribution-graphs/biology.html",
        "title": "On the Biology of a Large Language Model",
        "authors": "",
        "abstract": "We investigate the internal mechanisms used by Claude 3.5 Haiku \u2014 Anthropic's lightweight production model \u2014 in a variety of contexts, using our circuit tracing methodology.",
        "timestamp": "2025-03-29T13:44:49.367Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-29T13:44:49+00:00",
        "updated_at": "2025-03-29T22:40:06+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.15776": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.15776",
        "url": "https://arxiv.org/pdf/2503.15776",
        "title": "2503.15776",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-29T08:11:28.334Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-29T08:11:28+00:00",
        "updated_at": "2025-03-29T22:40:06+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.16611": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.16611",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-29T14:48:57.978Z",
            "data": {
              "session_id": "session_1743259737958_azlr3o7",
              "source_id": "arxiv",
              "paper_id": "2503.16611",
              "start_time": "2025-03-29T14:48:50.625Z",
              "end_time": "2025-03-29T14:48:57.958Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-29T14:48:52+00:00",
        "updated_at": "2025-03-29T22:40:03+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2503.16611": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.16611",
        "url": "https://arxiv.org/html/2503.16611v1",
        "title": "A Recipe for Generating 3D Worlds From a Single Image",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-29T14:48:15.589Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-29T14:48:16+00:00",
        "updated_at": "2025-03-29T22:40:04+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2502.18781": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.18781",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-29T14:51:39.202Z",
            "data": {
              "session_id": "session_1743259898420_hxsdn7e",
              "source_id": "arxiv",
              "paper_id": "2502.18781",
              "start_time": "2025-03-29T14:51:28.664Z",
              "end_time": "2025-03-29T14:51:38.420Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 5,
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-29T16:07:37.908Z",
            "data": {
              "session_id": "session_1743264457545_frleno5",
              "source_id": "arxiv",
              "paper_id": "2502.18781",
              "start_time": "2025-03-29T16:07:29.590Z",
              "end_time": "2025-03-29T16:07:37.545Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-29T14:51:40+00:00",
        "updated_at": "2025-03-29T22:40:02+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2502.18781": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.18781",
        "url": "https://arxiv.org/abs/2502.18781",
        "title": "The distribution of galaxy rotation in JWST Advanced Deep Extragalactic Survey",
        "authors": "Shamir, Lior",
        "abstract": "JWST provides a view of the Universe never seen before, and specifically fine details of galaxies in deep space. JWST Advanced Deep Extragalactic Survey (JADES) is a deep field survey, providing unprecedentedly detailed view of galaxies in the early Universe. The field is also in relatively close proximity to the Galactic pole. Analysis of spiral galaxies by their direction of rotation in JADES shows that the number of galaxies in that field that rotate in the opposite direction relative to the Milky Way galaxy is ~50% higher than the number of galaxies that rotate in the same direction relative to the Milky Way. The analysis is done using a computer-aided quantitative method, but the difference is so extreme that it can be noticed and inspected even by the unaided human eye. These observations are in excellent agreement with deep fields taken at around the same footprint by HST and JWST. The reason for the difference may be related to the structure of the early Universe, but it can also be related to the physics of galaxy rotation and the internal structure of galaxies. In that case the observation can provide possible explanations to other puzzling anomalies such as the Ho tension and the observation of massive mature galaxies at very high redshifts.",
        "timestamp": "2025-03-29T14:51:29.101Z",
        "rating": "novote",
        "publishedDate": "2025/02/26",
        "tags": [],
        "doi": "10.1093/mnras/staf292",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-29T14:51:29+00:00",
        "updated_at": "2025-03-29T22:40:03+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2503.11990": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.11990",
        "url": "https://arxiv.org/abs/2503.11990",
        "title": "Testing Stochastic Block Models Based on Maximum Sampling Entry-Wise Deviations",
        "authors": "Wu, Yujia, Lan, Wei, Feng, Long, Tsai, Chih-Ling",
        "abstract": "The stochastic block model (SBM) has been widely used to analyze network data. Various goodness-of-fit tests have been proposed to assess the adequacy of model structures. To the best of our knowledge, however, none of the existing approaches are applicable for sparse networks in which the connection probability of any two communities is of order log n/n, and the number of communities is divergent. To fill this gap, we propose a novel goodness-of-fit test for the stochastic block model. The key idea is to construct statistics by sampling the maximum entry-deviations of the adjacency matrix that the negative impacts of network sparsity are alleviated by the sampling process. We demonstrate theoretically that the proposed test statistic converges to the Type-I extreme value distribution under the null hypothesis regardless of the network structure. Accordingly, it can be applied to both dense and sparse networks. In addition, we obtain the asymptotic power against alternatives. Moreover, we introduce a bootstrap-corrected test statistic to improve the finite sample performance, recommend an augmented test statistic to increase the power, and extend the proposed test to the degree-corrected SBM. Simulation studies and two empirical examples with both dense and sparse networks indicate that the proposed method performs well.",
        "timestamp": "2025-03-29T19:11:33.206Z",
        "rating": "novote",
        "publishedDate": "2025/03/15",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-29T19:11:33+00:00",
        "updated_at": "2025-03-29T22:40:02+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.21774": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.21774",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-30T00:10:11.492Z",
            "data": {
              "session_id": "session_1743293411479_ly79b3a",
              "source_id": "arxiv",
              "paper_id": "2503.21774",
              "start_time": "2025-03-30T00:10:03.240Z",
              "end_time": "2025-03-30T00:10:11.479Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-30T00:14:26.433Z",
            "data": {
              "session_id": "session_1743293665869_4dhwj8o",
              "source_id": "arxiv",
              "paper_id": "2503.21774",
              "start_time": "2025-03-30T00:11:40.527Z",
              "end_time": "2025-03-30T00:14:25.869Z",
              "heartbeat_count": 33,
              "duration_seconds": 165,
              "idle_seconds": 0,
              "total_elapsed_seconds": 165
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-30T00:09:44+00:00",
        "updated_at": "2025-03-30T07:13:13+00:00",
        "version": 6
      }
    },
    "paper:arxiv.2503.21774": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.21774",
        "url": "https://arxiv.org/abs/2503.21774",
        "title": "Optimal Stepsize for Diffusion Sampling",
        "authors": "Pei, Jianning, Hu, Han, Gu, Shuyang",
        "abstract": "Diffusion models achieve remarkable generation quality but suffer from computational intensive sampling due to suboptimal step discretization. While existing works focus on optimizing denoising directions, we address the principled design of stepsize schedules. This paper proposes Optimal Stepsize Distillation, a dynamic programming framework that extracts theoretically optimal schedules by distilling knowledge from reference trajectories. By reformulating stepsize optimization as recursive error minimization, our method guarantees global discretization bounds through optimal substructure exploitation. Crucially, the distilled schedules demonstrate strong robustness across architectures, ODE solvers, and noise schedules. Experiments show 10x accelerated text-to-image generation while preserving 99.4% performance on GenEval. Our code is available at https://github.com/bebebe666/OptimalSteps.",
        "timestamp": "2025-03-30T00:09:32.698Z",
        "rating": "novote",
        "publishedDate": "2025/03/27",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T00:09:33+00:00",
        "updated_at": "2025-03-30T07:13:13+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2006.15191": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2006.15191",
        "url": "https://arxiv.org/abs/2006.15191",
        "title": "Is SGD a Bayesian sampler? Well, almost",
        "authors": "Mingard, Chris, Valle-P\u00e9rez, Guillermo, Skalse, Joar, Louis, Ard A.",
        "abstract": "Overparameterised deep neural networks (DNNs) are highly expressive and so can, in principle, generate almost any function that fits a training dataset with zero error. The vast majority of these functions will perform poorly on unseen data, and yet in practice DNNs often generalise remarkably well. This success suggests that a trained DNN must have a strong inductive bias towards functions with low generalisation error. Here we empirically investigate this inductive bias by calculating, for a range of architectures and datasets, the probability $P_{SGD}(f\\mid S)$ that an overparameterised DNN, trained with stochastic gradient descent (SGD) or one of its variants, converges on a function $f$ consistent with a training set $S$. We also use Gaussian processes to estimate the Bayesian posterior probability $P_B(f\\mid S)$ that the DNN expresses $f$ upon random sampling of its parameters, conditioned on $S$. Our main findings are that $P_{SGD}(f\\mid S)$ correlates remarkably well with $P_B(f\\mid S)$ and that $P_B(f\\mid S)$ is strongly biased towards low-error and low complexity functions. These results imply that strong inductive bias in the parameter-function map (which determines $P_B(f\\mid S)$), rather than a special property of SGD, is the primary explanation for why DNNs generalise so well in the overparameterised regime. While our results suggest that the Bayesian posterior $P_B(f\\mid S)$ is the first order determinant of $P_{SGD}(f\\mid S)$, there remain second order differences that are sensitive to hyperparameter tuning. A function probability picture, based on $P_{SGD}(f\\mid S)$ and/or $P_B(f\\mid S)$, can shed new light on the way that variations in architecture or hyperparameter settings such as batch size, learning rate, and optimiser choice, affect DNN performance.",
        "timestamp": "2025-03-30T02:05:10.208Z",
        "rating": "novote",
        "publishedDate": "2020/06/26",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T02:05:10+00:00",
        "updated_at": "2025-03-30T07:13:14+00:00",
        "version": 2
      }
    },
    "paper:url.6191417C": {
      "data": {
        "sourceId": "url",
        "paperId": "6191417C",
        "url": "https://openreview.net/forum?id=UV58hNygne",
        "title": "HoSNNs: Adversarially-Robust Homeostatic Spiking Neural Networks with Adaptive Firing Thresholds",
        "authors": "Hejia Geng, Peng Li",
        "abstract": "While spiking neural networks (SNNs) offer a promising  neurally-inspired model of computation, they are vulnerable to  adversarial attacks. We present the first study that draws inspiration from neural homeostasis to design a  threshold-adapting leaky integrate-and-fire (TA-LIF) neuron model and utilize TA-LIF neurons  to construct the adversarially robust homeostatic SNNs (HoSNNs) for improved robustness. The TA-LIF model incorporates a self-stabilizing dynamic thresholding mechanism, offering a local feedback control solution to the minimization of each neuron's membrane potential error caused by adversarial disturbance.   Theoretical analysis demonstrates favorable dynamic properties of TA-LIF neurons in terms of  the bounded-input bounded-output stability and suppressed time growth of membrane potential error, underscoring their superior  robustness compared with the standard LIF neurons. When trained with weak FGSM attacks (\\(\\epsilon = 2/255\\)), our HoSNNs significantly outperform conventionally trained LIF-based SNNs across multiple datasets. Furthermore, under significantly stronger PGD7 attacks (\\(\\epsilon = 8/255\\)), HoSNN achieves notable improvements in accuracy, increasing from 30.90% to 74.91% on FashionMNIST, 0.44% to 36.82% on SVHN, 0.54% to 43.33% on CIFAR10, and 0.04% to 16.66% on CIFAR100.",
        "timestamp": "2025-03-30T01:44:41.206Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "Transactions on Machine Learning Research",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T01:44:41+00:00",
        "updated_at": "2025-03-30T07:13:13+00:00",
        "version": 2
      }
    },
    "paper:url.2578F8B9": {
      "data": {
        "sourceId": "url",
        "paperId": "2578F8B9",
        "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=rdznfHEAAAAJ&sortby=pubdate&citation_for_view=rdznfHEAAAAJ:-f6ydRqryjwC",
        "title": "\u202aThe applied epistemology of conspiracy theories: An overview\u202c",
        "authors": "",
        "abstract": "\u202aMRX Dentith, BL Keeley\u202c, \u202aThe routledge handbook of applied epistemology, 2018\u202c - \u202aCited by 37\u202c",
        "timestamp": "2025-03-30T17:15:22.377Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T17:15:22+00:00",
        "updated_at": "2025-03-30T17:24:18+00:00",
        "version": 2
      }
    },
    "interactions:url.563CB721": {
      "data": {
        "sourceId": "url",
        "paperId": "563CB721",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-30T17:14:59.384Z",
            "data": {
              "session_id": "session_1743354898461_aplu3wo",
              "source_id": "url",
              "paper_id": "563CB721",
              "start_time": "2025-03-30T17:12:37.523Z",
              "end_time": "2025-03-30T17:14:58.461Z",
              "heartbeat_count": 28,
              "duration_seconds": 140,
              "idle_seconds": 1,
              "total_elapsed_seconds": 141
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-30T17:15:00+00:00",
        "updated_at": "2025-03-30T17:24:17+00:00",
        "version": 3
      }
    },
    "paper:url.563CB721": {
      "data": {
        "sourceId": "url",
        "paperId": "563CB721",
        "url": "https://philarchive.org/archive/DESSDO-4",
        "title": "563CB721",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-30T17:12:37.979Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T17:12:38+00:00",
        "updated_at": "2025-03-30T17:24:17+00:00",
        "version": 2
      }
    },
    "paper:url.9508B2A": {
      "data": {
        "sourceId": "url",
        "paperId": "9508B2A",
        "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=K9aFDbMAAAAJ&citation_for_view=K9aFDbMAAAAJ:dhFuZR0502QC",
        "title": "\u202aStatus distrust of scientific experts\u202c",
        "authors": "",
        "abstract": "\u202aH Desmond\u202c, \u202aSocial Epistemology, 2022\u202c - \u202aCited by 16\u202c",
        "timestamp": "2025-03-30T17:12:30.561Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T17:12:31+00:00",
        "updated_at": "2025-03-30T17:24:17+00:00",
        "version": 2
      }
    },
    "interactions:url.27EAF32": {
      "data": {
        "sourceId": "url",
        "paperId": "27EAF32",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-30T17:10:11.273Z",
            "data": {
              "session_id": "session_1743354610468_jrzgm7a",
              "source_id": "url",
              "paper_id": "27EAF32",
              "start_time": "2025-03-30T17:10:00.652Z",
              "end_time": "2025-03-30T17:10:10.468Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 5,
              "total_elapsed_seconds": 10
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-30T17:10:12+00:00",
        "updated_at": "2025-03-30T17:24:16+00:00",
        "version": 3
      }
    },
    "paper:url.27EAF32": {
      "data": {
        "sourceId": "url",
        "paperId": "27EAF32",
        "url": "https://philpapers.org/rec/KEECTA-3",
        "title": "Conspiracy Theories and Public Trust",
        "authors": "Brian L. Keeley",
        "abstract": "What is the relationship between belief in (or other forms of engagement with) conspiratorial thinking and trust? To what extent does engagement with conspiracy theories lead to an erosion of trust ...",
        "timestamp": "2025-03-30T17:10:00.390Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T17:10:00+00:00",
        "updated_at": "2025-03-30T17:24:16+00:00",
        "version": 2
      }
    },
    "paper:url.E52BD3": {
      "data": {
        "sourceId": "url",
        "paperId": "E52BD3",
        "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=rdznfHEAAAAJ&sortby=pubdate&citation_for_view=rdznfHEAAAAJ:lSLTfruPkqcC",
        "title": "\u202aConspiracy Theories and Public Trust\u202c",
        "authors": "",
        "abstract": "\u202aBL Keeley, 2023\u202c - \u202aCited by 1\u202c",
        "timestamp": "2025-03-30T17:09:47.328Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T17:09:47+00:00",
        "updated_at": "2025-03-30T17:24:15+00:00",
        "version": 2
      }
    },
    "interactions:url.733F9369": {
      "data": {
        "sourceId": "url",
        "paperId": "733F9369",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-30T17:07:28.438Z",
            "data": {
              "session_id": "session_1743354447520_tw3dnjp",
              "source_id": "url",
              "paper_id": "733F9369",
              "start_time": "2025-03-30T17:07:19.413Z",
              "end_time": "2025-03-30T17:07:27.520Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-30T17:07:29+00:00",
        "updated_at": "2025-03-30T17:24:15+00:00",
        "version": 3
      }
    },
    "paper:url.733F9369": {
      "data": {
        "sourceId": "url",
        "paperId": "733F9369",
        "url": "https://muse.jhu.edu/pub/63/article/228147/summary",
        "title": "God as the Ultimate Conspiracy Theory",
        "authors": "Brian L. Keeley",
        "abstract": "<p>Traditional secular conspiracy theories and explanations of worldly events in terms of supernatural agency share interesting epistemic features. This paper explores what can be called \"supernatural conspiracy theories,\" by considering such supernatural explanations through the lens of recent work on the epistemology of secular conspiracy theories. After considering the similarities and the differences between the two types of theories, the prospects for agnosticism both with respect to secular conspiracy theories and the existence of God are then considered. Arguments regarding secular conspiracy theories suggest ways to defend agnosticism with respect to God from arguments that agnosticism is not a logically stable position and that it ultimately collapses into atheism, as has been argued by N. Russell Hanson and others. I conclude that such attacks on religious agnosticism fail to appreciate the conspiratorial features of God's alleged role in the universe. \t</p>",
        "timestamp": "2025-03-30T17:07:17.632Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "Episteme: A Journal of Social Epistemology",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T17:07:18+00:00",
        "updated_at": "2025-03-30T17:24:15+00:00",
        "version": 2
      }
    },
    "interactions:url.2497DAC7": {
      "data": {
        "sourceId": "url",
        "paperId": "2497DAC7",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-30T17:02:19.224Z",
            "data": {
              "session_id": "session_1743354138378_r2lbhgz",
              "source_id": "url",
              "paper_id": "2497DAC7",
              "start_time": "2025-03-30T17:02:12.078Z",
              "end_time": "2025-03-30T17:02:18.378Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-30T17:02:20+00:00",
        "updated_at": "2025-03-30T17:24:14+00:00",
        "version": 3
      }
    },
    "paper:url.2497DAC7": {
      "data": {
        "sourceId": "url",
        "paperId": "2497DAC7",
        "url": "https://www.tandfonline.com/doi/abs/10.1080/0020174X.2024.2375771",
        "title": "Conspiracy theorists are not the problem; Conspiracy liars are",
        "authors": "",
        "abstract": "In an opinion piece in the Los Angeles Times (08/06/2022), entitled Alex Jones is no kind of \u2018theorist\u2019, LZ Granderson writes that although the ubiquitous recent \u2018conspiracy theorist\u2019 of American j...",
        "timestamp": "2025-03-30T17:02:09.231Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "Conspiracy theories",
          "lies",
          "bullshit",
          "scientific explanation"
        ],
        "doi": "",
        "journalName": "Inquiry",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T17:02:09+00:00",
        "updated_at": "2025-03-30T17:24:14+00:00",
        "version": 2
      }
    },
    "interactions:url.4D5F5775": {
      "data": {
        "sourceId": "url",
        "paperId": "4D5F5775",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-30T17:00:56.262Z",
            "data": {
              "session_id": "session_1743354055207_vyezl25",
              "source_id": "url",
              "paper_id": "4D5F5775",
              "start_time": "2025-03-30T16:59:11.679Z",
              "end_time": "2025-03-30T17:00:55.207Z",
              "heartbeat_count": 20,
              "duration_seconds": 100,
              "idle_seconds": 4,
              "total_elapsed_seconds": 104
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-30T17:00:57+00:00",
        "updated_at": "2025-03-30T17:24:13+00:00",
        "version": 3
      }
    },
    "paper:url.4D5F5775": {
      "data": {
        "sourceId": "url",
        "paperId": "4D5F5775",
        "url": "https://web.archive.org/web/20050414075924id_/http://mugwump.pitzer.edu:80/~bkeeley/work/PUBS/ct/CONSPIR.PDF",
        "title": "4D5F5775",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-30T16:59:12.205Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "created_at": "2025-03-30T16:59:12+00:00",
        "updated_at": "2025-03-30T17:24:13+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.19058": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.19058",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-30T16:45:42.211Z",
            "data": {
              "session_id": "session_1743353141373_915y5lt",
              "source_id": "arxiv",
              "paper_id": "2503.19058",
              "start_time": "2025-03-30T16:45:30.041Z",
              "end_time": "2025-03-30T16:45:41.373Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-30T16:45:43+00:00",
        "updated_at": "2025-03-30T17:24:13+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.19058": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.19058",
        "url": "https://arxiv.org/abs/2503.19058",
        "title": "Coding Malware in Fancy Programming Languages for Fun and Profit",
        "authors": "Apostolopoulos, Theodoros, Koutsokostas, Vasilios, Totosis, Nikolaos, Patsakis, Constantinos, Smaragdakis, Georgios",
        "abstract": "The continuous increase in malware samples, both in sophistication and number, presents many challenges for organizations and analysts, who must cope with thousands of new heterogeneous samples daily. This requires robust methods to quickly determine whether a file is malicious. Due to its speed and efficiency, static analysis is the first line of defense. In this work, we illustrate how the practical state-of-the-art methods used by antivirus solutions may fail to detect evident malware traces. The reason is that they highly depend on very strict signatures where minor deviations prevent them from detecting shellcodes that otherwise would immediately be flagged as malicious. Thus, our findings illustrate that malware authors may drastically decrease the detections by converting the code base to less-used programming languages. To this end, we study the features that such programming languages introduce in executables and the practical issues that arise for practitioners to detect malicious activity.",
        "timestamp": "2025-03-30T16:45:30.666Z",
        "rating": "novote",
        "publishedDate": "2025/03/24",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T16:45:31+00:00",
        "updated_at": "2025-03-30T17:24:12+00:00",
        "version": 2
      }
    },
    "paper:url.12FA1C92": {
      "data": {
        "sourceId": "url",
        "paperId": "12FA1C92",
        "url": "https://joss.theoj.org/papers/10.21105/joss.07668",
        "title": "Mesa 3: Agent-based modeling with Python in 2025",
        "authors": "Ewout ter Hoeven, Jan Kwakkel, Vincent Hess, Thomas Pike, Boyu Wang, Rht, Jackie Kazil",
        "abstract": "ter Hoeven et al., (2025). Mesa 3: Agent-based modeling with Python in 2025. Journal of Open Source Software, 10(107), 7668, https://doi.org/10.21105/joss.07668",
        "timestamp": "2025-03-30T16:24:15.979Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.21105/joss.07668",
        "journalName": "Journal of Open Source Software",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T16:24:16+00:00",
        "updated_at": "2025-03-30T17:24:12+00:00",
        "version": 2
      }
    },
    "interactions:url.417CBFA2": {
      "data": {
        "sourceId": "url",
        "paperId": "417CBFA2",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-30T16:20:44.280Z",
            "data": {
              "session_id": "session_1743351643400_v6x6za0",
              "source_id": "url",
              "paper_id": "417CBFA2",
              "start_time": "2025-03-30T16:20:12.188Z",
              "end_time": "2025-03-30T16:20:43.400Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 1,
              "total_elapsed_seconds": 31
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-30T16:20:45+00:00",
        "updated_at": "2025-03-30T17:24:11+00:00",
        "version": 3
      }
    },
    "paper:url.417CBFA2": {
      "data": {
        "sourceId": "url",
        "paperId": "417CBFA2",
        "url": "http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf",
        "title": "417CBFA2",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-30T16:20:12.711Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "created_at": "2025-03-30T16:20:13+00:00",
        "updated_at": "2025-03-30T17:24:11+00:00",
        "version": 2
      }
    },
    "paper:url.CF39E38": {
      "data": {
        "sourceId": "url",
        "paperId": "CF39E38",
        "url": "https://link.springer.com/chapter/10.1007/978-1-4899-0718-9_37",
        "title": "Every Good Regulator of a System Must Be a Model of That System",
        "authors": "Conant, Roger C., Ashby, W. Ross",
        "abstract": "Today, as a step towards the control of complex dynamic systems, models are being used ubiquitously. Being modelled, for instance, are the air traffic flows around New York, the endocrine balances of the pregnant sheep, and the flows of money among the banking centres.",
        "timestamp": "2025-03-30T16:19:57.335Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1007/978-1-4899-0718-9_37",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T16:19:57+00:00",
        "updated_at": "2025-03-30T17:24:11+00:00",
        "version": 2
      }
    },
    "interactions:url.77BA8972": {
      "data": {
        "sourceId": "url",
        "paperId": "77BA8972",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-30T16:19:07.421Z",
            "data": {
              "session_id": "session_1743351546563_ppj8ba6",
              "source_id": "url",
              "paper_id": "77BA8972",
              "start_time": "2025-03-30T16:17:11.612Z",
              "end_time": "2025-03-30T16:19:06.563Z",
              "heartbeat_count": 22,
              "duration_seconds": 110,
              "idle_seconds": 5,
              "total_elapsed_seconds": 115
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-30T16:19:08+00:00",
        "updated_at": "2025-03-30T17:24:10+00:00",
        "version": 3
      }
    },
    "paper:url.77BA8972": {
      "data": {
        "sourceId": "url",
        "paperId": "77BA8972",
        "url": "https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.14.3.137",
        "title": "77BA8972",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-30T16:17:12.019Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T16:17:12+00:00",
        "updated_at": "2025-03-30T17:24:10+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2503.16581": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.16581",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-30T15:49:20.895Z",
            "data": {
              "session_id": "session_1743349760019_9raklw3",
              "source_id": "arxiv",
              "paper_id": "2503.16581",
              "start_time": "2025-03-30T15:48:12.632Z",
              "end_time": "2025-03-30T15:49:20.019Z",
              "heartbeat_count": 13,
              "duration_seconds": 65,
              "idle_seconds": 2,
              "total_elapsed_seconds": 67
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-30T15:49:21+00:00",
        "updated_at": "2025-03-30T17:24:09+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2503.16581": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.16581",
        "url": "https://arxiv.org/html/2503.16581v1",
        "title": "Investigating Retrieval-Augmented Generation in Quranic Studies: A Study of 13 Open-Source Large Language Models",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-30T15:48:13.154Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "Large-Language-Models",
          "Retrieval-Augmented Generation",
          "Question Answering",
          "Quranic Studies",
          "Islamic Teachings"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T15:48:13+00:00",
        "updated_at": "2025-03-30T17:24:09+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2502.13047": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.13047",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-30T15:41:55.382Z",
            "data": {
              "session_id": "session_1743349314560_evs5y8y",
              "source_id": "arxiv",
              "paper_id": "2502.13047",
              "start_time": "2025-03-30T15:41:23.676Z",
              "end_time": "2025-03-30T15:41:54.560Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 1,
              "total_elapsed_seconds": 31
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-30T15:41:56+00:00",
        "updated_at": "2025-03-30T17:24:09+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2502.13047": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.13047",
        "url": "https://arxiv.org/abs/2502.13047",
        "title": "Development of systematic uncertainty-aware neural network trainings for binned-likelihood analyses at the LHC",
        "authors": "CMS Collaboration",
        "abstract": "We propose a neural network training method capable of accounting for the effects of systematic variations of the data model in the training process and describe its extension towards neural network multiclass classification. The procedure is evaluated on the realistic case of the measurement of Higgs boson production via gluon fusion and vector boson fusion in the $\\tau\\tau$ decay channel at the CMS experiment. The neural network output functions are used to infer the signal strengths for inclusive production of Higgs bosons as well as for their production via gluon fusion and vector boson fusion. We observe improvements of 12 and 16% in the uncertainty in the signal strengths for gluon and vector-boson fusion, respectively, compared with a conventional neural network training based on cross-entropy.",
        "timestamp": "2025-03-30T15:41:24.075Z",
        "rating": "novote",
        "publishedDate": "2025/02/18",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T15:41:24+00:00",
        "updated_at": "2025-03-30T17:25:42+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2311.16863": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2311.16863",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-30T15:38:27.684Z",
            "data": {
              "session_id": "session_1743349107664_z1fa5s9",
              "source_id": "arxiv",
              "paper_id": "2311.16863",
              "start_time": "2025-03-30T15:38:02.230Z",
              "end_time": "2025-03-30T15:38:27.664Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 0,
              "total_elapsed_seconds": 25
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-30T15:38:03+00:00",
        "updated_at": "2025-03-30T17:25:41+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2311.16863": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2311.16863",
        "url": "https://arxiv.org/pdf/2311.16863",
        "title": "2311.16863",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-03-30T15:37:10.002Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T15:37:10+00:00",
        "updated_at": "2025-03-30T17:25:41+00:00",
        "version": 2
      }
    },
    "interactions:url.48DD6CB4": {
      "data": {
        "sourceId": "url",
        "paperId": "48DD6CB4",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-30T15:33:24.233Z",
            "data": {
              "session_id": "session_1743348803425_vx3jjhg",
              "source_id": "url",
              "paper_id": "48DD6CB4",
              "start_time": "2025-03-30T15:31:23.985Z",
              "end_time": "2025-03-30T15:33:23.425Z",
              "heartbeat_count": 23,
              "duration_seconds": 115,
              "idle_seconds": 4,
              "total_elapsed_seconds": 119
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-30T15:33:25+00:00",
        "updated_at": "2025-03-30T17:25:40+00:00",
        "version": 3
      }
    },
    "paper:url.48DD6CB4": {
      "data": {
        "sourceId": "url",
        "paperId": "48DD6CB4",
        "url": "https://docs.oasis-open.org/legaldocml/akn-core/v1.0/cos01/part1-vocabulary/akn-core-v1.0-cos01-part1-vocabulary.html",
        "title": "Akoma Ntoso Version 1.0. Part 1: XML Vocabulary",
        "authors": "",
        "abstract": "This document defines the Akoma Ntoso XML standard.",
        "timestamp": "2025-03-30T15:31:23.343Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T15:31:23+00:00",
        "updated_at": "2025-03-30T17:25:40+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2406.14935": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.14935",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-30T15:30:28.845Z",
            "data": {
              "session_id": "session_1743348627946_hylgzrq",
              "source_id": "arxiv",
              "paper_id": "2406.14935",
              "start_time": "2025-03-30T15:29:34.929Z",
              "end_time": "2025-03-30T15:30:27.946Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 3,
              "total_elapsed_seconds": 53
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-30T15:30:29+00:00",
        "updated_at": "2025-03-30T17:25:39+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2406.14935": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.14935",
        "url": "https://arxiv.org/abs/2406.14935",
        "title": "Modelling Legislative Systems into Property Graphs to Enable Advanced Pattern Detection",
        "authors": "Colombo, Andrea, Bernasconi, Anna, Ceri, Stefano",
        "abstract": "Legislative systems face growing complexity due to the ever-increasing number of laws and intricate interdependencies between them. Traditional methods of storing and analyzing legal systems, mainly based on RDF, struggle with this complexity, hindering efficient knowledge discovery, as required by domain experts. In this paper, we propose to model legislation into a property graph, where edges represent citations, modifications, and abrogations between laws and their articles or attachments, both represented as nodes and edges with properties. As a practical use case, we implement the model in the Italian legislative system. First, we describe our approach to extracting knowledge from legal texts. To this aim, we leverage the recently internationally adopted XML law standard, Akoma Ntoso, to parse and identify entities, relationships and properties. Next, we describe the model and the schema implemented using Neo4j, the market-leading graph database management system. The schema is designed to capture the structure and hierarchy of laws, together with their interdependencies. We show how such a property graph enables an efficient answer to complex and relevant queries previously impractical on raw text. By leveraging other implementations of the Akoma Ntoso standard and the proposed property graph approach, we are confident that this work will facilitate a comprehensive comparison of legislative systems and their complexities.",
        "timestamp": "2025-03-30T15:29:35.462Z",
        "rating": "novote",
        "publishedDate": "2024/06/21",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T15:29:35+00:00",
        "updated_at": "2025-03-30T17:25:39+00:00",
        "version": 2
      }
    },
    "paper:url.6480833D": {
      "data": {
        "sourceId": "url",
        "paperId": "6480833D",
        "url": "https://www.pnas.org/doi/abs/10.1073/pnas.2015509117",
        "title": "Prevalence of neural collapse during the terminal phase of deep learning training",
        "authors": "Papyan, Vardan, Han, X. Y., Donoho, David L.",
        "abstract": "Modern practice for training classification deepnets involves a terminal phase of\ntraining (TPT), which begins at the epoch where training error fi...",
        "timestamp": "2025-03-30T15:14:14.144Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "deep learning",
          "inductive bias",
          "adversarial robustness",
          "simplex equiangular tight frame",
          "nearest class center"
        ],
        "doi": "10.1073/pnas.2015509117",
        "journalName": "Proceedings of the National Academy of Sciences",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T15:14:14+00:00",
        "updated_at": "2025-03-30T17:25:39+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2206.04041": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2206.04041",
        "url": "https://arxiv.org/abs/2206.04041",
        "title": "Neural Collapse: A Review on Modelling Principles and Generalization",
        "authors": "Kothapalli, Vignesh",
        "abstract": "Deep classifier neural networks enter the terminal phase of training (TPT) when training error reaches zero and tend to exhibit intriguing Neural Collapse (NC) properties. Neural collapse essentially represents a state at which the within-class variability of final hidden layer outputs is infinitesimally small and their class means form a simplex equiangular tight frame. This simplifies the last layer behaviour to that of a nearest-class center decision rule. Despite the simplicity of this state, the dynamics and implications of reaching it are yet to be fully understood. In this work, we review the principles which aid in modelling neural collapse, followed by the implications of this state on generalization and transfer learning capabilities of neural networks. Finally, we conclude by discussing potential avenues and directions for future research.",
        "timestamp": "2025-03-30T15:13:57.969Z",
        "rating": "novote",
        "publishedDate": "2022/06/08",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T15:13:58+00:00",
        "updated_at": "2025-03-30T17:25:38+00:00",
        "version": 2
      }
    },
    "paper:url.7754815": {
      "data": {
        "sourceId": "url",
        "paperId": "7754815",
        "url": "https://openreview.net/forum?id=y5W8tpojhtJ",
        "title": "Neural Collapse Inspired Feature-Classifier Alignment for Few-Shot Class-Incremental Learning",
        "authors": "Yibo Yang, Haobo Yuan, Xiangtai Li, Zhouchen Lin, Philip Torr, Dacheng Tao",
        "abstract": "Few-shot class-incremental learning (FSCIL) has been a challenging problem as only a few training samples are accessible for each novel class in the new sessions. Finetuning the backbone or adjusting the classifier prototypes trained in the prior sessions would inevitably cause a misalignment between the feature and classifier of old classes, which explains the well-known catastrophic forgetting problem. In this paper, we deal with this misalignment dilemma in FSCIL inspired by the recently discovered phenomenon named neural collapse, which reveals that the last-layer features of the same class will collapse into a vertex, and the vertices of all classes are aligned with the classifier prototypes, which are formed as a simplex equiangular tight frame (ETF). It corresponds to an optimal geometric structure for classification due to the maximized Fisher Discriminant Ratio. We propose a neural collapse inspired framework for FSCIL. A group of classifier prototypes are pre-assigned as a simplex ETF for the whole label space, including the base session and all the incremental sessions. During training, the classifier prototypes are not learnable, and we adopt a novel loss function that drives the features into their corresponding prototypes. Theoretical analysis shows that our method holds the neural collapse optimality and does not break the feature-classifier alignment in an incremental fashion. Experiments on the miniImageNet, CUB-200, and CIFAR-100 datasets demonstrate that our proposed framework outperforms the state-of-the-art performances. Code address: https://github.com/NeuralCollapseApplications/FSCIL ",
        "timestamp": "2025-03-30T15:10:32.274Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T15:10:32+00:00",
        "updated_at": "2025-03-30T17:25:38+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2309.10313": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2309.10313",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-30T15:10:24.433Z",
            "data": {
              "session_id": "session_1743347423726_wtz7kia",
              "source_id": "arxiv",
              "paper_id": "2309.10313",
              "start_time": "2025-03-30T15:07:58.319Z",
              "end_time": "2025-03-30T15:10:23.726Z",
              "heartbeat_count": 29,
              "duration_seconds": 145,
              "idle_seconds": 0,
              "total_elapsed_seconds": 145
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-30T15:10:25+00:00",
        "updated_at": "2025-03-30T17:25:38+00:00",
        "version": 3
      }
    },
    "interactions:arxiv.2208.05512": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2208.05512",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-30T15:07:42.544Z",
            "data": {
              "session_id": "session_1743347261763_ds22n41",
              "source_id": "arxiv",
              "paper_id": "2208.05512",
              "start_time": "2025-03-30T15:06:42.332Z",
              "end_time": "2025-03-30T15:07:41.763Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 4,
              "total_elapsed_seconds": 59
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-30T15:07:43+00:00",
        "updated_at": "2025-03-30T17:25:37+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2208.05512": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2208.05512",
        "url": "https://arxiv.org/abs/2208.05512",
        "title": "Imbalance Trouble: Revisiting Neural-Collapse Geometry",
        "authors": "Thrampoulidis, Christos, Kini, Ganesh R., Vakilian, Vala, Behnia, Tina",
        "abstract": "Neural Collapse refers to the remarkable structural properties characterizing the geometry of class embeddings and classifier weights, found by deep nets when trained beyond zero training error. However, this characterization only holds for balanced data. Here we thus ask whether it can be made invariant to class imbalances. Towards this end, we adopt the unconstrained-features model (UFM), a recent theoretical model for studying neural collapse, and introduce Simplex-Encoded-Labels Interpolation (SELI) as an invariant characterization of the neural collapse phenomenon. Specifically, we prove for the UFM with cross-entropy loss and vanishing regularization that, irrespective of class imbalances, the embeddings and classifiers always interpolate a simplex-encoded label matrix and that their individual geometries are determined by the SVD factors of this same label matrix. We then present extensive experiments on synthetic and real datasets that confirm convergence to the SELI geometry. However, we caution that convergence worsens with increasing imbalances. We theoretically support this finding by showing that unlike the balanced case, when minorities are present, ridge-regularization plays a critical role in tweaking the geometry. This defines new questions and motivates further investigations into the impact of class imbalances on the rates at which first-order methods converge to their asymptotically preferred solutions.",
        "timestamp": "2025-03-30T15:06:42.865Z",
        "rating": "novote",
        "publishedDate": "2022/08/10",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T15:06:43+00:00",
        "updated_at": "2025-03-30T17:25:37+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2309.10313": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2309.10313",
        "url": "https://arxiv.org/abs/2309.10313",
        "title": "Investigating the Catastrophic Forgetting in Multimodal Large Language Models",
        "authors": "Zhai, Yuexiang, Tong, Shengbang, Li, Xiao, Cai, Mu, Qu, Qing, Lee, Yong Jae, Ma, Yi",
        "abstract": "Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-stage fine-tuning on an image dataset improves performance across other image datasets, by enhancing the alignment of text and visual features. However, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in a significant loss of generalizability, even when the image encoder remains frozen. Our results suggest that MLLMs have yet to demonstrate performance on par with their vision models on standard image classification tasks and the current MLLM fine-tuning procedure still has room for improvement.",
        "timestamp": "2025-03-30T15:05:53.461Z",
        "rating": "novote",
        "publishedDate": "2023/09/19",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T15:05:54+00:00",
        "updated_at": "2025-03-30T17:25:36+00:00",
        "version": 2
      }
    },
    "paper:url.7F3D0B7": {
      "data": {
        "sourceId": "url",
        "paperId": "7F3D0B7",
        "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/dfc310e81992d2e4cedc09ac47eff13e-Abstract-Conference.html",
        "title": "Towards Understanding Grokking: An Effective Theory of Representation Learning",
        "authors": "Liu, Ziming, Kitouni, Ouail, Nolte, Niklas S, Michaud, Eric, Tegmark, Max, Williams, Mike",
        "abstract": "",
        "timestamp": "2025-03-30T15:02:41.621Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "Advances in Neural Information Processing Systems",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T15:02:42+00:00",
        "updated_at": "2025-03-30T17:25:36+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2106.04972": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2106.04972",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-30T15:00:03.320Z",
            "data": {
              "session_id": "session_1743346802801_xkilnxm",
              "source_id": "arxiv",
              "paper_id": "2106.04972",
              "start_time": "2025-03-30T14:59:34.342Z",
              "end_time": "2025-03-30T15:00:02.801Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 3,
              "total_elapsed_seconds": 28
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-30T15:00:04+00:00",
        "updated_at": "2025-03-30T17:25:36+00:00",
        "version": 3
      }
    },
    "interactions:arxiv.2008.08186": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2008.08186",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-30T14:55:43.180Z",
            "data": {
              "session_id": "session_1743346543172_98qzzs5",
              "source_id": "arxiv",
              "paper_id": "2008.08186",
              "start_time": "2025-03-30T14:55:22.862Z",
              "end_time": "2025-03-30T14:55:43.171Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 0,
              "total_elapsed_seconds": 20
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-03-30T14:59:32.202Z",
            "data": {
              "session_id": "session_1743346771606_bt6rwcc",
              "source_id": "arxiv",
              "paper_id": "2008.08186",
              "start_time": "2025-03-30T14:55:57.988Z",
              "end_time": "2025-03-30T14:59:31.606Z",
              "heartbeat_count": 42,
              "duration_seconds": 210,
              "idle_seconds": 4,
              "total_elapsed_seconds": 214
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-30T14:55:44+00:00",
        "updated_at": "2025-03-30T17:25:35+00:00",
        "version": 4
      }
    },
    "paper:arxiv.2008.08186": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2008.08186",
        "url": "https://arxiv.org/abs/2008.08186",
        "title": "Prevalence of Neural Collapse during the terminal phase of deep learning training",
        "authors": "Papyan, Vardan, Han, X. Y., Donoho, David L.",
        "abstract": "Modern practice for training classification deepnets involves a Terminal Phase of Training (TPT), which begins at the epoch where training error first vanishes; During TPT, the training error stays effectively zero while training loss is pushed towards zero. Direct measurements of TPT, for three prototypical deepnet architectures and across seven canonical classification datasets, expose a pervasive inductive bias we call Neural Collapse, involving four deeply interconnected phenomena: (NC1) Cross-example within-class variability of last-layer training activations collapses to zero, as the individual activations themselves collapse to their class-means; (NC2) The class-means collapse to the vertices of a Simplex Equiangular Tight Frame (ETF); (NC3) Up to rescaling, the last-layer classifiers collapse to the class-means, or in other words to the Simplex ETF, i.e. to a self-dual configuration; (NC4) For a given activation, the classifier's decision collapses to simply choosing whichever class has the closest train class-mean, i.e. the Nearest Class Center (NCC) decision rule. The symmetric and very simple geometry induced by the TPT confers important benefits, including better generalization performance, better robustness, and better interpretability.",
        "timestamp": "2025-03-30T14:55:23.359Z",
        "rating": "novote",
        "publishedDate": "2020/08/18",
        "tags": [],
        "doi": "10.1073/pnas.2015509117",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T14:55:23+00:00",
        "updated_at": "2025-03-30T17:25:35+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2402.03979": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.03979",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-30T14:54:32.339Z",
            "data": {
              "session_id": "session_1743346471379_ewosh1p",
              "source_id": "arxiv",
              "paper_id": "2402.03979",
              "start_time": "2025-03-30T14:53:36.397Z",
              "end_time": "2025-03-30T14:54:31.379Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 5,
              "total_elapsed_seconds": 55
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-30T14:54:33+00:00",
        "updated_at": "2025-03-30T17:25:35+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2402.03979": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.03979",
        "url": "https://arxiv.org/abs/2402.03979",
        "title": "Cross Entropy versus Label Smoothing: A Neural Collapse Perspective",
        "authors": "Guo, Li, Ross, Keith, Zhao, Zifan, Andriopoulos, George, Ling, Shuyang, Xu, Yufeng, Dong, Zixuan",
        "abstract": "Label smoothing loss is a widely adopted technique to mitigate overfitting in deep neural networks. This paper studies label smoothing from the perspective of Neural Collapse (NC), a powerful empirical and theoretical framework which characterizes model behavior during the terminal phase of training. We first show empirically that models trained with label smoothing converge faster to neural collapse solutions and attain a stronger level of neural collapse. Additionally, we show that at the same level of NC1, models under label smoothing loss exhibit intensified NC2. These findings provide valuable insights into the performance benefits and enhanced model calibration under label smoothing loss. We then leverage the unconstrained feature model to derive closed-form solutions for the global minimizers for both loss functions and further demonstrate that models under label smoothing have a lower conditioning number and, therefore, theoretically converge faster. Our study, combining empirical evidence and theoretical results, not only provides nuanced insights into the differences between label smoothing and cross-entropy losses, but also serves as an example of how the powerful neural collapse framework can be used to improve our understanding of DNNs.",
        "timestamp": "2025-03-30T14:53:36.408Z",
        "rating": "novote",
        "publishedDate": "2024/02/06",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T14:53:36+00:00",
        "updated_at": "2025-03-30T17:25:34+00:00",
        "version": 2
      }
    },
    "paper:arxiv.2106.04972": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2106.04972",
        "url": "https://arxiv.org/abs/2106.04972",
        "title": "Understanding Softmax Confidence and Uncertainty",
        "authors": "Pearce, Tim, Brintrup, Alexandra, Zhu, Jun",
        "abstract": "It is often remarked that neural networks fail to increase their uncertainty when predicting on data far from the training distribution. Yet naively using softmax confidence as a proxy for uncertainty achieves modest success in tasks exclusively testing for this, e.g., out-of-distribution (OOD) detection. This paper investigates this contradiction, identifying two implicit biases that do encourage softmax confidence to correlate with epistemic uncertainty: 1) Approximately optimal decision boundary structure, and 2) Filtering effects of deep networks. It describes why low-dimensional intuitions about softmax confidence are misleading. Diagnostic experiments quantify reasons softmax confidence can fail, finding that extrapolations are less to blame than overlap between training and OOD data in final-layer representations. Pre-trained/fine-tuned networks reduce this overlap.",
        "timestamp": "2025-03-30T14:53:19.905Z",
        "rating": "novote",
        "publishedDate": "2021/06/09",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T14:53:20+00:00",
        "updated_at": "2025-03-30T17:25:33+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2403.09635": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2403.09635",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-30T14:30:18.967Z",
            "data": {
              "session_id": "session_1743345018227_uw0ng2b",
              "source_id": "arxiv",
              "paper_id": "2403.09635",
              "start_time": "2025-03-30T14:29:52.247Z",
              "end_time": "2025-03-30T14:30:18.227Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 1,
              "total_elapsed_seconds": 26
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-30T14:30:19+00:00",
        "updated_at": "2025-03-30T17:25:33+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2403.09635": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2403.09635",
        "url": "https://arxiv.org/abs/2403.09635",
        "title": "Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models",
        "authors": "Kedia, Akhil, Zaidi, Mohd Abbas, Khyalia, Sushil, Jung, Jungho, Goka, Harshith, Lee, Haejun",
        "abstract": "In spite of their huge success, transformer models remain difficult to scale in depth. In this work, we develop a unified signal propagation theory and provide formulae that govern the moments of the forward and backward signal through the transformer model. Our framework can be used to understand and mitigate vanishing/exploding gradients, rank collapse, and instability associated with high attention scores. We also propose DeepScaleLM, an initialization and scaling scheme that conserves unit output/gradient moments throughout the model, enabling the training of very deep models with 1000 layers. We find that transformer models could be much deeper - our deep models with fewer parameters outperform shallow models in Language Modeling, Speech Translation, and Image Classification, across encoder-only, decoder-only and encoder-decoder variants, for both Pre-LN and Post-LN transformers, for multiple datasets and model sizes. These improvements also translate into improved performance on downstream Question Answering tasks and improved robustness for Image Classification.",
        "timestamp": "2025-03-30T14:29:52.649Z",
        "rating": "novote",
        "publishedDate": "2024/03/14",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T14:29:53+00:00",
        "updated_at": "2025-03-30T17:25:33+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2501.04697": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.04697",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-30T14:22:30.248Z",
            "data": {
              "session_id": "session_1743344549523_6vt3eas",
              "source_id": "arxiv",
              "paper_id": "2501.04697",
              "start_time": "2025-03-30T14:22:20.733Z",
              "end_time": "2025-03-30T14:22:29.523Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-30T14:22:31+00:00",
        "updated_at": "2025-03-30T17:26:01+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2501.04697": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.04697",
        "url": "https://arxiv.org/abs/2501.04697",
        "title": "Grokking at the Edge of Numerical Stability",
        "authors": "Prieto, Lucas, Barsbey, Melih, Mediano, Pedro A. M., Birdal, Tolga",
        "abstract": "Grokking, the sudden generalization that occurs after prolonged overfitting, is a surprising phenomenon challenging our understanding of deep learning. Although significant progress has been made in understanding grokking, the reasons behind the delayed generalization and its dependence on regularization remain unclear. In this work, we argue that without regularization, grokking tasks push models to the edge of numerical stability, introducing floating point errors in the Softmax function, which we refer to as Softmax Collapse (SC). We demonstrate that SC prevents grokking and that mitigating SC enables grokking without regularization. Investigating the root cause of SC, we find that beyond the point of overfitting, the gradients strongly align with what we call the na\\\"ive loss minimization (NLM) direction. This component of the gradient does not alter the model's predictions but decreases the loss by scaling the logits, typically by scaling the weights along their current direction. We show that this scaling of the logits explains the delay in generalization characteristic of grokking and eventually leads to SC, halting further learning. To validate our hypotheses, we introduce two key contributions that address the challenges in grokking tasks: StableMax, a new activation function that prevents SC and enables grokking without regularization, and $\\perp$Grad, a training algorithm that promotes quick generalization in grokking tasks by preventing NLM altogether. These contributions provide new insights into grokking, elucidating its delayed generalization, reliance on regularization, and the effectiveness of existing grokking-inducing methods. Code for this paper is available at https://github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability.",
        "timestamp": "2025-03-30T14:22:21.202Z",
        "rating": "novote",
        "publishedDate": "2025/01/08",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T14:22:21+00:00",
        "updated_at": "2025-03-30T17:26:00+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2107.09133": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2107.09133",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-30T14:19:10.519Z",
            "data": {
              "session_id": "session_1743344350139_fsb7l6z",
              "source_id": "arxiv",
              "paper_id": "2107.09133",
              "start_time": "2025-03-30T14:18:43.099Z",
              "end_time": "2025-03-30T14:19:10.139Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 2,
              "total_elapsed_seconds": 27
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-30T14:19:11+00:00",
        "updated_at": "2025-03-30T17:26:00+00:00",
        "version": 3
      }
    },
    "interactions:arxiv.2203.03466": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2203.03466",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-30T21:03:08.610Z",
            "data": {
              "session_id": "session_1743368588600_62w2kci",
              "source_id": "arxiv",
              "paper_id": "2203.03466",
              "start_time": "2025-03-30T21:03:02.078Z",
              "end_time": "2025-03-30T21:03:08.600Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-03-30T21:03:09+00:00",
        "updated_at": "2025-03-30T21:04:08+00:00",
        "version": 3
      }
    },
    "paper:arxiv.2203.03466": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2203.03466",
        "url": "https://arxiv.org/abs/2203.03466",
        "title": "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer",
        "authors": "Yang, Greg, Hu, Edward J., Babuschkin, Igor, Sidor, Szymon, Liu, Xiaodong, Farhi, David, Ryder, Nick, Pachocki, Jakub, Chen, Weizhu, Gao, Jianfeng",
        "abstract": "Hyperparameter (HP) tuning in deep learning is an expensive process, prohibitively so for neural networks (NNs) with billions of parameters. We show that, in the recently discovered Maximal Update Parametrization (muP), many optimal HPs remain stable even as model size changes. This leads to a new HP tuning paradigm we call muTransfer: parametrize the target model in muP, tune the HP indirectly on a smaller model, and zero-shot transfer them to the full-sized model, i.e., without directly tuning the latter at all. We verify muTransfer on Transformer and ResNet. For example, 1) by transferring pretraining HPs from a model of 13M parameters, we outperform published numbers of BERT-large (350M parameters), with a total tuning cost equivalent to pretraining BERT-large once; 2) by transferring from 40M parameters, we outperform published numbers of the 6.7B GPT-3 model, with tuning cost only 7% of total pretraining cost. A Pytorch implementation of our technique can be found at github.com/microsoft/mup and installable via `pip install mup`.",
        "timestamp": "2025-03-30T21:02:59.480Z",
        "rating": "novote",
        "publishedDate": "2022/03/07",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "created_at": "2025-03-30T21:02:59+00:00",
        "updated_at": "2025-03-30T21:03:02+00:00",
        "version": 2
      }
    },
    "interactions:arxiv.2310.01425": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2310.01425",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-31T15:13:51.710Z",
            "data": {
              "session_id": "session_1743434030864_egedind",
              "source_id": "arxiv",
              "paper_id": "2310.01425",
              "start_time": "2025-03-31T15:13:08.938Z",
              "end_time": "2025-03-31T15:13:50.864Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 2,
              "total_elapsed_seconds": 42
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2392,
        "object_id": "interactions:arxiv.2310.01425",
        "created_at": "2025-03-31T15:13:52+00:00",
        "updated_at": "2025-03-31T15:14:56+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2310.01425": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2310.01425",
        "url": "https://arxiv.org/abs/2310.01425",
        "title": "Borges and AI",
        "authors": "Bottou, L\u00e9on, Sch\u00f6lkopf, Bernhard",
        "abstract": "Many believe that Large Language Models (LLMs) open the era of Artificial Intelligence (AI). Some see opportunities while others see dangers. Yet both proponents and opponents grasp AI through the imagery popularised by science fiction. Will the machine become sentient and rebel against its creators? Will we experience a paperclip apocalypse? Before answering such questions, we should first ask whether this mental imagery provides a good description of the phenomenon at hand. Understanding weather patterns through the moods of the gods only goes so far. The present paper instead advocates understanding LLMs and their connection to AI through the imagery of Jorge Luis Borges, a master of 20th century literature, forerunner of magical realism, and precursor to postmodern literature. This exercise leads to a new perspective that illuminates the relation between language modelling and artificial intelligence.",
        "timestamp": "2025-03-31T15:13:09.589Z",
        "rating": "novote",
        "publishedDate": "2023/09/27",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2391,
        "object_id": "paper:arxiv.2310.01425",
        "created_at": "2025-03-31T15:13:10+00:00",
        "updated_at": "2025-03-31T15:13:13+00:00",
        "version": 1
      }
    },
    "interactions:url.27A837D6": {
      "data": {
        "sourceId": "url",
        "paperId": "27A837D6",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-31T15:03:29.186Z",
            "data": {
              "session_id": "session_1743433408186_4ol57te",
              "source_id": "url",
              "paper_id": "27A837D6",
              "start_time": "2025-03-31T15:03:19.918Z",
              "end_time": "2025-03-31T15:03:28.186Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2390,
        "object_id": "interactions:url.27A837D6",
        "created_at": "2025-03-31T15:03:30+00:00",
        "updated_at": "2025-03-31T15:04:31+00:00",
        "version": 1
      }
    },
    "paper:url.27A837D6": {
      "data": {
        "sourceId": "url",
        "paperId": "27A837D6",
        "url": "https://www.nature.com/articles/s41593-025-01899-1",
        "title": "Expectation-driven sensory adaptations support enhanced acuity during categorical perception",
        "authors": "Sainburg, Tim, McPherson, Trevor S., Arneodo, Ezequiel M., Rudraraju, Srihita, Turvey, Michael, Theilman, Bradley H., Tostado Marcos, Pablo, Thielk, Marvin, Gentner, Timothy Q.",
        "abstract": "Bayesian models explain how context biases perceptual behavior toward expected categories, but sensory neurons do not reflect this bias. Instead, expectation sharpens sensory acuity, independent of downstream decision making.",
        "timestamp": "2025-03-31T15:03:19.319Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1038/s41593-025-01899-1",
        "journalName": "Nature Neuroscience",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2389,
        "object_id": "paper:url.27A837D6",
        "created_at": "2025-03-31T15:03:20+00:00",
        "updated_at": "2025-03-31T15:03:23+00:00",
        "version": 1
      }
    },
    "paper:url.6F0D819D": {
      "data": {
        "sourceId": "url",
        "paperId": "6F0D819D",
        "url": "https://openreview.net/forum?id=IssPhpUsKt&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2025%2FConference%2FAuthors%23your-submissions)",
        "title": "Improving Reasoning Performance in Large Language Models via Representation Engineering",
        "authors": "Bertram H\u00f8jer, Oliver Simon Jarvis, Stefan Heinrich",
        "abstract": "Recent advancements in large language models (LLMs) have resulted in increasingly anthropomorphic language concerning the ability of LLMs to reason. Whether \\textit{reasoning} in LLMs should be understood to be inherently different is, however, widely debated. We propose utilizing a representation engineering approach wherein model activations are read from the residual stream of an LLM when processing a reasoning task. The activations are used to derive a control vector that is applied to the model as an inference-time intervention, modulating the representational space of the model, to improve performance on the specified task. We publish the code for deriving control vectors and analyzing model representations. The method allows us to improve performance on reasoning benchmarks and assess how control vectors influence the final logit distribution of a model via metrics such as KL divergence and entropy. We apply control vectors to Mistral-7B-Instruct and a range of Pythia models on an inductive, a deductive and mathematical reasoning task. We show that an LLM can, to a certain degree, be controlled to improve its perceived reasoning ability by modulating activations. The intervention is dependent upon the ability to reliably extract the model's typical state when correctly solving a task. Our results suggest that reasoning performance can be modulated in the same manner as other information-processing tasks performed by LLMs and demonstrate that we are capable of improving performance on specific tasks via a simple intervention on the residual stream with no additional training.",
        "timestamp": "2025-03-31T14:54:29.286Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2388,
        "object_id": "paper:url.6F0D819D",
        "created_at": "2025-03-31T14:54:29+00:00",
        "updated_at": "2025-03-31T14:54:32+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.1912.02292": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1912.02292",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-31T09:35:04.023Z",
            "data": {
              "session_id": "session_1743413703511_59amiep",
              "source_id": "arxiv",
              "paper_id": "1912.02292",
              "start_time": "2025-03-31T09:33:25.199Z",
              "end_time": "2025-03-31T09:35:03.511Z",
              "heartbeat_count": 19,
              "duration_seconds": 95,
              "idle_seconds": 3,
              "total_elapsed_seconds": 98
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2387,
        "object_id": "interactions:arxiv.1912.02292",
        "created_at": "2025-03-31T09:33:15+00:00",
        "updated_at": "2025-03-31T09:35:28+00:00",
        "version": 1
      }
    },
    "paper:arxiv.1912.02292": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1912.02292",
        "url": "https://arxiv.org/abs/1912.02292",
        "title": "Deep Double Descent: Where Bigger Models and More Data Hurt",
        "authors": "Nakkiran, Preetum, Kaplun, Gal, Bansal, Yamini, Yang, Tristan, Barak, Boaz, Sutskever, Ilya",
        "abstract": "We show that a variety of modern deep learning tasks exhibit a \"double-descent\" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.",
        "timestamp": "2025-03-31T09:33:08.038Z",
        "rating": "novote",
        "publishedDate": "2019/12/04",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2386,
        "object_id": "paper:arxiv.1912.02292",
        "created_at": "2025-03-31T09:33:08+00:00",
        "updated_at": "2025-03-31T09:33:11+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.1912.08286": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1912.08286",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-31T09:32:59.199Z",
            "data": {
              "session_id": "session_1743413578233_u0nvski",
              "source_id": "arxiv",
              "paper_id": "1912.08286",
              "start_time": "2025-03-31T09:32:49.955Z",
              "end_time": "2025-03-31T09:32:58.233Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2385,
        "object_id": "interactions:arxiv.1912.08286",
        "created_at": "2025-03-31T09:33:00+00:00",
        "updated_at": "2025-03-31T09:33:59+00:00",
        "version": 1
      }
    },
    "paper:arxiv.1912.08286": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1912.08286",
        "url": "https://arxiv.org/abs/1912.08286",
        "title": "On the Bias-Variance Tradeoff: Textbooks Need an Update",
        "authors": "Neal, Brady",
        "abstract": "The main goal of this thesis is to point out that the bias-variance tradeoff is not always true (e.g. in neural networks). We advocate for this lack of universality to be acknowledged in textbooks and taught in introductory courses that cover the tradeoff. We first review the history of the bias-variance tradeoff, its prevalence in textbooks, and some of the main claims made about the bias-variance tradeoff. Through extensive experiments and analysis, we show a lack of a bias-variance tradeoff in neural networks when increasing network width. Our findings seem to contradict the claims of the landmark work by Geman et al. (1992). Motivated by this contradiction, we revisit the experimental measurements in Geman et al. (1992). We discuss that there was never strong evidence for a tradeoff in neural networks when varying the number of parameters. We observe a similar phenomenon beyond supervised learning, with a set of deep reinforcement learning experiments. We argue that textbook and lecture revisions are in order to convey this nuanced modern understanding of the bias-variance tradeoff.",
        "timestamp": "2025-03-31T09:32:50.571Z",
        "rating": "novote",
        "publishedDate": "2019/12/17",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2384,
        "object_id": "paper:arxiv.1912.08286",
        "created_at": "2025-03-31T09:32:51+00:00",
        "updated_at": "2025-03-31T09:32:54+00:00",
        "version": 1
      }
    },
    "interactions:url.35F95ECF": {
      "data": {
        "sourceId": "url",
        "paperId": "35F95ECF",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-31T09:29:22.899Z",
            "data": {
              "session_id": "session_1743413361962_ks06t6b",
              "source_id": "url",
              "paper_id": "35F95ECF",
              "start_time": "2025-03-31T09:28:30.558Z",
              "end_time": "2025-03-31T09:29:21.962Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 1,
              "total_elapsed_seconds": 51
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2383,
        "object_id": "interactions:url.35F95ECF",
        "created_at": "2025-03-31T09:29:23+00:00",
        "updated_at": "2025-03-31T09:30:24+00:00",
        "version": 1
      }
    },
    "paper:url.35F95ECF": {
      "data": {
        "sourceId": "url",
        "paperId": "35F95ECF",
        "url": "https://www.nature.com/articles/s43247-024-01974-8?fromPaywallRec=false",
        "title": "The changing language and sentiment of conversations about climate change in Reddit posts over sixteen years",
        "authors": "Fariello, Gabriele, Jemielniak, Dariusz",
        "abstract": "The proportion of posts on Reddit on the topic of climate change has declined between 2005 and 2021, according to analyses of 16 years of Reddit discussions that include 11.5 billion posts",
        "timestamp": "2025-03-31T09:28:29.252Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1038/s43247-024-01974-8",
        "journalName": "Communications Earth & Environment",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2382,
        "object_id": "paper:url.35F95ECF",
        "created_at": "2025-03-31T09:28:29+00:00",
        "updated_at": "2025-03-31T09:28:33+00:00",
        "version": 1
      }
    },
    "interactions:url.7D2519FA": {
      "data": {
        "sourceId": "url",
        "paperId": "7D2519FA",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-31T09:28:18.330Z",
            "data": {
              "session_id": "session_1743413297499_kbu36fw",
              "source_id": "url",
              "paper_id": "7D2519FA",
              "start_time": "2025-03-31T09:27:54.979Z",
              "end_time": "2025-03-31T09:28:17.499Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 3,
              "total_elapsed_seconds": 23
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2381,
        "object_id": "interactions:url.7D2519FA",
        "created_at": "2025-03-31T09:28:19+00:00",
        "updated_at": "2025-03-31T09:29:21+00:00",
        "version": 1
      }
    },
    "paper:url.7D2519FA": {
      "data": {
        "sourceId": "url",
        "paperId": "7D2519FA",
        "url": "https://www.nature.com/articles/s41598-022-21720-4",
        "title": "The language of opinion change on social media under the lens of communicative action",
        "authors": "Monti, Corrado, Aiello, Luca Maria, De Francisci Morales, Gianmarco, Bonchi, Francesco",
        "abstract": "Which messages are more effective at inducing a change of opinion in the listener? We approach this question within the frame of Habermas\u2019 theory of communicative action, which posits that the illocutionary intent of the message (its pragmatic meaning) is the key. Thanks to recent advances in natural language processing, we are able to operationalize this theory by extracting the latent social dimensions of a message, namely archetypes of social intent of language, that come from social exchange theory. We identify key ingredients to opinion change by looking at more than 46k posts and more than 3.5M comments on Reddit\u2019s r/ChangeMyView, a debate forum where people try to change each other\u2019s opinion and explicitly mark opinion-changing comments with a special flag called delta. Comments that express no intent are about 77% less likely to change the mind of the recipient, compared to comments that convey at least one social dimension. Among the various social dimensions, the ones that are most likely to produce an opinion change are knowledge, similarity, and trust, which resonates with Habermas\u2019 theory of communicative action. We also find other new important dimensions, such as appeals to power or empathetic expressions of support. Finally, in line with theories of constructive conflict, yet contrary to the popular characterization of conflict as the bane of modern social media, our findings show that voicing conflict in the context of a structured public debate can promote integration, especially when it is used to counter another conflictive stance. By leveraging recent advances in natural language processing, our work provides an empirical framework for Habermas\u2019 theory, finds concrete examples of its effects in the wild, and suggests its possible extension with a more faceted understanding of intent interpreted as social dimensions of language.",
        "timestamp": "2025-03-31T09:27:54.736Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1038/s41598-022-21720-4",
        "journalName": "Scientific Reports",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2380,
        "object_id": "paper:url.7D2519FA",
        "created_at": "2025-03-31T09:27:55+00:00",
        "updated_at": "2025-03-31T09:27:58+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2403.02563": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2403.02563",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-31T20:28:28.358Z",
            "data": {
              "session_id": "session_1743452907538_0bzhsem",
              "source_id": "arxiv",
              "paper_id": "2403.02563",
              "start_time": "2025-03-31T20:28:07.434Z",
              "end_time": "2025-03-31T20:28:27.538Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 0,
              "total_elapsed_seconds": 20
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2397,
        "object_id": "interactions:arxiv.2403.02563",
        "created_at": "2025-03-31T20:28:29+00:00",
        "updated_at": "2025-03-31T20:29:31+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2403.02563": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2403.02563",
        "url": "https://arxiv.org/abs/2403.02563",
        "title": "Systemic Biases in Sign Language AI Research: A Deaf-Led Call to Reevaluate Research Agendas",
        "authors": "Desai, Aashaka, De Meulder, Maartje, Hochgesang, Julie A., Kocab, Annemarie, Lu, Alex X.",
        "abstract": "Growing research in sign language recognition, generation, and translation AI has been accompanied by calls for ethical development of such technologies. While these works are crucial to helping individual researchers do better, there is a notable lack of discussion of systemic biases or analysis of rhetoric that shape the research questions and methods in the field, especially as it remains dominated by hearing non-signing researchers. Therefore, we conduct a systematic review of 101 recent papers in sign language AI. Our analysis identifies significant biases in the current state of sign language AI research, including an overfocus on addressing perceived communication barriers, a lack of use of representative datasets, use of annotations lacking linguistic foundations, and development of methods that build on flawed models. We take the position that the field lacks meaningful input from Deaf stakeholders, and is instead driven by what decisions are the most convenient or perceived as important to hearing researchers. We end with a call to action: the field must make space for Deaf researchers to lead the conversation in sign language AI.",
        "timestamp": "2025-03-31T20:28:08.006Z",
        "rating": "novote",
        "publishedDate": "2024/03/05",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2396,
        "object_id": "paper:arxiv.2403.02563",
        "created_at": "2025-03-31T20:28:08+00:00",
        "updated_at": "2025-03-31T20:28:11+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.21934": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.21934",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-31T20:27:16.831Z",
            "data": {
              "session_id": "session_1743452835834_1q7c9kk",
              "source_id": "arxiv",
              "paper_id": "2503.21934",
              "start_time": "2025-03-31T20:27:09.548Z",
              "end_time": "2025-03-31T20:27:15.834Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2395,
        "object_id": "interactions:arxiv.2503.21934",
        "created_at": "2025-03-31T20:27:17+00:00",
        "updated_at": "2025-03-31T20:28:19+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.21934": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.21934",
        "url": "https://arxiv.org/abs/2503.21934v1",
        "title": "Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad",
        "authors": "Petrov, Ivo, Dekoninck, Jasper, Baltadzhiev, Lyuben, Drencheva, Maria, Minchev, Kristian, Balunovi\u0107, Mislav, Jovanovi\u0107, Nikola, Vechev, Martin",
        "abstract": "Recent math benchmarks for large language models (LLMs) such as MathArena indicate that state-of-the-art reasoning models achieve impressive performance on mathematical competitions like AIME, with the leading model, o3-mini, achieving scores comparable to top human competitors. However, these benchmarks evaluate models solely based on final numerical answers, neglecting rigorous reasoning and proof generation which are essential for real-world mathematical tasks. To address this, we introduce the first comprehensive evaluation of full-solution reasoning for challenging mathematical problems. Using expert human annotators, we evaluated several state-of-the-art reasoning models on the six problems from the 2025 USAMO within hours of their release. Our results reveal that all tested models struggled significantly, achieving less than 5% on average. Through detailed analysis of reasoning traces, we identify the most common failure modes and find several unwanted artifacts arising from the optimization strategies employed during model training. Overall, our results suggest that current LLMs are inadequate for rigorous mathematical reasoning tasks, highlighting the need for substantial improvements in reasoning and proof generation capabilities.",
        "timestamp": "2025-03-31T20:27:08.522Z",
        "rating": "novote",
        "publishedDate": "2025/03/27",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2393,
        "object_id": "paper:arxiv.2503.21934",
        "created_at": "2025-03-31T20:27:09+00:00",
        "updated_at": "2025-03-31T20:27:12+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.21985": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.21985",
        "url": "https://arxiv.org/abs/2503.21985",
        "title": "Improving Equivariant Networks with Probabilistic Symmetry Breaking",
        "authors": "Lawrence, Hannah, Portilheiro, Vasco, Zhang, Yan, Kaba, S\u00e9kou-Oumar",
        "abstract": "Equivariance encodes known symmetries into neural networks, often enhancing generalization. However, equivariant networks cannot break symmetries: the output of an equivariant network must, by definition, have at least the same self-symmetries as the input. This poses an important problem, both (1) for prediction tasks on domains where self-symmetries are common, and (2) for generative models, which must break symmetries in order to reconstruct from highly symmetric latent spaces. This fundamental limitation can be addressed by considering equivariant conditional distributions, instead of equivariant functions. We present novel theoretical results that establish necessary and sufficient conditions for representing such distributions. Concretely, this representation provides a practical framework for breaking symmetries in any equivariant network via randomized canonicalization. Our method, SymPE (Symmetry-breaking Positional Encodings), admits a simple interpretation in terms of positional encodings. This approach expands the representational power of equivariant networks while retaining the inductive bias of symmetry, which we justify through generalization bounds. Experimental results demonstrate that SymPE significantly improves performance of group-equivariant and graph neural networks across diffusion models for graphs, graph autoencoders, and lattice spin system modeling.",
        "timestamp": "2025-03-31T20:43:29.294Z",
        "rating": "novote",
        "publishedDate": "2025/03/27",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2398,
        "object_id": "paper:arxiv.2503.21985",
        "created_at": "2025-03-31T20:43:29+00:00",
        "updated_at": "2025-03-31T20:43:32+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2405.02985": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2405.02985",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-31T21:22:33.186Z",
            "data": {
              "session_id": "session_1743456152183_dui5t78",
              "source_id": "arxiv",
              "paper_id": "2405.02985",
              "start_time": "2025-03-31T21:22:21.991Z",
              "end_time": "2025-03-31T21:22:32.183Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 0,
              "total_elapsed_seconds": 10
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2401,
        "object_id": "interactions:arxiv.2405.02985",
        "created_at": "2025-03-31T21:22:34+00:00",
        "updated_at": "2025-03-31T21:23:34+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2405.02985": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2405.02985",
        "url": "https://arxiv.org/abs/2405.02985",
        "title": "Can Large Language Models Make the Grade? An Empirical Study Evaluating LLMs Ability to Mark Short Answer Questions in K-12 Education",
        "authors": "Henkel, Owen, Boxer, Adam, Hills, Libby, Roberts, Bill",
        "abstract": "This paper presents reports on a series of experiments with a novel dataset evaluating how well Large Language Models (LLMs) can mark (i.e. grade) open text responses to short answer questions, Specifically, we explore how well different combinations of GPT version and prompt engineering strategies performed at marking real student answers to short answer across different domain areas (Science and History) and grade-levels (spanning ages 5-16) using a new, never-used-before dataset from Carousel, a quizzing platform. We found that GPT-4, with basic few-shot prompting performed well (Kappa, 0.70) and, importantly, very close to human-level performance (0.75). This research builds on prior findings that GPT-4 could reliably score short answer reading comprehension questions at a performance-level very close to that of expert human raters. The proximity to human-level performance, across a variety of subjects and grade levels suggests that LLMs could be a valuable tool for supporting low-stakes formative assessment tasks in K-12 education and has important implications for real-world education delivery.",
        "timestamp": "2025-03-31T21:22:22.701Z",
        "rating": "novote",
        "publishedDate": "2024/05/05",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2400,
        "object_id": "paper:arxiv.2405.02985",
        "created_at": "2025-03-31T21:22:23+00:00",
        "updated_at": "2025-03-31T21:22:26+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.02058": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.02058",
        "url": "https://arxiv.org/abs/2503.02058",
        "title": "RiboGen: RNA Sequence and Structure Co-Generation with Equivariant MultiFlow",
        "authors": "Rubin, Dana, Costa, Allan dos Santos, Ponnapati, Manvitha, Jacobson, Joseph",
        "abstract": "Ribonucleic acid (RNA) plays fundamental roles in biological systems, from carrying genetic information to performing enzymatic function. Understanding and designing RNA can enable novel therapeutic application and biotechnological innovation. To enhance RNA design, in this paper we introduce RiboGen, the first deep learning model to simultaneously generate RNA sequence and all-atom 3D structure. RiboGen leverages the standard Flow Matching with Discrete Flow Matching in a multimodal data representation. RiboGen is based on Euclidean Equivariant neural networks for efficiently processing and learning three-dimensional geometry. Our experiments show that RiboGen can efficiently generate chemically plausible and self-consistent RNA samples. Our results suggest that co-generation of sequence and structure is a competitive approach for modeling RNA.",
        "timestamp": "2025-03-31T22:24:23.798Z",
        "rating": "novote",
        "publishedDate": "2025/03/03",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2403,
        "object_id": "paper:arxiv.2503.02058",
        "created_at": "2025-03-31T22:24:24+00:00",
        "updated_at": "2025-03-31T22:24:27+00:00",
        "version": 1
      }
    },
    "paper:url.377DF229": {
      "data": {
        "sourceId": "url",
        "paperId": "377DF229",
        "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/85069585133c4c168c865e65d72e9775-Abstract-Conference.html",
        "title": "Break It Down:  Evidence for Structural Compositionality in Neural Networks",
        "authors": "Lepori, Michael, Serre, Thomas, Pavlick, Ellie",
        "abstract": "",
        "timestamp": "2025-03-31T22:24:11.547Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "Advances in Neural Information Processing Systems",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2402,
        "object_id": "paper:url.377DF229",
        "created_at": "2025-03-31T22:24:12+00:00",
        "updated_at": "2025-03-31T22:24:15+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.24159": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.24159",
        "url": "https://arxiv.org/abs/2503.24159",
        "title": "A system level approach to generalised feedback Nash equilibrium seeking in partially-observed games",
        "authors": "Neto, Otacilio B. L., Mulas, Michela, Corona, Francesco",
        "abstract": "This work proposes an algorithm for seeking generalised feedback Nash equilibria (GFNE) in noncooperative dynamic games. The focus is on cyber-physical systems with dynamics which are linear, stochastic, potentially unstable, and partially observed. We employ System Level Synthesis (SLS) to reformulate the problem as the search for an equilibrium profile of closed-loop responses to noise, which can then be used to reconstruct a stabilising output-feedback policy. Under this setup, we leverage monotone operator theory to design a GFNE-seeking algorithm capable to enforce closed-loop stability, operational constraints, and communication constraints onto the control policies. This algorithm is amenable to numerical implementation and we provide conditions for its convergence. We demonstrate our approach in a simulated experiment on the noncooperative stabilisation of a decentralised power-grid.",
        "timestamp": "2025-04-01T05:00:54.888Z",
        "rating": "novote",
        "publishedDate": "2025/03/31",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2410,
        "object_id": "paper:arxiv.2503.24159",
        "created_at": "2025-04-01T05:00:55+00:00",
        "updated_at": "2025-04-01T05:00:58+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.24153": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.24153",
        "url": "https://arxiv.org/abs/2503.24153",
        "title": "Convexity of chance constraints for elliptical and skewed distributions with copula structures dependent on decision variables",
        "authors": "Zhang, Heng, Lisser, Abdel",
        "abstract": "Chance constraints describe a set of given random inequalities depending on the decision vector satisfied with a large enough probability. They are widely used in decision making under uncertain data in many engineering problems. This paper aims to derive the convexity of chance constraints with row dependent elliptical and skewed random variables via a copula depending on decision vectors. We obtain best thresholds of the $r$-concavity for any real number $r$ and improve probability thresholds of the eventual convexity. We prove the eventual convexity with elliptical distributions and a Gumbel-Hougaard copula despite the copula's singularity near the origin. We determine the $\\alpha$-decreasing densities of generalized hyperbolic distributions by estimating the modified Bessel functions. By applying the $\\alpha$-decreasing property and a radial decomposition, we achieve the eventual convexity for three types of skewed distributions. Finally, we provide an example to illustrate the eventual convexity of a feasible set containing the origin.",
        "timestamp": "2025-04-01T05:00:26.204Z",
        "rating": "novote",
        "publishedDate": "2025/03/31",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2408,
        "object_id": "paper:arxiv.2503.24153",
        "created_at": "2025-04-01T05:00:26+00:00",
        "updated_at": "2025-04-01T05:00:29+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.23922": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.23922",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-01T04:58:56.945Z",
            "data": {
              "session_id": "session_1743483536286_2w3i7rr",
              "source_id": "arxiv",
              "paper_id": "2503.23922",
              "start_time": "2025-04-01T04:58:26.090Z",
              "end_time": "2025-04-01T04:58:56.286Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 0,
              "total_elapsed_seconds": 30
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-02T06:42:26.721Z",
            "data": {
              "session_id": "session_1743576146141_fukjke8",
              "source_id": "arxiv",
              "paper_id": "2503.23922",
              "start_time": "2025-04-02T06:42:20.930Z",
              "end_time": "2025-04-02T06:42:26.141Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2407,
        "object_id": "interactions:arxiv.2503.23922",
        "created_at": "2025-04-01T04:58:57+00:00",
        "updated_at": "2025-04-02T06:43:33+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.24044": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.24044",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-01T04:58:26.502Z",
            "data": {
              "session_id": "session_1743483506058_1pn3572",
              "source_id": "arxiv",
              "paper_id": "2503.24044",
              "start_time": "2025-04-01T04:58:20.931Z",
              "end_time": "2025-04-01T04:58:26.058Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2406,
        "object_id": "interactions:arxiv.2503.24044",
        "created_at": "2025-04-01T04:58:27+00:00",
        "updated_at": "2025-04-01T04:59:41+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.24044": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.24044",
        "url": "https://arxiv.org/abs/2503.24044",
        "title": "Bi-Level Route Optimization and Path Planning with Hazard Exploration",
        "authors": "Choi, Jimin, Stagg, Grant, Peterson, Cameron K., Li, Max Z.",
        "abstract": "Effective risk monitoring in dynamic environments such as disaster zones requires an adaptive exploration strategy to detect hidden threats. We propose a bi-level unmanned aerial vehicle (UAV) monitoring strategy that efficiently integrates high-level route optimization with low-level path planning for known and unknown hazards. At the high level, we formulate the route optimization as a vehicle routing problem (VRP) to determine the optimal sequence for visiting known hazard locations. To strategically incorporate exploration efficiency, we introduce an edge-based centroidal Voronoi tessellation (CVT), which refines baseline routes using pseudo-nodes and allocates path budgets based on the UAV's battery capacity using a line segment Voronoi diagram. At the low level, path planning maximizes information gain within the allocated path budget by generating kinematically feasible B-spline trajectories. Bayesian inference is applied to dynamically update hazard probabilities, enabling the UAVs to prioritize unexplored regions. Simulation results demonstrate that edge-based CVT improves spatial coverage and route uniformity compared to the node-based method. Additionally, our optimized path planning consistently outperforms baselines in hazard discovery rates across a diverse set of scenarios.",
        "timestamp": "2025-04-01T04:56:11.153Z",
        "rating": "novote",
        "publishedDate": "2025/03/31",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2405,
        "object_id": "paper:arxiv.2503.24044",
        "created_at": "2025-04-01T04:56:11+00:00",
        "updated_at": "2025-04-01T04:56:14+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.23922": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.23922",
        "url": "https://arxiv.org/abs/2503.23922",
        "title": "Distributionally Robust Model Order Reduction for Linear Systems",
        "authors": "Liu, Le, Kawano, Yu, Dou, Yangming, Cao, Ming",
        "abstract": "In this paper, we investigate distributionally robust model order reduction for linear, discrete-time, time-invariant systems. The external input is assumed to follow an uncertain distribution within a Wasserstein ambiguity set. We begin by considering the case where the distribution is certain and formulate an optimization problem to obtain the reduced model. When the distribution is uncertain, the interaction between the reduced-order model and the distribution is modeled by a Stackelberg game. To ensure solvability, we first introduce the Gelbrich distance and demonstrate that the Stackelberg game within a Wasserstein ambiguity set is equivalent to that within a Gelbrich ambiguity set. Then, we propose a nested optimization problem to solve the Stackelberg game. Furthermore, the nested optimization problem is relaxed into a nested convex optimization problem, ensuring computational feasibility. Finally, a simulation is presented to illustrate the effectiveness of the proposed method.",
        "timestamp": "2025-04-01T04:56:03.066Z",
        "rating": "novote",
        "publishedDate": "2025/03/31",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2404,
        "object_id": "paper:arxiv.2503.23922",
        "created_at": "2025-04-01T04:56:03+00:00",
        "updated_at": "2025-04-01T04:56:06+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.24159": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.24159",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-01T05:06:40.203Z",
            "data": {
              "session_id": "session_1743484000195_e65fx2e",
              "source_id": "arxiv",
              "paper_id": "2503.24159",
              "start_time": "2025-04-01T05:06:32.091Z",
              "end_time": "2025-04-01T05:06:40.195Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-01T05:13:14.203Z",
            "data": {
              "session_id": "session_1743484393774_n4gjjzq",
              "source_id": "arxiv",
              "paper_id": "2503.24159",
              "start_time": "2025-04-01T05:12:17.044Z",
              "end_time": "2025-04-01T05:13:13.774Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 2,
              "total_elapsed_seconds": 57
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2411,
        "object_id": "interactions:arxiv.2503.24159",
        "created_at": "2025-04-01T05:06:12+00:00",
        "updated_at": "2025-04-01T05:14:13+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.21985": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.21985",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-03-31T20:43:46.309Z",
            "data": {
              "session_id": "session_1743453825277_4d1ror9",
              "source_id": "arxiv",
              "paper_id": "2503.21985",
              "start_time": "2025-03-31T20:43:28.669Z",
              "end_time": "2025-03-31T20:43:45.277Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 2,
              "total_elapsed_seconds": 17
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2399,
        "object_id": "interactions:arxiv.2503.21985",
        "created_at": "2025-03-31T20:43:47+00:00",
        "updated_at": "2025-04-01T05:47:24+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2004.09458": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2004.09458",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-01T17:26:04.684Z",
            "data": {
              "session_id": "session_1743528364665_u7zmuby",
              "source_id": "arxiv",
              "paper_id": "2004.09458",
              "start_time": "2025-04-01T17:25:05.118Z",
              "end_time": "2025-04-01T17:26:04.665Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 5,
              "total_elapsed_seconds": 60
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-01T19:26:33.799Z",
            "data": {
              "session_id": "session_1743535593571_207il9u",
              "source_id": "arxiv",
              "paper_id": "2004.09458",
              "start_time": "2025-04-01T19:26:02.640Z",
              "end_time": "2025-04-01T19:26:33.571Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 1,
              "total_elapsed_seconds": 31
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2415,
        "object_id": "interactions:arxiv.2004.09458",
        "created_at": "2025-04-01T17:26:05+00:00",
        "updated_at": "2025-04-01T19:27:35+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2004.09458": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2004.09458",
        "url": "https://arxiv.org/abs/2004.09458",
        "title": "Noise-Induced Randomization in Regression Discontinuity Designs",
        "authors": "Eckles, Dean, Ignatiadis, Nikolaos, Wager, Stefan, Wu, Han",
        "abstract": "Regression discontinuity designs assess causal effects in settings where treatment is determined by whether an observed running variable crosses a pre-specified threshold. Here we propose a new approach to identification, estimation, and inference in regression discontinuity designs that uses knowledge about exogenous noise (e.g., measurement error) in the running variable. In our strategy, we weight treated and control units to balance a latent variable of which the running variable is a noisy measure. Our approach is driven by effective randomization provided by the noise in the running variable, and complements standard formal analyses that appeal to continuity arguments while ignoring the stochastic nature of the assignment mechanism.",
        "timestamp": "2025-04-01T17:24:44.648Z",
        "rating": "novote",
        "publishedDate": "2020/04/20",
        "tags": [],
        "doi": "10.1093/biomet/asaf003",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2414,
        "object_id": "paper:arxiv.2004.09458",
        "created_at": "2025-04-01T17:24:45+00:00",
        "updated_at": "2025-04-01T17:24:48+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.24391": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.24391",
        "url": "https://arxiv.org/abs/2503.24391",
        "title": "Easi3R: Estimating Disentangled Motion from DUSt3R Without Training",
        "authors": "Chen, Xingyu, Chen, Yue, Xiu, Yuliang, Geiger, Andreas, Chen, Anpei",
        "abstract": "Recent advances in DUSt3R have enabled robust estimation of dense point clouds and camera parameters of static scenes, leveraging Transformer network architectures and direct supervision on large-scale 3D datasets. In contrast, the limited scale and diversity of available 4D datasets present a major bottleneck for training a highly generalizable 4D model. This constraint has driven conventional 4D methods to fine-tune 3D models on scalable dynamic video data with additional geometric priors such as optical flow and depths. In this work, we take an opposite path and introduce Easi3R, a simple yet efficient training-free method for 4D reconstruction. Our approach applies attention adaptation during inference, eliminating the need for from-scratch pre-training or network fine-tuning. We find that the attention layers in DUSt3R inherently encode rich information about camera and object motion. By carefully disentangling these attention maps, we achieve accurate dynamic region segmentation, camera pose estimation, and 4D dense point map reconstruction. Extensive experiments on real-world dynamic videos demonstrate that our lightweight attention adaptation significantly outperforms previous state-of-the-art methods that are trained or finetuned on extensive dynamic datasets. Our code is publicly available for research purpose at https://easi3r.github.io/",
        "timestamp": "2025-04-01T06:54:33.436Z",
        "rating": "novote",
        "publishedDate": "2025/03/31",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2413,
        "object_id": "paper:arxiv.2503.24391",
        "created_at": "2025-04-01T06:54:34+00:00",
        "updated_at": "2025-04-01T06:54:37+00:00",
        "version": 1
      }
    },
    "paper:url.74756212": {
      "data": {
        "sourceId": "url",
        "paperId": "74756212",
        "url": "https://snap.stanford.edu/class/cs224w-readings/Newman02Modularity.pdf",
        "title": "74756212",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-01T19:24:30.796Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 2418,
        "object_id": "paper:url.74756212",
        "created_at": "2025-04-01T19:24:31+00:00",
        "updated_at": "2025-04-01T19:24:34+00:00",
        "version": 1
      }
    },
    "interactions:url.690ED10": {
      "data": {
        "sourceId": "url",
        "paperId": "690ED10",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-01T19:24:07.905Z",
            "data": {
              "session_id": "session_1743535447123_41qo2vj",
              "source_id": "url",
              "paper_id": "690ED10",
              "start_time": "2025-04-01T19:23:44.133Z",
              "end_time": "2025-04-01T19:24:07.123Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 3,
              "total_elapsed_seconds": 23
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2417,
        "object_id": "interactions:url.690ED10",
        "created_at": "2025-04-01T19:24:09+00:00",
        "updated_at": "2025-04-01T19:25:13+00:00",
        "version": 1
      }
    },
    "paper:url.690ED10": {
      "data": {
        "sourceId": "url",
        "paperId": "690ED10",
        "url": "https://www.pnas.org/doi/full/10.1073/pnas.0601602103",
        "title": "Modularity and community structure in networks",
        "authors": "Newman, M. E. J.",
        "abstract": "Many networks of interest in the sciences, including social networks, computer networks,\nand metabolic and regulatory networks, are found to divide...",
        "timestamp": "2025-04-01T19:23:44.160Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "clustering",
          "partitioning",
          "modules",
          "metabolic network",
          "social network"
        ],
        "doi": "10.1073/pnas.0601602103",
        "journalName": "Proceedings of the National Academy of Sciences",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2416,
        "object_id": "paper:url.690ED10",
        "created_at": "2025-04-01T19:23:44+00:00",
        "updated_at": "2025-04-01T19:23:47+00:00",
        "version": 1
      }
    },
    "interactions:url.20A2D0BE": {
      "data": {
        "sourceId": "url",
        "paperId": "20A2D0BE",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-01T19:28:25.048Z",
            "data": {
              "session_id": "session_1743535704363_d9zkixs",
              "source_id": "url",
              "paper_id": "20A2D0BE",
              "start_time": "2025-04-01T19:27:20.955Z",
              "end_time": "2025-04-01T19:28:24.363Z",
              "heartbeat_count": 12,
              "duration_seconds": 60,
              "idle_seconds": 3,
              "total_elapsed_seconds": 63
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2421,
        "object_id": "interactions:url.20A2D0BE",
        "created_at": "2025-04-01T19:27:22+00:00",
        "updated_at": "2025-04-01T19:29:25+00:00",
        "version": 1
      }
    },
    "paper:url.20A2D0BE": {
      "data": {
        "sourceId": "url",
        "paperId": "20A2D0BE",
        "url": "https://skewed.de/lab/posts/modularity-harmful/#ref-newman_modularity_2006",
        "title": "Modularity maximization considered harmful \u2013 Tiago P. Peixoto",
        "authors": "Tiago P. Peixoto",
        "abstract": "Inverse Complexity Lab",
        "timestamp": "2025-04-01T19:27:06.745Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2420,
        "object_id": "paper:url.20A2D0BE",
        "created_at": "2025-04-01T19:27:07+00:00",
        "updated_at": "2025-04-01T19:27:10+00:00",
        "version": 1
      }
    },
    "interactions:url.74756212": {
      "data": {
        "sourceId": "url",
        "paperId": "74756212",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-01T19:26:03.604Z",
            "data": {
              "session_id": "session_1743535562640_7kxxxh6",
              "source_id": "url",
              "paper_id": "74756212",
              "start_time": "2025-04-01T19:24:30.407Z",
              "end_time": "2025-04-01T19:26:02.640Z",
              "heartbeat_count": 17,
              "duration_seconds": 85,
              "idle_seconds": 7,
              "total_elapsed_seconds": 92
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2419,
        "object_id": "interactions:url.74756212",
        "created_at": "2025-04-01T19:26:04+00:00",
        "updated_at": "2025-04-01T19:27:08+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2401.13782": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2401.13782",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-01T20:05:03.950Z",
            "data": {
              "session_id": "session_1743537903099_2p238is",
              "source_id": "arxiv",
              "paper_id": "2401.13782",
              "start_time": "2025-04-01T20:04:46.144Z",
              "end_time": "2025-04-01T20:05:03.099Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 2,
              "total_elapsed_seconds": 17
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-01T20:09:07.911Z",
            "data": {
              "session_id": "session_1743538147634_8unffle",
              "source_id": "arxiv",
              "paper_id": "2401.13782",
              "start_time": "2025-04-01T20:08:14.897Z",
              "end_time": "2025-04-01T20:09:07.634Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 3,
              "total_elapsed_seconds": 53
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2424,
        "object_id": "interactions:arxiv.2401.13782",
        "created_at": "2025-04-01T20:05:05+00:00",
        "updated_at": "2025-04-01T20:10:08+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2401.13782": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2401.13782",
        "url": "https://arxiv.org/pdf/2401.13782",
        "title": "2401.13782",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-01T20:04:45.275Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2422,
        "object_id": "paper:arxiv.2401.13782",
        "created_at": "2025-04-01T20:04:45+00:00",
        "updated_at": "2025-04-01T20:04:49+00:00",
        "version": 1
      }
    },
    "interactions:url.6DA213A8": {
      "data": {
        "sourceId": "url",
        "paperId": "6DA213A8",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-01T20:21:33.410Z",
            "data": {
              "session_id": "session_1743538892561_6a90m2e",
              "source_id": "url",
              "paper_id": "6DA213A8",
              "start_time": "2025-04-01T20:21:26.776Z",
              "end_time": "2025-04-01T20:21:32.561Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2427,
        "object_id": "interactions:url.6DA213A8",
        "created_at": "2025-04-01T20:21:34+00:00",
        "updated_at": "2025-04-01T20:22:32+00:00",
        "version": 1
      }
    },
    "paper:url.6DA213A8": {
      "data": {
        "sourceId": "url",
        "paperId": "6DA213A8",
        "url": "https://mcgill-nlp.github.io/thoughtology/Deepseek_R1_Thoughtology.pdf",
        "title": "6DA213A8",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-01T20:21:27.155Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 2426,
        "object_id": "paper:url.6DA213A8",
        "created_at": "2025-04-01T20:21:27+00:00",
        "updated_at": "2025-04-01T20:21:30+00:00",
        "version": 1
      }
    },
    "paper:url.50C5515C": {
      "data": {
        "sourceId": "url",
        "paperId": "50C5515C",
        "url": "https://mcgill-nlp.github.io/thoughtology/",
        "title": "DeepSeek-R1 Thoughtology: Let\u2019s think about LLM reasoning",
        "authors": "McGill NLP Member(s)",
        "abstract": "Large Reasoning Models like DeepSeek-R1 mark a fundamental shift in how LLMs approach complex problems. Instead of directly producing an answer for a given input, DeepSeek-R1 creates detailed multi-step reasoning chains, seemingly \u201cthinking\u201d about a problem before providing an answer. This reasoning process is publicly available to the user, creating endless opportunities for studying the reasoning behaviour of the model and opening up the field of Thoughtology. Starting from a taxonomy of DeepSeek-R1\u2019s basic building blocks of reasoning, our analyses on DeepSeek-R1 investigate the impact and controllability of thought length, management of long or confusing contexts, cultural and safety concerns, and the status of DeepSeek-R1 vis-\u00e0-vis cognitive phenomena, such as human-like language processing and world modelling. Our findings paint a nuanced picture. Notably, we show DeepSeek-R1 has a \u2018sweet spot\u2019 of reasoning, where extra inference time can impair model performance. Furthermore, we find a tendency for DeepSeek-R1 to persistently ruminate on previously explored problem formulations, obstructing further exploration. We also note strong safety vulnerabilities of DeepSeek-R1 compared to its non-reasoning counterpart, which can also compromise safety-aligned LLMs.",
        "timestamp": "2025-04-01T20:21:18.895Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2425,
        "object_id": "paper:url.50C5515C",
        "created_at": "2025-04-01T20:21:19+00:00",
        "updated_at": "2025-04-01T20:21:22+00:00",
        "version": 1
      }
    },
    "interactions:url.79617106": {
      "data": {
        "sourceId": "url",
        "paperId": "79617106",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-01T23:11:15.044Z",
            "data": {
              "session_id": "session_1743549074118_6sjrjyb",
              "source_id": "url",
              "paper_id": "79617106",
              "start_time": "2025-04-01T23:10:03.865Z",
              "end_time": "2025-04-01T23:11:14.118Z",
              "heartbeat_count": 14,
              "duration_seconds": 70,
              "idle_seconds": 0,
              "total_elapsed_seconds": 70
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2429,
        "object_id": "interactions:url.79617106",
        "created_at": "2025-04-01T23:11:16+00:00",
        "updated_at": "2025-04-01T23:12:28+00:00",
        "version": 1
      }
    },
    "paper:url.79617106": {
      "data": {
        "sourceId": "url",
        "paperId": "79617106",
        "url": "https://www.sciencedirect.com/science/article/pii/S2666659621000032?via%3Dihub",
        "title": "Facebook's Project Aria indicates problems for responsible innovation when broadly deploying AR and other pervasive technology in the Commons",
        "authors": "",
        "abstract": "Nearly every week, a technology company is introducing a new surveillance technology, varying from applying facial recognition to observing and catalo\u2026",
        "timestamp": "2025-04-01T23:10:03.038Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1016/j.jrt.2021.100010",
        "journalName": "Journal of Responsible Technology",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2428,
        "object_id": "paper:url.79617106",
        "created_at": "2025-04-01T23:10:03+00:00",
        "updated_at": "2025-04-01T23:10:07+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2502.09716": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.09716",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-02T14:08:12.891Z",
            "data": {
              "session_id": "session_1743602891924_9t4sw2t",
              "source_id": "arxiv",
              "paper_id": "2502.09716",
              "start_time": "2025-04-02T14:07:46.917Z",
              "end_time": "2025-04-02T14:08:11.924Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 15,
              "total_elapsed_seconds": 25
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2433,
        "object_id": "interactions:arxiv.2502.09716",
        "created_at": "2025-04-02T14:08:14+00:00",
        "updated_at": "2025-04-02T14:09:17+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.09716": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.09716",
        "url": "https://arxiv.org/abs/2502.09716",
        "title": "Genetic Data Governance in Crisis: Policy Recommendations for Safeguarding Privacy and Preventing Discrimination",
        "authors": "Ramanan, Vivek, Vinod, Ria, Williams, Cole, Ramachandran, Sohini, Venkatasubramanian, Suresh",
        "abstract": "Genetic data collection has become ubiquitous today. The ability to meaningfully interpret genetic data has motivated its widespread use, providing crucial insights into human health and ancestry while driving important public health initiatives. Easy access to genetic testing has fueled a rapid expansion of recreational direct-to-consumer offerings. However, the growth of genetic datasets and their applications has created significant privacy and discrimination risks, as our understanding of the scientific basis for genetic traits continues to evolve. In this paper, we organize the uses of genetic data along four distinct \"pillars\": clinical practice, research, forensic and government use, and recreational use. Using our scientific understanding of genetics, genetic inference methods and their associated risks, and current public protections, we build a risk assessment framework that identifies key values that any governance system must preserve. We analyze case studies using this framework to assess how well existing regulatory frameworks preserve desired values. Our investigation reveals critical gaps in these frameworks and identifies specific threats to privacy and personal liberties, particularly through genetic discrimination. We propose comprehensive policy reforms to: (1) update the legal definition of genetic data to protect against modern technological capabilities, (2) expand the Genetic Information Nondiscrimination Act (GINA) to cover currently unprotected domains, and (3) establish a unified regulatory framework under a single governing body to oversee all applications of genetic data. We conclude with three open questions about genetic data: the challenges posed by its relational nature, including consent for relatives and minors; the complexities of international data transfer; and its potential integration into large language models.",
        "timestamp": "2025-04-02T14:07:47.970Z",
        "rating": "novote",
        "publishedDate": "2025/02/13",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2432,
        "object_id": "paper:arxiv.2502.09716",
        "created_at": "2025-04-02T14:07:48+00:00",
        "updated_at": "2025-04-02T14:07:52+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.00506": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.00506",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-02T16:32:48.293Z",
            "data": {
              "session_id": "session_1743611567722_svzujpn",
              "source_id": "arxiv",
              "paper_id": "2504.00506",
              "start_time": "2025-04-02T16:32:09.097Z",
              "end_time": "2025-04-02T16:32:47.722Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 4,
              "total_elapsed_seconds": 39
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2435,
        "object_id": "interactions:arxiv.2504.00506",
        "created_at": "2025-04-02T16:32:49+00:00",
        "updated_at": "2025-04-02T16:33:49+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.00506": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.00506",
        "url": "https://arxiv.org/abs/2504.00506",
        "title": "Higher multipoles of the cow",
        "authors": "Lehmann, Benjamin V.",
        "abstract": "The spherical cow approximation is widely used in the literature, but is rarely justified. Here, I propose several schemes for extending the spherical cow approximation to a full multipole expansion, in which the spherical cow is simply the first term. This allows for the computation of bovine potentials and interactions beyond spherical symmetry, and also provides a scheme for defining the geometry of the cow itself at higher multipole moments. This is especially important for the treatment of physical processes that are suppressed by spherical symmetry, such as the spindown of a rotating cow due to the emission of gravitational waves. I demonstrate the computation of multipole coefficients for a benchmark cow, and illustrate the applicability of the multipolar cow to several important problems.",
        "timestamp": "2025-04-02T16:00:25.632Z",
        "rating": "novote",
        "publishedDate": "2025/04/01",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2434,
        "object_id": "paper:arxiv.2504.00506",
        "created_at": "2025-04-02T16:00:26+00:00",
        "updated_at": "2025-04-02T20:31:16+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.00927": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.00927",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-03T13:18:32.862Z",
            "data": {
              "session_id": "session_1743686311855_n2r7c9c",
              "source_id": "arxiv",
              "paper_id": "2504.00927",
              "start_time": "2025-04-03T13:17:38.049Z",
              "end_time": "2025-04-03T13:18:31.855Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 4,
              "total_elapsed_seconds": 54
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2446,
        "object_id": "interactions:arxiv.2504.00927",
        "created_at": "2025-04-03T13:18:34+00:00",
        "updated_at": "2025-04-03T13:19:42+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.00927": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.00927",
        "url": "https://arxiv.org/abs/2504.00927",
        "title": "Multi-Token Attention",
        "authors": "Golovneva, Olga, Wang, Tianlu, Weston, Jason, Sukhbaatar, Sainbayar",
        "abstract": "Soft attention is a critical mechanism powering LLMs to locate relevant parts within a given context. However, individual attention weights are determined by the similarity of only a single query and key token vector. This \"single token attention\" bottlenecks the amount of information used in distinguishing a relevant part from the rest of the context. To address this issue, we propose a new attention method, Multi-Token Attention (MTA), which allows LLMs to condition their attention weights on multiple query and key vectors simultaneously. This is achieved by applying convolution operations over queries, keys and heads, allowing nearby queries and keys to affect each other's attention weights for more precise attention. As a result, our method can locate relevant context using richer, more nuanced information that can exceed a single vector's capacity. Through extensive evaluations, we demonstrate that MTA achieves enhanced performance on a range of popular benchmarks. Notably, it outperforms Transformer baseline models on standard language modeling tasks, and on tasks that require searching for information within long contexts, where our method's ability to leverage richer information proves particularly beneficial.",
        "timestamp": "2025-04-03T13:17:38.864Z",
        "rating": "novote",
        "publishedDate": "2025/04/01",
        "tags": [
          "cs.CL"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2445,
        "object_id": "paper:arxiv.2504.00927",
        "created_at": "2025-04-03T13:17:39+00:00",
        "updated_at": "2025-04-07T02:24:52+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.01840": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.01840",
        "url": "https://arxiv.org/abs/2504.01840",
        "title": "LARGE: Legal Retrieval Augmented Generation Evaluation Tool",
        "authors": "Park, Minhu, Oh, Hongseok, Choi, Eunkyung, Hwang, Wonseok",
        "abstract": "Recently, building retrieval-augmented generation (RAG) systems to enhance the capability of large language models (LLMs) has become a common practice. Especially in the legal domain, previous judicial decisions play a significant role under the doctrine of stare decisis which emphasizes the importance of making decisions based on (retrieved) prior documents. However, the overall performance of RAG system depends on many components: (1) retrieval corpora, (2) retrieval algorithms, (3) rerankers, (4) LLM backbones, and (5) evaluation metrics. Here we propose LRAGE, an open-source tool for holistic evaluation of RAG systems focusing on the legal domain. LRAGE provides GUI and CLI interfaces to facilitate seamless experiments and investigate how changes in the aforementioned five components affect the overall accuracy. We validated LRAGE using multilingual legal benches including Korean (KBL), English (LegalBench), and Chinese (LawBench) by demonstrating how the overall accuracy changes when varying the five components mentioned above. The source code is available at https://github.com/hoorangyee/LRAGE.",
        "timestamp": "2025-04-03T05:03:39.517Z",
        "rating": "novote",
        "publishedDate": "2025/04/02",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2442,
        "object_id": "paper:arxiv.2504.01840",
        "created_at": "2025-04-03T05:03:39+00:00",
        "updated_at": "2025-04-03T05:03:42+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.01017": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.01017",
        "url": "https://arxiv.org/abs/2504.01017",
        "title": "Scaling Language-Free Visual Representation Learning",
        "authors": "Fan, David, Tong, Shengbang, Zhu, Jiachen, Sinha, Koustuv, Liu, Zhuang, Chen, Xinlei, Rabbat, Michael, Ballas, Nicolas, LeCun, Yann, Bar, Amir, Xie, Saining",
        "abstract": "Visual Self-Supervised Learning (SSL) currently underperforms Contrastive Language-Image Pretraining (CLIP) in multimodal settings such as Visual Question Answering (VQA). This multimodal gap is often attributed to the semantics introduced by language supervision, even though visual SSL and CLIP models are often trained on different data. In this work, we ask the question: \"Do visual self-supervised approaches lag behind CLIP due to the lack of language supervision, or differences in the training data?\" We study this question by training both visual SSL and CLIP models on the same MetaCLIP data, and leveraging VQA as a diverse testbed for vision encoders. In this controlled setup, visual SSL models scale better than CLIP models in terms of data and model capacity, and visual SSL performance does not saturate even after scaling up to 7B parameters. Consequently, we observe visual SSL methods achieve CLIP-level performance on a wide range of VQA and classic vision benchmarks. These findings demonstrate that pure visual SSL can match language-supervised visual pretraining at scale, opening new opportunities for vision-centric representation learning.",
        "timestamp": "2025-04-03T04:44:46.049Z",
        "rating": "novote",
        "publishedDate": "2025/04/01",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2440,
        "object_id": "paper:arxiv.2504.01017",
        "created_at": "2025-04-03T04:44:46+00:00",
        "updated_at": "2025-04-03T04:44:49+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.16992": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.16992",
        "url": "https://arxiv.org/abs/2503.16992",
        "title": "Friend or Foe? Navigating and Re-configuring \"Snipers' Alley\"",
        "authors": "Dwyer, Andrew C, Coles-Kemp, Lizzie, Crivellaro, Clara, Heath, Claude P R",
        "abstract": "In a 'digital by default' society, essential services must be accessed online. This opens users to digital deception not only from criminal fraudsters but from a range of actors in a marketised digital economy. Using grounded empirical research from northern England, we show how supposedly 'trusted' actors, such as governments,(re)produce the insecurities and harms that they seek to prevent. Enhanced by a weakening of social institutions amid a drive for efficiency and scale, this has built a constricted, unpredictable digital channel. We conceptualise this as a \"snipers' alley\". Four key snipers articulated by participants' lived experiences are examined: 1) Governments; 2) Business; 3) Criminal Fraudsters; and 4) Friends and Family to explore how snipers are differentially experienced and transfigure through this constricted digital channel. We discuss strategies to re-configure the alley, and how crafting and adopting opportunity models can enable more equitable forms of security for all.",
        "timestamp": "2025-04-02T23:46:54.669Z",
        "rating": "novote",
        "publishedDate": "2025/03/21",
        "tags": [
          "cs.HC",
          "cs.CR"
        ],
        "doi": "10.1145/3706598.3713317",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2438,
        "object_id": "paper:arxiv.2503.16992",
        "created_at": "2025-04-02T23:46:55+00:00",
        "updated_at": "2025-04-07T06:38:19+00:00",
        "version": 1
      }
    },
    "interactions:url.739894CC": {
      "data": {
        "sourceId": "url",
        "paperId": "739894CC",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-03T14:13:06.413Z",
            "data": {
              "session_id": "session_1743689585344_grw35ut",
              "source_id": "url",
              "paper_id": "739894CC",
              "start_time": "2025-04-03T14:11:24.950Z",
              "end_time": "2025-04-03T14:13:05.344Z",
              "heartbeat_count": 20,
              "duration_seconds": 100,
              "idle_seconds": 0,
              "total_elapsed_seconds": 100
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2448,
        "object_id": "interactions:url.739894CC",
        "created_at": "2025-04-03T14:13:07+00:00",
        "updated_at": "2025-04-03T14:14:21+00:00",
        "version": 1
      }
    },
    "paper:url.739894CC": {
      "data": {
        "sourceId": "url",
        "paperId": "739894CC",
        "url": "https://www.researchgate.net/publication/378142766_Order_parameter_dynamics_in_complex_systems_From_models_to_data",
        "title": "(PDF) Order parameter dynamics in complex systems: From models to data",
        "authors": "",
        "abstract": "PDF | Collective ordering behaviors are typical macroscopic manifestations embedded in complex systems and can be ubiquitously observed across various... | Find, read and cite all the research you need on ResearchGate",
        "timestamp": "2025-04-03T14:11:24.734Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2447,
        "object_id": "paper:url.739894CC",
        "created_at": "2025-04-03T14:11:25+00:00",
        "updated_at": "2025-04-03T14:11:28+00:00",
        "version": 1
      }
    },
    "interactions:url.2C4A821D": {
      "data": {
        "sourceId": "url",
        "paperId": "2C4A821D",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-04T04:00:58.685Z",
            "data": {
              "session_id": "session_1743739257680_tqkqjyb",
              "source_id": "url",
              "paper_id": "2C4A821D",
              "start_time": "2025-04-04T04:00:33.980Z",
              "end_time": "2025-04-04T04:00:57.680Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 4,
              "total_elapsed_seconds": 24
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2452,
        "object_id": "interactions:url.2C4A821D",
        "created_at": "2025-04-04T04:00:59+00:00",
        "updated_at": "2025-04-04T04:02:05+00:00",
        "version": 1
      }
    },
    "paper:url.2C4A821D": {
      "data": {
        "sourceId": "url",
        "paperId": "2C4A821D",
        "url": "https://www.science.org/doi/10.1126/science.adu0047",
        "title": "Tissue-like multicellular development triggered by mechanical compression in archaea",
        "authors": "",
        "abstract": "The advent of clonal multicellularity is a critical evolutionary milestone, seen often in eukaryotes, rarely in bacteria, and only once in archaea. We show that uniaxial compression induces clonal multicellularity in haloarchaea, forming tissue-like ...",
        "timestamp": "2025-04-04T04:00:33.167Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "Science",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2451,
        "object_id": "paper:url.2C4A821D",
        "created_at": "2025-04-04T04:00:33+00:00",
        "updated_at": "2025-04-04T04:00:36+00:00",
        "version": 1
      }
    },
    "interactions:url.5B4C1B56": {
      "data": {
        "sourceId": "url",
        "paperId": "5B4C1B56",
        "interactions": []
      },
      "meta": {
        "issue_number": 2450,
        "object_id": "interactions:url.5B4C1B56",
        "created_at": "2025-04-04T00:22:32+00:00",
        "updated_at": "2025-04-04T00:22:35+00:00",
        "version": 1
      }
    },
    "paper:url.5B4C1B56": {
      "data": {
        "sourceId": "url",
        "paperId": "5B4C1B56",
        "url": "https://jods.mitpress.mit.edu/pub/ic90uta1/release/4",
        "title": "To Unreality\u2014and Beyond",
        "authors": "Peter Pomerantsev",
        "abstract": "An examination of the \u201cpropaganda of unreality\u201d and why age-old principles of resistance to manipulation don\u2019t work against today\u2019s style of unreality.",
        "timestamp": "2025-04-04T00:15:30.289Z",
        "rating": "novote",
        "publishedDate": "Wed Oct 23 2019 19:36:37 GMT+0000 (Coordinated Universal Time)",
        "tags": [],
        "doi": "doi:10.21428/7808da6b.274f05e6",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2449,
        "object_id": "paper:url.5B4C1B56",
        "created_at": "2025-04-04T00:15:30+00:00",
        "updated_at": "2025-04-04T00:15:33+00:00",
        "version": 1
      }
    },
    "interactions:url.430A8C32": {
      "data": {
        "sourceId": "url",
        "paperId": "430A8C32",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-04T15:30:05.370Z",
            "data": {
              "session_id": "session_1743780604494_13wvm2x",
              "source_id": "url",
              "paper_id": "430A8C32",
              "start_time": "2025-04-04T15:29:59.250Z",
              "end_time": "2025-04-04T15:30:04.494Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2454,
        "object_id": "interactions:url.430A8C32",
        "created_at": "2025-04-04T15:30:06+00:00",
        "updated_at": "2025-04-04T15:31:08+00:00",
        "version": 1
      }
    },
    "paper:url.430A8C32": {
      "data": {
        "sourceId": "url",
        "paperId": "430A8C32",
        "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4778120",
        "title": "Social Media and Job Market Success: A Field Experiment on Twitter",
        "authors": "Qiu, Jingyi, Chen, Yan, Cohn, Alain, Roth, Alvin E.",
        "abstract": "To examine the impact of social media promotion of job market papers on job outcomes, we conduct a field experiment on Twitter (now X). Specifically, we examine",
        "timestamp": "2025-04-04T15:29:58.908Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "SSRN",
          "Social Media and Job Market Success: A Field Experiment on Twitter",
          "Jingyi Qiu",
          "Yan Chen",
          "Alain Cohn",
          "Alvin E. Roth"
        ],
        "doi": "10.2139/ssrn.4778120",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2453,
        "object_id": "paper:url.430A8C32",
        "created_at": "2025-04-04T15:29:59+00:00",
        "updated_at": "2025-04-04T15:30:03+00:00",
        "version": 1
      }
    },
    "paper:url.39EA95AC": {
      "data": {
        "sourceId": "url",
        "paperId": "39EA95AC",
        "url": "https://philpapers.org/archive/GRIDSH.pdf",
        "title": "39EA95AC",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-04T20:36:04.185Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 2458,
        "object_id": "paper:url.39EA95AC",
        "created_at": "2025-04-04T20:36:04+00:00",
        "updated_at": "2025-04-04T20:36:07+00:00",
        "version": 1
      }
    },
    "paper:url.6AD940C2": {
      "data": {
        "sourceId": "url",
        "paperId": "6AD940C2",
        "url": "https://philpapers.org/rec/GRIDSH",
        "title": "Distributional Semantics, Holism, and the Instability of Meaning",
        "authors": "Jumbly Grindrod, J. D. Porter, Nat Hansen",
        "abstract": "Large Language Models are built on the so-called distributional semantic approach to linguistic meaning that has the distributional hypothesis at its core. The distributional hypothesis involves a holistic conception of word ...",
        "timestamp": "2025-04-04T20:35:59.644Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "distributional semantics;large language models;meaning holism;instability;experimental philosophy of language"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2457,
        "object_id": "paper:url.6AD940C2",
        "created_at": "2025-04-04T20:36:00+00:00",
        "updated_at": "2025-04-04T20:36:02+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2408.15925": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2408.15925",
        "url": "https://arxiv.org/abs/2408.15925",
        "title": "Explicit Folded Reed-Solomon and Multiplicity Codes Achieve Relaxed Generalized Singleton Bounds",
        "authors": "Chen, Yeyuan, Zhang, Zihan",
        "abstract": "In this paper, we prove that explicit FRS codes and multiplicity codes achieve relaxed generalized Singleton bounds for list size $L\\ge1.$ Specifically, we show the following: (1) FRS code of length $n$ and rate $R$ over the alphabet $\\mathbb{F}_q^s$ with distinct evaluation points is $\\left(\\frac{L}{L+1}\\left(1-\\frac{sR}{s-L+1}\\right),L\\right)$ list-decodable (LD) for list size $L\\in[s]$. (2) Multiplicity code of length $n$ and rate $R$ over the alphabet $\\mathbb{F}_p^s$ with distinct evaluation points is $\\left(\\frac{L}{L+1}\\left(1-\\frac{sR}{s-L+1}\\right),L\\right)$ LD for list size $L\\in[s]$. Choosing $s=\\Theta(1/\\epsilon^2)$ and $L=O(1/\\epsilon)$, our results imply that both FRS codes and multiplicity codes achieve LD capacity $1-R-\\epsilon$ with optimal list size $O(1/\\epsilon)$. This exponentially improves the previous state of the art $(1/\\epsilon)^{O(1/\\epsilon)}$ established by Kopparty et. al. (FOCS 2018) and Tamo (IEEE TIT, 2024). In particular, our results on FRS codes fully resolve a open problem proposed by Guruswami and Rudra (STOC 2006). Furthermore, our results imply the first explicit constructions of $(1-R-\\epsilon,O(1/\\epsilon))$ LD codes of rate $R$ with poly-sized alphabets. Our method can also be extended to analyze the list-recoverability (LR) of FRS codes. We provide a tighter radius upper bound that FRS codes cannot be $(\\frac{L+1-\\ell}{L+1}(1-\\frac{mR}{m-1})+o(1),\\ell, L)$ LR where $m=\\lceil\\log_{\\ell}{(L+1)}\\rceil$. We conjecture this bound is almost tight when $L+1=\\ell^a$ for any $a\\in\\mathbb{N}^{\\ge 2}$. To give some evidences, we show FRS codes are $\\left(\\frac{1}{2}-\\frac{sR}{s-2},2,3\\right)$ LR, which proves the tightness in the smallest non-trivial case. Our bound refutes the possibility that FRS codes could achieve LR capacity $(1-R-\\epsilon, \\ell, O(\\frac{\\ell}{\\epsilon}))$. This implies an intrinsic separation between LD and LR of FRS codes.",
        "timestamp": "2025-04-04T20:30:56.055Z",
        "rating": "novote",
        "publishedDate": "2024/08/28",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2456,
        "object_id": "paper:arxiv.2408.15925",
        "created_at": "2025-04-04T20:30:56+00:00",
        "updated_at": "2025-04-04T20:30:59+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.20098": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.20098",
        "url": "https://arxiv.org/abs/2503.20098",
        "title": "Fundamental Limits of Perfect Concept Erasure",
        "authors": "Chowdhury, Somnath Basu Roy, Dubey, Avinava, Beirami, Ahmad, Kidambi, Rahul, Monath, Nicholas, Ahmed, Amr, Chaturvedi, Snigdha",
        "abstract": "Concept erasure is the task of erasing information about a concept (e.g., gender or race) from a representation set while retaining the maximum possible utility -- information from original representations. Concept erasure is useful in several applications, such as removing sensitive concepts to achieve fairness and interpreting the impact of specific concepts on a model's performance. Previous concept erasure techniques have prioritized robustly erasing concepts over retaining the utility of the resultant representations. However, there seems to be an inherent tradeoff between erasure and retaining utility, making it unclear how to achieve perfect concept erasure while maintaining high utility. In this paper, we offer a fresh perspective toward solving this problem by quantifying the fundamental limits of concept erasure through an information-theoretic lens. Using these results, we investigate constraints on the data distribution and the erasure functions required to achieve the limits of perfect concept erasure. Empirically, we show that the derived erasure functions achieve the optimal theoretical bounds. Additionally, we show that our approach outperforms existing methods on a range of synthetic and real-world datasets using GPT-4 representations.",
        "timestamp": "2025-04-04T20:28:41.369Z",
        "rating": "novote",
        "publishedDate": "2025/03/25",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2455,
        "object_id": "paper:arxiv.2503.20098",
        "created_at": "2025-04-04T20:28:41+00:00",
        "updated_at": "2025-04-04T20:28:44+00:00",
        "version": 1
      }
    },
    "interactions:url.3606F333": {
      "data": {
        "sourceId": "url",
        "paperId": "3606F333",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-04T20:48:39.760Z",
            "data": {
              "session_id": "session_1743799719006_4zlatj2",
              "source_id": "url",
              "paper_id": "3606F333",
              "start_time": "2025-04-04T20:48:30.463Z",
              "end_time": "2025-04-04T20:48:39.006Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2460,
        "object_id": "interactions:url.3606F333",
        "created_at": "2025-04-04T20:48:40+00:00",
        "updated_at": "2025-04-04T20:49:41+00:00",
        "version": 1
      }
    },
    "paper:url.3606F333": {
      "data": {
        "sourceId": "url",
        "paperId": "3606F333",
        "url": "https://www.cs.unc.edu/~somnath/blogs/pef",
        "title": "[Somnath Basu Roy Chowdhury]",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-04T20:48:29.453Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2459,
        "object_id": "paper:url.3606F333",
        "created_at": "2025-04-04T20:48:29+00:00",
        "updated_at": "2025-04-04T20:48:32+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2406.15881": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.15881",
        "url": "https://arxiv.org/abs/2406.15881",
        "title": "Fast Tree-Field Integrators: From Low Displacement Rank to Topological Transformers",
        "authors": "Choromanski, Krzysztof, Sehanobish, Arijit, Chowdhury, Somnath Basu Roy, Lin, Han, Dubey, Avinava, Sarlos, Tamas, Chaturvedi, Snigdha",
        "abstract": "We present a new class of fast polylog-linear algorithms based on the theory of structured matrices (in particular low displacement rank) for integrating tensor fields defined on weighted trees. Several applications of the resulting fast tree-field integrators (FTFIs) are presented, including (a) approximation of graph metrics with tree metrics, (b) graph classification, (c) modeling on meshes, and finally (d) Topological Transformers (TTs) (Choromanski et al., 2022) for images. For Topological Transformers, we propose new relative position encoding (RPE) masking mechanisms with as few as three extra learnable parameters per Transformer layer, leading to 1.0-1.5%+ accuracy gains. Importantly, most of FTFIs are exact methods, thus numerically equivalent to their brute-force counterparts. When applied to graphs with thousands of nodes, those exact algorithms provide 5.7-13x speedups. We also provide an extensive theoretical analysis of our methods.",
        "timestamp": "2025-04-04T20:54:55.959Z",
        "rating": "novote",
        "publishedDate": "2024/06/22",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2464,
        "object_id": "paper:arxiv.2406.15881",
        "created_at": "2025-04-04T20:54:56+00:00",
        "updated_at": "2025-04-04T20:54:59+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2406.17740": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.17740",
        "url": "https://arxiv.org/abs/2406.17740",
        "title": "Structured Unrestricted-Rank Matrices for Parameter Efficient Fine-tuning",
        "authors": "Sehanobish, Arijit, Dubey, Avinava, Choromanski, Krzysztof, Chowdhury, Somnath Basu Roy, Jain, Deepali, Sindhwani, Vikas, Chaturvedi, Snigdha",
        "abstract": "Recent efforts to scale Transformer models have demonstrated rapid progress across a wide range of tasks (Wei et al., 2022). However, fine-tuning these models for downstream tasks is expensive due to their large parameter counts. Parameter-efficient fine-tuning (PEFT) approaches have emerged as a viable alternative by allowing us to fine-tune models by updating only a small number of parameters. In this work, we propose a general framework for parameter efficient fine-tuning (PEFT), based on structured unrestricted-rank matrices (SURM) which can serve as a drop-in replacement for popular approaches such as Adapters and LoRA. Unlike other methods like LoRA, SURMs provides more flexibility in finding the right balance between compactness and expressiveness. This is achieved by using low displacement rank matrices (LDRMs), which hasn't been used in this context before. SURMs remain competitive with baselines, often providing significant quality improvements while using a smaller parameter budget. SURMs achieve 5-7% accuracy gains on various image classification tasks while replacing low-rank matrices in LoRA. It also results in up to 12x reduction of the number of parameters in adapters (with virtually no loss in quality) on the GLUE benchmark.",
        "timestamp": "2025-04-04T20:54:50.783Z",
        "rating": "novote",
        "publishedDate": "2024/06/25",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2463,
        "object_id": "paper:arxiv.2406.17740",
        "created_at": "2025-04-04T20:54:51+00:00",
        "updated_at": "2025-04-04T20:54:53+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2406.16257": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.16257",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-04T20:53:39.508Z",
            "data": {
              "session_id": "session_1743800018724_1aewtqg",
              "source_id": "arxiv",
              "paper_id": "2406.16257",
              "start_time": "2025-04-04T20:53:04.693Z",
              "end_time": "2025-04-04T20:53:38.724Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 4,
              "total_elapsed_seconds": 34
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2462,
        "object_id": "interactions:arxiv.2406.16257",
        "created_at": "2025-04-04T20:53:40+00:00",
        "updated_at": "2025-04-04T20:54:43+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2406.16257": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.16257",
        "url": "https://arxiv.org/abs/2406.16257",
        "title": "Towards Scalable Exact Machine Unlearning Using Parameter-Efficient Fine-Tuning",
        "authors": "Chowdhury, Somnath Basu Roy, Choromanski, Krzysztof, Sehanobish, Arijit, Dubey, Avinava, Chaturvedi, Snigdha",
        "abstract": "Machine unlearning is the process of efficiently removing the influence of a training data instance from a trained machine learning model without retraining it from scratch. A popular subclass of unlearning approaches is exact machine unlearning, which focuses on techniques that explicitly guarantee the removal of the influence of a data instance from a model. Exact unlearning approaches use a machine learning model in which individual components are trained on disjoint subsets of the data. During deletion, exact unlearning approaches only retrain the affected components rather than the entire model. While existing approaches reduce retraining costs, it can still be expensive for an organization to retrain a model component as it requires halting a system in production, which leads to service failure and adversely impacts customers. To address these challenges, we introduce an exact unlearning framework -- Sequence-aware Sharded Sliced Training (S3T), which is designed to enhance the deletion capabilities of an exact unlearning system while minimizing the impact on model's performance. At the core of S3T, we utilize a lightweight parameter-efficient fine-tuning approach that enables parameter isolation by sequentially training layers with disjoint data slices. This enables efficient unlearning by simply deactivating the layers affected by data deletion. Furthermore, to reduce the retraining cost and improve model performance, we train the model on multiple data sequences, which allows S3T to handle an increased number of deletion requests. Both theoretically and empirically, we demonstrate that S3T attains superior deletion capabilities and enhanced performance compared to baselines across a wide range of settings.",
        "timestamp": "2025-04-04T20:53:05.344Z",
        "rating": "novote",
        "publishedDate": "2024/06/24",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2461,
        "object_id": "paper:arxiv.2406.16257",
        "created_at": "2025-04-04T20:53:05+00:00",
        "updated_at": "2025-04-04T20:53:08+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2406.15881": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.15881",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-04T20:57:48.887Z",
            "data": {
              "session_id": "session_1743800268874_r3ibxvu",
              "source_id": "arxiv",
              "paper_id": "2406.15881",
              "start_time": "2025-04-04T20:57:40.350Z",
              "end_time": "2025-04-04T20:57:48.874Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2466,
        "object_id": "interactions:arxiv.2406.15881",
        "created_at": "2025-04-04T20:57:49+00:00",
        "updated_at": "2025-04-04T20:58:59+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2406.17740": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.17740",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-04T20:57:33.096Z",
            "data": {
              "session_id": "session_1743800252437_kwaw6jy",
              "source_id": "arxiv",
              "paper_id": "2406.17740",
              "start_time": "2025-04-04T20:54:56.103Z",
              "end_time": "2025-04-04T20:57:32.437Z",
              "heartbeat_count": 31,
              "duration_seconds": 155,
              "idle_seconds": 1,
              "total_elapsed_seconds": 156
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2465,
        "object_id": "interactions:arxiv.2406.17740",
        "created_at": "2025-04-04T20:57:34+00:00",
        "updated_at": "2025-04-04T20:58:38+00:00",
        "version": 1
      }
    },
    "interactions:url.706ED175": {
      "data": {
        "sourceId": "url",
        "paperId": "706ED175",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-04T21:25:43.738Z",
            "data": {
              "session_id": "session_1743801942933_f83e2qn",
              "source_id": "url",
              "paper_id": "706ED175",
              "start_time": "2025-04-04T21:25:37.357Z",
              "end_time": "2025-04-04T21:25:42.933Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2468,
        "object_id": "interactions:url.706ED175",
        "created_at": "2025-04-04T21:25:44+00:00",
        "updated_at": "2025-04-04T21:26:44+00:00",
        "version": 1
      }
    },
    "paper:url.706ED175": {
      "data": {
        "sourceId": "url",
        "paperId": "706ED175",
        "url": "https://www.journalofaccountancy.com/issues/2022/nov/amortizing-r-e-expenditures-under-tcja/",
        "title": "Amortizing R&E expenditures under the TCJA - Journal of Accountancy",
        "authors": "By Richard Ray, CPA, Ph.D.",
        "abstract": "The change this year from immediate expensing under Sec. 174 sends ripples through affected taxpayers\u2019 returns and may affect financial reporting.",
        "timestamp": "2025-04-04T21:25:37.778Z",
        "rating": "novote",
        "publishedDate": "2022-11-01T05:00:00-04:00",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2467,
        "object_id": "paper:url.706ED175",
        "created_at": "2025-04-04T21:25:38+00:00",
        "updated_at": "2025-04-04T21:25:40+00:00",
        "version": 1
      }
    },
    "interactions:url.29CA2781": {
      "data": {
        "sourceId": "url",
        "paperId": "29CA2781",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-04T21:52:46.650Z",
            "data": {
              "session_id": "session_1743803565620_djdm3y3",
              "source_id": "url",
              "paper_id": "29CA2781",
              "start_time": "2025-04-04T21:51:54.494Z",
              "end_time": "2025-04-04T21:52:45.620Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 1,
              "total_elapsed_seconds": 51
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2471,
        "object_id": "interactions:url.29CA2781",
        "created_at": "2025-04-04T21:52:47+00:00",
        "updated_at": "2025-04-04T21:53:47+00:00",
        "version": 1
      }
    },
    "paper:url.29CA2781": {
      "data": {
        "sourceId": "url",
        "paperId": "29CA2781",
        "url": "https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf",
        "title": "29CA2781",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-04T21:51:54.865Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 2470,
        "object_id": "paper:url.29CA2781",
        "created_at": "2025-04-04T21:51:55+00:00",
        "updated_at": "2025-04-04T21:51:58+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.16948": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.16948",
        "url": "https://arxiv.org/pdf/2503.16948",
        "title": "2503.16948",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-04T21:41:45.753Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2469,
        "object_id": "paper:arxiv.2503.16948",
        "created_at": "2025-04-04T21:41:46+00:00",
        "updated_at": "2025-04-04T21:41:49+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2410.04596": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.04596",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-05T00:17:05.941Z",
            "data": {
              "session_id": "session_1743812225049_urrqrjt",
              "source_id": "arxiv",
              "paper_id": "2410.04596",
              "start_time": "2025-04-05T00:16:43.795Z",
              "end_time": "2025-04-05T00:17:05.049Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 1,
              "total_elapsed_seconds": 21
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2474,
        "object_id": "interactions:arxiv.2410.04596",
        "created_at": "2025-04-05T00:17:07+00:00",
        "updated_at": "2025-04-05T00:18:13+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2410.04596": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.04596",
        "url": "https://arxiv.org/abs/2410.04596",
        "title": "Need Help? Designing Proactive AI Assistants for Programming",
        "authors": "Chen, Valerie, Zhu, Alan, Zhao, Sebastian, Mozannar, Hussein, Sontag, David, Talwalkar, Ameet",
        "abstract": "While current chat-based AI assistants primarily operate reactively, responding only when prompted by users, there is significant potential for these systems to proactively assist in tasks without explicit invocation, enabling a mixed-initiative interaction. This work explores the design and implementation of proactive AI assistants powered by large language models. We first outline the key design considerations for building effective proactive assistants. As a case study, we propose a proactive chat-based programming assistant that automatically provides suggestions and facilitates their integration into the programmer's code. The programming context provides a shared workspace enabling the assistant to offer more relevant suggestions. We conducted a randomized experimental study examining the impact of various design elements of the proactive assistant on programmer productivity and user experience. Our findings reveal significant benefits of incorporating proactive chat assistants into coding environments and uncover important nuances that influence their usage and effectiveness.",
        "timestamp": "2025-04-05T00:16:44.536Z",
        "rating": "novote",
        "publishedDate": "2024/10/06",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2473,
        "object_id": "paper:arxiv.2410.04596",
        "created_at": "2025-04-05T00:16:45+00:00",
        "updated_at": "2025-04-05T00:16:47+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2206.13901": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2206.13901",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-05T00:32:55.253Z",
            "data": {
              "session_id": "session_1743813174122_g2r6mz8",
              "source_id": "arxiv",
              "paper_id": "2206.13901",
              "start_time": "2025-04-05T00:31:16.033Z",
              "end_time": "2025-04-05T00:32:54.122Z",
              "heartbeat_count": 19,
              "duration_seconds": 95,
              "idle_seconds": 3,
              "total_elapsed_seconds": 98
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2476,
        "object_id": "interactions:arxiv.2206.13901",
        "created_at": "2025-04-05T00:32:56+00:00",
        "updated_at": "2025-04-05T00:33:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2206.13901": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2206.13901",
        "url": "https://arxiv.org/abs/2206.13901",
        "title": "Value Function Decomposition for Iterative Design of Reinforcement Learning Agents",
        "authors": "MacGlashan, James, Archer, Evan, Devlic, Alisa, Seno, Takuma, Sherstan, Craig, Wurman, Peter R., Stone, Peter",
        "abstract": "Designing reinforcement learning (RL) agents is typically a difficult process that requires numerous design iterations. Learning can fail for a multitude of reasons, and standard RL methods provide too few tools to provide insight into the exact cause. In this paper, we show how to integrate value decomposition into a broad class of actor-critic algorithms and use it to assist in the iterative agent-design process. Value decomposition separates a reward function into distinct components and learns value estimates for each. These value estimates provide insight into an agent's learning and decision-making process and enable new training methods to mitigate common problems. As a demonstration, we introduce SAC-D, a variant of soft actor-critic (SAC) adapted for value decomposition. SAC-D maintains similar performance to SAC, while learning a larger set of value predictions. We also introduce decomposition-based tools that exploit this information, including a new reward influence metric, which measures each reward component's effect on agent decision-making. Using these tools, we provide several demonstrations of decomposition's use in identifying and addressing problems in the design of both environments and agents. Value decomposition is broadly applicable and easy to incorporate into existing algorithms and workflows, making it a powerful tool in an RL practitioner's toolbox.",
        "timestamp": "2025-04-05T00:31:16.557Z",
        "rating": "novote",
        "publishedDate": "2022/06/24",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2475,
        "object_id": "paper:arxiv.2206.13901",
        "created_at": "2025-04-05T00:31:17+00:00",
        "updated_at": "2025-04-05T00:31:19+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.02495": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.02495",
        "url": "https://arxiv.org/abs/2504.02495",
        "title": "Inference-Time Scaling for Generalist Reward Modeling",
        "authors": "Liu, Zijun, Wang, Peiyi, Xu, Runxin, Ma, Shirong, Ruan, Chong, Li, Peng, Liu, Yang, Wu, Yu",
        "abstract": "Reinforcement learning (RL) has been widely adopted in post-training for large language models (LLMs) at scale. Recently, the incentivization of reasoning capabilities in LLMs from RL indicates that $\\textit{proper learning methods could enable effective inference-time scalability}$. A key challenge of RL is to obtain accurate reward signals for LLMs in various domains beyond verifiable questions or artificial rules. In this work, we investigate how to improve reward modeling (RM) with more inference compute for general queries, i.e. the $\\textbf{inference-time scalability of generalist RM}$, and further, how to improve the effectiveness of performance-compute scaling with proper learning methods. For the RM approach, we adopt pointwise generative reward modeling (GRM) to enable flexibility for different input types and potential for inference-time scaling. For the learning method, we propose Self-Principled Critique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs through online RL, to generate principles adaptively and critiques accurately, resulting in $\\textbf{DeepSeek-GRM}$ models. Furthermore, for effective inference-time scaling, we use parallel sampling to expand compute usage, and introduce a meta RM to guide voting process for better scaling performance. Empirically, we show that SPCT significantly improves the quality and scalability of GRMs, outperforming existing methods and models in various RM benchmarks without severe biases, and could achieve better performance compared to training-time scaling. DeepSeek-GRM still meets challenges in some tasks, which we believe can be addressed by future efforts in generalist reward systems. The models will be released and open-sourced.",
        "timestamp": "2025-04-05T00:52:22.603Z",
        "rating": "novote",
        "publishedDate": "2025/04/03",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2479,
        "object_id": "paper:arxiv.2504.02495",
        "created_at": "2025-04-05T00:52:23+00:00",
        "updated_at": "2025-04-05T00:52:25+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.20170": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.20170",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-05T00:50:32.748Z",
            "data": {
              "session_id": "session_1743814231853_ptk2g8x",
              "source_id": "arxiv",
              "paper_id": "2503.20170",
              "start_time": "2025-04-05T00:49:38.591Z",
              "end_time": "2025-04-05T00:50:31.853Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 3,
              "total_elapsed_seconds": 53
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2478,
        "object_id": "interactions:arxiv.2503.20170",
        "created_at": "2025-04-05T00:50:33+00:00",
        "updated_at": "2025-04-05T00:51:41+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.20170": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.20170",
        "url": "https://arxiv.org/abs/2503.20170",
        "title": "Decomposing a factorial into large factors",
        "authors": "Tao, Terence",
        "abstract": "Let $t(N)$ denote the largest number such that $N!$ can be expressed as the product of $N$ numbers greater than or equal to $t(N)$. The bound $t(N)/N = 1/e-o(1)$ was apparently established in unpublished work of Erd\\H{o}s, Selfridge, and Straus; but the proof is lost. Here we obtain the more precise asymptotic bounds $$ \\frac{1}{e} - \\frac{O(1)}{\\log N} \\leq \\frac{t(N)}{N} \\leq \\frac{1}{e} - \\frac{c_0+o(1)}{\\log N}$$ for an explicit constant $c_0 \\approx 0.3044$, answering a question of Erd\\H{o}s and Graham. Our methods are elementary, aside from the use of the prime number theorem (with classical error term). Using an effective version of the upper bound argument, we also show that $t(N)/N < 1/e$ for $N \\neq 1,2,4$, answering a question of Guy and Selfridge.",
        "timestamp": "2025-04-05T00:49:39.221Z",
        "rating": "novote",
        "publishedDate": "2025/03/26",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2477,
        "object_id": "paper:arxiv.2503.20170",
        "created_at": "2025-04-05T00:49:39+00:00",
        "updated_at": "2025-04-05T00:49:42+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.02495": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.02495",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-05T00:53:01.321Z",
            "data": {
              "session_id": "session_1743814380504_rpuweo1",
              "source_id": "arxiv",
              "paper_id": "2504.02495",
              "start_time": "2025-04-05T00:52:21.982Z",
              "end_time": "2025-04-05T00:53:00.504Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 4,
              "total_elapsed_seconds": 39
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2480,
        "object_id": "interactions:arxiv.2504.02495",
        "created_at": "2025-04-05T00:53:02+00:00",
        "updated_at": "2025-04-05T00:54:04+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.01017": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.01017",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-03T05:11:26.187Z",
            "data": {
              "session_id": "session_1743657085923_kss8x0l",
              "source_id": "arxiv",
              "paper_id": "2504.01017",
              "start_time": "2025-04-03T05:11:13.629Z",
              "end_time": "2025-04-03T05:11:25.923Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2444,
        "object_id": "interactions:arxiv.2504.01017",
        "created_at": "2025-04-03T05:11:27+00:00",
        "updated_at": "2025-04-10T04:37:47+00:00",
        "version": 1
      }
    },
    "interactions:url.6911BB25": {
      "data": {
        "sourceId": "url",
        "paperId": "6911BB25",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-05T01:09:27.985Z",
            "data": {
              "session_id": "session_1743815367024_q98fbid",
              "source_id": "url",
              "paper_id": "6911BB25",
              "start_time": "2025-04-05T01:08:15.518Z",
              "end_time": "2025-04-05T01:09:27.024Z",
              "heartbeat_count": 14,
              "duration_seconds": 70,
              "idle_seconds": 2,
              "total_elapsed_seconds": 72
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2483,
        "object_id": "interactions:url.6911BB25",
        "created_at": "2025-04-05T01:09:29+00:00",
        "updated_at": "2025-04-05T01:10:35+00:00",
        "version": 1
      }
    },
    "paper:url.6911BB25": {
      "data": {
        "sourceId": "url",
        "paperId": "6911BB25",
        "url": "https://journals.aps.org/prx/abstract/10.1103/PhysRevX.11.031058",
        "title": "Games in Rigged Economies",
        "authors": "Lu\u00eds F. Seoane",
        "abstract": "A game-theoretical model of a rigged economy predicts the emergence of cartels followed by a risk of instability as the economy becomes more complex.",
        "timestamp": "2025-04-05T01:08:15.909Z",
        "rating": "novote",
        "publishedDate": "2021/09/15",
        "tags": [],
        "doi": "10.1103/PhysRevX.11.031058",
        "journalName": "Physical Review X",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2482,
        "object_id": "paper:url.6911BB25",
        "created_at": "2025-04-05T01:08:16+00:00",
        "updated_at": "2025-04-05T01:08:19+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.24322": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.24322",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-05T03:59:01.812Z",
            "data": {
              "session_id": "session_1743825541105_clq53mz",
              "source_id": "arxiv",
              "paper_id": "2503.24322",
              "start_time": "2025-04-05T03:58:32.339Z",
              "end_time": "2025-04-05T03:59:01.105Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 4,
              "total_elapsed_seconds": 29
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-05T04:11:30.398Z",
            "data": {
              "session_id": "session_1743826290078_vqw0zwu",
              "source_id": "arxiv",
              "paper_id": "2503.24322",
              "start_time": "2025-04-05T04:11:23.494Z",
              "end_time": "2025-04-05T04:11:30.078Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T14:47:30.880Z",
            "data": {
              "session_id": "session_1743950850446_l7r315p",
              "source_id": "arxiv",
              "paper_id": "2503.24322",
              "start_time": "2025-04-06T14:46:53.361Z",
              "end_time": "2025-04-06T14:47:30.446Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 2,
              "total_elapsed_seconds": 37
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T15:24:43.657Z",
            "data": {
              "session_id": "session_1743953083407_a75lg83",
              "source_id": "arxiv",
              "paper_id": "2503.24322",
              "start_time": "2025-04-06T15:24:09.656Z",
              "end_time": "2025-04-06T15:24:43.407Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 4,
              "total_elapsed_seconds": 34
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T15:27:10.191Z",
            "data": {
              "session_id": "session_1743953229639_8xwdla4",
              "source_id": "arxiv",
              "paper_id": "2503.24322",
              "start_time": "2025-04-06T15:25:54.937Z",
              "end_time": "2025-04-06T15:27:09.639Z",
              "heartbeat_count": 14,
              "duration_seconds": 70,
              "idle_seconds": 5,
              "total_elapsed_seconds": 75
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T15:32:59.577Z",
            "data": {
              "session_id": "session_1743953579037_40s5eq2",
              "source_id": "arxiv",
              "paper_id": "2503.24322",
              "start_time": "2025-04-06T15:29:31.338Z",
              "end_time": "2025-04-06T15:32:59.037Z",
              "heartbeat_count": 41,
              "duration_seconds": 205,
              "idle_seconds": 3,
              "total_elapsed_seconds": 208
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-08T17:44:24.012Z",
            "data": {
              "session_id": "session_1744134263332_9yrld3m",
              "source_id": "arxiv",
              "paper_id": "2503.24322",
              "start_time": "2025-04-08T17:40:06.367Z",
              "end_time": "2025-04-08T17:44:23.332Z",
              "heartbeat_count": 51,
              "duration_seconds": 255,
              "idle_seconds": 2,
              "total_elapsed_seconds": 257
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-08T17:45:44.062Z",
            "data": {
              "session_id": "session_1744134343462_7jslvgr",
              "source_id": "arxiv",
              "paper_id": "2503.24322",
              "start_time": "2025-04-08T17:44:54.208Z",
              "end_time": "2025-04-08T17:45:43.462Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 4,
              "total_elapsed_seconds": 49
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2485,
        "object_id": "interactions:arxiv.2503.24322",
        "created_at": "2025-04-05T03:59:02+00:00",
        "updated_at": "2025-04-08T17:46:49+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.24322": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.24322",
        "url": "https://arxiv.org/abs/2503.24322",
        "title": "NoProp: Training Neural Networks without Back-propagation or Forward-propagation",
        "authors": "Li, Qinyu, Teh, Yee Whye, Pascanu, Razvan",
        "abstract": "The canonical deep learning approach for learning requires computing a gradient term at each layer by back-propagating the error signal from the output towards each learnable parameter. Given the stacked structure of neural networks, where each layer builds on the representation of the layer below, this approach leads to hierarchical representations. More abstract features live on the top layers of the model, while features on lower layers are expected to be less abstract. In contrast to this, we introduce a new learning method named NoProp, which does not rely on either forward or backwards propagation. Instead, NoProp takes inspiration from diffusion and flow matching methods, where each layer independently learns to denoise a noisy target. We believe this work takes a first step towards introducing a new family of gradient-free learning methods, that does not learn hierarchical representations -- at least not in the usual sense. NoProp needs to fix the representation at each layer beforehand to a noised version of the target, learning a local denoising process that can then be exploited at inference. We demonstrate the effectiveness of our method on MNIST, CIFAR-10, and CIFAR-100 image classification benchmarks. Our results show that NoProp is a viable learning algorithm which achieves superior accuracy, is easier to use and computationally more efficient compared to other existing back-propagation-free methods. By departing from the traditional gradient based learning paradigm, NoProp alters how credit assignment is done within the network, enabling more efficient distributed learning as well as potentially impacting other characteristics of the learning process.",
        "timestamp": "2025-04-05T03:58:32.352Z",
        "rating": "novote",
        "publishedDate": "2025/03/31",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2484,
        "object_id": "paper:arxiv.2503.24322",
        "created_at": "2025-04-05T03:58:32+00:00",
        "updated_at": "2025-04-05T03:58:35+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2103.00564": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2103.00564",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-05T05:12:18.823Z",
            "data": {
              "session_id": "session_1743829938299_t56jlaw",
              "source_id": "arxiv",
              "paper_id": "2103.00564",
              "start_time": "2025-04-05T05:11:42.840Z",
              "end_time": "2025-04-05T05:12:18.298Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 0,
              "total_elapsed_seconds": 35
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2487,
        "object_id": "interactions:arxiv.2103.00564",
        "created_at": "2025-04-05T05:12:20+00:00",
        "updated_at": "2025-04-05T05:13:34+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2103.00564": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2103.00564",
        "url": "https://arxiv.org/pdf/2103.00564",
        "title": "2103.00564",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-05T04:50:19.164Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2486,
        "object_id": "paper:arxiv.2103.00564",
        "created_at": "2025-04-05T04:50:19+00:00",
        "updated_at": "2025-04-05T04:50:22+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2312.06709": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2312.06709",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-05T14:21:31.695Z",
            "data": {
              "session_id": "session_1743862890597_gme44hf",
              "source_id": "arxiv",
              "paper_id": "2312.06709",
              "start_time": "2025-04-05T14:20:07.887Z",
              "end_time": "2025-04-05T14:21:30.597Z",
              "heartbeat_count": 16,
              "duration_seconds": 80,
              "idle_seconds": 3,
              "total_elapsed_seconds": 83
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2489,
        "object_id": "interactions:arxiv.2312.06709",
        "created_at": "2025-04-05T14:21:32+00:00",
        "updated_at": "2025-04-05T14:22:34+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2312.06709": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2312.06709",
        "url": "https://arxiv.org/abs/2312.06709",
        "title": "AM-RADIO: Agglomerative Vision Foundation Model -- Reduce All Domains Into One",
        "authors": "Ranzinger, Mike, Heinrich, Greg, Kautz, Jan, Molchanov, Pavlo",
        "abstract": "A handful of visual foundation models (VFMs) have recently emerged as the backbones for numerous downstream tasks. VFMs like CLIP, DINOv2, SAM are trained with distinct objectives, exhibiting unique characteristics for various downstream tasks. We find that despite their conceptual differences, these models can be effectively merged into a unified model through multi-teacher distillation. We name this approach AM-RADIO (Agglomerative Model -- Reduce All Domains Into One). This integrative approach not only surpasses the performance of individual teacher models but also amalgamates their distinctive features, such as zero-shot vision-language comprehension, detailed pixel-level understanding, and open vocabulary segmentation capabilities. In pursuit of the most hardware-efficient backbone, we evaluated numerous architectures in our multi-teacher distillation pipeline using the same training recipe. This led to the development of a novel architecture (E-RADIO) that exceeds the performance of its predecessors and is at least 7x faster than the teacher models. Our comprehensive benchmarking process covers downstream tasks including ImageNet classification, ADE20k semantic segmentation, COCO object detection and LLaVa-1.5 framework. Code: https://github.com/NVlabs/RADIO",
        "timestamp": "2025-04-05T14:20:05.338Z",
        "rating": "novote",
        "publishedDate": "2023/12/10",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2488,
        "object_id": "paper:arxiv.2312.06709",
        "created_at": "2025-04-05T14:20:05+00:00",
        "updated_at": "2025-04-05T14:20:09+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2412.07679": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2412.07679",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-05T14:40:21.311Z",
            "data": {
              "session_id": "session_1743864021082_m20qqqu",
              "source_id": "arxiv",
              "paper_id": "2412.07679",
              "start_time": "2025-04-05T14:40:15.189Z",
              "end_time": "2025-04-05T14:40:21.082Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2491,
        "object_id": "interactions:arxiv.2412.07679",
        "created_at": "2025-04-05T14:40:22+00:00",
        "updated_at": "2025-04-05T14:41:27+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2412.07679": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2412.07679",
        "url": "https://arxiv.org/abs/2412.07679",
        "title": "RADIOv2.5: Improved Baselines for Agglomerative Vision Foundation Models",
        "authors": "Heinrich, Greg, Ranzinger, Mike, Hongxu, Yin, Lu, Yao, Kautz, Jan, Tao, Andrew, Catanzaro, Bryan, Molchanov, Pavlo",
        "abstract": "Agglomerative models have recently emerged as a powerful approach to training vision foundation models, leveraging multi-teacher distillation from existing models such as CLIP, DINO, and SAM. This strategy enables the efficient creation of robust models, combining the strengths of individual teachers while significantly reducing computational and resource demands. In this paper, we thoroughly analyze state-of-the-art agglomerative models, identifying critical challenges including resolution mode shifts, teacher imbalance, idiosyncratic teacher artifacts, and an excessive number of output tokens. To address these issues, we propose several novel solutions: multi-resolution training, mosaic augmentation, and improved balancing of teacher loss functions. Specifically, in the context of Vision Language Models, we introduce a token compression technique to maintain high-resolution information within a fixed token count. We release our top-performing variants at multiple scales (-B, -L, -H, and -g), along with inference code and pretrained weights",
        "timestamp": "2025-04-05T14:28:24.453Z",
        "rating": "novote",
        "publishedDate": "2024/12/10",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2490,
        "object_id": "paper:arxiv.2412.07679",
        "created_at": "2025-04-05T14:28:25+00:00",
        "updated_at": "2025-04-05T14:28:27+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.01128": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.01128",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T05:32:32.857Z",
            "data": {
              "session_id": "session_1743917552100_so34hyq",
              "source_id": "arxiv",
              "paper_id": "2504.01128",
              "start_time": "2025-04-06T05:32:22.609Z",
              "end_time": "2025-04-06T05:32:32.100Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2494,
        "object_id": "interactions:arxiv.2504.01128",
        "created_at": "2025-04-06T05:32:33+00:00",
        "updated_at": "2025-04-06T05:33:36+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.01128": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.01128",
        "url": "https://arxiv.org/html/2504.01128v2",
        "title": "RipVIS: Rip Currents Video Instance Segmentation Benchmark for Beach Monitoring and Safety",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-06T05:32:22.637Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2492,
        "object_id": "paper:arxiv.2504.01128",
        "created_at": "2025-04-06T05:32:23+00:00",
        "updated_at": "2025-04-06T05:32:26+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2402.05070": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.05070",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T05:37:02.308Z",
            "data": {
              "session_id": "session_1743917821421_vn01rsb",
              "source_id": "arxiv",
              "paper_id": "2402.05070",
              "start_time": "2025-04-06T05:36:51.704Z",
              "end_time": "2025-04-06T05:37:01.421Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 5,
              "total_elapsed_seconds": 10
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2497,
        "object_id": "interactions:arxiv.2402.05070",
        "created_at": "2025-04-06T05:37:03+00:00",
        "updated_at": "2025-04-06T05:38:04+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2402.05070": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.05070",
        "url": "https://arxiv.org/abs/2402.05070",
        "title": "A Roadmap to Pluralistic Alignment",
        "authors": "Sorensen, Taylor, Moore, Jared, Fisher, Jillian, Gordon, Mitchell, Mireshghallah, Niloofar, Rytting, Christopher Michael, Ye, Andre, Jiang, Liwei, Lu, Ximing, Dziri, Nouha, Althoff, Tim, Choi, Yejin",
        "abstract": "With increased power and prevalence of AI systems, it is ever more critical that AI systems are designed to serve all, i.e., people with diverse values and perspectives. However, aligning models to serve pluralistic human values remains an open research question. In this piece, we propose a roadmap to pluralistic alignment, specifically using language models as a test bed. We identify and formalize three possible ways to define and operationalize pluralism in AI systems: 1) Overton pluralistic models that present a spectrum of reasonable responses; 2) Steerably pluralistic models that can steer to reflect certain perspectives; and 3) Distributionally pluralistic models that are well-calibrated to a given population in distribution. We also formalize and discuss three possible classes of pluralistic benchmarks: 1) Multi-objective benchmarks, 2) Trade-off steerable benchmarks, which incentivize models to steer to arbitrary trade-offs, and 3) Jury-pluralistic benchmarks which explicitly model diverse human ratings. We use this framework to argue that current alignment techniques may be fundamentally limited for pluralistic AI; indeed, we highlight empirical evidence, both from our own experiments and from other work, that standard alignment procedures might reduce distributional pluralism in models, motivating the need for further research on pluralistic alignment.",
        "timestamp": "2025-04-06T05:36:51.424Z",
        "rating": "novote",
        "publishedDate": "2024/02/07",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2495,
        "object_id": "paper:arxiv.2402.05070",
        "created_at": "2025-04-06T05:36:51+00:00",
        "updated_at": "2025-04-06T05:36:55+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.23126": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.23126",
        "url": "https://arxiv.org/abs/2503.23126",
        "title": "pastamarkers 2: pasta sauce colormaps for your flavorful results",
        "authors": "PASTA Collaboration, Rosignoli, L., Della Croce, A., Leitinger, E., Leuzzi, L., Papini, G., Traina, A., Sartori, S., Borghi, N., Ceccarelli, E.",
        "abstract": "In the big data era of Astrophysics, the improvement of visualization techniques can greatly enhance the ability to identify and interpret key features in complex datasets. This aspect of data analysis will become even more relevant in the near future, with the expected growth of data volumes. With our studies, we aim to drive progress in this field and inspire further research. We present the second release of pastamarkers, a Python-based matplotlib package that we initially presented last year. In this new release we focus on big data visualization and update the content of our first release. We find that analyzing complex problems and mining large data sets becomes significantly more intuitive and engaging when using the familiar and appetizing colors of pasta sauces instead of traditional colormaps.",
        "timestamp": "2025-04-06T05:51:19.354Z",
        "rating": "novote",
        "publishedDate": "2025/03/29",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2502,
        "object_id": "paper:arxiv.2503.23126",
        "created_at": "2025-04-06T05:51:19+00:00",
        "updated_at": "2025-04-06T05:51:23+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2401.00104": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2401.00104",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T05:49:40.265Z",
            "data": {
              "session_id": "session_1743918580248_aqj5ni8",
              "source_id": "arxiv",
              "paper_id": "2401.00104",
              "start_time": "2025-04-06T05:49:12.743Z",
              "end_time": "2025-04-06T05:49:40.248Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 3,
              "total_elapsed_seconds": 28
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T05:53:16.355Z",
            "data": {
              "session_id": "session_1743918796320_q6of90m",
              "source_id": "arxiv",
              "paper_id": "2401.00104",
              "start_time": "2025-04-06T05:53:10.305Z",
              "end_time": "2025-04-06T05:53:16.320Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2501,
        "object_id": "interactions:arxiv.2401.00104",
        "created_at": "2025-04-06T05:49:41+00:00",
        "updated_at": "2025-04-06T05:54:15+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2107.14042": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2107.14042",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T05:49:13.447Z",
            "data": {
              "session_id": "session_1743918552718_pos5luv",
              "source_id": "arxiv",
              "paper_id": "2107.14042",
              "start_time": "2025-04-06T05:48:47.083Z",
              "end_time": "2025-04-06T05:49:12.718Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 1,
              "total_elapsed_seconds": 26
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2500,
        "object_id": "interactions:arxiv.2107.14042",
        "created_at": "2025-04-06T05:49:14+00:00",
        "updated_at": "2025-04-06T05:50:12+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2401.00104": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2401.00104",
        "url": "https://arxiv.org/abs/2401.00104",
        "title": "Causal State Distillation for Explainable Reinforcement Learning",
        "authors": "Lu, Wenhao, Zhao, Xufeng, Fryen, Thilo, Lee, Jae Hee, Li, Mengdi, Magg, Sven, Wermter, Stefan",
        "abstract": "Reinforcement learning (RL) is a powerful technique for training intelligent agents, but understanding why these agents make specific decisions can be quite challenging. This lack of transparency in RL models has been a long-standing problem, making it difficult for users to grasp the reasons behind an agent's behaviour. Various approaches have been explored to address this problem, with one promising avenue being reward decomposition (RD). RD is appealing as it sidesteps some of the concerns associated with other methods that attempt to rationalize an agent's behaviour in a post-hoc manner. RD works by exposing various facets of the rewards that contribute to the agent's objectives during training. However, RD alone has limitations as it primarily offers insights based on sub-rewards and does not delve into the intricate cause-and-effect relationships that occur within an RL agent's neural model. In this paper, we present an extension of RD that goes beyond sub-rewards to provide more informative explanations. Our approach is centred on a causal learning framework that leverages information-theoretic measures for explanation objectives that encourage three crucial properties of causal factors: causal sufficiency, sparseness, and orthogonality. These properties help us distill the cause-and-effect relationships between the agent's states and actions or rewards, allowing for a deeper understanding of its decision-making processes. Our framework is designed to generate local explanations and can be applied to a wide range of RL tasks with multiple reward channels. Through a series of experiments, we demonstrate that our approach offers more meaningful and insightful explanations for the agent's action selections.",
        "timestamp": "2025-04-06T05:49:13.200Z",
        "rating": "novote",
        "publishedDate": "2023/12/30",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2499,
        "object_id": "paper:arxiv.2401.00104",
        "created_at": "2025-04-06T05:49:13+00:00",
        "updated_at": "2025-04-06T05:49:16+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2107.14042": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2107.14042",
        "url": "https://arxiv.org/abs/2107.14042",
        "title": "The brain is a computer is a brain: neuroscience's internal debate and the social significance of the Computational Metaphor",
        "authors": "Baria, Alexis T., Cross, Keith",
        "abstract": "The Computational Metaphor, comparing the brain to the computer and vice versa, is the most prominent metaphor in neuroscience and artificial intelligence (AI). Its appropriateness is highly debated in both fields, particularly with regards to whether it is useful for the advancement of science and technology. Considerably less attention, however, has been devoted to how the Computational Metaphor is used outside of the lab, and particularly how it may shape society's interactions with AI. As such, recently publicized concerns over AI's role in perpetuating racism, genderism, and ableism suggest that the term \"artificial intelligence\" is misplaced, and that a new lexicon is needed to describe these computational systems. Thus, there is an essential question about the Computational Metaphor that is rarely asked by neuroscientists: whom does it help and whom does it harm? This essay invites the neuroscience community to consider the social implications of the field's most controversial metaphor.",
        "timestamp": "2025-04-06T05:48:47.506Z",
        "rating": "novote",
        "publishedDate": "2021/07/18",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2498,
        "object_id": "paper:arxiv.2107.14042",
        "created_at": "2025-04-06T05:48:47+00:00",
        "updated_at": "2025-04-06T05:48:51+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.23126": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.23126",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T05:51:26.496Z",
            "data": {
              "session_id": "session_1743918685698_dun8zcf",
              "source_id": "arxiv",
              "paper_id": "2503.23126",
              "start_time": "2025-04-06T05:51:18.815Z",
              "end_time": "2025-04-06T05:51:25.698Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2503,
        "object_id": "interactions:arxiv.2503.23126",
        "created_at": "2025-04-06T05:51:27+00:00",
        "updated_at": "2025-04-06T05:52:37+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2101.03961": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2101.03961",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T05:58:30.667Z",
            "data": {
              "session_id": "session_1743919109779_rsluoc9",
              "source_id": "arxiv",
              "paper_id": "2101.03961",
              "start_time": "2025-04-06T05:57:39.764Z",
              "end_time": "2025-04-06T05:58:29.779Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 5,
              "total_elapsed_seconds": 50
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2506,
        "object_id": "interactions:arxiv.2101.03961",
        "created_at": "2025-04-06T05:58:31+00:00",
        "updated_at": "2025-04-06T05:59:29+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2101.03961": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2101.03961",
        "url": "https://arxiv.org/abs/2101.03961",
        "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
        "authors": "Fedus, William, Zoph, Barret, Shazeer, Noam",
        "abstract": "In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the \"Colossal Clean Crawled Corpus\" and achieve a 4x speedup over the T5-XXL model.",
        "timestamp": "2025-04-06T05:57:39.541Z",
        "rating": "novote",
        "publishedDate": "2021/01/11",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2504,
        "object_id": "paper:arxiv.2101.03961",
        "created_at": "2025-04-06T05:57:39+00:00",
        "updated_at": "2025-04-06T05:57:42+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2005.14050": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2005.14050",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T06:16:10.551Z",
            "data": {
              "session_id": "session_1743920169836_q72i6zo",
              "source_id": "arxiv",
              "paper_id": "2005.14050",
              "start_time": "2025-04-06T06:16:03.419Z",
              "end_time": "2025-04-06T06:16:09.836Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2509,
        "object_id": "interactions:arxiv.2005.14050",
        "created_at": "2025-04-06T06:16:11+00:00",
        "updated_at": "2025-04-06T06:17:12+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2005.14050": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2005.14050",
        "url": "https://arxiv.org/abs/2005.14050",
        "title": "Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP",
        "authors": "Blodgett, Su Lin, Barocas, Solon, Daum\u00e9 III, Hal, Wallach, Hanna",
        "abstract": "We survey 146 papers analyzing \"bias\" in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing \"bias\" is an inherently normative process. We further find that these papers' proposed quantitative techniques for measuring or mitigating \"bias\" are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing \"bias\" in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of \"bias\"---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements---and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.",
        "timestamp": "2025-04-06T06:16:03.112Z",
        "rating": "novote",
        "publishedDate": "2020/05/28",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2507,
        "object_id": "paper:arxiv.2005.14050",
        "created_at": "2025-04-06T06:16:03+00:00",
        "updated_at": "2025-04-06T06:16:06+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.00952": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.00952",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T06:27:58.379Z",
            "data": {
              "session_id": "session_1743920877818_64kl1tr",
              "source_id": "arxiv",
              "paper_id": "2504.00952",
              "start_time": "2025-04-06T06:23:22.788Z",
              "end_time": "2025-04-06T06:27:57.818Z",
              "heartbeat_count": 52,
              "duration_seconds": 260,
              "idle_seconds": 15,
              "total_elapsed_seconds": 275
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2511,
        "object_id": "interactions:arxiv.2504.00952",
        "created_at": "2025-04-06T06:27:59+00:00",
        "updated_at": "2025-04-06T06:29:00+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.00952": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.00952",
        "url": "https://arxiv.org/html/2504.00952v1",
        "title": "Personalized Federated Training of Diffusion Models with Privacy Guarantees",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-06T06:23:19.729Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2510,
        "object_id": "paper:arxiv.2504.00952",
        "created_at": "2025-04-06T06:23:20+00:00",
        "updated_at": "2025-04-06T06:23:23+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.08685": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.08685",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T07:27:38.037Z",
            "data": {
              "session_id": "session_1743924457225_855fw1z",
              "source_id": "arxiv",
              "paper_id": "2503.08685",
              "start_time": "2025-04-06T07:27:25.204Z",
              "end_time": "2025-04-06T07:27:37.225Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2516,
        "object_id": "interactions:arxiv.2503.08685",
        "created_at": "2025-04-06T07:27:38+00:00",
        "updated_at": "2025-04-06T07:28:37+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.08685": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.08685",
        "url": "https://arxiv.org/abs/2503.08685",
        "title": "\"Principal Components\" Enable A New Language of Images",
        "authors": "Wen, Xin, Zhao, Bingchen, Elezi, Ismail, Deng, Jiankang, Qi, Xiaojuan",
        "abstract": "We introduce a novel visual tokenization framework that embeds a provable PCA-like structure into the latent token space. While existing visual tokenizers primarily optimize for reconstruction fidelity, they often neglect the structural properties of the latent space -- a critical factor for both interpretability and downstream tasks. Our method generates a 1D causal token sequence for images, where each successive token contributes non-overlapping information with mathematically guaranteed decreasing explained variance, analogous to principal component analysis. This structural constraint ensures the tokenizer extracts the most salient visual features first, with each subsequent token adding diminishing yet complementary information. Additionally, we identified and resolved a semantic-spectrum coupling effect that causes the unwanted entanglement of high-level semantic content and low-level spectral details in the tokens by leveraging a diffusion decoder. Experiments demonstrate that our approach achieves state-of-the-art reconstruction performance and enables better interpretability to align with the human vision system. Moreover, auto-regressive models trained on our token sequences achieve performance comparable to current state-of-the-art methods while requiring fewer tokens for training and inference.",
        "timestamp": "2025-04-06T07:27:25.546Z",
        "rating": "novote",
        "publishedDate": "2025/03/11",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2515,
        "object_id": "paper:arxiv.2503.08685",
        "created_at": "2025-04-06T07:27:25+00:00",
        "updated_at": "2025-04-06T07:27:28+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2502.10841": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.10841",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T07:26:48.654Z",
            "data": {
              "session_id": "session_1743924407949_7bzxo2e",
              "source_id": "arxiv",
              "paper_id": "2502.10841",
              "start_time": "2025-04-06T07:26:41.701Z",
              "end_time": "2025-04-06T07:26:47.949Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2514,
        "object_id": "interactions:arxiv.2502.10841",
        "created_at": "2025-04-06T07:26:49+00:00",
        "updated_at": "2025-04-06T07:27:48+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.10841": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.10841",
        "url": "https://arxiv.org/abs/2502.10841",
        "title": "SkyReels-A1: Expressive Portrait Animation in Video Diffusion Transformers",
        "authors": "Qiu, Di, Fei, Zhengcong, Wang, Rui, Bai, Jialin, Yu, Changqian, Fan, Mingyuan, Chen, Guibin, Wen, Xiang",
        "abstract": "We present SkyReels-A1, a simple yet effective framework built upon video diffusion Transformer to facilitate portrait image animation. Existing methodologies still encounter issues, including identity distortion, background instability, and unrealistic facial dynamics, particularly in head-only animation scenarios. Besides, extending to accommodate diverse body proportions usually leads to visual inconsistencies or unnatural articulations. To address these challenges, SkyReels-A1 capitalizes on the strong generative capabilities of video DiT, enhancing facial motion transfer precision, identity retention, and temporal coherence. The system incorporates an expression-aware conditioning module that enables seamless video synthesis driven by expression-guided landmark inputs. Integrating the facial image-text alignment module strengthens the fusion of facial attributes with motion trajectories, reinforcing identity preservation. Additionally, SkyReels-A1 incorporates a multi-stage training paradigm to incrementally refine the correlation between expressions and motion while ensuring stable identity reproduction. Extensive empirical evaluations highlight the model's ability to produce visually coherent and compositionally diverse results, making it highly applicable to domains such as virtual avatars, remote communication, and digital media generation.",
        "timestamp": "2025-04-06T07:26:41.174Z",
        "rating": "novote",
        "publishedDate": "2025/02/15",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2512,
        "object_id": "paper:arxiv.2502.10841",
        "created_at": "2025-04-06T07:26:41+00:00",
        "updated_at": "2025-04-06T07:26:44+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.01990": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.01990",
        "url": "https://arxiv.org/abs/2504.01990",
        "title": "Advances and Challenges in Foundation Agents: From Brain-Inspired Intelligence to Evolutionary, Collaborative, and Safe Systems",
        "authors": "Liu, Bang, Li, Xinfeng, Zhang, Jiayi, Wang, Jinlin, He, Tanjin, Hong, Sirui, Liu, Hongzhang, Zhang, Shaokun, Song, Kaitao, Zhu, Kunlun, Cheng, Yuheng, Wang, Suyuchen, Wang, Xiaoqiang, Luo, Yuyu, Jin, Haibo, Zhang, Peiyan, Liu, Ollie, Chen, Jiaqi, Zhang, Huan, Yu, Zhaoyang, Shi, Haochen, Li, Boyan, Wu, Dekun, Teng, Fengwei, Jia, Xiaojun, Xu, Jiawei, Xiang, Jinyu, Lin, Yizhang, Liu, Tianming, Liu, Tongliang, Su, Yu, Sun, Huan, Berseth, Glen, Nie, Jianyun, Foster, Ian, Ward, Logan, Wu, Qingyun, Gu, Yu, Zhuge, Mingchen, Tang, Xiangru, Wang, Haohan, You, Jiaxuan, Wang, Chi, Pei, Jian, Yang, Qiang, Qi, Xiaoliang, Wu, Chenglin",
        "abstract": "The advent of large language models (LLMs) has catalyzed a transformative shift in artificial intelligence, paving the way for advanced intelligent agents capable of sophisticated reasoning, robust perception, and versatile action across diverse domains. As these agents increasingly drive AI research and practical applications, their design, evaluation, and continuous improvement present intricate, multifaceted challenges. This survey provides a comprehensive overview, framing intelligent agents within a modular, brain-inspired architecture that integrates principles from cognitive science, neuroscience, and computational research. We structure our exploration into four interconnected parts. First, we delve into the modular foundation of intelligent agents, systematically mapping their cognitive, perceptual, and operational modules onto analogous human brain functionalities, and elucidating core components such as memory, world modeling, reward processing, and emotion-like systems. Second, we discuss self-enhancement and adaptive evolution mechanisms, exploring how agents autonomously refine their capabilities, adapt to dynamic environments, and achieve continual learning through automated optimization paradigms, including emerging AutoML and LLM-driven optimization strategies. Third, we examine collaborative and evolutionary multi-agent systems, investigating the collective intelligence emerging from agent interactions, cooperation, and societal structures, highlighting parallels to human social dynamics. Finally, we address the critical imperative of building safe, secure, and beneficial AI systems, emphasizing intrinsic and extrinsic security threats, ethical alignment, robustness, and practical mitigation strategies necessary for trustworthy real-world deployment.",
        "timestamp": "2025-04-06T07:33:24.045Z",
        "rating": "novote",
        "publishedDate": "2025/03/31",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2519,
        "object_id": "paper:arxiv.2504.01990",
        "created_at": "2025-04-06T07:33:24+00:00",
        "updated_at": "2025-04-06T07:33:27+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2501.18795": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.18795",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T07:32:01.370Z",
            "data": {
              "session_id": "session_1743924720692_thbro9c",
              "source_id": "arxiv",
              "paper_id": "2501.18795",
              "start_time": "2025-04-06T07:31:49.895Z",
              "end_time": "2025-04-06T07:32:00.692Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T07:34:11.763Z",
            "data": {
              "session_id": "session_1743924851583_prvm53p",
              "source_id": "arxiv",
              "paper_id": "2501.18795",
              "start_time": "2025-04-06T07:34:06.456Z",
              "end_time": "2025-04-06T07:34:11.583Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2518,
        "object_id": "interactions:arxiv.2501.18795",
        "created_at": "2025-04-06T07:32:02+00:00",
        "updated_at": "2025-04-06T07:35:10+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2501.18795": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.18795",
        "url": "https://arxiv.org/abs/2501.18795",
        "title": "Rope to Nope and Back Again: A New Hybrid Attention Strategy",
        "authors": "Yang, Bowen, Venkitesh, Bharat, Talupuru, Dwarak, Lin, Hangyu, Cairuz, David, Blunsom, Phil, Locatelli, Acyr",
        "abstract": "Long-context large language models (LLMs) have achieved remarkable advancements, driven by techniques like Rotary Position Embedding (RoPE) (Su et al., 2023) and its extensions (Chen et al., 2023; Liu et al., 2024c; Peng et al., 2023). By adjusting RoPE parameters and incorporating training data with extended contexts, we can train performant models with considerably longer input sequences. However, existing RoPE-based methods exhibit performance limitations when applied to extended context lengths. This paper presents a comprehensive analysis of various attention mechanisms, including RoPE, No Positional Embedding (NoPE), and Query-Key Normalization (QK-Norm), identifying their strengths and shortcomings in long-context modeling. Our investigation identifies distinctive attention patterns in these methods and highlights their impact on long-context performance, providing valuable insights for architectural design. Building on these findings, we propose a novel architectural based on a hybrid attention mechanism that not only surpasses conventional RoPE-based transformer models in long context tasks but also achieves competitive performance on benchmarks requiring shorter context lengths.",
        "timestamp": "2025-04-06T07:31:50.541Z",
        "rating": "novote",
        "publishedDate": "2025/01/30",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2517,
        "object_id": "paper:arxiv.2501.18795",
        "created_at": "2025-04-06T07:31:51+00:00",
        "updated_at": "2025-04-06T07:31:54+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2409.19606": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2409.19606",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T15:34:05.227Z",
            "data": {
              "session_id": "session_1743953644444_4l54afp",
              "source_id": "arxiv",
              "paper_id": "2409.19606",
              "start_time": "2025-04-06T15:33:52.455Z",
              "end_time": "2025-04-06T15:34:04.444Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2523,
        "object_id": "interactions:arxiv.2409.19606",
        "created_at": "2025-04-06T15:34:06+00:00",
        "updated_at": "2025-04-06T15:35:10+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2409.19606": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2409.19606",
        "url": "https://arxiv.org/abs/2409.19606",
        "title": "Hyper-Connections",
        "authors": "Zhu, Defa, Huang, Hongzhi, Huang, Zihao, Zeng, Yutao, Mao, Yunyao, Wu, Banggu, Min, Qiyang, Zhou, Xun",
        "abstract": "We present hyper-connections, a simple yet effective method that can serve as an alternative to residual connections. This approach specifically addresses common drawbacks observed in residual connection variants, such as the seesaw effect between gradient vanishing and representation collapse. Theoretically, hyper-connections allow the network to adjust the strength of connections between features at different depths and dynamically rearrange layers. We conduct experiments focusing on the pre-training of large language models, including dense and sparse models, where hyper-connections show significant performance improvements over residual connections. Additional experiments conducted on vision tasks also demonstrate similar improvements. We anticipate that this method will be broadly applicable and beneficial across a wide range of AI problems.",
        "timestamp": "2025-04-06T15:32:59.578Z",
        "rating": "novote",
        "publishedDate": "2024/09/29",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2520,
        "object_id": "paper:arxiv.2409.19606",
        "created_at": "2025-04-06T15:33:00+00:00",
        "updated_at": "2025-04-06T15:33:02+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2411.04997": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.04997",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T15:46:34.409Z",
            "data": {
              "session_id": "session_1743954393638_efl9lit",
              "source_id": "arxiv",
              "paper_id": "2411.04997",
              "start_time": "2025-04-06T15:45:46.503Z",
              "end_time": "2025-04-06T15:46:33.638Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 2,
              "total_elapsed_seconds": 47
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T15:49:16.263Z",
            "data": {
              "session_id": "session_1743954555743_wile7np",
              "source_id": "arxiv",
              "paper_id": "2411.04997",
              "start_time": "2025-04-06T15:47:51.902Z",
              "end_time": "2025-04-06T15:49:15.743Z",
              "heartbeat_count": 16,
              "duration_seconds": 80,
              "idle_seconds": 4,
              "total_elapsed_seconds": 84
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T15:52:17.241Z",
            "data": {
              "session_id": "session_1743954737004_vq4tvui",
              "source_id": "arxiv",
              "paper_id": "2411.04997",
              "start_time": "2025-04-06T15:52:11.593Z",
              "end_time": "2025-04-06T15:52:17.004Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2526,
        "object_id": "interactions:arxiv.2411.04997",
        "created_at": "2025-04-06T15:46:35+00:00",
        "updated_at": "2025-04-06T15:53:14+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2411.04997": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.04997",
        "url": "https://arxiv.org/abs/2411.04997",
        "title": "LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation",
        "authors": "Huang, Weiquan, Wu, Aoqi, Yang, Yifan, Luo, Xufang, Yang, Yuqing, Hu, Liang, Dai, Qi, Dai, Xiyang, Chen, Dongdong, Luo, Chong, Qiu, Lili",
        "abstract": "CLIP is a foundational multimodal model that aligns image and text features into a shared space using contrastive learning on large-scale image-text pairs. Its strength lies in leveraging natural language as a rich supervisory signal. With the rapid progress of large language models (LLMs), we explore their potential to further enhance CLIP's multimodal representation learning. This work introduces a fine-tuning approach that integrates LLMs with the pretrained CLIP visual encoder, leveraging LLMs' advanced text understanding and open-world knowledge to improve CLIP's ability to process long and complex captions. To address the challenge of LLMs' autoregressive nature, we propose a caption-to-caption contrastive learning framework to enhance the discriminative power of their outputs. Our method achieves substantial performance gains on various downstream tasks, demonstrating the effectiveness of combining LLMs with CLIP for enhanced multimodal learning.",
        "timestamp": "2025-04-06T15:45:45.734Z",
        "rating": "novote",
        "publishedDate": "2024/11/07",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2524,
        "object_id": "paper:arxiv.2411.04997",
        "created_at": "2025-04-06T15:45:46+00:00",
        "updated_at": "2025-04-06T15:45:49+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.16484": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.16484",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T16:03:13.476Z",
            "data": {
              "session_id": "session_1743955392546_avquapz",
              "source_id": "arxiv",
              "paper_id": "2503.16484",
              "start_time": "2025-04-06T16:03:06.403Z",
              "end_time": "2025-04-06T16:03:12.546Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2528,
        "object_id": "interactions:arxiv.2503.16484",
        "created_at": "2025-04-06T16:03:14+00:00",
        "updated_at": "2025-04-06T16:04:17+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.16484": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.16484",
        "url": "https://arxiv.org/abs/2503.16484",
        "title": "AI-Powered Episodic Future Thinking",
        "authors": "Ahmadi, Sareh, Rockwell, Michelle, Stuart, Megan, Tegge, Allison, Wang, Xuan, Stein, Jeffrey, Fox, Edward A.",
        "abstract": "Episodic Future Thinking (EFT) is an intervention that involves vividly imagining personal future events and experiences in detail. It has shown promise as an intervention to reduce delay discounting - the tendency to devalue delayed rewards in favor of immediate gratification - and to promote behavior change in a range of maladaptive health behaviors. We present EFTeacher, an AI chatbot powered by the GPT-4-Turbo large language model, designed to generate EFT cues for users with lifestyle-related conditions. To evaluate the chatbot, we conducted a user study that included usability assessments and user evaluations based on content characteristics questionnaires, followed by semi-structured interviews. The study provides qualitative insights into participants' experiences and interactions with the chatbot and its usability. Our findings highlight the potential application of AI chatbots based on Large Language Models (LLMs) in EFT interventions, and offer design guidelines for future behavior-oriented applications.",
        "timestamp": "2025-04-06T16:03:06.897Z",
        "rating": "novote",
        "publishedDate": "2025/03/08",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2527,
        "object_id": "paper:arxiv.2503.16484",
        "created_at": "2025-04-06T16:03:07+00:00",
        "updated_at": "2025-04-06T16:03:10+00:00",
        "version": 1
      }
    },
    "interactions:url.602D4F43": {
      "data": {
        "sourceId": "url",
        "paperId": "602D4F43",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T16:18:32.023Z",
            "data": {
              "session_id": "session_1743956311148_b1n90tg",
              "source_id": "url",
              "paper_id": "602D4F43",
              "start_time": "2025-04-06T16:18:06.136Z",
              "end_time": "2025-04-06T16:18:31.148Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 15,
              "total_elapsed_seconds": 25
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2533,
        "object_id": "interactions:url.602D4F43",
        "created_at": "2025-04-06T16:18:32+00:00",
        "updated_at": "2025-04-06T16:19:36+00:00",
        "version": 1
      }
    },
    "paper:url.602D4F43": {
      "data": {
        "sourceId": "url",
        "paperId": "602D4F43",
        "url": "https://link.springer.com/article/10.1007/s12064-020-00313-7",
        "title": "The information theory of individuality",
        "authors": "Krakauer, David, Bertschinger, Nils, Olbrich, Eckehard, Flack, Jessica C., Ay, Nihat",
        "abstract": "Despite the near universal assumption of individuality in biology, there is little agreement about what individuals are and few rigorous quantitative methods for their identification. Here, we propose that individuals are aggregates that preserve a measure of temporal integrity, i.e., \u201cpropagate\u201d information from their past into their futures. We formalize this idea using information theory and graphical models. This mathematical formulation yields three principled and distinct forms of individuality\u2014an organismal, a colonial, and a driven form\u2014each of which varies in the degree of environmental dependence and inherited information. This approach can be thought of as a Gestalt approach to evolution where selection makes figure-ground (agent\u2013environment) distinctions using suitable information-theoretic lenses. A benefit of the approach is that it expands the scope of allowable individuals to include adaptive aggregations in systems that are multi-scale, highly distributed, and do not necessarily have physical boundaries such as cell walls or clonal somatic tissue. Such individuals might be visible to selection but hard to detect by observers without suitable measurement principles. The information theory of individuality allows for the identification of individuals at all levels of organization from molecular to cultural and provides a basis for testing assumptions about the natural scales of a system and argues for the importance of uncertainty reduction through coarse-graining in adaptive systems.",
        "timestamp": "2025-04-06T16:18:05.304Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1007/s12064-020-00313-7",
        "journalName": "Theory in Biosciences",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2532,
        "object_id": "paper:url.602D4F43",
        "created_at": "2025-04-06T16:18:05+00:00",
        "updated_at": "2025-04-06T16:18:08+00:00",
        "version": 1
      }
    },
    "interactions:url.15C07065": {
      "data": {
        "sourceId": "url",
        "paperId": "15C07065",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T16:16:24.784Z",
            "data": {
              "session_id": "session_1743956183729_s8mklo9",
              "source_id": "url",
              "paper_id": "15C07065",
              "start_time": "2025-04-06T16:15:28.722Z",
              "end_time": "2025-04-06T16:16:23.729Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 5,
              "total_elapsed_seconds": 55
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2531,
        "object_id": "interactions:url.15C07065",
        "created_at": "2025-04-06T16:16:25+00:00",
        "updated_at": "2025-04-06T16:17:36+00:00",
        "version": 1
      }
    },
    "paper:url.15C07065": {
      "data": {
        "sourceId": "url",
        "paperId": "15C07065",
        "url": "https://link.springer.com/content/pdf/10.1007/s12064-020-00313-7.pdf",
        "title": "15C07065",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-06T16:15:29.124Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 2530,
        "object_id": "paper:url.15C07065",
        "created_at": "2025-04-06T16:15:29+00:00",
        "updated_at": "2025-04-06T16:15:32+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.23758": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.23758",
        "url": "https://arxiv.org/pdf/2503.23758",
        "title": "2503.23758",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-06T16:10:13.448Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2529,
        "object_id": "paper:arxiv.2503.23758",
        "created_at": "2025-04-06T16:10:13+00:00",
        "updated_at": "2025-04-06T16:10:17+00:00",
        "version": 1
      }
    },
    "paper:url.1284B9C0": {
      "data": {
        "sourceId": "url",
        "paperId": "1284B9C0",
        "url": "https://www.pnas.org/doi/abs/10.1073/pnas.2300888120",
        "title": "Partial entropy decomposition reveals higher-order information structures in human brain activity",
        "authors": "Varley, Thomas F., Pope, Maria, Maria Grazia, null, Joshua, null, Sporns, Olaf",
        "abstract": "The standard approach to modeling the human brain as a complex system is with a network,\nwhere the basic unit of interaction is a pairwise link bet...",
        "timestamp": "2025-04-06T16:20:25.393Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "information theory",
          "synergy",
          "higher-order network",
          "fMRI",
          "neuroscience"
        ],
        "doi": "10.1073/pnas.2300888120",
        "journalName": "Proceedings of the National Academy of Sciences",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2534,
        "object_id": "paper:url.1284B9C0",
        "created_at": "2025-04-06T16:20:25+00:00",
        "updated_at": "2025-04-06T16:20:28+00:00",
        "version": 1
      }
    },
    "paper:url.44D45584": {
      "data": {
        "sourceId": "url",
        "paperId": "44D45584",
        "url": "https://link.springer.com/article/10.1007/s10539-021-09815-0",
        "title": "Unknotting reciprocal causation between organism and environment",
        "authors": "Baedke, Jan, F\u00e1bregas-Tejeda, Alejandro, Prieto, Guido I.",
        "abstract": "In recent years, biologists and philosophers of science have argued that evolutionary theory should incorporate more seriously the idea of \u2018reciprocal causation.\u2019 This notion refers to feedback loops whereby organisms change their experiences of the environment or alter the physical properties of their surroundings. In these loops, in particular niche constructing activities are central, since they may alter selection pressures acting on organisms, and thus affect their evolutionary trajectories. This paper discusses long-standing problems that emerge when studying such reciprocal causal processes between organisms and environments. By comparing past approaches to reciprocal causation from the early twentieth century with contemporary ones in niche construction theory, we identify two central reoccurring problems: All of these approaches have not been able to provide a conceptual framework that allows (i) maintaining meaningful boundaries between organisms and environments, instead of merging the two, and (ii) integrating experiential and physical kinds of reciprocal causation. By building on case studies of niche construction research, we provide a model that is able to solve these two problems. It allows distinguishing between mutually interacting organisms and environments in complex scenarios, as well as integrating various forms of experiential and physical niche construction.",
        "timestamp": "2025-04-06T16:22:40.422Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1007/s10539-021-09815-0",
        "journalName": "Biology & Philosophy",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2538,
        "object_id": "paper:url.44D45584",
        "created_at": "2025-04-06T16:22:40+00:00",
        "updated_at": "2025-04-06T16:22:43+00:00",
        "version": 1
      }
    },
    "paper:url.479954A9": {
      "data": {
        "sourceId": "url",
        "paperId": "479954A9",
        "url": "https://www.frontiersin.org/journals/ecology-and-evolution/articles/10.3389/fevo.2019.00219/full",
        "title": "Principles of Ecology Revisited: Integrating Information and Ecological Theories for a More Unified Science",
        "authors": "O'Connor, Mary I., Pennell, Matthew W., Altermatt, Florian, Matthews, Blake, Meli\u00e1n, Carlos J., Gonzalez, Andrew",
        "abstract": "<p>The persistence of ecological systems in changing environments requires energy, materials, and information. Although the importance of information to ecological function has been widely recognized, the fundamental principles of ecological science as commonly expressed do not reflect this central role of information processing. We articulate five fundamental principles of ecology that integrate information with energy and material constraints across scales of organization in living systems. We show how these principles outline new theoretical and empirical research challenges, and offer one novel attempt to incorporate them in a theoretical model. To provide adequate background for the principles, we review major concepts and identify common themes and key differences in information theories spanning physics, biology and semiotics. We structured our review around a series of questions about the role information may play in ecological systems: (i) what is information? (ii) how is information related to uncertainty? (iii) what is information processing? (iv) does information processing link ecological systems across scales? We highlight two aspects of information that capture its dual roles: <italic>syntactic information</italic> defining the processes that encode, filter and process information stored in biological structure and <italic>semiotic information</italic> associated with structures and their context. We argue that the principles of information in living systems promote a unified approach to understanding living systems in terms of first principles of biology and physics, and promote much needed theoretical and empirical advances in ecological research to unify understanding across disciplines and scales.</p>",
        "timestamp": "2025-04-06T16:22:19.080Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "Ecology",
          "information",
          "semantic",
          "syntactic",
          "Community",
          "ecosystem",
          "evolution",
          "Behavior",
          "Population",
          "global change"
        ],
        "doi": "10.3389/fevo.2019.00219",
        "journalName": "Frontiers in Ecology and Evolution",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2537,
        "object_id": "paper:url.479954A9",
        "created_at": "2025-04-06T16:22:19+00:00",
        "updated_at": "2025-04-06T16:22:22+00:00",
        "version": 1
      }
    },
    "paper:url.69860140": {
      "data": {
        "sourceId": "url",
        "paperId": "69860140",
        "url": "https://www.pnas.org/doi/pdf/10.1073/pnas.2300888120",
        "title": "69860140",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-06T16:20:58.425Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2536,
        "object_id": "paper:url.69860140",
        "created_at": "2025-04-06T16:20:58+00:00",
        "updated_at": "2025-04-06T16:21:01+00:00",
        "version": 1
      }
    },
    "interactions:url.1284B9C0": {
      "data": {
        "sourceId": "url",
        "paperId": "1284B9C0",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T16:20:39.872Z",
            "data": {
              "session_id": "session_1743956439178_tbdm7l2",
              "source_id": "url",
              "paper_id": "1284B9C0",
              "start_time": "2025-04-06T16:20:25.829Z",
              "end_time": "2025-04-06T16:20:39.178Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 3,
              "total_elapsed_seconds": 13
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2535,
        "object_id": "interactions:url.1284B9C0",
        "created_at": "2025-04-06T16:20:40+00:00",
        "updated_at": "2025-04-06T16:21:48+00:00",
        "version": 1
      }
    },
    "paper:url.1D5FEC41": {
      "data": {
        "sourceId": "url",
        "paperId": "1D5FEC41",
        "url": "https://royalsocietypublishing.org/doi/full/10.1098/rsfs.2018.0041",
        "title": "Semantic information, autonomous agency and non-equilibrium statistical physics | Interface Focus",
        "authors": "",
        "abstract": " Shannon information theory provides various measures of so-called syntactic information,\nwhich reflect the amount of statistical correlation between systems. By contrast,\nthe concept of \u2018semantic information\u2019 refers to those correlations which carry ...",
        "timestamp": "2025-04-06T16:23:53.125Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "information theory",
          "semantic information",
          "agency",
          "autonomy",
          "non-equilibrium",
          "entropy"
        ],
        "doi": "",
        "journalName": "Interface Focus",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2540,
        "object_id": "paper:url.1D5FEC41",
        "created_at": "2025-04-06T16:23:53+00:00",
        "updated_at": "2025-04-06T16:23:56+00:00",
        "version": 1
      }
    },
    "interactions:url.44D45584": {
      "data": {
        "sourceId": "url",
        "paperId": "44D45584",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T16:23:31.527Z",
            "data": {
              "session_id": "session_1743956610776_r9vmclg",
              "source_id": "url",
              "paper_id": "44D45584",
              "start_time": "2025-04-06T16:22:40.072Z",
              "end_time": "2025-04-06T16:23:30.776Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 1,
              "total_elapsed_seconds": 51
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2539,
        "object_id": "interactions:url.44D45584",
        "created_at": "2025-04-06T16:23:32+00:00",
        "updated_at": "2025-04-06T16:24:40+00:00",
        "version": 1
      }
    },
    "paper:url.14D6EC5": {
      "data": {
        "sourceId": "url",
        "paperId": "14D6EC5",
        "url": "https://www.pnas.org/doi/full/10.1073/pnas.2300888120",
        "title": "Partial entropy decomposition reveals higher-order information structures in human brain activity",
        "authors": "Varley, Thomas F., Pope, Maria, Maria Grazia, null, Joshua, null, Sporns, Olaf",
        "abstract": "The standard approach to modeling the human brain as a complex system is with a network,\nwhere the basic unit of interaction is a pairwise link bet...",
        "timestamp": "2025-04-06T16:26:46.518Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "information theory",
          "synergy",
          "higher-order network",
          "fMRI",
          "neuroscience"
        ],
        "doi": "10.1073/pnas.2300888120",
        "journalName": "Proceedings of the National Academy of Sciences",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2543,
        "object_id": "paper:url.14D6EC5",
        "created_at": "2025-04-06T16:26:46+00:00",
        "updated_at": "2025-04-06T16:26:49+00:00",
        "version": 1
      }
    },
    "paper:url.7C3D8AD6": {
      "data": {
        "sourceId": "url",
        "paperId": "7C3D8AD6",
        "url": "https://journals.sagepub.com/doi/full/10.1177/26339137231222481",
        "title": "Designing ecosystems of intelligence from first principles - Karl J Friston, Maxwell JD Ramstead, Alex B Kiefer, Alexander Tschantz, Christopher L Buckley, Mahault Albarracin, Riddhi J Pitliya, Conor Heins, Brennan Klein, Beren Millidge, Dalton AR Sakthivadivel, Toby St Clere Smithe, Magnus Koudahl, Safae Essafi Tremblay, Capm Petersen, Kaiser Fung, Jason G Fox, Steven Swanson, Dan Mapes, Gabriel Ren\u00e9, 2024",
        "authors": "",
        "abstract": "This white paper lays out a vision of research and development in the field of artificial intelligence for the next decade (and beyond). Its denouement is a cyb...",
        "timestamp": "2025-04-06T16:26:22.811Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "Active inference",
          "free energy principle",
          "artificial intelligence",
          "belief updating",
          "belief propagation"
        ],
        "doi": "",
        "journalName": "Collective Intelligence",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2542,
        "object_id": "paper:url.7C3D8AD6",
        "created_at": "2025-04-06T16:26:23+00:00",
        "updated_at": "2025-04-06T16:26:25+00:00",
        "version": 1
      }
    },
    "interactions:url.1D5FEC41": {
      "data": {
        "sourceId": "url",
        "paperId": "1D5FEC41",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T16:25:37.453Z",
            "data": {
              "session_id": "session_1743956736567_i77ph8e",
              "source_id": "url",
              "paper_id": "1D5FEC41",
              "start_time": "2025-04-06T16:23:53.312Z",
              "end_time": "2025-04-06T16:25:36.567Z",
              "heartbeat_count": 20,
              "duration_seconds": 100,
              "idle_seconds": 3,
              "total_elapsed_seconds": 103
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2541,
        "object_id": "interactions:url.1D5FEC41",
        "created_at": "2025-04-06T16:25:38+00:00",
        "updated_at": "2025-04-06T16:26:37+00:00",
        "version": 1
      }
    },
    "paper:url.7ADEE461": {
      "data": {
        "sourceId": "url",
        "paperId": "7ADEE461",
        "url": "https://www.mdpi.com/1099-4300/25/1/54",
        "title": "Flickering Emergences: The Question of Locality in Information-Theoretic Approaches to Emergence",
        "authors": "Varley, Thomas F.",
        "abstract": "\u201cEmergence\u201d, the phenomenon where a complex system displays properties, behaviours, or dynamics not trivially reducible to its constituent elements, is one of the defining properties of complex systems. Recently, there has been a concerted effort to formally define emergence using the mathematical framework of information theory, which proposes that emergence can be understood in terms of how the states of wholes and parts collectively disclose information about the system\u2019s collective future. In this paper, we show how a common, foundational component of information-theoretic approaches to emergence implies an inherent instability to emergent properties, which we call flickering emergence. A system may, on average, display a meaningful emergent property (be it an informative coarse-graining, or higher-order synergy), but for particular configurations, that emergent property falls apart and becomes misinformative. We show existence proofs that flickering emergence occurs in two different frameworks (one based on coarse-graining and another based on multivariate information decomposition) and argue that any approach based on temporal mutual information will display it. Finally, we argue that flickering emergence should not be a disqualifying property of any model of emergence, but that it should be accounted for when attempting to theorize about how emergence relates to practical models of the natural world.",
        "timestamp": "2025-04-06T16:28:33.927Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.3390/e25010054",
        "journalName": "Entropy",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2545,
        "object_id": "paper:url.7ADEE461",
        "created_at": "2025-04-06T16:28:34+00:00",
        "updated_at": "2025-04-06T16:28:37+00:00",
        "version": 1
      }
    },
    "interactions:url.14D6EC5": {
      "data": {
        "sourceId": "url",
        "paperId": "14D6EC5",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T16:27:45.632Z",
            "data": {
              "session_id": "session_1743956864660_2ukziu2",
              "source_id": "url",
              "paper_id": "14D6EC5",
              "start_time": "2025-04-06T16:26:47.178Z",
              "end_time": "2025-04-06T16:27:44.660Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 2,
              "total_elapsed_seconds": 57
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2544,
        "object_id": "interactions:url.14D6EC5",
        "created_at": "2025-04-06T16:27:46+00:00",
        "updated_at": "2025-04-06T16:28:49+00:00",
        "version": 1
      }
    },
    "paper:url.285965E2": {
      "data": {
        "sourceId": "url",
        "paperId": "285965E2",
        "url": "https://onlinelibrary.wiley.com/doi/epdf/10.1111/cogs.13183",
        "title": "The Emergence of Cultural Attractors: How Dynamic Populations of Learners Achieve Collective Cognitive Alignment",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-06T16:34:26.327Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2552,
        "object_id": "paper:url.285965E2",
        "created_at": "2025-04-06T16:34:26+00:00",
        "updated_at": "2025-04-06T16:34:29+00:00",
        "version": 1
      }
    },
    "paper:url.1A45A980": {
      "data": {
        "sourceId": "url",
        "paperId": "1A45A980",
        "url": "https://onlinelibrary.wiley.com/doi/full/10.1111/cogs.13183",
        "title": "The Emergence of Cultural Attractors: How Dynamic Populations of Learners Achieve Collective Cognitive Alignment",
        "authors": "J. Benjamin Falandays, Paul E. Smaldino",
        "abstract": "When a population exhibits collective cognitive alignment, such that group members tend to perceive, remember, and reproduce information in similar ways, the features of socially transmitted variants...",
        "timestamp": "2025-04-06T16:34:11.503Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1111/cogs.13183",
        "journalName": "Cognitive Science",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2551,
        "object_id": "paper:url.1A45A980",
        "created_at": "2025-04-06T16:34:12+00:00",
        "updated_at": "2025-04-06T16:34:14+00:00",
        "version": 1
      }
    },
    "interactions:url.604D8D9F": {
      "data": {
        "sourceId": "url",
        "paperId": "604D8D9F",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T16:32:32.582Z",
            "data": {
              "session_id": "session_1743957151825_w9jrvmn",
              "source_id": "url",
              "paper_id": "604D8D9F",
              "start_time": "2025-04-06T16:32:23.618Z",
              "end_time": "2025-04-06T16:32:31.825Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2550,
        "object_id": "interactions:url.604D8D9F",
        "created_at": "2025-04-06T16:32:33+00:00",
        "updated_at": "2025-04-06T16:33:31+00:00",
        "version": 1
      }
    },
    "paper:url.604D8D9F": {
      "data": {
        "sourceId": "url",
        "paperId": "604D8D9F",
        "url": "https://helda.helsinki.fi/server/api/core/bitstreams/120a9e03-5748-426d-9f73-64279080c77d/content",
        "title": "604D8D9F",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-06T16:32:24.049Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2549,
        "object_id": "paper:url.604D8D9F",
        "created_at": "2025-04-06T16:32:24+00:00",
        "updated_at": "2025-04-06T16:32:26+00:00",
        "version": 1
      }
    },
    "interactions:url.66E8B87E": {
      "data": {
        "sourceId": "url",
        "paperId": "66E8B87E",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T16:32:01.964Z",
            "data": {
              "session_id": "session_1743957121278_lzntxgi",
              "source_id": "url",
              "paper_id": "66E8B87E",
              "start_time": "2025-04-06T16:31:54.371Z",
              "end_time": "2025-04-06T16:32:01.278Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2548,
        "object_id": "interactions:url.66E8B87E",
        "created_at": "2025-04-06T16:32:02+00:00",
        "updated_at": "2025-04-06T16:33:04+00:00",
        "version": 1
      }
    },
    "paper:url.66E8B87E": {
      "data": {
        "sourceId": "url",
        "paperId": "66E8B87E",
        "url": "https://researchportal.helsinki.fi/en/publications/all-intelligence-is-collective-intelligence",
        "title": "All Intelligence is Collective Intelligence",
        "authors": "Falandays J. Benjamin, Roope Oskari Kaaronen, Cody Moser, Wiktor Rorot, Joshua Tan, Vishwanath Varma, Tevin Williams, Mason Youngblood",
        "abstract": "",
        "timestamp": "2025-04-06T16:31:51.696Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.56280/1564736810",
        "journalName": "Journal of Multiscale Neuroscience",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2547,
        "object_id": "paper:url.66E8B87E",
        "created_at": "2025-04-06T16:31:52+00:00",
        "updated_at": "2025-04-06T16:31:54+00:00",
        "version": 1
      }
    },
    "paper:url.68D21D3": {
      "data": {
        "sourceId": "url",
        "paperId": "68D21D3",
        "url": "https://journals.aps.org/pre/abstract/10.1103/PhysRevE.108.014304",
        "title": "Dynamical independence: Discovering emergent macroscopic processes in complex dynamical systems",
        "authors": "L. Barnett, A. K. Seth",
        "abstract": "We introduce a notion of emergence for macroscopic variables associated with highly multivariate microscopic dynamical processes. Dynamical independence instantiates the intuition of an emergent macroscopic process as one possessing the characteristics of a dynamical system ``in its own right,'' with its own dynamical laws distinct from those of the underlying microscopic dynamics. We quantify (departure from) dynamical independence by a transformation-invariant Shannon information-based measure of dynamical dependence. We emphasize the data-driven discovery of dynamically independent macroscopic variables, and introduce the idea of a multiscale ``emergence portrait'' for complex systems. We show how dynamical dependence may be computed explicitly for linear systems in both time and frequency domains, facilitating discovery of emergent phenomena across spatiotemporal scales, and outline application of the linear operationalization to inference of emergence portraits for neural systems from neurophysiological time-series data. We discuss dynamical independence for discrete- and continuous-time deterministic dynamics, with potential application to Hamiltonian mechanics and classical complex systems such as flocking and cellular automata.",
        "timestamp": "2025-04-06T16:30:58.831Z",
        "rating": "novote",
        "publishedDate": "2023/07/17",
        "tags": [],
        "doi": "10.1103/PhysRevE.108.014304",
        "journalName": "Physical Review E",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2546,
        "object_id": "paper:url.68D21D3",
        "created_at": "2025-04-06T16:30:59+00:00",
        "updated_at": "2025-04-06T16:31:02+00:00",
        "version": 1
      }
    },
    "interactions:url.285965E2": {
      "data": {
        "sourceId": "url",
        "paperId": "285965E2",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T16:34:55.097Z",
            "data": {
              "session_id": "session_1743957294345_ce6cfi6",
              "source_id": "url",
              "paper_id": "285965E2",
              "start_time": "2025-04-06T16:34:26.996Z",
              "end_time": "2025-04-06T16:34:54.345Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 2,
              "total_elapsed_seconds": 27
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2553,
        "object_id": "interactions:url.285965E2",
        "created_at": "2025-04-06T16:34:55+00:00",
        "updated_at": "2025-04-06T16:35:57+00:00",
        "version": 1
      }
    },
    "interactions:url.1A45A980": {
      "data": {
        "sourceId": "url",
        "paperId": "1A45A980",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T16:49:34.830Z",
            "data": {
              "session_id": "session_1743958174486_di83au9",
              "source_id": "url",
              "paper_id": "1A45A980",
              "start_time": "2025-04-06T16:49:13.613Z",
              "end_time": "2025-04-06T16:49:34.486Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 1,
              "total_elapsed_seconds": 21
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2555,
        "object_id": "interactions:url.1A45A980",
        "created_at": "2025-04-06T16:49:35+00:00",
        "updated_at": "2025-04-06T16:50:38+00:00",
        "version": 1
      }
    },
    "paper:arxiv.1412.2447": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1412.2447",
        "url": "https://arxiv.org/abs/1412.2447",
        "title": "The Information Theory of Individuality",
        "authors": "Krakauer, David, Bertschinger, Nils, Olbrich, Eckehard, Ay, Nihat, Flack, Jessica C.",
        "abstract": "We consider biological individuality in terms of information theoretic and graphical principles. Our purpose is to extract through an algorithmic decomposition system-environment boundaries supporting individuality. We infer or detect evolved individuals rather than assume that they exist. Given a set of consistent measurements over time, we discover a coarse-grained or quantized description on a system, inducing partitions (which can be nested). Legitimate individual partitions will propagate information from the past into the future, whereas spurious aggregations will not. Individuals are therefore defined in terms of ongoing, bounded information processing units rather than lists of static features or conventional replication-based definitions which tend to fail in the case of cultural change. One virtue of this approach is that it could expand the scope of what we consider adaptive or biological phenomena, particularly in the microscopic and macroscopic regimes of molecular and social phenomena.",
        "timestamp": "2025-04-06T16:43:27.115Z",
        "rating": "novote",
        "publishedDate": "2014/12/08",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2554,
        "object_id": "paper:arxiv.1412.2447",
        "created_at": "2025-04-06T16:43:27+00:00",
        "updated_at": "2025-04-06T16:43:30+00:00",
        "version": 1
      }
    },
    "paper:url.FAEF314": {
      "data": {
        "sourceId": "url",
        "paperId": "FAEF314",
        "url": "https://par.nsf.gov/servlets/purl/10514653",
        "title": "FAEF314",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-06T16:54:01.661Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2559,
        "object_id": "paper:url.FAEF314",
        "created_at": "2025-04-06T16:54:02+00:00",
        "updated_at": "2025-04-06T16:54:04+00:00",
        "version": 1
      }
    },
    "interactions:url.625F5538": {
      "data": {
        "sourceId": "url",
        "paperId": "625F5538",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T16:53:51.886Z",
            "data": {
              "session_id": "session_1743958431158_2bzfl8l",
              "source_id": "url",
              "paper_id": "625F5538",
              "start_time": "2025-04-06T16:53:40.009Z",
              "end_time": "2025-04-06T16:53:51.158Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2558,
        "object_id": "interactions:url.625F5538",
        "created_at": "2025-04-06T16:53:52+00:00",
        "updated_at": "2025-04-06T16:54:53+00:00",
        "version": 1
      }
    },
    "paper:url.625F5538": {
      "data": {
        "sourceId": "url",
        "paperId": "625F5538",
        "url": "https://link.springer.com/article/10.1007/s11571-023-09988-2",
        "title": "A potential mechanism for Gibsonian resonance: behavioral entrainment emerges from local homeostasis in an unsupervised reservoir network",
        "authors": "Falandays, J. Benjamin, Yoshimi, Jeffrey, Warren, William H., Spivey, Michael J.",
        "abstract": "While the cognitivist school of thought holds that the mind is analogous to a computer, performing logical operations over internal representations, the tradition of ecological psychology contends that organisms can directly \u201cresonate\u201d to information for action and perception without the need for a representational intermediary. The concept of resonance has played an important role in ecological psychology, but it remains a metaphor. Supplying a mechanistic account of resonance requires a non-representational account of central nervous system (CNS) dynamics. Towards this, we present a series of simple models in which a reservoir network with homeostatic nodes is used to control a simple agent embedded in an environment. This network spontaneously produces behaviors that are adaptive in each context, including (1) visually tracking a moving object, (2) substantially above-chance performance in the arcade game Pong, (2) and avoiding walls while controlling a mobile agent. Upon analyzing the dynamics of the networks, we find that behavioral stability can be maintained without the formation of stable or recurring patterns of network activity that could be identified as neural representations. These results may represent a useful step towards a mechanistic grounding of resonance and a view of the CNS that is compatible with ecological psychology.",
        "timestamp": "2025-04-06T16:53:39.288Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1007/s11571-023-09988-2",
        "journalName": "Cognitive Neurodynamics",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2557,
        "object_id": "paper:url.625F5538",
        "created_at": "2025-04-06T16:53:39+00:00",
        "updated_at": "2025-04-06T16:53:42+00:00",
        "version": 1
      }
    },
    "paper:url.C59ED1B": {
      "data": {
        "sourceId": "url",
        "paperId": "C59ED1B",
        "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JlcD1lEAAAAJ&citation_for_view=JlcD1lEAAAAJ:Se3iqnhoufwC",
        "title": "\u202aA potential mechanism for Gibsonian resonance: Behavioral entrainment emerges from local homeostasis in an unsupervised reservoir network\u202c",
        "authors": "",
        "abstract": "\u202aJB Falandays, J Yoshimi, WH Warren, MJ Spivey\u202c, \u202aCognitive Neurodynamics, 2024\u202c - \u202aCited by 16\u202c",
        "timestamp": "2025-04-06T16:53:29.876Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2556,
        "object_id": "paper:url.C59ED1B",
        "created_at": "2025-04-06T16:53:30+00:00",
        "updated_at": "2025-04-06T16:53:33+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2401.11380": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2401.11380",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T17:30:37.701Z",
            "data": {
              "session_id": "session_1743960636841_hwuermw",
              "source_id": "arxiv",
              "paper_id": "2401.11380",
              "start_time": "2025-04-06T17:28:53.265Z",
              "end_time": "2025-04-06T17:30:36.841Z",
              "heartbeat_count": 20,
              "duration_seconds": 100,
              "idle_seconds": 4,
              "total_elapsed_seconds": 104
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T17:34:56.427Z",
            "data": {
              "session_id": "session_1743960896385_s2g20dw",
              "source_id": "arxiv",
              "paper_id": "2401.11380",
              "start_time": "2025-04-06T17:34:50.765Z",
              "end_time": "2025-04-06T17:34:56.385Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2562,
        "object_id": "interactions:arxiv.2401.11380",
        "created_at": "2025-04-06T17:30:38+00:00",
        "updated_at": "2025-04-06T17:39:29+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2401.11380": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2401.11380",
        "url": "https://arxiv.org/abs/2401.11380",
        "title": "MoMA: Model-based Mirror Ascent for Offline Reinforcement Learning",
        "authors": "Hong, Mao, Zhang, Zhiyue, Wu, Yue, Xu, Yanxun",
        "abstract": "Model-based offline reinforcement learning methods (RL) have achieved state-of-the-art performance in many decision-making problems thanks to their sample efficiency and generalizability. Despite these advancements, existing model-based offline RL approaches either focus on theoretical studies without developing practical algorithms or rely on a restricted parametric policy space, thus not fully leveraging the advantages of an unrestricted policy space inherent to model-based methods. To address this limitation, we develop MoMA, a model-based mirror ascent algorithm with general function approximations under partial coverage of offline data. MoMA distinguishes itself from existing literature by employing an unrestricted policy class. In each iteration, MoMA conservatively estimates the value function by a minimization procedure within a confidence set of transition models in the policy evaluation step, then updates the policy with general function approximations instead of commonly-used parametric policy classes in the policy improvement step. Under some mild assumptions, we establish theoretical guarantees of MoMA by proving an upper bound on the suboptimality of the returned policy. We also provide a practically implementable, approximate version of the algorithm. The effectiveness of MoMA is demonstrated via numerical studies.",
        "timestamp": "2025-04-06T17:28:53.754Z",
        "rating": "novote",
        "publishedDate": "2024/01/21",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2561,
        "object_id": "paper:arxiv.2401.11380",
        "created_at": "2025-04-06T17:28:54+00:00",
        "updated_at": "2025-04-06T17:28:57+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.19672": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.19672",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T18:09:20.928Z",
            "data": {
              "session_id": "session_1743962960191_zvkg5b2",
              "source_id": "arxiv",
              "paper_id": "2503.19672",
              "start_time": "2025-04-06T18:09:10.001Z",
              "end_time": "2025-04-06T18:09:20.191Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 0,
              "total_elapsed_seconds": 10
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2564,
        "object_id": "interactions:arxiv.2503.19672",
        "created_at": "2025-04-06T18:09:21+00:00",
        "updated_at": "2025-04-06T18:10:21+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.19672": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.19672",
        "url": "https://arxiv.org/abs/2503.19672",
        "title": "Reframing the Free Will Debate: The Universe is Not Deterministic",
        "authors": "Potter, Henry D., Ellis, George F. R., Mitchell, Kevin J.",
        "abstract": "Free will discourse is primarily centred around the thesis of determinism. Much of the literature takes determinism as its starting premise, assuming it true for the sake of discussion, and then proceeds to present arguments for why, if determinism is true, free will would be either possible or impossible. This is reflected in the theoretical terrain of the debate, with the primary distinction currently being between compatibilists and incompatibilists and not, as one might expect, between free will realists and skeptics. The aim of this paper is twofold. First, we argue that there is no reason to accept such a framing. We show that, on the basis of modern physics, there is no good evidence that physical determinism of any variety provides an accurate description of our universe and lots of evidence against such a view. Moreover, we show that this analysis extends equally to the sort of indeterministic worldviews endorsed by many libertarian philosophers and their skeptics, a worldview which we refer to as determinism plus randomness. The papers secondary aim is therefore to present an alternative conception of indeterminism, which is more in line with the empirical evidence from physics. It is this indeterministic worldview, we suggest, that ought to be the central focus of a reframed philosophy of free will.",
        "timestamp": "2025-04-06T18:09:10.614Z",
        "rating": "novote",
        "publishedDate": "2025/03/25",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2563,
        "object_id": "paper:arxiv.2503.19672",
        "created_at": "2025-04-06T18:09:11+00:00",
        "updated_at": "2025-04-06T18:09:13+00:00",
        "version": 1
      }
    },
    "paper:url.378367A7": {
      "data": {
        "sourceId": "url",
        "paperId": "378367A7",
        "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=_e-p2usAAAAJ&sortby=pubdate&citation_for_view=_e-p2usAAAAJ:lYAcb2jw7qUC",
        "title": "\u202aDivergent and convergent creativity are different kinds of foraging\u202c",
        "authors": "",
        "abstract": "\u202aS Malaie, MJ Spivey, T Marghetis\u202c, \u202aPsychological Science, 2024\u202c - \u202aCited by 5\u202c",
        "timestamp": "2025-04-06T18:33:49.742Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2569,
        "object_id": "paper:url.378367A7",
        "created_at": "2025-04-06T18:33:50+00:00",
        "updated_at": "2025-04-06T18:33:53+00:00",
        "version": 1
      }
    },
    "paper:url.387E9159": {
      "data": {
        "sourceId": "url",
        "paperId": "387E9159",
        "url": "https://journals.sagepub.com/doi/pdf/10.1177/09567976241245695",
        "title": "387E9159",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-06T18:33:44.083Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2568,
        "object_id": "paper:url.387E9159",
        "created_at": "2025-04-06T18:33:44+00:00",
        "updated_at": "2025-04-06T18:33:47+00:00",
        "version": 1
      }
    },
    "paper:url.8241EB1": {
      "data": {
        "sourceId": "url",
        "paperId": "8241EB1",
        "url": "https://onlinelibrary.wiley.com/doi/epdf/10.1111/tops.12763",
        "title": "Team Cognition Research Is Transforming Cognitive Science",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-06T18:32:37.913Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2567,
        "object_id": "paper:url.8241EB1",
        "created_at": "2025-04-06T18:32:38+00:00",
        "updated_at": "2025-04-06T18:32:40+00:00",
        "version": 1
      }
    },
    "interactions:url.3A7AF0B1": {
      "data": {
        "sourceId": "url",
        "paperId": "3A7AF0B1",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T18:32:21.309Z",
            "data": {
              "session_id": "session_1743964340331_5iut3t0",
              "source_id": "url",
              "paper_id": "3A7AF0B1",
              "start_time": "2025-04-06T18:31:45.458Z",
              "end_time": "2025-04-06T18:32:20.331Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 5,
              "total_elapsed_seconds": 35
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2566,
        "object_id": "interactions:url.3A7AF0B1",
        "created_at": "2025-04-06T18:32:22+00:00",
        "updated_at": "2025-04-06T18:35:24+00:00",
        "version": 1
      }
    },
    "paper:url.3A7AF0B1": {
      "data": {
        "sourceId": "url",
        "paperId": "3A7AF0B1",
        "url": "https://onlinelibrary.wiley.com/doi/full/10.1111/tops.12763",
        "title": "Team Cognition Research Is Transforming Cognitive Science",
        "authors": "Michael J. Spivey",
        "abstract": "Van Gelder's Dynamical Hypothesis from 30 years ago triggered a domino effect where the analysis of continuous cognitive dynamics inside one organism could be extended to an analysis of continuous co...",
        "timestamp": "2025-04-06T18:31:45.513Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1111/tops.12763",
        "journalName": "Topics in Cognitive Science",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2565,
        "object_id": "paper:url.3A7AF0B1",
        "created_at": "2025-04-06T18:31:45+00:00",
        "updated_at": "2025-04-06T18:31:48+00:00",
        "version": 1
      }
    },
    "interactions:url.247D73F0": {
      "data": {
        "sourceId": "url",
        "paperId": "247D73F0",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T19:07:39.404Z",
            "data": {
              "session_id": "session_1743966458480_fd60b1q",
              "source_id": "url",
              "paper_id": "247D73F0",
              "start_time": "2025-04-06T19:06:37.670Z",
              "end_time": "2025-04-06T19:07:38.480Z",
              "heartbeat_count": 12,
              "duration_seconds": 60,
              "idle_seconds": 1,
              "total_elapsed_seconds": 61
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2572,
        "object_id": "interactions:url.247D73F0",
        "created_at": "2025-04-06T19:07:40+00:00",
        "updated_at": "2025-04-06T19:08:48+00:00",
        "version": 1
      }
    },
    "paper:url.247D73F0": {
      "data": {
        "sourceId": "url",
        "paperId": "247D73F0",
        "url": "https://escholarship.org/content/qt6r15q2ds/qt6r15q2ds.pdf",
        "title": "247D73F0",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-06T19:06:38.016Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 2571,
        "object_id": "paper:url.247D73F0",
        "created_at": "2025-04-06T19:06:38+00:00",
        "updated_at": "2025-04-06T19:06:41+00:00",
        "version": 1
      }
    },
    "paper:url.1CAF1473": {
      "data": {
        "sourceId": "url",
        "paperId": "1CAF1473",
        "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=_e-p2usAAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=_e-p2usAAAAJ:mel-f30kHHgC",
        "title": "\u202aCan Correlates of a Memory be Transferred Between Human Subjects? A New False Memory Paradigm.\u202c",
        "authors": "",
        "abstract": "\u202aJ Rotondo-Valentine, JB Falandays, MJ Spivey, J Wilson\u202c, \u202aUC Merced Undergraduate Research Journal, 2023\u202c",
        "timestamp": "2025-04-06T19:06:27.465Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2570,
        "object_id": "paper:url.1CAF1473",
        "created_at": "2025-04-06T19:06:27+00:00",
        "updated_at": "2025-04-06T19:06:30+00:00",
        "version": 1
      }
    },
    "paper:url.372F409F": {
      "data": {
        "sourceId": "url",
        "paperId": "372F409F",
        "url": "https://www.sciencedirect.com/science/article/abs/pii/S1053810018300515",
        "title": "Bridgemanian space constancy as a precursor to extended cognition",
        "authors": "",
        "abstract": "A few decades ago, cognitive psychologists generally took for granted that the reason we perceive our visual environment as one contiguous stable whol\u2026",
        "timestamp": "2025-04-06T19:13:39.143Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1016/j.concog.2018.04.003",
        "journalName": "Consciousness and Cognition",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2576,
        "object_id": "paper:url.372F409F",
        "created_at": "2025-04-06T19:13:39+00:00",
        "updated_at": "2025-04-06T19:13:42+00:00",
        "version": 1
      }
    },
    "interactions:url.444184C6": {
      "data": {
        "sourceId": "url",
        "paperId": "444184C6",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T19:12:15.221Z",
            "data": {
              "session_id": "session_1743966734521_jn46h4x",
              "source_id": "url",
              "paper_id": "444184C6",
              "start_time": "2025-04-06T19:12:08.903Z",
              "end_time": "2025-04-06T19:12:14.521Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2575,
        "object_id": "interactions:url.444184C6",
        "created_at": "2025-04-06T19:12:16+00:00",
        "updated_at": "2025-04-06T19:13:26+00:00",
        "version": 1
      }
    },
    "paper:url.444184C6": {
      "data": {
        "sourceId": "url",
        "paperId": "444184C6",
        "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=_e-p2usAAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=_e-p2usAAAAJ:DyXnQzXoVgIC",
        "title": "\u202aWho you are: The science of connectedness\u202c",
        "authors": "",
        "abstract": "\u202aMJ Spivey, 2020\u202c - \u202aCited by 32\u202c",
        "timestamp": "2025-04-06T19:12:08.755Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2574,
        "object_id": "paper:url.444184C6",
        "created_at": "2025-04-06T19:12:09+00:00",
        "updated_at": "2025-04-06T19:12:12+00:00",
        "version": 1
      }
    },
    "paper:url.31C84D86": {
      "data": {
        "sourceId": "url",
        "paperId": "31C84D86",
        "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=_e-p2usAAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=_e-p2usAAAAJ:0aBXIfxlw9sC",
        "title": "\u202aCoordination Dynamics of Multi-Agent Human Interaction\u202c",
        "authors": "",
        "abstract": "\u202aS Proksch, M Reeves, R Balasubramaniam, M Spivey, 2021\u202c - \u202aCited by 1\u202c",
        "timestamp": "2025-04-06T19:11:08.942Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2573,
        "object_id": "paper:url.31C84D86",
        "created_at": "2025-04-06T19:11:09+00:00",
        "updated_at": "2025-04-06T19:11:11+00:00",
        "version": 1
      }
    },
    "interactions:url.372F409F": {
      "data": {
        "sourceId": "url",
        "paperId": "372F409F",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T19:13:50.158Z",
            "data": {
              "session_id": "session_1743966829477_pqb2job",
              "source_id": "url",
              "paper_id": "372F409F",
              "start_time": "2025-04-06T19:13:41.577Z",
              "end_time": "2025-04-06T19:13:49.477Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2577,
        "object_id": "interactions:url.372F409F",
        "created_at": "2025-04-06T19:13:50+00:00",
        "updated_at": "2025-04-06T19:14:53+00:00",
        "version": 1
      }
    },
    "interactions:url.48B458C1": {
      "data": {
        "sourceId": "url",
        "paperId": "48B458C1",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-06T19:17:38.968Z",
            "data": {
              "session_id": "session_1743967058224_5of9hce",
              "source_id": "url",
              "paper_id": "48B458C1",
              "start_time": "2025-04-06T19:17:31.809Z",
              "end_time": "2025-04-06T19:17:38.224Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2580,
        "object_id": "interactions:url.48B458C1",
        "created_at": "2025-04-06T19:17:40+00:00",
        "updated_at": "2025-04-06T19:18:39+00:00",
        "version": 1
      }
    },
    "paper:url.48B458C1": {
      "data": {
        "sourceId": "url",
        "paperId": "48B458C1",
        "url": "https://co-mind.org/rdmaterials/php.cv/pdfs/incollection/dale_spivey_systemhood.pdf",
        "title": "48B458C1",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-06T19:17:32.171Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 2579,
        "object_id": "paper:url.48B458C1",
        "created_at": "2025-04-06T19:17:32+00:00",
        "updated_at": "2025-04-06T19:17:35+00:00",
        "version": 1
      }
    },
    "paper:url.6B5D559": {
      "data": {
        "sourceId": "url",
        "paperId": "6B5D559",
        "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=_e-p2usAAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=_e-p2usAAAAJ:kzcSZmkxUKAC",
        "title": "\u202aWeaving oneself into others\u202c",
        "authors": "",
        "abstract": "\u202aR Dale, MJ Spivey, G Br\u00f4ne, B Oben\u202c, \u202aEye-tracking in interaction. Studies on the role of eye gaze in dialogue, 2018\u202c - \u202aCited by 8\u202c",
        "timestamp": "2025-04-06T19:17:25.319Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2578,
        "object_id": "paper:url.6B5D559",
        "created_at": "2025-04-06T19:17:25+00:00",
        "updated_at": "2025-04-06T19:17:28+00:00",
        "version": 1
      }
    },
    "interactions:url.24CF1DD8": {
      "data": {
        "sourceId": "url",
        "paperId": "24CF1DD8",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-07T00:16:19.643Z",
            "data": {
              "session_id": "session_1743984978911_mvfkw9b",
              "source_id": "url",
              "paper_id": "24CF1DD8",
              "start_time": "2025-04-07T00:15:53.904Z",
              "end_time": "2025-04-07T00:16:18.911Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 15,
              "total_elapsed_seconds": 25
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2586,
        "object_id": "interactions:url.24CF1DD8",
        "created_at": "2025-04-07T00:16:20+00:00",
        "updated_at": "2025-04-07T00:17:28+00:00",
        "version": 1
      }
    },
    "paper:url.24CF1DD8": {
      "data": {
        "sourceId": "url",
        "paperId": "24CF1DD8",
        "url": "https://bpb-us-w2.wpmucdn.com/u.osu.edu/dist/6/60429/files/2018/07/psychrev78-1ohb1a5.pdf",
        "title": "24CF1DD8",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-07T00:15:54.254Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 2585,
        "object_id": "paper:url.24CF1DD8",
        "created_at": "2025-04-07T00:15:54+00:00",
        "updated_at": "2025-04-07T00:15:57+00:00",
        "version": 1
      }
    },
    "interactions:url.7AB1A901": {
      "data": {
        "sourceId": "url",
        "paperId": "7AB1A901",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-07T00:15:46.567Z",
            "data": {
              "session_id": "session_1743984945807_4z7cppd",
              "source_id": "url",
              "paper_id": "7AB1A901",
              "start_time": "2025-04-07T00:15:40.502Z",
              "end_time": "2025-04-07T00:15:45.807Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2584,
        "object_id": "interactions:url.7AB1A901",
        "created_at": "2025-04-07T00:15:47+00:00",
        "updated_at": "2025-04-07T00:16:56+00:00",
        "version": 1
      }
    },
    "paper:url.7AB1A901": {
      "data": {
        "sourceId": "url",
        "paperId": "7AB1A901",
        "url": "https://psycnet.apa.org/record/1978-30970-001",
        "title": "A theory of memory retrieval.",
        "authors": "",
        "abstract": "Develops a theory of memory retrieval and shows that it applies over a range of experimental paradigms. Access to memory traces is viewed in terms of a resonance metaphor. The probe item evokes the search set on the basis of probe\u2013memory item relatedness, just as a ringing tuning fork evokes sympathetic vibrations in other tuning forks. Evidence is accumulated in parallel from each probe\u2013memory item comparison, and each comparison is modeled by a continuous random walk process. In item recognition, the decision process is self-terminating on matching comparisons and exhaustive on nonmatching comparisons. The mathematical model produces predictions about accuracy, mean reaction time, error latency, and reaction time distributions that are in good accord with data from 2 experiments conducted with 6 undergraduates. The theory is applied to 4 item recognition paradigms (Sternberg, prememorized list, study\u2013test, and continuous) and to speed\u2013accuracy paradigms; results are found to provide a basis for comparison of these paradigms. It is noted that neural network models can be interfaced to the retrieval theory with little difficulty and that semantic memory models may benefit from such a retrieval scheme. (PsycINFO Database Record (c) 2016 APA, all rights reserved)",
        "timestamp": "2025-04-07T00:15:40.177Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1037/0033-295X.85.2.59",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2583,
        "object_id": "paper:url.7AB1A901",
        "created_at": "2025-04-07T00:15:40+00:00",
        "updated_at": "2025-04-07T00:15:43+00:00",
        "version": 1
      }
    },
    "interactions:url.7E4AE943": {
      "data": {
        "sourceId": "url",
        "paperId": "7E4AE943",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-07T00:13:04.344Z",
            "data": {
              "session_id": "session_1743984783477_85chcqw",
              "source_id": "url",
              "paper_id": "7E4AE943",
              "start_time": "2025-04-07T00:12:09.188Z",
              "end_time": "2025-04-07T00:13:03.477Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 4,
              "total_elapsed_seconds": 54
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2582,
        "object_id": "interactions:url.7E4AE943",
        "created_at": "2025-04-07T00:13:05+00:00",
        "updated_at": "2025-04-07T00:14:05+00:00",
        "version": 1
      }
    },
    "paper:url.7E4AE943": {
      "data": {
        "sourceId": "url",
        "paperId": "7E4AE943",
        "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC4928591/",
        "title": "Diffusion Decision Model: Current Issues and History",
        "authors": "Roger Ratcliff, Philip L Smith, Scott D Brown, Gail McKoon",
        "abstract": "There is growing interest in diffusion models to represent the cognitive and neural processes of speeded decision making. Sequential-sampling models like the diffusion model have a long history in psychology. They view decision making as a process ...",
        "timestamp": "2025-04-07T00:12:08.837Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1016/j.tics.2016.01.007",
        "journalName": "Trends in cognitive sciences",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2581,
        "object_id": "paper:url.7E4AE943",
        "created_at": "2025-04-07T00:12:09+00:00",
        "updated_at": "2025-04-07T00:12:12+00:00",
        "version": 1
      }
    },
    "interactions:url.DCEEEDC": {
      "data": {
        "sourceId": "url",
        "paperId": "DCEEEDC",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-07T00:46:28.748Z",
            "data": {
              "session_id": "session_1743986787764_hjp49j5",
              "source_id": "url",
              "paper_id": "DCEEEDC",
              "start_time": "2025-04-07T00:44:57.146Z",
              "end_time": "2025-04-07T00:46:27.764Z",
              "heartbeat_count": 18,
              "duration_seconds": 90,
              "idle_seconds": 1,
              "total_elapsed_seconds": 91
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2588,
        "object_id": "interactions:url.DCEEEDC",
        "created_at": "2025-04-07T00:46:29+00:00",
        "updated_at": "2025-04-07T00:47:32+00:00",
        "version": 1
      }
    },
    "paper:url.DCEEEDC": {
      "data": {
        "sourceId": "url",
        "paperId": "DCEEEDC",
        "url": "https://www.pnas.org/doi/epub/10.1073/pnas.2102157118",
        "title": "The geometry of decision-making in individuals and collectives",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-07T00:44:56.453Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2587,
        "object_id": "paper:url.DCEEEDC",
        "created_at": "2025-04-07T00:44:56+00:00",
        "updated_at": "2025-04-07T00:44:59+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2301.11445": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2301.11445",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-07T01:43:00.463Z",
            "data": {
              "session_id": "session_1743990179402_cw8qz6i",
              "source_id": "arxiv",
              "paper_id": "2301.11445",
              "start_time": "2025-04-07T01:42:39.737Z",
              "end_time": "2025-04-07T01:42:59.402Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 5,
              "total_elapsed_seconds": 20
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2591,
        "object_id": "interactions:arxiv.2301.11445",
        "created_at": "2025-04-07T01:43:01+00:00",
        "updated_at": "2025-04-07T01:44:00+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2301.11445": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2301.11445",
        "url": "https://arxiv.org/abs/2301.11445",
        "title": "3DShape2VecSet: A 3D Shape Representation for Neural Fields and Generative Diffusion Models",
        "authors": "Zhang, Biao, Tang, Jiapeng, Niessner, Matthias, Wonka, Peter",
        "abstract": "We introduce 3DShape2VecSet, a novel shape representation for neural fields designed for generative diffusion models. Our shape representation can encode 3D shapes given as surface models or point clouds, and represents them as neural fields. The concept of neural fields has previously been combined with a global latent vector, a regular grid of latent vectors, or an irregular grid of latent vectors. Our new representation encodes neural fields on top of a set of vectors. We draw from multiple concepts, such as the radial basis function representation and the cross attention and self-attention function, to design a learnable representation that is especially suitable for processing with transformers. Our results show improved performance in 3D shape encoding and 3D shape generative modeling tasks. We demonstrate a wide variety of generative applications: unconditioned generation, category-conditioned generation, text-conditioned generation, point-cloud completion, and image-conditioned generation.",
        "timestamp": "2025-04-07T01:42:39.095Z",
        "rating": "novote",
        "publishedDate": "2023/01/26",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2589,
        "object_id": "paper:arxiv.2301.11445",
        "created_at": "2025-04-07T01:42:39+00:00",
        "updated_at": "2025-04-07T01:42:42+00:00",
        "version": 1
      }
    },
    "interactions:url.FAEF314": {
      "data": {
        "sourceId": "url",
        "paperId": "FAEF314",
        "interactions": []
      },
      "meta": {
        "issue_number": 2560,
        "object_id": "interactions:url.FAEF314",
        "created_at": "2025-04-06T16:56:08+00:00",
        "updated_at": "2025-04-07T02:34:16+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2305.14314": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2305.14314",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-07T05:31:42.334Z",
            "data": {
              "session_id": "session_1744003901557_0giyu2x",
              "source_id": "arxiv",
              "paper_id": "2305.14314",
              "start_time": "2025-04-07T05:31:34.381Z",
              "end_time": "2025-04-07T05:31:41.557Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2593,
        "object_id": "interactions:arxiv.2305.14314",
        "created_at": "2025-04-07T05:31:43+00:00",
        "updated_at": "2025-04-07T05:32:48+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2305.14314": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2305.14314",
        "url": "https://arxiv.org/abs/2305.14314",
        "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
        "authors": "Dettmers, Tim, Pagnoni, Artidoro, Holtzman, Ari, Zettlemoyer, Luke",
        "abstract": "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.",
        "timestamp": "2025-04-07T05:31:35.175Z",
        "rating": "novote",
        "publishedDate": "2023/05/23",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2592,
        "object_id": "paper:arxiv.2305.14314",
        "created_at": "2025-04-07T05:31:35+00:00",
        "updated_at": "2025-04-07T05:31:38+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.1902.04450": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1902.04450",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-07T05:57:26.432Z",
            "data": {
              "session_id": "session_1744005445499_ut0w20g",
              "source_id": "arxiv",
              "paper_id": "1902.04450",
              "start_time": "2025-04-07T05:56:35.960Z",
              "end_time": "2025-04-07T05:57:25.499Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 5,
              "total_elapsed_seconds": 50
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2595,
        "object_id": "interactions:arxiv.1902.04450",
        "created_at": "2025-04-07T05:57:27+00:00",
        "updated_at": "2025-04-07T05:58:28+00:00",
        "version": 1
      }
    },
    "paper:arxiv.1902.04450": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1902.04450",
        "url": "https://arxiv.org/abs/1902.04450",
        "title": "The Fermi Paradox and the Aurora Effect: Exo-civilization Settlement, Expansion and Steady States",
        "authors": "Carroll-Nellenback, Jonathan, Frank, Adam, Wright, Jason, Scharf, Caleb",
        "abstract": "We model the settlement of the galaxy by space-faring civilizations in order to address issues related to the Fermi Paradox. We explore the problem in a way that avoids assumptions about the intent and motivation of any exo-civilization seeking to settle other planetary systems. We first consider the speed of an advancing settlement via probes of finite velocity and range to determine if the galaxy can become inhabited with space-faring civilizations on timescales shorter than its age. We also include the effect of stellar motions on the long term behavior of the settlement front which adds a diffusive component to its advance. The results of these models demonstrate that the Milky Way can be readily 'filled-in' with settled stellar systems under conservative assumptions about interstellar spacecraft velocities and launch rates. We then consider the question of the galactic steady-state achieved in terms of the fraction of settled planets. We do this by considering the effect of finite settlement civilization lifetimes on the steady states. We find a range of parameters for which the galaxy supports a population of interstellar space-faring civilizations even though some settleable systems are uninhabited. Both results point to ways in which Earth might remain unvisited in the midst of an inhabited galaxy. Finally we consider how our results can be combined with the finite horizon for evidence of previous settlements in Earth's geologic record. Our steady-state model can constrain the probabilities for an Earth visit by a settling civilization before a given time horizon. These results break the link between Hart's famous \"Fact A\" (no interstellar visitors on Earth now) and the conclusion that humans must, therefore, be the only technological civilization in the galaxy.",
        "timestamp": "2025-04-07T05:56:36.784Z",
        "rating": "novote",
        "publishedDate": "2019/02/12",
        "tags": [],
        "doi": "10.3847/1538-3881/ab31a3",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2594,
        "object_id": "paper:arxiv.1902.04450",
        "created_at": "2025-04-07T05:56:37+00:00",
        "updated_at": "2025-04-07T05:56:40+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2006.16734": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2006.16734",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-07T06:00:51.789Z",
            "data": {
              "session_id": "session_1744005651781_jle3839",
              "source_id": "arxiv",
              "paper_id": "2006.16734",
              "start_time": "2025-04-07T06:00:24.679Z",
              "end_time": "2025-04-07T06:00:51.781Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 2,
              "total_elapsed_seconds": 27
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2597,
        "object_id": "interactions:arxiv.2006.16734",
        "created_at": "2025-04-07T06:00:52+00:00",
        "updated_at": "2025-04-07T06:02:02+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2006.16734": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2006.16734",
        "url": "https://arxiv.org/abs/2006.16734",
        "title": "Dyson Spheres",
        "authors": "Wright, Jason T.",
        "abstract": "I review the origins and development of the idea of Dyson spheres, their purpose, their engineering, and their detectability. I explicate the ways in which the popular imagining of them as monolithic objects would make them dynamically unstable under gravity and radiation pressure, and mechanically unstable to buckling. I develop a model for the radiative coupling between a star and large amounts of material orbiting it, and connect the observational features of a star plus Dyson sphere system to the gross radiative properties of the sphere itself. I discuss the still-unexplored problem of the effects of radiative feedback on the central star's structure and luminosity. Finally, I discuss the optimal sizes of Dyson spheres under various assumptions about their purpose as sources of low-entropy emission, dissipative work, or computation.",
        "timestamp": "2025-04-07T05:59:57.430Z",
        "rating": "novote",
        "publishedDate": "2020/06/30",
        "tags": [],
        "doi": "10.2298/SAJ2000001W",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2596,
        "object_id": "paper:arxiv.2006.16734",
        "created_at": "2025-04-07T05:59:57+00:00",
        "updated_at": "2025-04-07T06:00:01+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.03058": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.03058",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-07T06:30:33.455Z",
            "data": {
              "session_id": "session_1744007432082_8byl31o",
              "source_id": "arxiv",
              "paper_id": "2504.03058",
              "start_time": "2025-04-07T06:28:22.323Z",
              "end_time": "2025-04-07T06:30:32.082Z",
              "heartbeat_count": 25,
              "duration_seconds": 125,
              "idle_seconds": 5,
              "total_elapsed_seconds": 130
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2600,
        "object_id": "interactions:arxiv.2504.03058",
        "created_at": "2025-04-07T06:30:34+00:00",
        "updated_at": "2025-04-07T06:31:36+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.03058": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.03058",
        "url": "https://arxiv.org/abs/2504.03058",
        "title": "Global Continuation of Stable Periodic Orbits in Systems of Competing Predators",
        "authors": "Church, Kevin E. M., Dai, Jia-Yuan, H\u00e9not, Olivier, Lappicy, Phillipo, Vassena, Nicola",
        "abstract": "We develop a continuation technique to obtain global families of stable periodic orbits, delimited by transcritical bifurcations at both ends. To this end, we formulate a zero-finding problem whose zeros correspond to families of periodic orbits. We then define a Newton-like fixed-point operator and establish its contraction near a numerically computed approximation of the family. To verify the contraction, we derive sufficient conditions expressed as inequalities on the norms of the fixed-point operator, and involving the numerical approximation. These inequalities are then rigorously checked by the computer via interval arithmetic. To show the efficacy of our approach, we prove the existence of global families in an ecosystem with Holling's type II functional response, and thereby solve a stable connection problem proposed by Butler and Waltler in 1981. Our method does not rely on restricting the choice of parameters and is applicable to many other systems that numerically exhibit global families.",
        "timestamp": "2025-04-07T06:28:22.995Z",
        "rating": "novote",
        "publishedDate": "2025/04/03",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2599,
        "object_id": "paper:arxiv.2504.03058",
        "created_at": "2025-04-07T06:28:23+00:00",
        "updated_at": "2025-04-07T06:28:26+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.03120": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.03120",
        "url": "https://arxiv.org/abs/2504.03120",
        "title": "Distributed Resilience-Aware Control in Multi-Robot Networks",
        "authors": "Lee, Haejoon, Panagou, Dimitra",
        "abstract": "Ensuring resilient consensus in multi-robot systems with misbehaving agents remains a challenge, as many existing network resilience properties are inherently combinatorial and globally defined. While previous works have proposed control laws to enhance or preserve resilience in multi-robot networks, they often assume a fixed topology with known resilience properties, or require global state knowledge. These assumptions may be impractical in physically-constrained environments, where safety and resilience requirements are conflicting, or when misbehaving agents corrupt the shared information. In this work, we propose a distributed control law that enables each robot to guarantee resilient consensus and safety during its navigation without fixed topologies using only locally available information. To this end, we establish a new sufficient condition for resilient consensus in time-varying networks based on the degree of non-misbehaving or normal agents. Using this condition, we design a Control Barrier Function (CBF)-based controller that guarantees resilient consensus and collision avoidance without requiring estimates of global state and/or control actions of all other robots. Finally, we validate our method through simulations.",
        "timestamp": "2025-04-07T06:14:12.983Z",
        "rating": "novote",
        "publishedDate": "2025/04/04",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2598,
        "object_id": "paper:arxiv.2504.03120",
        "created_at": "2025-04-07T06:14:13+00:00",
        "updated_at": "2025-04-07T06:14:16+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.03460": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.03460",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-07T06:52:35.502Z",
            "data": {
              "session_id": "session_1744008754706_fwvzmq9",
              "source_id": "arxiv",
              "paper_id": "2504.03460",
              "start_time": "2025-04-07T06:52:14.277Z",
              "end_time": "2025-04-07T06:52:34.705Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 0,
              "total_elapsed_seconds": 20
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2605,
        "object_id": "interactions:arxiv.2504.03460",
        "created_at": "2025-04-07T06:52:36+00:00",
        "updated_at": "2025-04-07T06:53:39+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.03460": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.03460",
        "url": "https://arxiv.org/abs/2504.03460",
        "title": "Verified Program Extraction in Number Theory: The Fundamental Theorem of Arithmetic and Relatives",
        "authors": "Wiesnet, Franziskus",
        "abstract": "This article revisits standard theorems from elementary number theory through a constructive, algorithmic, and proof-theoretic lens, within the theory of computable functionals. Key examples include B\\`ezout's identity, the fundamental theorem of arithmetic, and Fermat's factorization method. All definitions and theorems are fully formalized in the proof assistant Minlog, thereby laying the foundation for a comprehensive formal framework for number theory within Minlog. While formalization ensures correctness, our primary emphasis is on the computational content of proofs. Leveraging Minlog's built-in program extraction, we obtain executable terms that are exported as Haskell code. Efficiency of the extracted programs plays a central role. We show how performance considerations influence even the initial formulation of theorems and proofs. In particular, we compare formalizations based on binary encodings of natural numbers with those using the traditional unary (successor-based) representation. We present several core proofs in detail and reflect on the challenges that arise from formalization in contrast to informal reasoning. The complete formalisation is available online and linked for reference. Minlog's tactic scripts are designed to follow the structure of natural-language proofs, allowing each derivation step to be traced precisely and helping to bridge the gap between formal and classical mathematical reasoning.",
        "timestamp": "2025-04-07T06:52:13.980Z",
        "rating": "novote",
        "publishedDate": "2025/04/04",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2603,
        "object_id": "paper:arxiv.2504.03460",
        "created_at": "2025-04-07T06:52:14+00:00",
        "updated_at": "2025-04-07T06:52:17+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.02128": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.02128",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-07T06:51:54.902Z",
            "data": {
              "session_id": "session_1744008714080_hf9fc6r",
              "source_id": "arxiv",
              "paper_id": "2504.02128",
              "start_time": "2025-04-07T06:51:44.669Z",
              "end_time": "2025-04-07T06:51:54.080Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2602,
        "object_id": "interactions:arxiv.2504.02128",
        "created_at": "2025-04-07T06:51:56+00:00",
        "updated_at": "2025-04-07T06:53:00+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.02128": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.02128",
        "url": "https://arxiv.org/html/2504.02128v1",
        "title": "Achieving Unanimous Consensus in Decision Making Using Multi-Agents",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-07T06:51:45.362Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "Blockchain",
          "Consensus",
          "Deliberation",
          "Multi-Agent",
          "LLMs",
          "Decision-Making."
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2601,
        "object_id": "paper:arxiv.2504.02128",
        "created_at": "2025-04-07T06:51:45+00:00",
        "updated_at": "2025-04-07T06:51:48+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.03604": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.03604",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-07T06:55:48.164Z",
            "data": {
              "session_id": "session_1744008947165_kkg87ip",
              "source_id": "arxiv",
              "paper_id": "2504.03604",
              "start_time": "2025-04-07T06:55:41.699Z",
              "end_time": "2025-04-07T06:55:47.165Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2607,
        "object_id": "interactions:arxiv.2504.03604",
        "created_at": "2025-04-07T06:55:49+00:00",
        "updated_at": "2025-04-07T06:56:58+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.03604": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.03604",
        "url": "https://arxiv.org/abs/2504.03604",
        "title": "Epicast 2.0: A large-scale, demographically detailed, agent-based model for simulating respiratory pathogen spread in the United States",
        "authors": "Alexander, Prescott C., Harris, Thomas J., Kitson, Joy, Tuccillo, Joseph V., Del Valle, Sara Y., Germann, Timothy C.",
        "abstract": "The recent history of respiratory pathogen epidemics, including those caused by influenza and SARS-CoV-2, has highlighted the urgent need for advanced modeling approaches that can accurately capture heterogeneous disease dynamics and outcomes at the national scale, thereby enhancing the effectiveness of resource allocation and decision-making. In this paper, we describe Epicast 2.0, an agent-based model that utilizes a highly detailed, synthetic population and high-performance computing techniques to simulate respiratory pathogen transmission across the entire United States. This model replicates the contact patterns of over 320 million agents as they engage in daily activities at school, work, and within their communities. Epicast 2.0 supports vaccination and an array of non-pharmaceutical interventions that can be promoted or relaxed via highly granular, user specified policies. We illustrate the model's capabilities using a wide range of outbreak scenarios, highlighting the model's varied dynamics as well as its extensive support for policy exploration. This model provides a robust platform for conducting what if scenario analysis and providing insights into potential strategies for mitigating the impacts of infectious diseases.",
        "timestamp": "2025-04-07T06:55:42.386Z",
        "rating": "novote",
        "publishedDate": "2025/04/04",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2606,
        "object_id": "paper:arxiv.2504.03604",
        "created_at": "2025-04-07T06:55:42+00:00",
        "updated_at": "2025-04-07T06:55:45+00:00",
        "version": 1
      }
    },
    "interactions:url.2893554D": {
      "data": {
        "sourceId": "url",
        "paperId": "2893554D",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-07T12:44:49.308Z",
            "data": {
              "session_id": "session_1744029888399_bdu9479",
              "source_id": "url",
              "paper_id": "2893554D",
              "start_time": "2025-04-07T12:44:39.894Z",
              "end_time": "2025-04-07T12:44:48.399Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2609,
        "object_id": "interactions:url.2893554D",
        "created_at": "2025-04-07T12:44:50+00:00",
        "updated_at": "2025-04-07T12:46:28+00:00",
        "version": 1
      }
    },
    "paper:url.2893554D": {
      "data": {
        "sourceId": "url",
        "paperId": "2893554D",
        "url": "https://www.biorxiv.org/content/10.1101/2025.04.02.646835v1",
        "title": "The extended language network: Language selective brain areas whose contributions to language remain to be discovered",
        "authors": "Agata Wolna, Aaron Wright, Benjamin Lipkin, Evelina Fedorenko",
        "abstract": "Although language neuroscience has largely focused on \u2018core\u2019 left frontal and temporal brain areas and their right-hemisphere homotopes, numerous other areas\u2014cortical, subcortical, and cerebellar\u2014have been implicated in linguistic processing. However, these areas\u2019 contributions to language remain unclear given that the evidence for their recruitment comes from diverse paradigms, many of which conflate language processing with perceptual, motor, or task-related cognitive processes. Using fMRI data from 772 participants performing an extensively-validated language \u2018localizer\u2019 paradigm that isolates language processing from other processes, we a) delineate a comprehensive set of areas that respond reliably to language across written and auditory modalities, and b) evaluate these areas\u2019 selectivity for language relative to a demanding non-linguistic task. In line with prior claims, many areas outside the core fronto-temporal network respond during language processing, and most of them show selectivity for language relative to general task demands. These language-selective areas of the extended language network include areas around the temporal poles, in the medial frontal cortex, in the hippocampus, and in the cerebellum, among others. Although distributed across many parts of the brain, the extended language-selective network still only comprises \u223c1.2% of the brain\u2019s volume and is about the size of a strawberry, challenging the view that language processing is broadly distributed across the cortical surface. These newly identified language-selective areas can now be systematically characterized to decipher their contributions to language processing, including testing whether these contributions differ from those of the core language areas.\n\nSignificance statement Language processing consistently recruits a left-lateralized fronto-temporal brain network, but language tasks often engage areas outside this core system. In an fMRI dataset of 772 participants performing a validated language localizer task, we identified 17 brain areas outside the core network that respond to both auditory and written language. Most of these areas show selectivity for language, including regions in the temporal poles, medial frontal cortex, hippocampus, and cerebellum. Despite its large number of components, this extended language network still only takes up \u223c1.2% of the brain\u2019s volume, challenging the view that language processing is broadly distributed in the brain. These findings lay the foundation for systematically characterizing these newly identified non-canonical language-selective areas.\n\n### Competing Interest Statement\n\nThe authors have declared no competing interest.",
        "timestamp": "2025-04-07T12:44:39.245Z",
        "rating": "novote",
        "publishedDate": "2025-04-03",
        "tags": [],
        "doi": "10.1101/2025.04.02.646835",
        "journalName": "bioRxiv",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2608,
        "object_id": "paper:url.2893554D",
        "created_at": "2025-04-07T12:44:39+00:00",
        "updated_at": "2025-04-07T12:44:42+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.1910.10683": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1910.10683",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-07T19:47:59.115Z",
            "data": {
              "session_id": "session_1744055278369_xxpitjs",
              "source_id": "arxiv",
              "paper_id": "1910.10683",
              "start_time": "2025-04-07T19:47:38.452Z",
              "end_time": "2025-04-07T19:47:58.369Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 5,
              "total_elapsed_seconds": 20
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-07T19:53:13.376Z",
            "data": {
              "session_id": "session_1744055592478_y2oqe3g",
              "source_id": "arxiv",
              "paper_id": "1910.10683",
              "start_time": "2025-04-07T19:50:05.551Z",
              "end_time": "2025-04-07T19:53:12.478Z",
              "heartbeat_count": 37,
              "duration_seconds": 185,
              "idle_seconds": 2,
              "total_elapsed_seconds": 187
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2612,
        "object_id": "interactions:arxiv.1910.10683",
        "created_at": "2025-04-07T19:48:00+00:00",
        "updated_at": "2025-04-07T19:54:08+00:00",
        "version": 1
      }
    },
    "paper:arxiv.1910.10683": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1910.10683",
        "url": "https://arxiv.org/abs/1910.10683",
        "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "authors": "Raffel, Colin, Shazeer, Noam, Roberts, Adam, Lee, Katherine, Narang, Sharan, Matena, Michael, Zhou, Yanqi, Li, Wei, Liu, Peter J.",
        "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",
        "timestamp": "2025-04-07T19:47:38.887Z",
        "rating": "novote",
        "publishedDate": "2019/10/23",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2611,
        "object_id": "paper:arxiv.1910.10683",
        "created_at": "2025-04-07T19:47:39+00:00",
        "updated_at": "2025-04-07T19:47:42+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.24366": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.24366",
        "url": "https://arxiv.org/abs/2503.24366",
        "title": "StochasticSplats: Stochastic Rasterization for Sorting-Free 3D Gaussian Splatting",
        "authors": "Kheradmand, Shakiba, Vicini, Delio, Kopanas, George, Lagun, Dmitry, Yi, Kwang Moo, Matthews, Mark, Tagliasacchi, Andrea",
        "abstract": "3D Gaussian splatting (3DGS) is a popular radiance field method, with many application-specific extensions. Most variants rely on the same core algorithm: depth-sorting of Gaussian splats then rasterizing in primitive order. This ensures correct alpha compositing, but can cause rendering artifacts due to built-in approximations. Moreover, for a fixed representation, sorted rendering offers little control over render cost and visual fidelity. For example, and counter-intuitively, rendering a lower-resolution image is not necessarily faster. In this work, we address the above limitations by combining 3D Gaussian splatting with stochastic rasterization. Concretely, we leverage an unbiased Monte Carlo estimator of the volume rendering equation. This removes the need for sorting, and allows for accurate 3D blending of overlapping Gaussians. The number of Monte Carlo samples further imbues 3DGS with a way to trade off computation time and quality. We implement our method using OpenGL shaders, enabling efficient rendering on modern GPU hardware. At a reasonable visual quality, our method renders more than four times faster than sorted rasterization.",
        "timestamp": "2025-04-07T19:41:57.495Z",
        "rating": "novote",
        "publishedDate": "2025/03/31",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2610,
        "object_id": "paper:arxiv.2503.24366",
        "created_at": "2025-04-07T19:41:58+00:00",
        "updated_at": "2025-04-07T19:42:01+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2203.17189": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2203.17189",
        "url": "https://arxiv.org/abs/2203.17189",
        "title": "Scaling Up Models and Data with $\\texttt{t5x}$ and $\\texttt{seqio}$",
        "authors": "Roberts, Adam, Chung, Hyung Won, Levskaya, Anselm, Mishra, Gaurav, Bradbury, James, Andor, Daniel, Narang, Sharan, Lester, Brian, Gaffney, Colin, Mohiuddin, Afroz, Hawthorne, Curtis, Lewkowycz, Aitor, Salcianu, Alex, van Zee, Marc, Austin, Jacob, Goodman, Sebastian, Soares, Livio Baldini, Hu, Haitang, Tsvyashchenko, Sasha, Chowdhery, Aakanksha, Bastings, Jasmijn, Bulian, Jannis, Garcia, Xavier, Ni, Jianmo, Chen, Andrew, Kenealy, Kathleen, Clark, Jonathan H., Lee, Stephan, Garrette, Dan, Lee-Thorp, James, Raffel, Colin, Shazeer, Noam, Ritter, Marvin, Bosma, Maarten, Passos, Alexandre, Maitin-Shepard, Jeremy, Fiedel, Noah, Omernick, Mark, Saeta, Brennan, Sepassi, Ryan, Spiridonov, Alexander, Newlan, Joshua, Gesmundo, Andrea",
        "abstract": "Recent neural network-based language models have benefited greatly from scaling up the size of training datasets and the number of parameters in the models themselves. Scaling can be complicated due to various factors including the need to distribute computation on supercomputer clusters (e.g., TPUs), prevent bottlenecks when infeeding data, and ensure reproducible results. In this work, we present two software libraries that ease these issues: $\\texttt{t5x}$ simplifies the process of building and training large language models at scale while maintaining ease of use, and $\\texttt{seqio}$ provides a task-based API for simple creation of fast and reproducible training data and evaluation pipelines. These open-source libraries have been used to train models with hundreds of billions of parameters on datasets with multiple terabytes of training data. Along with the libraries, we release configurations and instructions for T5-like encoder-decoder models as well as GPT-like decoder-only architectures. $\\texttt{t5x}$ and $\\texttt{seqio}$ are open source and available at https://github.com/google-research/t5x and https://github.com/google/seqio, respectively.",
        "timestamp": "2025-04-07T19:53:39.474Z",
        "rating": "novote",
        "publishedDate": "2022/03/31",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2613,
        "object_id": "paper:arxiv.2203.17189",
        "created_at": "2025-04-07T19:53:39+00:00",
        "updated_at": "2025-04-07T19:53:42+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2203.17189": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2203.17189",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-07T19:54:21.496Z",
            "data": {
              "session_id": "session_1744055660515_3v8233v",
              "source_id": "arxiv",
              "paper_id": "2203.17189",
              "start_time": "2025-04-07T19:53:39.673Z",
              "end_time": "2025-04-07T19:54:20.515Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 1,
              "total_elapsed_seconds": 41
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2615,
        "object_id": "interactions:arxiv.2203.17189",
        "created_at": "2025-04-07T19:54:22+00:00",
        "updated_at": "2025-04-07T19:55:23+00:00",
        "version": 1
      }
    },
    "paper:url.6DE4F59": {
      "data": {
        "sourceId": "url",
        "paperId": "6DE4F59",
        "url": "https://www.biorxiv.org/content/10.1101/2025.03.24.645064v1",
        "title": "Recasting adaptation as strategy inference",
        "authors": "Sami Beaumont, Mehdi Khamassi, Philippe Domenech",
        "abstract": "Flexible adaptation to uncertain and changing environments requires dynamic adjustments in behavioral strategies. While classical learning theories emphasize incremental strengthening of local stimulus-action associations in adaptation, emerging evidence suggests that global-level strategy representations may enable rapid inference of adaptive behaviors, thus promoting efficient decision-making. However, it remains unclear to what extent direct inference over putative strategies can fully account for human adaptation across diverse statistical contexts. Here, we demonstrate clear behavioral markers supporting the broad use of inference over strategies in human adapting to rapid changes. These markers are fully explained solely by a novel model of inference over a structured space of strategies. We further show that inference over strategies is influenced by latent contextual statistics that are beyond the scope of models based on incremental learning. Taken together, these results establish the importance of direct inference over an abstract strategy space for flexible adaptation in humans.\n\n### Competing Interest Statement\n\nThe authors have declared no competing interest.",
        "timestamp": "2025-04-08T06:31:03.561Z",
        "rating": "novote",
        "publishedDate": "2025-03-25",
        "tags": [],
        "doi": "10.1101/2025.03.24.645064",
        "journalName": "bioRxiv",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2618,
        "object_id": "paper:url.6DE4F59",
        "created_at": "2025-04-08T06:31:04+00:00",
        "updated_at": "2025-04-08T06:31:06+00:00",
        "version": 1
      }
    },
    "paper:url.DEFF937": {
      "data": {
        "sourceId": "url",
        "paperId": "DEFF937",
        "url": "https://psycnet.apa.org/fulltext/2025-57011-001.pdf",
        "title": "DEFF937",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-08T06:29:27.992Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 2617,
        "object_id": "paper:url.DEFF937",
        "created_at": "2025-04-08T06:29:28+00:00",
        "updated_at": "2025-04-08T06:29:31+00:00",
        "version": 1
      }
    },
    "paper:url.4FED6CB4": {
      "data": {
        "sourceId": "url",
        "paperId": "4FED6CB4",
        "url": "https://psycnet.apa.org/fulltext/2025-57011-001.html",
        "title": "APA PsycNet FullTextHTML page",
        "authors": "",
        "abstract": "APA PsycNet FullTextHTML page",
        "timestamp": "2025-04-08T06:29:12.015Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2616,
        "object_id": "paper:url.4FED6CB4",
        "created_at": "2025-04-08T06:29:12+00:00",
        "updated_at": "2025-04-08T06:29:15+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.03950": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.03950",
        "url": "https://arxiv.org/abs/2503.03950",
        "title": "The Nature of Organization in Living Systems",
        "authors": "M\u00e1rquez-Zacar\u00edas, Pedro, Ortiz-Mu\u00f1oz, Andr\u00e9s, Bingham, Emma P.",
        "abstract": "Living systems are thermodynamically open but closed in their organization. In other words, even though their material components turn over constantly, a material-independent property persists, which we call organization. Moreover, organization comes from within organisms themselves, which requires us to explain how this self-organization is established and maintained. In this paper we propose a mathematical and conceptual framework to understand the kinds of organized systems that living systems are, aiming to explain how self-organization emerges from more basic elemental processes. Additionally, we map our own notions to existing traditions in theoretical biology and philosophy, aiming to bring the main formal ideas into conceptual congruence.",
        "timestamp": "2025-04-09T00:15:05.970Z",
        "rating": "novote",
        "publishedDate": "2025/03/05",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2622,
        "object_id": "paper:arxiv.2503.03950",
        "created_at": "2025-04-09T00:15:06+00:00",
        "updated_at": "2025-04-09T00:15:09+00:00",
        "version": 1
      }
    },
    "paper:url.2EE21EC3": {
      "data": {
        "sourceId": "url",
        "paperId": "2EE21EC3",
        "url": "https://download.ssrn.com/24/05/13/ssrn_id4826613_code375637.pdf?response-content-disposition=inline&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEAgaCXVzLWVhc3QtMSJIMEYCIQDrJN9pk31LrCFvAAkXI%2F3o9xSAY%2BwAlrY1S3%2BPS98waAIhAL1Dy3TbvaOYjVCMCDDvzpNfTnnmxngqU4Xyaaj7bdB4KscFCIH%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQBBoMMzA4NDc1MzAxMjU3IgwC7PgyvmfhO9GrYiYqmwUc%2BwqSgQezULuh75M7ePeaWkHlN0LJMhYbH%2BQ2z0L48mqmOCHu4Mt2626ej5xgFmBJV6q6uuDlvpPUc%2BirJdhTZyPkooj1ipGQCHjVsyDKjpU5RzMvzx40CbqGTsXmaY6HczqsYrQq2tAPPTSWbDlWgW1gVtG7Czbnws23jRnk%2FkNvNGg9f%2F%2FK60vnTkX9XYVmhez0cO%2BeDSpnnGla0P9E90e0VfxqP7kEzy%2BHLWHSv7h7TDuK7A9KQAcrJAYhvqHHLSPraVQA8%2BjtIQKEqaRmWOZB9kQi4VGZvXz1gOzXvLubFYg96oRjbcrLsJTtlMb7TuYzBME1ral4VDSCGnDEyYIYvO07tnlp4d0WCN61e2x40i0N84MQfBk0Xy1nPeaPvJwT87PstJr9MmZvoifM4Qr1dn9JqGU7%2Fw2of8lzULBnbJZwKSzPj2etVz%2BPL9tWiJouek6ZfQSp3YnnEMSpLg3jqn5naYXilfA%2FTfm2qibXjmlLTCUSIHxJN4UtteOu4%2F%2FUS3fscndva9RbJJsEWr1UEhmfvg7Uo9wxFoKlJaCNOry82hRFWlDFxr8%2BVvnmzJpnTGgsvmY7bbKpNzdJEjEpfsxtSzQFM6gZJiffV%2FB5S2GGv2dUrs%2FEj6DOLwYkRVwf9x7XpPwJnB1x90xWsEVKEYKnWtCgQkR%2F8EUG3CblOS3UK6gNnPKz9%2Bqw63n4Ge5rL2YXgSOBvgt0ltywuQCAcvn0MvKzXgApwo5MyVM4gM8yi4Djm34bvFUmXh6HRz%2B9aMiU7jSV%2Bgd2bcUmL%2BJexHITHHwxGkojGqwsJurpYBsApQNRihxrhFp9LqALkFR74pBXhHp%2B05PB0Ef2odGYOGo7wmwB7hvxB81GoThtQC8%2BbUFdDfIUMMXv1r8GOrABKuKI%2FSxh%2FEZLAcl73bO9lU4rllRAXhLbe1buIAUFY6W0Zz%2B28d2eNWz0sIeZHfPBrYln6b2lwEVAF7PuJcj5jGLEeScwyfqYVnSwXKgI8JJJcLD4JHtuAIrrXphzW%2FKiJj9%2FXEP4L%2FSL5caFkkqt70LsS%2FgbgnX51ImX6IDjAMXfx%2F3qW4ZuYLFKtMOOcMatcs7FivbViXzsjrNsG5brwjU7vtV6Rbd1UsEu0bWD7N8%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250409T001408Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAUPUUPRWERYZRPMDD%2F20250409%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=8b139e29d048aaf571d2bb63e9d712cfca90d8a346a006b488b0c93784a7f451&abstractId=4826613",
        "title": "2EE21EC3",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-09T00:14:18.175Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2621,
        "object_id": "paper:url.2EE21EC3",
        "created_at": "2025-04-09T00:14:18+00:00",
        "updated_at": "2025-04-09T00:14:21+00:00",
        "version": 1
      }
    },
    "interactions:url.419CFF2D": {
      "data": {
        "sourceId": "url",
        "paperId": "419CFF2D",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-09T00:14:08.463Z",
            "data": {
              "session_id": "session_1744157647587_uvq2drc",
              "source_id": "url",
              "paper_id": "419CFF2D",
              "start_time": "2025-04-09T00:13:58.907Z",
              "end_time": "2025-04-09T00:14:07.587Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2620,
        "object_id": "interactions:url.419CFF2D",
        "created_at": "2025-04-09T00:14:09+00:00",
        "updated_at": "2025-04-09T00:15:08+00:00",
        "version": 1
      }
    },
    "paper:url.419CFF2D": {
      "data": {
        "sourceId": "url",
        "paperId": "419CFF2D",
        "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4826613",
        "title": "Debunking Robot Rights Metaphysically, Ethically, and Legally",
        "authors": "Birhane, Abeba, van Dijk, Jelle, Pasquale, Frank",
        "abstract": "In this work we challenge the argument for robot rights on metaphysical, ethical and legal grounds. Metaphysically, we argue that machines are not the kinds of ",
        "timestamp": "2025-04-09T00:13:57.142Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "SSRN",
          "Debunking Robot Rights Metaphysically",
          "Ethically",
          "and Legally",
          "Abeba Birhane",
          "Jelle van Dijk",
          "Frank Pasquale"
        ],
        "doi": "10.2139/ssrn.4826613",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2619,
        "object_id": "paper:url.419CFF2D",
        "created_at": "2025-04-09T00:13:57+00:00",
        "updated_at": "2025-04-09T00:14:00+00:00",
        "version": 1
      }
    },
    "paper:url.67FCF3E5": {
      "data": {
        "sourceId": "url",
        "paperId": "67FCF3E5",
        "url": "https://www.cl.cam.ac.uk/~jgd1000/metaphors.pdf",
        "title": "67FCF3E5",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-09T00:19:03.167Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 2627,
        "object_id": "paper:url.67FCF3E5",
        "created_at": "2025-04-09T00:19:03+00:00",
        "updated_at": "2025-04-14T06:00:55+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.00797": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.00797",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-09T00:18:01.884Z",
            "data": {
              "session_id": "session_1744157881306_jtaibns",
              "source_id": "arxiv",
              "paper_id": "2504.00797",
              "start_time": "2025-04-09T00:17:42.424Z",
              "end_time": "2025-04-09T00:18:01.306Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 4,
              "total_elapsed_seconds": 19
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2626,
        "object_id": "interactions:arxiv.2504.00797",
        "created_at": "2025-04-09T00:18:02+00:00",
        "updated_at": "2025-04-09T00:19:11+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.00797": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.00797",
        "url": "https://arxiv.org/abs/2504.00797",
        "title": "Bridging the Gap: Integrating Ethics and Environmental Sustainability in AI Research and Practice",
        "authors": "Luccioni, Alexandra Sasha, Pistilli, Giada, Sefala, Raesetje, Moorosi, Nyalleng",
        "abstract": "As the possibilities for Artificial Intelligence (AI) have grown, so have concerns regarding its impacts on society and the environment. However, these issues are often raised separately; i.e. carbon footprint analyses of AI models typically do not consider how the pursuit of scale has contributed towards building models that are both inaccessible to most researchers in terms of cost and disproportionately harmful to the environment. On the other hand, model audits that aim to evaluate model performance and disparate impacts mostly fail to engage with the environmental ramifications of AI models and how these fit into their auditing approaches. In this separation, both research directions fail to capture the depth of analysis that can be explored by considering the two in parallel and the potential solutions for making informed choices that can be developed at their convergence. In this essay, we build upon work carried out in AI and in sister communities, such as philosophy and sustainable development, to make more deliberate connections around topics such as generalizability, transparency, evaluation and equity across AI research and practice. We argue that the efforts aiming to study AI's ethical ramifications should be made in tandem with those evaluating its impacts on the environment, and we conclude with a proposal of best practices to better integrate AI ethics and sustainability in AI research and practice.",
        "timestamp": "2025-04-09T00:17:41.457Z",
        "rating": "novote",
        "publishedDate": "2025/04/01",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2625,
        "object_id": "paper:arxiv.2504.00797",
        "created_at": "2025-04-09T00:17:41+00:00",
        "updated_at": "2025-04-09T00:17:44+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.03950": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.03950",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-09T00:15:29.616Z",
            "data": {
              "session_id": "session_1744157728869_ztolalj",
              "source_id": "arxiv",
              "paper_id": "2503.03950",
              "start_time": "2025-04-09T00:15:05.625Z",
              "end_time": "2025-04-09T00:15:28.869Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 3,
              "total_elapsed_seconds": 23
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-09T00:17:17.745Z",
            "data": {
              "session_id": "session_1744157837160_40m09ws",
              "source_id": "arxiv",
              "paper_id": "2503.03950",
              "start_time": "2025-04-09T00:15:35.416Z",
              "end_time": "2025-04-09T00:17:17.159Z",
              "heartbeat_count": 20,
              "duration_seconds": 100,
              "idle_seconds": 2,
              "total_elapsed_seconds": 102
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2624,
        "object_id": "interactions:arxiv.2503.03950",
        "created_at": "2025-04-09T00:15:30+00:00",
        "updated_at": "2025-04-09T00:18:25+00:00",
        "version": 1
      }
    },
    "interactions:url.9C27909": {
      "data": {
        "sourceId": "url",
        "paperId": "9C27909",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-09T00:21:11.512Z",
            "data": {
              "session_id": "session_1744158070812_h9ydgcx",
              "source_id": "url",
              "paper_id": "9C27909",
              "start_time": "2025-04-09T00:21:03.355Z",
              "end_time": "2025-04-09T00:21:10.812Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2629,
        "object_id": "interactions:url.9C27909",
        "created_at": "2025-04-09T00:21:12+00:00",
        "updated_at": "2025-04-09T00:22:12+00:00",
        "version": 1
      }
    },
    "paper:url.9C27909": {
      "data": {
        "sourceId": "url",
        "paperId": "9C27909",
        "url": "https://www.nature.com/articles/s41467-025-58043-7",
        "title": "Deep reinforcement learning can promote sustainable human behaviour in a common-pool resource problem",
        "authors": "Koster, Raphael, P\u00eeslar, Miruna, Tacchetti, Andrea, Balaguer, Jan, Liu, Leqi, Elie, Romuald, Hauser, Oliver P., Tuyls, Karl, Botvinick, Matt, Summerfield, Christopher",
        "abstract": "Koster et al introduce a deep reinforcement learning (RL) mechanism designed to manage common-pool resources successfully encourages sustainable cooperation among human participants by dynamically adjusting resource allocations based on the current state of the resource pool. The RL-derived policy outperforms traditional allocation methods by balancing generosity when resources are abundant and applying temporary sanctions to discourage free-riding, ultimately maximizing social welfare and fairness.",
        "timestamp": "2025-04-09T00:21:02.726Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1038/s41467-025-58043-7",
        "journalName": "Nature Communications",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2628,
        "object_id": "paper:url.9C27909",
        "created_at": "2025-04-09T00:21:03+00:00",
        "updated_at": "2025-04-09T00:21:06+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2410.23179": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.23179",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-09T06:15:05.887Z",
            "data": {
              "session_id": "session_1744179305136_phrj2d9",
              "source_id": "arxiv",
              "paper_id": "2410.23179",
              "start_time": "2025-04-09T06:14:49.931Z",
              "end_time": "2025-04-09T06:15:05.136Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 0,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2633,
        "object_id": "interactions:arxiv.2410.23179",
        "created_at": "2025-04-09T06:15:07+00:00",
        "updated_at": "2025-04-09T06:16:11+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2410.23179": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.23179",
        "url": "https://arxiv.org/abs/2410.23179",
        "title": "Does equivariance matter at scale?",
        "authors": "Brehmer, Johann, Behrends, S\u00f6nke, de Haan, Pim, Cohen, Taco",
        "abstract": "Given large data sets and sufficient compute, is it beneficial to design neural architectures for the structure and symmetries of each problem? Or is it more efficient to learn them from data? We study empirically how equivariant and non-equivariant networks scale with compute and training samples. Focusing on a benchmark problem of rigid-body interactions and on general-purpose transformer architectures, we perform a series of experiments, varying the model size, training steps, and dataset size. We find evidence for three conclusions. First, equivariance improves data efficiency, but training non-equivariant models with data augmentation can close this gap given sufficient epochs. Second, scaling with compute follows a power law, with equivariant models outperforming non-equivariant ones at each tested compute budget. Finally, the optimal allocation of a compute budget onto model size and training duration differs between equivariant and non-equivariant models.",
        "timestamp": "2025-04-09T06:14:50.041Z",
        "rating": "novote",
        "publishedDate": "2024/10/30",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2631,
        "object_id": "paper:arxiv.2410.23179",
        "created_at": "2025-04-09T06:14:50+00:00",
        "updated_at": "2025-04-09T06:14:53+00:00",
        "version": 1
      }
    },
    "interactions:url.41E58C89": {
      "data": {
        "sourceId": "url",
        "paperId": "41E58C89",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-09T06:26:48.052Z",
            "data": {
              "session_id": "session_1744180007243_3ulz0gu",
              "source_id": "url",
              "paper_id": "41E58C89",
              "start_time": "2025-04-09T06:26:40.271Z",
              "end_time": "2025-04-09T06:26:47.243Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2637,
        "object_id": "interactions:url.41E58C89",
        "created_at": "2025-04-09T06:26:49+00:00",
        "updated_at": "2025-04-09T06:27:50+00:00",
        "version": 1
      }
    },
    "paper:url.41E58C89": {
      "data": {
        "sourceId": "url",
        "paperId": "41E58C89",
        "url": "https://washingtonmonthly.com/2025/04/08/trumps-tariffs-arent-economics-theyre-a-cultural-purge/",
        "title": "Trump's Tariffs Aren't Economics. They're a Cultural Purge | Washington Monthly",
        "authors": "David Atkins",
        "abstract": "Trump's blunderbuss tariff policies are a declaration of war on the half of America that didn\u2019t vote for him.",
        "timestamp": "2025-04-09T06:26:40.841Z",
        "rating": "novote",
        "publishedDate": "2025-04-08T14:24:36+00:00",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2636,
        "object_id": "paper:url.41E58C89",
        "created_at": "2025-04-09T06:26:41+00:00",
        "updated_at": "2025-04-09T06:26:44+00:00",
        "version": 1
      }
    },
    "paper:url.588B45FD": {
      "data": {
        "sourceId": "url",
        "paperId": "588B45FD",
        "url": "https://journals.sagepub.com/doi/abs/10.1177/10911421241250088",
        "title": "Estimating the Negative Credit Impacts of Police-involved Fatalities - Tatyana Guzman, Benjamin Y. Clark, 2025",
        "authors": "",
        "abstract": "George Floyd's killing in Minneapolis spotlighted the role of police-involved fatalities that cause immeasurable harm to victims\u2019 families and communities,...",
        "timestamp": "2025-04-09T06:19:11.686Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "Cities\u2019 credit ratings",
          "municipal fiscal health",
          "police-involved fatalities"
        ],
        "doi": "",
        "journalName": "Public Finance Review",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2635,
        "object_id": "paper:url.588B45FD",
        "created_at": "2025-04-09T06:19:12+00:00",
        "updated_at": "2025-04-09T06:19:15+00:00",
        "version": 1
      }
    },
    "paper:url.3F663409": {
      "data": {
        "sourceId": "url",
        "paperId": "3F663409",
        "url": "https://pubs.aeaweb.org/doi/pdf/10.1257/jep.35.3.157",
        "title": "3F663409",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-09T06:18:07.061Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2634,
        "object_id": "paper:url.3F663409",
        "created_at": "2025-04-09T06:18:07+00:00",
        "updated_at": "2025-04-09T06:18:10+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.05304": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.05304",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-09T06:43:31.711Z",
            "data": {
              "session_id": "session_1744181010578_gx9bsx5",
              "source_id": "arxiv",
              "paper_id": "2504.05304",
              "start_time": "2025-04-09T06:42:04.318Z",
              "end_time": "2025-04-09T06:43:30.578Z",
              "heartbeat_count": 17,
              "duration_seconds": 85,
              "idle_seconds": 1,
              "total_elapsed_seconds": 86
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2639,
        "object_id": "interactions:arxiv.2504.05304",
        "created_at": "2025-04-09T06:43:33+00:00",
        "updated_at": "2025-04-09T06:44:34+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.05304": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.05304",
        "url": "https://arxiv.org/abs/2504.05304",
        "title": "Gaussian Mixture Flow Matching Models",
        "authors": "Chen, Hansheng, Zhang, Kai, Tan, Hao, Xu, Zexiang, Luan, Fujun, Guibas, Leonidas, Wetzstein, Gordon, Bi, Sai",
        "abstract": "Diffusion models approximate the denoising distribution as a Gaussian and predict its mean, whereas flow matching models reparameterize the Gaussian mean as flow velocity. However, they underperform in few-step sampling due to discretization error and tend to produce over-saturated colors under classifier-free guidance (CFG). To address these limitations, we propose a novel Gaussian mixture flow matching (GMFlow) model: instead of predicting the mean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a multi-modal flow velocity distribution, which can be learned with a KL divergence loss. We demonstrate that GMFlow generalizes previous diffusion and flow matching models where a single Gaussian is learned with an $L_2$ denoising loss. For inference, we derive GM-SDE/ODE solvers that leverage analytic denoising distributions and velocity fields for precise few-step sampling. Furthermore, we introduce a novel probabilistic guidance scheme that mitigates the over-saturation issues of CFG and improves image generation quality. Extensive experiments demonstrate that GMFlow consistently outperforms flow matching baselines in generation quality, achieving a Precision of 0.942 with only 6 sampling steps on ImageNet 256$\\times$256.",
        "timestamp": "2025-04-09T06:42:04.863Z",
        "rating": "novote",
        "publishedDate": "2025/04/07",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2638,
        "object_id": "paper:arxiv.2504.05304",
        "created_at": "2025-04-09T06:42:05+00:00",
        "updated_at": "2025-04-09T07:13:14+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.05579": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.05579",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-09T14:37:58.955Z",
            "data": {
              "session_id": "session_1744209478027_hapvf0s",
              "source_id": "arxiv",
              "paper_id": "2504.05579",
              "start_time": "2025-04-09T14:37:49.106Z",
              "end_time": "2025-04-09T14:37:58.027Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2641,
        "object_id": "interactions:arxiv.2504.05579",
        "created_at": "2025-04-09T14:38:00+00:00",
        "updated_at": "2025-04-09T14:39:08+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.05579": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.05579",
        "url": "https://arxiv.org/abs/2504.05579",
        "title": "TAPNext: Tracking Any Point (TAP) as Next Token Prediction",
        "authors": "Zholus, Artem, Doersch, Carl, Yang, Yi, Koppula, Skanda, Patraucean, Viorica, He, Xu Owen, Rocco, Ignacio, Sajjadi, Mehdi S. M., Chandar, Sarath, Goroshin, Ross",
        "abstract": "Tracking Any Point (TAP) in a video is a challenging computer vision problem with many demonstrated applications in robotics, video editing, and 3D reconstruction. Existing methods for TAP rely heavily on complex tracking-specific inductive biases and heuristics, limiting their generality and potential for scaling. To address these challenges, we present TAPNext, a new approach that casts TAP as sequential masked token decoding. Our model is causal, tracks in a purely online fashion, and removes tracking-specific inductive biases. This enables TAPNext to run with minimal latency, and removes the temporal windowing required by many existing state of art trackers. Despite its simplicity, TAPNext achieves a new state-of-the-art tracking performance among both online and offline trackers. Finally, we present evidence that many widely used tracking heuristics emerge naturally in TAPNext through end-to-end training.",
        "timestamp": "2025-04-09T14:37:49.588Z",
        "rating": "novote",
        "publishedDate": "2025/04/08",
        "tags": [
          "cs.CV"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2640,
        "object_id": "paper:arxiv.2504.05579",
        "created_at": "2025-04-09T14:37:50+00:00",
        "updated_at": "2025-04-10T02:20:24+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.02808": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.02808",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-09T15:05:28.293Z",
            "data": {
              "session_id": "session_1744211127375_4atm32l",
              "source_id": "arxiv",
              "paper_id": "2504.02808",
              "start_time": "2025-04-09T15:04:36.582Z",
              "end_time": "2025-04-09T15:05:27.375Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 1,
              "total_elapsed_seconds": 51
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2643,
        "object_id": "interactions:arxiv.2504.02808",
        "created_at": "2025-04-09T15:05:29+00:00",
        "updated_at": "2025-04-09T15:06:35+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.02808": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.02808",
        "url": "https://arxiv.org/abs/2504.02808",
        "title": "Quantum theory does not need complex numbers",
        "authors": "Hoffreumon, Timothee, Woods, Mischa P.",
        "abstract": "The longstanding debate over whether quantum theory fundamentally requires complex numbers--or if their use is merely a convenient choice--has persisted for decades. Until recently, this question was considered open. However, in [M.-O. Renou et al, Nature 600, 625-629, 2021], a decisive argument was presented asserting that quantum theory needs complex numbers. In this work, we demonstrate that a formulation of quantum theory based solely on real numbers is indeed possible while retaining key features such as theory-representation locality (i.e. local physical operations are represented by local changes to the states) and the positive semi-definiteness of its states and effects. We observe that the standard system combination rule--the tensor product--was derived after the development of single-system complex quantum theory. By starting from a single-system quantum theory using only real numbers, we derive a combination rule that produces a real quantum theory with properties analogous to those of conventional complex quantum theory. We also prove that the conventional tensor product rule can also lead to a real and representation-local theory, albeit with a modified characterization of the state space. We thus conclude that complex numbers are a mere convenience in quantum theory.",
        "timestamp": "2025-04-09T15:04:37.150Z",
        "rating": "novote",
        "publishedDate": "2025/04/03",
        "tags": [
          "quant-ph"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2642,
        "object_id": "paper:arxiv.2504.02808",
        "created_at": "2025-04-09T15:04:37+00:00",
        "updated_at": "2025-04-10T03:02:09+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.17486": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.17486",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-09T20:31:42.208Z",
            "data": {
              "session_id": "session_1744230701329_4oad7fe",
              "source_id": "arxiv",
              "paper_id": "2503.17486",
              "start_time": "2025-04-09T20:31:15.363Z",
              "end_time": "2025-04-09T20:31:41.329Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 1,
              "total_elapsed_seconds": 26
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2646,
        "object_id": "interactions:arxiv.2503.17486",
        "created_at": "2025-04-09T20:31:43+00:00",
        "updated_at": "2025-04-09T20:32:53+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.17486": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.17486",
        "url": "https://arxiv.org/abs/2503.17486",
        "title": "ProtoGS: Efficient and High-Quality Rendering with 3D Gaussian Prototypes",
        "authors": "Gao, Zhengqing, Hu, Dongting, Bian, Jia-Wang, Fu, Huan, Li, Yan, Liu, Tongliang, Gong, Mingming, Zhang, Kun",
        "abstract": "3D Gaussian Splatting (3DGS) has made significant strides in novel view synthesis but is limited by the substantial number of Gaussian primitives required, posing challenges for deployment on lightweight devices. Recent methods address this issue by compressing the storage size of densified Gaussians, yet fail to preserve rendering quality and efficiency. To overcome these limitations, we propose ProtoGS to learn Gaussian prototypes to represent Gaussian primitives, significantly reducing the total Gaussian amount without sacrificing visual quality. Our method directly uses Gaussian prototypes to enable efficient rendering and leverage the resulting reconstruction loss to guide prototype learning. To further optimize memory efficiency during training, we incorporate structure-from-motion (SfM) points as anchor points to group Gaussian primitives. Gaussian prototypes are derived within each group by clustering of K-means, and both the anchor points and the prototypes are optimized jointly. Our experiments on real-world and synthetic datasets prove that we outperform existing methods, achieving a substantial reduction in the number of Gaussians, and enabling high rendering speed while maintaining or even enhancing rendering fidelity.",
        "timestamp": "2025-04-09T20:31:15.047Z",
        "rating": "novote",
        "publishedDate": "2025/03/21",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2644,
        "object_id": "paper:arxiv.2503.17486",
        "created_at": "2025-04-09T20:31:15+00:00",
        "updated_at": "2025-04-09T20:31:18+00:00",
        "version": 1
      }
    },
    "paper:url.706D7C8A": {
      "data": {
        "sourceId": "url",
        "paperId": "706D7C8A",
        "url": "https://www.sciencedirect.com/science/article/abs/pii/S0047272703000239",
        "title": "Bad politicians",
        "authors": "",
        "abstract": "We present a simple theory of the quality (competence and honesty) of elected officials. Our theory offers three main insights. Low-quality citizens h\u2026",
        "timestamp": "2025-04-09T20:40:09.521Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1016/S0047-2727(03)00023-9",
        "journalName": "Journal of Public Economics",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2647,
        "object_id": "paper:url.706D7C8A",
        "created_at": "2025-04-09T20:40:10+00:00",
        "updated_at": "2025-04-09T20:40:13+00:00",
        "version": 1
      }
    },
    "paper:arxiv.1908.10530": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1908.10530",
        "url": "https://arxiv.org/abs/1908.10530",
        "title": "R\\'enyi Differential Privacy of the Sampled Gaussian Mechanism",
        "authors": "Mironov, Ilya, Talwar, Kunal, Zhang, Li",
        "abstract": "The Sampled Gaussian Mechanism (SGM)---a composition of subsampling and the additive Gaussian noise---has been successfully used in a number of machine learning applications. The mechanism's unexpected power is derived from privacy amplification by sampling where the privacy cost of a single evaluation diminishes quadratically, rather than linearly, with the sampling rate. Characterizing the precise privacy properties of SGM motivated development of several relaxations of the notion of differential privacy. This work unifies and fills in gaps in published results on SGM. We describe a numerically stable procedure for precise computation of SGM's R\\'enyi Differential Privacy and prove a nearly tight (within a small constant factor) closed-form bound.",
        "timestamp": "2025-04-11T04:38:13.856Z",
        "rating": "novote",
        "publishedDate": "2019/08/28",
        "tags": [
          "cs.LG",
          "cs.CR",
          "stat.ML"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2670,
        "object_id": "paper:arxiv.1908.10530",
        "created_at": "2025-04-11T04:38:14+00:00",
        "updated_at": "2025-04-11T04:39:40+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2401.15703": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2401.15703",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-11T04:16:22.830Z",
            "data": {
              "session_id": "session_1744344981995_eixu51u",
              "source_id": "arxiv",
              "paper_id": "2401.15703",
              "start_time": "2025-04-11T04:16:07.000Z",
              "end_time": "2025-04-11T04:16:21.995Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 5,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2668,
        "object_id": "interactions:arxiv.2401.15703",
        "created_at": "2025-04-11T04:16:24+00:00",
        "updated_at": "2025-04-11T04:17:29+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2401.15703": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2401.15703",
        "url": "https://arxiv.org/abs/2401.15703",
        "title": "A Bayesian multivariate extreme value mixture model",
        "authors": "Hu, Chenglei, Swallow, Ben, Castro-Camilo, Daniela",
        "abstract": "Impact assessment of natural hazards requires the consideration of both extreme and non-extreme events. Extensive research has been conducted on the joint modeling of bulk and tail in univariate settings; however, the corresponding body of research in the context of multivariate analysis is comparatively scant. This study extends the univariate joint modeling of bulk and tail to the multivariate framework. Specifically, it pertains to cases where multivariate observations exceed a high threshold in at least one component. We propose a multivariate mixture model that assumes a parametric model to capture the bulk of the distribution, which is in the max-domain of attraction (MDA) of a multivariate extreme value distribution (mGEVD). The tail is described by the multivariate generalized Pareto distribution, which is asymptotically justified to model multivariate threshold exceedances. We show that if all components exceed the threshold, our mixture model is in the MDA of an mGEVD. Bayesian inference based on multivariate random-walk Metropolis-Hastings and the automated factor slice sampler allows us to incorporate uncertainty from the threshold selection easily. Due to computational limitations, simulations and data applications are provided for dimension $d=2$, but a discussion is provided with views toward scalability based on pairwise likelihood.",
        "timestamp": "2025-04-11T04:16:07.415Z",
        "rating": "novote",
        "publishedDate": "2024/01/28",
        "tags": [
          "stat.ME",
          "stat.AP"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2667,
        "object_id": "paper:arxiv.2401.15703",
        "created_at": "2025-04-11T04:16:07+00:00",
        "updated_at": "2025-04-11T04:17:33+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.23674": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.23674",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-11T02:23:13.655Z",
            "data": {
              "session_id": "session_1744338193639_yjko66l",
              "source_id": "arxiv",
              "paper_id": "2503.23674",
              "start_time": "2025-04-11T02:22:33.305Z",
              "end_time": "2025-04-11T02:23:13.639Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 0,
              "total_elapsed_seconds": 40
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2666,
        "object_id": "interactions:arxiv.2503.23674",
        "created_at": "2025-04-11T02:22:34+00:00",
        "updated_at": "2025-04-11T02:23:36+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.23674": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.23674",
        "url": "https://arxiv.org/pdf/2503.23674",
        "title": "2503.23674",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-11T02:22:28.201Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2665,
        "object_id": "paper:arxiv.2503.23674",
        "created_at": "2025-04-11T02:22:28+00:00",
        "updated_at": "2025-04-11T02:22:31+00:00",
        "version": 1
      }
    },
    "paper:url.68AD64A3": {
      "data": {
        "sourceId": "url",
        "paperId": "68AD64A3",
        "url": "https://psycnet.apa.org/record/2025-98802-001",
        "title": "Intrinsically memorable words have unique associations with their meanings.",
        "authors": "",
        "abstract": "What makes a word memorable? An important claim from past work is that words are encoded by their meanings and not their forms. If true, then, following rational analysis, memorable words should uniquely pick out a particular meaning, which means they should have few or no synonyms, and they should be unambiguous. Across two large-scale recognition-memory experiments (2,222 target words and > 600 participants each, plus 3,780 participants for the norming experiments), we found that memory performance is overall high, and some words are consistently remembered better than others. Critically, the most memorable words indeed have a one-to-one relationship with their meanings\u2014with number of synonyms being a stronger contributor than number of meanings\u2014and number of synonyms outperforms other predictors (such as imageability, frequency, or contextual diversity) of memorability that have been proposed in the past. (PsycInfo Database Record (c) 2025 APA, all rights reserved)",
        "timestamp": "2025-04-11T01:49:46.024Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1037/xge0001742",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2664,
        "object_id": "paper:url.68AD64A3",
        "created_at": "2025-04-11T01:49:46+00:00",
        "updated_at": "2025-04-11T01:49:49+00:00",
        "version": 1
      }
    },
    "paper:url.2948FA7B": {
      "data": {
        "sourceId": "url",
        "paperId": "2948FA7B",
        "url": "https://www.sciencedirect.com/science/article/pii/S2211124725003183",
        "title": "Firing rates in visual cortex show representational drift, while temporal spike sequences remain stable",
        "authors": "",
        "abstract": "Neural firing-rate responses to sensory stimuli show progressive changes both within and across sessions, raising the question of how the brain mainta\u2026",
        "timestamp": "2025-04-11T01:49:04.086Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1016/j.celrep.2025.115547",
        "journalName": "Cell Reports",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2663,
        "object_id": "paper:url.2948FA7B",
        "created_at": "2025-04-11T01:49:04+00:00",
        "updated_at": "2025-04-11T01:49:08+00:00",
        "version": 1
      }
    },
    "interactions:url.71B03050": {
      "data": {
        "sourceId": "url",
        "paperId": "71B03050",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-10T15:10:15.251Z",
            "data": {
              "session_id": "session_1744297814260_7itlido",
              "source_id": "url",
              "paper_id": "71B03050",
              "start_time": "2025-04-10T15:08:05.572Z",
              "end_time": "2025-04-10T15:10:14.260Z",
              "heartbeat_count": 25,
              "duration_seconds": 125,
              "idle_seconds": 4,
              "total_elapsed_seconds": 129
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2662,
        "object_id": "interactions:url.71B03050",
        "created_at": "2025-04-10T15:10:16+00:00",
        "updated_at": "2025-04-10T15:11:19+00:00",
        "version": 1
      }
    },
    "paper:url.71B03050": {
      "data": {
        "sourceId": "url",
        "paperId": "71B03050",
        "url": "https://onlinelibrary.wiley.com/doi/10.1111/jomf.13097",
        "title": "Tracking types of non-parents in the United States",
        "authors": "Jennifer Watling Neal, Zachary P. Neal",
        "abstract": "\nBackground\nEfforts to document different types of non-parents have distinguished those who are voluntarily childless, involuntarily childless, and temporarily childless. However, an expanded approac...",
        "timestamp": "2025-04-10T15:08:04.726Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1111/jomf.13097",
        "journalName": "Journal of Marriage and Family",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2661,
        "object_id": "paper:url.71B03050",
        "created_at": "2025-04-10T15:08:05+00:00",
        "updated_at": "2025-04-10T15:08:08+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.07081": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.07081",
        "url": "https://arxiv.org/abs/2504.07081",
        "title": "Self-Steering Language Models",
        "authors": "Grand, Gabriel, Tenenbaum, Joshua B., Mansinghka, Vikash K., Lew, Alexander K., Andreas, Jacob",
        "abstract": "While test-time reasoning enables language models to tackle complex tasks, searching or planning in natural language can be slow, costly, and error-prone. But even when LMs struggle to emulate the precise reasoning steps needed to solve a problem, they often excel at describing its abstract structure--both how to verify solutions and how to search for them. This paper introduces DisCIPL, a method for \"self-steering\" LMs where a Planner model generates a task-specific inference program that is executed by a population of Follower models. Our approach equips LMs with the ability to write recursive search procedures that guide LM inference, enabling new forms of verifiable and efficient reasoning. When instantiated with a small Follower (e.g., Llama-3.2-1B), DisCIPL matches (and sometimes outperforms) much larger models, including GPT-4o and o1, on challenging constrained generation tasks. In decoupling planning from execution, our work opens up a design space of highly-parallelized Monte Carlo inference strategies that outperform standard best-of-N sampling, require no finetuning, and can be implemented automatically by existing LMs.",
        "timestamp": "2025-04-10T13:54:39.986Z",
        "rating": "novote",
        "publishedDate": "2025/04/09",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2660,
        "object_id": "paper:arxiv.2504.07081",
        "created_at": "2025-04-10T13:54:40+00:00",
        "updated_at": "2025-04-10T13:54:44+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.08688": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.08688",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-10T13:52:03.605Z",
            "data": {
              "session_id": "session_1744293122711_qhcs61m",
              "source_id": "arxiv",
              "paper_id": "2503.08688",
              "start_time": "2025-04-10T13:50:52.616Z",
              "end_time": "2025-04-10T13:52:02.711Z",
              "heartbeat_count": 14,
              "duration_seconds": 70,
              "idle_seconds": 0,
              "total_elapsed_seconds": 70
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2659,
        "object_id": "interactions:arxiv.2503.08688",
        "created_at": "2025-04-10T13:52:04+00:00",
        "updated_at": "2025-04-10T13:53:15+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.08688": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.08688",
        "url": "https://arxiv.org/abs/2503.08688v2",
        "title": "Randomness, Not Representation: The Unreliability of Evaluating Cultural Alignment in LLMs",
        "authors": "Khan, Ariba, Casper, Stephen, Hadfield-Menell, Dylan",
        "abstract": "Research on the 'cultural alignment' of Large Language Models (LLMs) has emerged in response to growing interest in understanding representation across diverse stakeholders. Current approaches to evaluating cultural alignment through survey-based assessments that borrow from social science methodologies often overlook systematic robustness checks. Here, we identify and test three assumptions behind current survey-based evaluation methods: (1) Stability: that cultural alignment is a property of LLMs rather than an artifact of evaluation design, (2) Extrapolability: that alignment with one culture on a narrow set of issues predicts alignment with that culture on others, and (3) Steerability: that LLMs can be reliably prompted to represent specific cultural perspectives. Through experiments examining both explicit and implicit preferences of leading LLMs, we find a high level of instability across presentation formats, incoherence between evaluated versus held-out cultural dimensions, and erratic behavior under prompt steering. We show that these inconsistencies can cause the results of an evaluation to be very sensitive to minor variations in methodology. Finally, we demonstrate in a case study on evaluation design that narrow experiments and a selective assessment of evidence can be used to paint an incomplete picture of LLMs' cultural alignment properties. Overall, these results highlight significant limitations of current survey-based approaches to evaluating the cultural alignment of LLMs and highlight a need for systematic robustness checks and red-teaming for evaluation results. Data and code are available at https://huggingface.co/datasets/akhan02/cultural-dimension-cover-letters and https://github.com/ariba-k/llm-cultural-alignment-evaluation, respectively.",
        "timestamp": "2025-04-10T13:50:53.146Z",
        "rating": "novote",
        "publishedDate": "2025/03/11",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2658,
        "object_id": "paper:arxiv.2503.08688",
        "created_at": "2025-04-10T13:50:53+00:00",
        "updated_at": "2025-04-10T13:50:57+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.06540": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.06540",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-10T06:40:05.520Z",
            "data": {
              "session_id": "session_1744267204671_79uyxft",
              "source_id": "arxiv",
              "paper_id": "2504.06540",
              "start_time": "2025-04-10T06:39:57.094Z",
              "end_time": "2025-04-10T06:40:04.671Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2657,
        "object_id": "interactions:arxiv.2504.06540",
        "created_at": "2025-04-10T06:40:06+00:00",
        "updated_at": "2025-04-10T06:41:07+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.06540": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.06540",
        "url": "https://arxiv.org/abs/2504.06540",
        "title": "Interconnections of Multimorbidity-Related Clinical Outcomes: Analysis of Health Administrative Claims Data with a Dynamic Network Approach",
        "authors": "Mei, Hao, Xiao, Haonan, Shia, Ben-Chang, Qiao, Guanzhong, Li, Yang",
        "abstract": "Given the rising complexity and burden of multimorbidity, it is crucial to provide evidence-based support for managing multimorbidity-related clinical outcomes. This study introduces a dynamic network approach to investigate conditional and time-varying interconnections in disease-specific clinical outcomes. Our method effectively tackles the issue of zero inflation, a frequent challenge in medical data that complicates traditional modeling techniques. The theoretical foundations of the proposed approach are rigorously developed and validated through extensive simulations. Using Taiwan's health administrative claims data from 2000 to 2013, we construct 14 yearly networks that are temporally correlated, featuring 125 nodes that represent different disease conditions. Key network properties, such as connectivity, module, and temporal variation are analyzed. To demonstrate how these networks can inform multimorbidity management, we focus on breast cancer and analyze the relevant network structures. The findings provide valuable clinical insights that enhance the current understanding of multimorbidity. The proposed methods offer promising applications in shaping treatment strategies, optimizing health resource allocation, and informing health policy development in the context of multimorbidity management.",
        "timestamp": "2025-04-10T06:39:56.926Z",
        "rating": "novote",
        "publishedDate": "2025/04/09",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2655,
        "object_id": "paper:arxiv.2504.06540",
        "created_at": "2025-04-10T06:39:57+00:00",
        "updated_at": "2025-04-10T06:40:00+00:00",
        "version": 1
      }
    },
    "interactions:url.2D741F57": {
      "data": {
        "sourceId": "url",
        "paperId": "2D741F57",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-10T06:09:48.837Z",
            "data": {
              "session_id": "session_1744265388090_62i57r7",
              "source_id": "url",
              "paper_id": "2D741F57",
              "start_time": "2025-04-10T06:09:37.362Z",
              "end_time": "2025-04-10T06:09:48.090Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2654,
        "object_id": "interactions:url.2D741F57",
        "created_at": "2025-04-10T06:09:49+00:00",
        "updated_at": "2025-04-10T06:10:51+00:00",
        "version": 1
      }
    },
    "paper:url.2D741F57": {
      "data": {
        "sourceId": "url",
        "paperId": "2D741F57",
        "url": "https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2003JE002088",
        "title": "Electric and magnetic signatures of dust devils from the 2000\u20132001 MATADOR desert tests",
        "authors": "W. M. Farrell, P. H. Smith, G. T. Delory, G. B. Hillard, J. R. Marshall, D. Catling, M. Hecht, D. M. Tratt, N. Renno, M. D. Desch, S. A. Cummer, J. G. Houser, B. Johnson",
        "abstract": "Dust devils are significant meteorological phenomena on Mars: They are ubiquitous, continually gardening the Martian surface, and may be the primary atmospheric dust-loading mechanism in nonstorm sea...",
        "timestamp": "2025-04-10T06:09:36.240Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1029/2003JE002088",
        "journalName": "Journal of Geophysical Research: Planets",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2653,
        "object_id": "paper:url.2D741F57",
        "created_at": "2025-04-10T06:09:36+00:00",
        "updated_at": "2025-04-10T06:09:39+00:00",
        "version": 1
      }
    },
    "interactions:url.4F481A80": {
      "data": {
        "sourceId": "url",
        "paperId": "4F481A80",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-10T06:07:51.668Z",
            "data": {
              "session_id": "session_1744265270900_697ubch",
              "source_id": "url",
              "paper_id": "4F481A80",
              "start_time": "2025-04-10T06:07:45.155Z",
              "end_time": "2025-04-10T06:07:50.900Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2652,
        "object_id": "interactions:url.4F481A80",
        "created_at": "2025-04-10T06:07:52+00:00",
        "updated_at": "2025-04-10T06:08:58+00:00",
        "version": 1
      }
    },
    "paper:url.4F481A80": {
      "data": {
        "sourceId": "url",
        "paperId": "4F481A80",
        "url": "https://www.hou.usra.edu/meetings/lpsc2025/pdf/1427.pdf",
        "title": "4F481A80",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-10T06:07:45.767Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 2651,
        "object_id": "paper:url.4F481A80",
        "created_at": "2025-04-10T06:07:46+00:00",
        "updated_at": "2025-04-10T06:07:49+00:00",
        "version": 1
      }
    },
    "paper:url.6731BA14": {
      "data": {
        "sourceId": "url",
        "paperId": "6731BA14",
        "url": "https://www.pnas.org/doi/10.1073/pnas.2409983121",
        "title": "Liquid water in the Martian mid-crust",
        "authors": "Wright, Vashan, Morzfeld, Matthias, Manga, Michael",
        "abstract": "Large volumes of liquid water transiently existed on the surface of Mars more than\n3 billion years ago. Much of this water is hypothesized to have ...",
        "timestamp": "2025-04-10T06:06:28.574Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "Mars",
          "water",
          "planetary geophysics",
          "InSight"
        ],
        "doi": "10.1073/pnas.2409983121",
        "journalName": "Proceedings of the National Academy of Sciences",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2650,
        "object_id": "paper:url.6731BA14",
        "created_at": "2025-04-10T06:06:29+00:00",
        "updated_at": "2025-04-10T06:06:32+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.06901": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.06901",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-10T05:01:10.144Z",
            "data": {
              "session_id": "session_1744261269308_aiol3nx",
              "source_id": "arxiv",
              "paper_id": "2504.06901",
              "start_time": "2025-04-10T05:00:05.307Z",
              "end_time": "2025-04-10T05:01:09.308Z",
              "heartbeat_count": 12,
              "duration_seconds": 60,
              "idle_seconds": 4,
              "total_elapsed_seconds": 64
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-10T05:11:35.345Z",
            "data": {
              "session_id": "session_1744261894723_ygbcgxc",
              "source_id": "arxiv",
              "paper_id": "2504.06901",
              "start_time": "2025-04-10T05:10:48.735Z",
              "end_time": "2025-04-10T05:11:34.723Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 1,
              "total_elapsed_seconds": 46
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2649,
        "object_id": "interactions:arxiv.2504.06901",
        "created_at": "2025-04-10T05:01:11+00:00",
        "updated_at": "2025-04-10T05:12:37+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.06901": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.06901",
        "url": "https://arxiv.org/abs/2504.06901",
        "title": "Classification of algebraic tangles",
        "authors": "Gren, Bartosz Ambrozy, Sulkowska, Joanna Ida, Gabrov\u0161ek, Bo\u0161tjan",
        "abstract": "We study algebraic tangles as fundamental components in knot theory, developing a systematic approach to classify and tabulate prime tangles using a novel canonical representation. The canonical representation enables us to distinguish mutant tangles, which fills the gaps in previous classifications. Moreover, we increase the classification of prime tangles up to 14 crossings and analyze tangle symmetry groups. We provide a database of our results: https://tangleinfo.cent.uw.edu.pl.",
        "timestamp": "2025-04-10T05:00:05.817Z",
        "rating": "novote",
        "publishedDate": "2025/04/09",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2648,
        "object_id": "paper:arxiv.2504.06901",
        "created_at": "2025-04-10T05:00:06+00:00",
        "updated_at": "2025-04-10T05:00:10+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.01840": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.01840",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-03T05:05:56.258Z",
            "data": {
              "session_id": "session_1743656755352_c0fpm1l",
              "source_id": "arxiv",
              "paper_id": "2504.01840",
              "start_time": "2025-04-03T05:03:38.856Z",
              "end_time": "2025-04-03T05:05:55.352Z",
              "heartbeat_count": 27,
              "duration_seconds": 135,
              "idle_seconds": 1,
              "total_elapsed_seconds": 136
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2443,
        "object_id": "interactions:arxiv.2504.01840",
        "created_at": "2025-04-03T05:05:57+00:00",
        "updated_at": "2025-04-10T04:37:24+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.16992": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.16992",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-02T23:47:36.570Z",
            "data": {
              "session_id": "session_1743637655746_ahaqs66",
              "source_id": "arxiv",
              "paper_id": "2503.16992",
              "start_time": "2025-04-02T23:46:54.657Z",
              "end_time": "2025-04-02T23:47:35.746Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 1,
              "total_elapsed_seconds": 41
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2439,
        "object_id": "interactions:arxiv.2503.16992",
        "created_at": "2025-04-02T23:47:37+00:00",
        "updated_at": "2025-04-10T03:52:05+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.07083": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.07083",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-11T04:44:12.672Z",
            "data": {
              "session_id": "session_1744346651560_rhuhwvn",
              "source_id": "arxiv",
              "paper_id": "2504.07083",
              "start_time": "2025-04-11T04:42:41.616Z",
              "end_time": "2025-04-11T04:44:11.560Z",
              "heartbeat_count": 17,
              "duration_seconds": 85,
              "idle_seconds": 5,
              "total_elapsed_seconds": 90
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2673,
        "object_id": "interactions:arxiv.2504.07083",
        "created_at": "2025-04-11T04:44:13+00:00",
        "updated_at": "2025-04-11T04:45:25+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.07184": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.07184",
        "url": "https://arxiv.org/abs/2504.07184",
        "title": "Hermite Reciprocity and Self-Duality of Generalized Eagon-Northcott Complexes",
        "authors": "Reed, Ethan",
        "abstract": "Previous examples of self-duality for generalized Eagon-Northcott complexes were given by computing the divisor class group for Hankel determinantal rings. We prove a new case of self-duality of generalized Eagon-Northcott complexes with input being a map defining a Koszul module with nice properties. This choice of Koszul module can be specialized to the Weyman module, which was used in a proof of the generic version of Green's conjecture. In this case, the proof uses a version of Hermite Reciprocity not previously defined in the literature.",
        "timestamp": "2025-04-11T04:48:18.596Z",
        "rating": "novote",
        "publishedDate": "2025/04/09",
        "tags": [
          "math.AC",
          "math.AG",
          "13D02, 13D07, 14F06, 15A69, 18G40, 20G05"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2674,
        "object_id": "paper:arxiv.2504.07184",
        "created_at": "2025-04-11T04:48:19+00:00",
        "updated_at": "2025-04-11T04:49:43+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.07341": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.07341",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-11T04:49:43.664Z",
            "data": {
              "session_id": "session_1744346982779_6i6gr0g",
              "source_id": "arxiv",
              "paper_id": "2504.07341",
              "start_time": "2025-04-11T04:48:30.966Z",
              "end_time": "2025-04-11T04:49:42.779Z",
              "heartbeat_count": 14,
              "duration_seconds": 70,
              "idle_seconds": 2,
              "total_elapsed_seconds": 72
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2677,
        "object_id": "interactions:arxiv.2504.07341",
        "created_at": "2025-04-11T04:49:44+00:00",
        "updated_at": "2025-04-11T04:50:47+00:00",
        "version": 1
      }
    },
    "interactions:url.2969854F": {
      "data": {
        "sourceId": "url",
        "paperId": "2969854F",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-11T04:53:37.523Z",
            "data": {
              "session_id": "session_1744347216504_ffqvqa3",
              "source_id": "url",
              "paper_id": "2969854F",
              "start_time": "2025-04-11T04:50:54.298Z",
              "end_time": "2025-04-11T04:53:36.504Z",
              "heartbeat_count": 32,
              "duration_seconds": 160,
              "idle_seconds": 2,
              "total_elapsed_seconds": 162
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2679,
        "object_id": "interactions:url.2969854F",
        "created_at": "2025-04-11T04:53:38+00:00",
        "updated_at": "2025-04-11T04:54:39+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.07728": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.07728",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-11T05:13:48.276Z",
            "data": {
              "session_id": "session_1744348427652_6hlkwqm",
              "source_id": "arxiv",
              "paper_id": "2504.07728",
              "start_time": "2025-04-11T05:13:33.492Z",
              "end_time": "2025-04-11T05:13:47.652Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2681,
        "object_id": "interactions:arxiv.2504.07728",
        "created_at": "2025-04-11T05:13:49+00:00",
        "updated_at": "2025-04-11T05:14:48+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.07728": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.07728",
        "url": "https://arxiv.org/abs/2504.07728",
        "title": "The Scaling Behaviors in Achieving High Reliability via Chance-Constrained Optimization",
        "authors": "Deo, Anand, Murthy, Karthyek",
        "abstract": "We study the problem of resource provisioning under stringent reliability or service level requirements, which arise in applications such as power distribution, emergency response, cloud server provisioning, and regulatory risk management. With chance-constrained optimization serving as a natural starting point for modeling this class of problems, our primary contribution is to characterize how the optimal costs and decisions scale for a generic joint chance-constrained model as the target probability of satisfying the service/reliability constraints approaches its maximal level. Beyond providing insights into the behavior of optimal solutions, our scaling framework has three key algorithmic implications. First, in distributionally robust optimization (DRO) modeling of chance constraints, we show that widely used approaches based on KL-divergences, Wasserstein distances, and moments heavily distort the scaling properties of optimal decisions, leading to exponentially higher costs. In contrast, incorporating marginal distributions or using appropriately chosen f-divergence balls preserves the correct scaling, ensuring decisions remain conservative by at most a constant or logarithmic factor. Second, we leverage the scaling framework to quantify the conservativeness of common inner approximations and propose a simple line search to refine their solutions, yielding near-optimal decisions. Finally, given N data samples, we demonstrate how the scaling framework enables the estimation of approximately Pareto-optimal decisions with constraint violation probabilities significantly smaller than the Omega(1/N)-barrier that arises in the absence of parametric assumptions",
        "timestamp": "2025-04-11T05:13:32.183Z",
        "rating": "novote",
        "publishedDate": "2025/04/10",
        "tags": [
          "math.OC",
          "math.PR",
          "q-fin.RM"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2680,
        "object_id": "paper:arxiv.2504.07728",
        "created_at": "2025-04-11T05:13:32+00:00",
        "updated_at": "2025-04-11T05:14:43+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.07457": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.07457",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-11T05:40:46.834Z",
            "data": {
              "session_id": "session_1744350046037_j2kz2lm",
              "source_id": "arxiv",
              "paper_id": "2504.07457",
              "start_time": "2025-04-11T05:40:25.577Z",
              "end_time": "2025-04-11T05:40:46.036Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 0,
              "total_elapsed_seconds": 20
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2683,
        "object_id": "interactions:arxiv.2504.07457",
        "created_at": "2025-04-11T05:40:47+00:00",
        "updated_at": "2025-04-11T05:42:10+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.07457": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.07457",
        "url": "https://arxiv.org/abs/2504.07457",
        "title": "CyberAlly: Leveraging LLMs and Knowledge Graphs to Empower Cyber Defenders",
        "authors": "Kim, Minjune, Wang, Jeff, Moore, Kristen, Goel, Diksha, Wang, Derui, Mohsin, Ahmad, Ibrahim, Ahmed, Doss, Robin, Camtepe, Seyit, Janicke, Helge",
        "abstract": "The increasing frequency and sophistication of cyberattacks demand innovative approaches to strengthen defense capabilities. Training on live infrastructure poses significant risks to organizations, making secure, isolated cyber ranges an essential tool for conducting Red vs. Blue Team training events. These events enable security teams to refine their skills without impacting operational environments. While such training provides a strong foundation, the ever-evolving nature of cyber threats necessitates additional support for effective defense. To address this challenge, we introduce CyberAlly, a knowledge graph-enhanced AI assistant designed to enhance the efficiency and effectiveness of Blue Teams during incident response. Integrated into our cyber range alongside an open-source SIEM platform, CyberAlly monitors alerts, tracks Blue Team actions, and suggests tailored mitigation recommendations based on insights from prior Red vs. Blue Team exercises. This demonstration highlights the feasibility and impact of CyberAlly in augmenting incident response and equipping defenders to tackle evolving threats with greater precision and confidence.",
        "timestamp": "2025-04-11T05:40:26.038Z",
        "rating": "novote",
        "publishedDate": "2025/04/10",
        "tags": [
          "cs.CR"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2682,
        "object_id": "paper:arxiv.2504.07457",
        "created_at": "2025-04-11T05:40:26+00:00",
        "updated_at": "2025-04-11T05:41:39+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.07543": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.07543",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-11T05:44:54.284Z",
            "data": {
              "session_id": "session_1744350293288_1bg06o0",
              "source_id": "arxiv",
              "paper_id": "2504.07543",
              "start_time": "2025-04-11T05:41:22.449Z",
              "end_time": "2025-04-11T05:44:53.288Z",
              "heartbeat_count": 42,
              "duration_seconds": 210,
              "idle_seconds": 1,
              "total_elapsed_seconds": 211
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-11T14:55:47.727Z",
            "data": {
              "session_id": "session_1744383346819_8gmffgw",
              "source_id": "arxiv",
              "paper_id": "2504.07543",
              "start_time": "2025-04-11T14:55:06.866Z",
              "end_time": "2025-04-11T14:55:46.819Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 5,
              "total_elapsed_seconds": 40
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2686,
        "object_id": "interactions:arxiv.2504.07543",
        "created_at": "2025-04-11T05:44:55+00:00",
        "updated_at": "2025-04-11T14:57:04+00:00",
        "version": 1
      }
    },
    "interactions:url.2EE72C2": {
      "data": {
        "sourceId": "url",
        "paperId": "2EE72C2",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-11T06:00:21.602Z",
            "data": {
              "session_id": "session_1744351220568_ph365wn",
              "source_id": "url",
              "paper_id": "2EE72C2",
              "start_time": "2025-04-11T06:00:13.381Z",
              "end_time": "2025-04-11T06:00:20.568Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-11T20:31:58.929Z",
            "data": {
              "session_id": "session_1744403518235_bqh96j4",
              "source_id": "url",
              "paper_id": "2EE72C2",
              "start_time": "2025-04-11T20:31:51.636Z",
              "end_time": "2025-04-11T20:31:58.235Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2688,
        "object_id": "interactions:url.2EE72C2",
        "created_at": "2025-04-11T06:00:22+00:00",
        "updated_at": "2025-04-11T20:32:58+00:00",
        "version": 1
      }
    },
    "interactions:url.4E8FECCF": {
      "data": {
        "sourceId": "url",
        "paperId": "4E8FECCF",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-11T14:54:17.606Z",
            "data": {
              "session_id": "session_1744383256372_mz8ceta",
              "source_id": "url",
              "paper_id": "4E8FECCF",
              "start_time": "2025-04-11T14:54:07.545Z",
              "end_time": "2025-04-11T14:54:16.372Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2691,
        "object_id": "interactions:url.4E8FECCF",
        "created_at": "2025-04-11T14:54:18+00:00",
        "updated_at": "2025-04-11T14:55:24+00:00",
        "version": 1
      }
    },
    "interactions:url.F230225": {
      "data": {
        "sourceId": "url",
        "paperId": "F230225",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-11T20:31:33.133Z",
            "data": {
              "session_id": "session_1744403492392_qf8ijne",
              "source_id": "url",
              "paper_id": "F230225",
              "start_time": "2025-04-11T20:31:26.044Z",
              "end_time": "2025-04-11T20:31:32.392Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2694,
        "object_id": "interactions:url.F230225",
        "created_at": "2025-04-11T20:31:34+00:00",
        "updated_at": "2025-04-11T20:32:35+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.03090": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.03090",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-11T20:37:41.131Z",
            "data": {
              "session_id": "session_1744403860412_0ilhxui",
              "source_id": "arxiv",
              "paper_id": "2504.03090",
              "start_time": "2025-04-11T20:37:15.152Z",
              "end_time": "2025-04-11T20:37:40.412Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 0,
              "total_elapsed_seconds": 25
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2697,
        "object_id": "interactions:arxiv.2504.03090",
        "created_at": "2025-04-11T20:37:42+00:00",
        "updated_at": "2025-04-11T20:38:49+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2412.17808": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2412.17808",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-12T02:51:42.677Z",
            "data": {
              "session_id": "session_1744426301373_wk5lufp",
              "source_id": "arxiv",
              "paper_id": "2412.17808",
              "start_time": "2025-04-12T02:50:44.903Z",
              "end_time": "2025-04-12T02:51:41.373Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 1,
              "total_elapsed_seconds": 56
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2701,
        "object_id": "interactions:arxiv.2412.17808",
        "created_at": "2025-04-12T02:51:43+00:00",
        "updated_at": "2025-04-12T02:52:39+00:00",
        "version": 1
      }
    },
    "paper:url.4684EF57": {
      "data": {
        "sourceId": "url",
        "paperId": "4684EF57",
        "url": "https://www.ianxmason.com/100style/",
        "title": "100STYLE",
        "authors": "Ian Mason",
        "abstract": "Artificial intelligence and machine learning research.",
        "timestamp": "2025-04-12T02:57:27.215Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2705,
        "object_id": "paper:url.4684EF57",
        "created_at": "2025-04-12T02:57:27+00:00",
        "updated_at": "2025-04-14T05:58:43+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2404.15121": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.15121",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-12T02:55:40.204Z",
            "data": {
              "session_id": "session_1744426539858_yat3i05",
              "source_id": "arxiv",
              "paper_id": "2404.15121",
              "start_time": "2025-04-12T02:55:32.134Z",
              "end_time": "2025-04-12T02:55:39.858Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-12T05:27:37.854Z",
            "data": {
              "session_id": "session_1744435657644_g0f4nwn",
              "source_id": "arxiv",
              "paper_id": "2404.15121",
              "start_time": "2025-04-12T05:27:19.224Z",
              "end_time": "2025-04-12T05:27:37.644Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 3,
              "total_elapsed_seconds": 18
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2703,
        "object_id": "interactions:arxiv.2404.15121",
        "created_at": "2025-04-12T02:55:33+00:00",
        "updated_at": "2025-04-12T05:28:38+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2404.15121": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.15121",
        "url": "https://arxiv.org/abs/2404.15121",
        "title": "Taming Diffusion Probabilistic Models for Character Control",
        "authors": "Chen, Rui, Shi, Mingyi, Huang, Shaoli, Tan, Ping, Komura, Taku, Chen, Xuelin",
        "abstract": "We present a novel character control framework that effectively utilizes motion diffusion probabilistic models to generate high-quality and diverse character animations, responding in real-time to a variety of dynamic user-supplied control signals. At the heart of our method lies a transformer-based Conditional Autoregressive Motion Diffusion Model (CAMDM), which takes as input the character's historical motion and can generate a range of diverse potential future motions conditioned on high-level, coarse user control. To meet the demands for diversity, controllability, and computational efficiency required by a real-time controller, we incorporate several key algorithmic designs. These include separate condition tokenization, classifier-free guidance on past motion, and heuristic future trajectory extension, all designed to address the challenges associated with taming motion diffusion probabilistic models for character control. As a result, our work represents the first model that enables real-time generation of high-quality, diverse character animations based on user interactive control, supporting animating the character in multiple styles with a single unified model. We evaluate our method on a diverse set of locomotion skills, demonstrating the merits of our method over existing character controllers. Project page and source codes: https://aiganimation.github.io/CAMDM/",
        "timestamp": "2025-04-12T02:55:27.171Z",
        "rating": "novote",
        "publishedDate": "2024/04/23",
        "tags": [
          "cs.GR",
          "cs.AI",
          "cs.CV"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2702,
        "object_id": "paper:arxiv.2404.15121",
        "created_at": "2025-04-12T02:55:28+00:00",
        "updated_at": "2025-04-12T02:56:36+00:00",
        "version": 1
      }
    },
    "interactions:url.2DF39D4D": {
      "data": {
        "sourceId": "url",
        "paperId": "2DF39D4D",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-12T02:57:49.528Z",
            "data": {
              "session_id": "session_1744426668684_awg8g4e",
              "source_id": "url",
              "paper_id": "2DF39D4D",
              "start_time": "2025-04-12T02:57:43.089Z",
              "end_time": "2025-04-12T02:57:48.684Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2708,
        "object_id": "interactions:url.2DF39D4D",
        "created_at": "2025-04-12T02:57:50+00:00",
        "updated_at": "2025-04-12T02:58:50+00:00",
        "version": 1
      }
    },
    "interactions:url.4684EF57": {
      "data": {
        "sourceId": "url",
        "paperId": "4684EF57",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-12T02:57:36.496Z",
            "data": {
              "session_id": "session_1744426655751_fzg4a6y",
              "source_id": "url",
              "paper_id": "4684EF57",
              "start_time": "2025-04-12T02:57:27.537Z",
              "end_time": "2025-04-12T02:57:35.751Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2706,
        "object_id": "interactions:url.4684EF57",
        "created_at": "2025-04-12T02:57:37+00:00",
        "updated_at": "2025-04-12T02:58:36+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2105.05233": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2105.05233",
        "url": "https://arxiv.org/pdf/2105.05233",
        "title": "2105.05233",
        "authors": [
          "Prafulla Dhariwal",
          "Alex Nichol"
        ],
        "abstract": "We show that diffusion models can achieve image sample quality superior to\nthe current state-of-the-art generative models. We achieve this on\nunconditional image synthesis by finding a better architecture through a series\nof ablations. For conditional image synthesis, we further improve sample\nquality with classifier guidance: a simple, compute-efficient method for\ntrading off diversity for fidelity using gradients from a classifier. We\nachieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet\n256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep\neven with as few as 25 forward passes per sample, all while maintaining better\ncoverage of the distribution. Finally, we find that classifier guidance\ncombines well with upsampling diffusion models, further improving FID to 3.94\non ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our\ncode at https://github.com/openai/guided-diffusion",
        "timestamp": "2025-04-12T03:00:12.957Z",
        "rating": "novote",
        "publishedDate": "2021-05-11T17:50:24+00:00",
        "tags": [
          "cs.LG",
          "cs.AI",
          "cs.CV",
          "stat.ML"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2709,
        "object_id": "paper:arxiv.2105.05233",
        "created_at": "2025-04-12T03:00:13+00:00",
        "updated_at": "2025-04-12T03:01:25+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2403.11641": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2403.11641",
        "url": "https://arxiv.org/abs/2403.11641",
        "title": "Arc2Face: A Foundation Model for ID-Consistent Human Faces",
        "authors": "Papantoniou, Foivos Paraperas, Lattas, Alexandros, Moschoglou, Stylianos, Deng, Jiankang, Kainz, Bernhard, Zafeiriou, Stefanos",
        "abstract": "This paper presents Arc2Face, an identity-conditioned face foundation model, which, given the ArcFace embedding of a person, can generate diverse photo-realistic images with an unparalleled degree of face similarity than existing models. Despite previous attempts to decode face recognition features into detailed images, we find that common high-resolution datasets (e.g. FFHQ) lack sufficient identities to reconstruct any subject. To that end, we meticulously upsample a significant portion of the WebFace42M database, the largest public dataset for face recognition (FR). Arc2Face builds upon a pretrained Stable Diffusion model, yet adapts it to the task of ID-to-face generation, conditioned solely on ID vectors. Deviating from recent works that combine ID with text embeddings for zero-shot personalization of text-to-image models, we emphasize on the compactness of FR features, which can fully capture the essence of the human face, as opposed to hand-crafted prompts. Crucially, text-augmented models struggle to decouple identity and text, usually necessitating some description of the given face to achieve satisfactory similarity. Arc2Face, however, only needs the discriminative features of ArcFace to guide the generation, offering a robust prior for a plethora of tasks where ID consistency is of paramount importance. As an example, we train a FR model on synthetic images from our model and achieve superior performance to existing synthetic datasets.",
        "timestamp": "2025-04-12T05:31:28.480Z",
        "rating": "novote",
        "publishedDate": "2024/03/18",
        "tags": [
          "cs.CV"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2710,
        "object_id": "paper:arxiv.2403.11641",
        "created_at": "2025-04-12T05:31:28+00:00",
        "updated_at": "2025-04-12T05:33:31+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2403.11641": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2403.11641",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-12T05:45:16.562Z",
            "data": {
              "session_id": "session_1744436716141_qu5qlrd",
              "source_id": "arxiv",
              "paper_id": "2403.11641",
              "start_time": "2025-04-12T05:45:09.429Z",
              "end_time": "2025-04-12T05:45:16.141Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2711,
        "object_id": "interactions:arxiv.2403.11641",
        "created_at": "2025-04-12T05:45:17+00:00",
        "updated_at": "2025-04-12T05:46:20+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.15699": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.15699",
        "url": "https://arxiv.org/pdf/2503.15699",
        "title": "2503.15699",
        "authors": [
          "Neehar Kondapaneni",
          "Oisin Mac Aodha",
          "Pietro Perona"
        ],
        "abstract": "How do two deep neural networks differ in how they arrive at a decision?\nMeasuring the similarity of deep networks has been a long-standing open\nquestion. Most existing methods provide a single number to measure the\nsimilarity of two networks at a given layer, but give no insight into what\nmakes them similar or dissimilar. We introduce an interpretable\nrepresentational similarity method (RSVC) to compare two networks. We use RSVC\nto discover shared and unique visual concepts between two models. We show that\nsome aspects of model differences can be attributed to unique concepts\ndiscovered by one model that are not well represented in the other. Finally, we\nconduct extensive evaluation across different vision model architectures and\ntraining protocols to demonstrate its effectiveness.",
        "timestamp": "2025-04-12T08:43:46.477Z",
        "rating": "novote",
        "publishedDate": "2025-03-19T21:21:45+00:00",
        "tags": [
          "cs.CV",
          "cs.AI",
          "q-bio.NC"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2712,
        "object_id": "paper:arxiv.2503.15699",
        "created_at": "2025-04-12T08:43:47+00:00",
        "updated_at": "2025-04-12T08:44:57+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.15699": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.15699",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-12T08:53:41.586Z",
            "data": {
              "session_id": "session_1744448021066_qum8x67",
              "source_id": "arxiv",
              "paper_id": "2503.15699",
              "start_time": "2025-04-12T08:53:06.058Z",
              "end_time": "2025-04-12T08:53:41.066Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 15,
              "total_elapsed_seconds": 35
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2713,
        "object_id": "interactions:arxiv.2503.15699",
        "created_at": "2025-04-12T08:53:42+00:00",
        "updated_at": "2025-04-12T08:54:45+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2410.17891": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.17891",
        "url": "https://arxiv.org/abs/2410.17891",
        "title": "Scaling Diffusion Language Models via Adaptation from Autoregressive Models",
        "authors": "Gong, Shansan, Agarwal, Shivam, Zhang, Yizhe, Ye, Jiacheng, Zheng, Lin, Li, Mukai, An, Chenxin, Zhao, Peilin, Bi, Wei, Han, Jiawei, Peng, Hao, Kong, Lingpeng",
        "abstract": "Diffusion Language Models (DLMs) have emerged as a promising new paradigm for text generative modeling, potentially addressing limitations of autoregressive (AR) models. However, current DLMs have been studied at a smaller scale compared to their AR counterparts and lack fair comparison on language modeling benchmarks. Additionally, training diffusion models from scratch at scale remains challenging. Given the prevalence of open-source AR language models, we propose adapting these models to build text diffusion models. We demonstrate connections between AR and diffusion modeling objectives and introduce a simple continual pre-training approach for training diffusion models. Through systematic evaluation on language modeling, reasoning, and commonsense benchmarks, we show that we can convert AR models ranging from 127M to 7B parameters (GPT2 and LLaMA) into diffusion models DiffuGPT and DiffuLLaMA, using less than 200B tokens for training. Our experimental results reveal that these models outperform earlier DLMs and are competitive with their AR counterparts. We release a suite of DLMs (127M-355M-7B) capable of generating fluent text, performing in-context learning, filling in the middle without prompt re-ordering, and following instructions https://github.com/HKUNLP/DiffuLLaMA.",
        "timestamp": "2025-04-12T14:07:49.782Z",
        "rating": "novote",
        "publishedDate": "2024/10/23",
        "tags": [
          "cs.CL"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2714,
        "object_id": "paper:arxiv.2410.17891",
        "created_at": "2025-04-12T14:07:50+00:00",
        "updated_at": "2025-04-12T14:08:57+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2410.17891": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.17891",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-12T14:09:06.599Z",
            "data": {
              "session_id": "session_1744466945631_jc35kub",
              "source_id": "arxiv",
              "paper_id": "2410.17891",
              "start_time": "2025-04-12T14:07:49.256Z",
              "end_time": "2025-04-12T14:09:05.631Z",
              "heartbeat_count": 15,
              "duration_seconds": 75,
              "idle_seconds": 1,
              "total_elapsed_seconds": 76
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2715,
        "object_id": "interactions:arxiv.2410.17891",
        "created_at": "2025-04-12T14:09:07+00:00",
        "updated_at": "2025-04-12T14:10:16+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2411.17525": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.17525",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-12T14:17:27.526Z",
            "data": {
              "session_id": "session_1744467446810_sgms8en",
              "source_id": "arxiv",
              "paper_id": "2411.17525",
              "start_time": "2025-04-12T14:17:10.443Z",
              "end_time": "2025-04-12T14:17:26.810Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 1,
              "total_elapsed_seconds": 16
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2718,
        "object_id": "interactions:arxiv.2411.17525",
        "created_at": "2025-04-12T14:17:28+00:00",
        "updated_at": "2025-04-12T14:18:33+00:00",
        "version": 1
      }
    },
    "interactions:url.4C8A9A43": {
      "data": {
        "sourceId": "url",
        "paperId": "4C8A9A43",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-12T14:24:54.676Z",
            "data": {
              "session_id": "session_1744467893778_p3lleec",
              "source_id": "url",
              "paper_id": "4C8A9A43",
              "start_time": "2025-04-12T14:20:56.508Z",
              "end_time": "2025-04-12T14:24:53.778Z",
              "heartbeat_count": 47,
              "duration_seconds": 235,
              "idle_seconds": 2,
              "total_elapsed_seconds": 237
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-12T14:32:29.821Z",
            "data": {
              "session_id": "session_1744468348889_r80ld8g",
              "source_id": "url",
              "paper_id": "4C8A9A43",
              "start_time": "2025-04-12T14:31:18.909Z",
              "end_time": "2025-04-12T14:32:28.889Z",
              "heartbeat_count": 13,
              "duration_seconds": 65,
              "idle_seconds": 5,
              "total_elapsed_seconds": 70
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2720,
        "object_id": "interactions:url.4C8A9A43",
        "created_at": "2025-04-12T14:24:55+00:00",
        "updated_at": "2025-04-12T14:33:28+00:00",
        "version": 1
      }
    },
    "interactions:url.659E2976": {
      "data": {
        "sourceId": "url",
        "paperId": "659E2976",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-12T14:34:07.386Z",
            "data": {
              "session_id": "session_1744468446622_rhuhg0b",
              "source_id": "url",
              "paper_id": "659E2976",
              "start_time": "2025-04-12T14:32:37.028Z",
              "end_time": "2025-04-12T14:34:06.622Z",
              "heartbeat_count": 17,
              "duration_seconds": 85,
              "idle_seconds": 5,
              "total_elapsed_seconds": 90
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2723,
        "object_id": "interactions:url.659E2976",
        "created_at": "2025-04-12T14:34:08+00:00",
        "updated_at": "2025-04-12T14:35:10+00:00",
        "version": 1
      }
    },
    "interactions:url.572B8B64": {
      "data": {
        "sourceId": "url",
        "paperId": "572B8B64",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-12T14:54:07.633Z",
            "data": {
              "session_id": "session_1744469646834_lzilwea",
              "source_id": "url",
              "paper_id": "572B8B64",
              "start_time": "2025-04-12T14:53:05.738Z",
              "end_time": "2025-04-12T14:54:06.834Z",
              "heartbeat_count": 12,
              "duration_seconds": 60,
              "idle_seconds": 1,
              "total_elapsed_seconds": 61
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2725,
        "object_id": "interactions:url.572B8B64",
        "created_at": "2025-04-12T14:54:08+00:00",
        "updated_at": "2025-04-12T14:55:08+00:00",
        "version": 1
      }
    },
    "interactions:url.4841805E": {
      "data": {
        "sourceId": "url",
        "paperId": "4841805E",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-12T19:00:50.001Z",
            "data": {
              "session_id": "session_1744484449283_3sbahgh",
              "source_id": "url",
              "paper_id": "4841805E",
              "start_time": "2025-04-12T19:00:42.063Z",
              "end_time": "2025-04-12T19:00:49.283Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2738,
        "object_id": "interactions:url.4841805E",
        "created_at": "2025-04-12T19:00:51+00:00",
        "updated_at": "2025-04-12T19:01:58+00:00",
        "version": 1
      }
    },
    "interactions:url.6F28EB05": {
      "data": {
        "sourceId": "url",
        "paperId": "6F28EB05",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-12T19:02:12.184Z",
            "data": {
              "session_id": "session_1744484531444_yj6q7dt",
              "source_id": "url",
              "paper_id": "6F28EB05",
              "start_time": "2025-04-12T19:02:00.362Z",
              "end_time": "2025-04-12T19:02:11.444Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2740,
        "object_id": "interactions:url.6F28EB05",
        "created_at": "2025-04-12T19:02:13+00:00",
        "updated_at": "2025-04-12T19:03:14+00:00",
        "version": 1
      }
    },
    "interactions:url.4636A927": {
      "data": {
        "sourceId": "url",
        "paperId": "4636A927",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-12T19:10:25.552Z",
            "data": {
              "session_id": "session_1744485024569_rf2pseh",
              "source_id": "url",
              "paper_id": "4636A927",
              "start_time": "2025-04-12T19:09:37.035Z",
              "end_time": "2025-04-12T19:10:24.569Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 3,
              "total_elapsed_seconds": 48
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2747,
        "object_id": "interactions:url.4636A927",
        "created_at": "2025-04-12T19:10:26+00:00",
        "updated_at": "2025-04-12T19:11:27+00:00",
        "version": 1
      }
    },
    "interactions:url.29495ADC": {
      "data": {
        "sourceId": "url",
        "paperId": "29495ADC",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-12T19:14:38.896Z",
            "data": {
              "session_id": "session_1744485277935_dw72vz6",
              "source_id": "url",
              "paper_id": "29495ADC",
              "start_time": "2025-04-12T19:13:20.847Z",
              "end_time": "2025-04-12T19:14:37.935Z",
              "heartbeat_count": 15,
              "duration_seconds": 75,
              "idle_seconds": 2,
              "total_elapsed_seconds": 77
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2753,
        "object_id": "interactions:url.29495ADC",
        "created_at": "2025-04-12T19:14:39+00:00",
        "updated_at": "2025-04-12T19:15:41+00:00",
        "version": 1
      }
    },
    "interactions:url.7F1F59AE": {
      "data": {
        "sourceId": "url",
        "paperId": "7F1F59AE",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-12T22:02:27.785Z",
            "data": {
              "session_id": "session_1744495347014_unx7f90",
              "source_id": "url",
              "paper_id": "7F1F59AE",
              "start_time": "2025-04-12T22:02:18.800Z",
              "end_time": "2025-04-12T22:02:27.014Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2765,
        "object_id": "interactions:url.7F1F59AE",
        "created_at": "2025-04-12T22:02:28+00:00",
        "updated_at": "2025-04-12T22:03:32+00:00",
        "version": 1
      }
    },
    "interactions:url.51CEEA8D": {
      "data": {
        "sourceId": "url",
        "paperId": "51CEEA8D",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-12T22:22:10.291Z",
            "data": {
              "session_id": "session_1744496529354_ybd1vvr",
              "source_id": "url",
              "paper_id": "51CEEA8D",
              "start_time": "2025-04-12T22:19:30.567Z",
              "end_time": "2025-04-12T22:22:09.354Z",
              "heartbeat_count": 31,
              "duration_seconds": 155,
              "idle_seconds": 4,
              "total_elapsed_seconds": 159
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2768,
        "object_id": "interactions:url.51CEEA8D",
        "created_at": "2025-04-12T22:22:11+00:00",
        "updated_at": "2025-04-12T22:23:16+00:00",
        "version": 1
      }
    },
    "interactions:url.7B3BF171": {
      "data": {
        "sourceId": "url",
        "paperId": "7B3BF171",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-12T22:24:28.752Z",
            "data": {
              "session_id": "session_1744496667986_o1lt1wy",
              "source_id": "url",
              "paper_id": "7B3BF171",
              "start_time": "2025-04-12T22:24:05.390Z",
              "end_time": "2025-04-12T22:24:27.986Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 3,
              "total_elapsed_seconds": 23
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2772,
        "object_id": "interactions:url.7B3BF171",
        "created_at": "2025-04-12T22:24:29+00:00",
        "updated_at": "2025-04-12T22:25:27+00:00",
        "version": 1
      }
    },
    "interactions:url.5C398FB3": {
      "data": {
        "sourceId": "url",
        "paperId": "5C398FB3",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-12T22:23:26.247Z",
            "data": {
              "session_id": "session_1744496606242_cofi2kc",
              "source_id": "url",
              "paper_id": "5C398FB3",
              "start_time": "2025-04-12T22:23:07.708Z",
              "end_time": "2025-04-12T22:23:26.242Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 4,
              "total_elapsed_seconds": 19
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2770,
        "object_id": "interactions:url.5C398FB3",
        "created_at": "2025-04-12T22:23:10+00:00",
        "updated_at": "2025-04-12T22:24:19+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.05426": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.05426",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-12T23:27:49.066Z",
            "data": {
              "session_id": "session_1744500468237_pr28emw",
              "source_id": "arxiv",
              "paper_id": "2504.05426",
              "start_time": "2025-04-12T23:27:23.378Z",
              "end_time": "2025-04-12T23:27:48.237Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 5,
              "total_elapsed_seconds": 25
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2775,
        "object_id": "interactions:arxiv.2504.05426",
        "created_at": "2025-04-12T23:27:49+00:00",
        "updated_at": "2025-04-12T23:28:55+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2406.00592": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.00592",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-13T00:19:49.688Z",
            "data": {
              "session_id": "session_1744503588969_tzqeegj",
              "source_id": "arxiv",
              "paper_id": "2406.00592",
              "start_time": "2025-04-13T00:19:36.273Z",
              "end_time": "2025-04-13T00:19:48.969Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 3,
              "total_elapsed_seconds": 13
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2779,
        "object_id": "interactions:arxiv.2406.00592",
        "created_at": "2025-04-13T00:19:50+00:00",
        "updated_at": "2025-04-13T00:20:52+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2406.00592": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.00592",
        "url": "https://arxiv.org/abs/2406.00592",
        "title": "Model Predictive Control and Reinforcement Learning: A Unified Framework Based on Dynamic Programming",
        "authors": "Bertsekas, Dimitri P.",
        "abstract": "In this paper we describe a new conceptual framework that connects approximate Dynamic Programming (DP), Model Predictive Control (MPC), and Reinforcement Learning (RL). This framework centers around two algorithms, which are designed largely independently of each other and operate in synergy through the powerful mechanism of Newton's method. We call them the off-line training and the on-line play algorithms. The names are borrowed from some of the major successes of RL involving games; primary examples are the recent (2017) AlphaZero program (which plays chess, [SHS17], [SSS17]), and the similarly structured and earlier (1990s) TD-Gammon program (which plays backgammon, [Tes94], [Tes95], [TeG96]). In these game contexts, the off-line training algorithm is the method used to teach the program how to evaluate positions and to generate good moves at any given position, while the on-line play algorithm is the method used to play in real time against human or computer opponents. Significantly, the synergy between off-line training and on-line play also underlies MPC (as well as other major classes of sequential decision problems), and indeed the MPC design architecture is very similar to the one of AlphaZero and TD-Gammon. This conceptual insight provides a vehicle for bridging the cultural gap between RL and MPC, and sheds new light on some fundamental issues in MPC. These include the enhancement of stability properties through rollout, the treatment of uncertainty through the use of certainty equivalence, the resilience of MPC in adaptive control settings that involve changing system parameters, and the insights provided by the superlinear performance bounds implied by Newton's method.",
        "timestamp": "2025-04-13T00:19:36.685Z",
        "rating": "novote",
        "publishedDate": "2024/06/02",
        "tags": [
          "eess.SY",
          "cs.AI",
          "cs.SY",
          "math.OC"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2778,
        "object_id": "paper:arxiv.2406.00592",
        "created_at": "2025-04-13T00:19:37+00:00",
        "updated_at": "2025-04-13T00:20:59+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.07965": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.07965",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-13T05:46:42.332Z",
            "data": {
              "session_id": "session_1744523202317_k50n3v2",
              "source_id": "arxiv",
              "paper_id": "2504.07965",
              "start_time": "2025-04-13T05:46:35.308Z",
              "end_time": "2025-04-13T05:46:42.317Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2782,
        "object_id": "interactions:arxiv.2504.07965",
        "created_at": "2025-04-13T05:46:11+00:00",
        "updated_at": "2025-04-13T05:47:12+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2211.12194": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2211.12194",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-13T06:16:37.115Z",
            "data": {
              "session_id": "session_1744524996420_b0eu84q",
              "source_id": "arxiv",
              "paper_id": "2211.12194",
              "start_time": "2025-04-13T06:16:30.083Z",
              "end_time": "2025-04-13T06:16:36.420Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2786,
        "object_id": "interactions:arxiv.2211.12194",
        "created_at": "2025-04-13T06:16:37+00:00",
        "updated_at": "2025-04-13T06:17:38+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2211.12194": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2211.12194",
        "url": "https://arxiv.org/abs/2211.12194",
        "title": "SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation",
        "authors": "Zhang, Wenxuan, Cun, Xiaodong, Wang, Xuan, Zhang, Yong, Shen, Xi, Guo, Yu, Shan, Ying, Wang, Fei",
        "abstract": "Generating talking head videos through a face image and a piece of speech audio still contains many challenges. ie, unnatural head movement, distorted expression, and identity modification. We argue that these issues are mainly because of learning from the coupled 2D motion fields. On the other hand, explicitly using 3D information also suffers problems of stiff expression and incoherent video. We present SadTalker, which generates 3D motion coefficients (head pose, expression) of the 3DMM from audio and implicitly modulates a novel 3D-aware face render for talking head generation. To learn the realistic motion coefficients, we explicitly model the connections between audio and different types of motion coefficients individually. Precisely, we present ExpNet to learn the accurate facial expression from audio by distilling both coefficients and 3D-rendered faces. As for the head pose, we design PoseVAE via a conditional VAE to synthesize head motion in different styles. Finally, the generated 3D motion coefficients are mapped to the unsupervised 3D keypoints space of the proposed face render, and synthesize the final video. We conducted extensive experiments to demonstrate the superiority of our method in terms of motion and video quality.",
        "timestamp": "2025-04-13T06:15:19.605Z",
        "rating": "novote",
        "publishedDate": "2022/11/22",
        "tags": [
          "cs.CV"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2784,
        "object_id": "paper:arxiv.2211.12194",
        "created_at": "2025-04-13T06:15:20+00:00",
        "updated_at": "2025-04-13T06:16:32+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2207.12393": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2207.12393",
        "url": "https://arxiv.org/abs/2207.12393",
        "title": "CelebV-HQ: A Large-Scale Video Facial Attributes Dataset",
        "authors": "Zhu, Hao, Wu, Wayne, Zhu, Wentao, Jiang, Liming, Tang, Siwei, Zhang, Li, Liu, Ziwei, Loy, Chen Change",
        "abstract": "Large-scale datasets have played indispensable roles in the recent success of face generation/editing and significantly facilitated the advances of emerging research fields. However, the academic community still lacks a video dataset with diverse facial attribute annotations, which is crucial for the research on face-related videos. In this work, we propose a large-scale, high-quality, and diverse video dataset with rich facial attribute annotations, named the High-Quality Celebrity Video Dataset (CelebV-HQ). CelebV-HQ contains 35,666 video clips with the resolution of 512x512 at least, involving 15,653 identities. All clips are labeled manually with 83 facial attributes, covering appearance, action, and emotion. We conduct a comprehensive analysis in terms of age, ethnicity, brightness stability, motion smoothness, head pose diversity, and data quality to demonstrate the diversity and temporal coherence of CelebV-HQ. Besides, its versatility and potential are validated on two representative tasks, i.e., unconditional video generation and video facial attribute editing. Furthermore, we envision the future potential of CelebV-HQ, as well as the new opportunities and challenges it would bring to related research directions. Data, code, and models are publicly available. Project page: https://celebv-hq.github.io.",
        "timestamp": "2025-04-13T06:18:43.256Z",
        "rating": "novote",
        "publishedDate": "2022/07/25",
        "tags": [
          "cs.CV"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2787,
        "object_id": "paper:arxiv.2207.12393",
        "created_at": "2025-04-13T06:18:43+00:00",
        "updated_at": "2025-04-13T06:19:49+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2211.06627": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2211.06627",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-13T06:40:59.493Z",
            "data": {
              "session_id": "session_1744526458338_qfpu7k7",
              "source_id": "arxiv",
              "paper_id": "2211.06627",
              "start_time": "2025-04-13T06:40:53.088Z",
              "end_time": "2025-04-13T06:40:58.338Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2790,
        "object_id": "interactions:arxiv.2211.06627",
        "created_at": "2025-04-13T06:41:00+00:00",
        "updated_at": "2025-04-13T06:42:01+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2211.06627": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2211.06627",
        "url": "https://arxiv.org/abs/2211.06627",
        "title": "MARLIN: Masked Autoencoder for facial video Representation LearnINg",
        "authors": "Cai, Zhixi, Ghosh, Shreya, Stefanov, Kalin, Dhall, Abhinav, Cai, Jianfei, Rezatofighi, Hamid, Haffari, Reza, Hayat, Munawar",
        "abstract": "This paper proposes a self-supervised approach to learn universal facial representations from videos, that can transfer across a variety of facial analysis tasks such as Facial Attribute Recognition (FAR), Facial Expression Recognition (FER), DeepFake Detection (DFD), and Lip Synchronization (LS). Our proposed framework, named MARLIN, is a facial video masked autoencoder, that learns highly robust and generic facial embeddings from abundantly available non-annotated web crawled facial videos. As a challenging auxiliary task, MARLIN reconstructs the spatio-temporal details of the face from the densely masked facial regions which mainly include eyes, nose, mouth, lips, and skin to capture local and global aspects that in turn help in encoding generic and transferable features. Through a variety of experiments on diverse downstream tasks, we demonstrate MARLIN to be an excellent facial video encoder as well as feature extractor, that performs consistently well across a variety of downstream tasks including FAR (1.13% gain over supervised benchmark), FER (2.64% gain over unsupervised benchmark), DFD (1.86% gain over unsupervised benchmark), LS (29.36% gain for Frechet Inception Distance), and even in low data regime. Our code and models are available at https://github.com/ControlNet/MARLIN .",
        "timestamp": "2025-04-13T06:40:53.640Z",
        "rating": "novote",
        "publishedDate": "2022/11/12",
        "tags": [
          "cs.CV"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2789,
        "object_id": "paper:arxiv.2211.06627",
        "created_at": "2025-04-13T06:40:54+00:00",
        "updated_at": "2025-04-13T06:42:11+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2401.08503": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2401.08503",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-13T06:53:12.342Z",
            "data": {
              "session_id": "session_1744527192338_fwjpizj",
              "source_id": "arxiv",
              "paper_id": "2401.08503",
              "start_time": "2025-04-13T06:53:05.365Z",
              "end_time": "2025-04-13T06:53:12.338Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2793,
        "object_id": "interactions:arxiv.2401.08503",
        "created_at": "2025-04-13T06:52:42+00:00",
        "updated_at": "2025-04-13T06:53:54+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2401.08503": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2401.08503",
        "url": "https://arxiv.org/abs/2401.08503",
        "title": "Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis",
        "authors": "Ye, Zhenhui, Zhong, Tianyun, Ren, Yi, Yang, Jiaqi, Li, Weichuang, Huang, Jiawei, Jiang, Ziyue, He, Jinzheng, Huang, Rongjie, Liu, Jinglin, Zhang, Chen, Yin, Xiang, Ma, Zejun, Zhao, Zhou",
        "abstract": "One-shot 3D talking portrait generation aims to reconstruct a 3D avatar from an unseen image, and then animate it with a reference video or audio to generate a talking portrait video. The existing methods fail to simultaneously achieve the goals of accurate 3D avatar reconstruction and stable talking face animation. Besides, while the existing works mainly focus on synthesizing the head part, it is also vital to generate natural torso and background segments to obtain a realistic talking portrait video. To address these limitations, we present Real3D-Potrait, a framework that (1) improves the one-shot 3D reconstruction power with a large image-to-plane model that distills 3D prior knowledge from a 3D face generative model; (2) facilitates accurate motion-conditioned animation with an efficient motion adapter; (3) synthesizes realistic video with natural torso movement and switchable background using a head-torso-background super-resolution model; and (4) supports one-shot audio-driven talking face generation with a generalizable audio-to-motion model. Extensive experiments show that Real3D-Portrait generalizes well to unseen identities and generates more realistic talking portrait videos compared to previous methods. Video samples and source code are available at https://real3dportrait.github.io .",
        "timestamp": "2025-04-13T06:51:44.452Z",
        "rating": "novote",
        "publishedDate": "2024/01/16",
        "tags": [
          "cs.CV"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2792,
        "object_id": "paper:arxiv.2401.08503",
        "created_at": "2025-04-13T06:51:44+00:00",
        "updated_at": "2025-04-13T06:52:30+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2403.17694": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2403.17694",
        "url": "https://arxiv.org/abs/2403.17694",
        "title": "AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation",
        "authors": "Wei, Huawei, Yang, Zejun, Wang, Zhisheng",
        "abstract": "In this study, we propose AniPortrait, a novel framework for generating high-quality animation driven by audio and a reference portrait image. Our methodology is divided into two stages. Initially, we extract 3D intermediate representations from audio and project them into a sequence of 2D facial landmarks. Subsequently, we employ a robust diffusion model, coupled with a motion module, to convert the landmark sequence into photorealistic and temporally consistent portrait animation. Experimental results demonstrate the superiority of AniPortrait in terms of facial naturalness, pose diversity, and visual quality, thereby offering an enhanced perceptual experience. Moreover, our methodology exhibits considerable potential in terms of flexibility and controllability, which can be effectively applied in areas such as facial motion editing or face reenactment. We release code and model weights at https://github.com/scutzzj/AniPortrait",
        "timestamp": "2025-04-13T06:51:19.582Z",
        "rating": "novote",
        "publishedDate": "2024/03/26",
        "tags": [
          "cs.CV",
          "cs.GR",
          "eess.IV"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2791,
        "object_id": "paper:arxiv.2403.17694",
        "created_at": "2025-04-13T06:51:20+00:00",
        "updated_at": "2025-04-13T06:52:42+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.07128": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.07128",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-13T07:14:28.301Z",
            "data": {
              "session_id": "session_1744528467513_s52fwgq",
              "source_id": "arxiv",
              "paper_id": "2504.07128",
              "start_time": "2025-04-13T07:14:12.896Z",
              "end_time": "2025-04-13T07:14:27.513Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 5,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2796,
        "object_id": "interactions:arxiv.2504.07128",
        "created_at": "2025-04-13T07:14:29+00:00",
        "updated_at": "2025-04-13T07:15:32+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2402.04398": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.04398",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-13T18:45:22.551Z",
            "data": {
              "session_id": "session_1744569922542_76jx2lf",
              "source_id": "arxiv",
              "paper_id": "2402.04398",
              "start_time": "2025-04-13T18:45:13.298Z",
              "end_time": "2025-04-13T18:45:22.542Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2800,
        "object_id": "interactions:arxiv.2402.04398",
        "created_at": "2025-04-13T18:45:04+00:00",
        "updated_at": "2025-04-13T18:46:08+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.05741": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.05741",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-14T02:11:00.614Z",
            "data": {
              "session_id": "session_1744596659780_n0l5412",
              "source_id": "arxiv",
              "paper_id": "2504.05741",
              "start_time": "2025-04-14T02:10:52.883Z",
              "end_time": "2025-04-14T02:10:59.780Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2804,
        "object_id": "interactions:arxiv.2504.05741",
        "created_at": "2025-04-14T02:11:01+00:00",
        "updated_at": "2025-04-14T02:12:08+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.05741": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.05741",
        "url": "https://arxiv.org/abs/2504.05741",
        "title": "DDT: Decoupled Diffusion Transformer",
        "authors": "Wang, Shuai, Tian, Zhi, Huang, Weilin, Wang, Limin",
        "abstract": "Diffusion transformers have demonstrated remarkable generation quality, albeit requiring longer training iterations and numerous inference steps. In each denoising step, diffusion transformers encode the noisy inputs to extract the lower-frequency semantic component and then decode the higher frequency with identical modules. This scheme creates an inherent optimization dilemma: encoding low-frequency semantics necessitates reducing high-frequency components, creating tension between semantic encoding and high-frequency decoding. To resolve this challenge, we propose a new \\textbf{\\color{ddt}D}ecoupled \\textbf{\\color{ddt}D}iffusion \\textbf{\\color{ddt}T}ransformer~(\\textbf{\\color{ddt}DDT}), with a decoupled design of a dedicated condition encoder for semantic extraction alongside a specialized velocity decoder. Our experiments reveal that a more substantial encoder yields performance improvements as model size increases. For ImageNet $256\\times256$, Our DDT-XL/2 achieves a new state-of-the-art performance of {1.31 FID}~(nearly $4\\times$ faster training convergence compared to previous diffusion transformers). For ImageNet $512\\times512$, Our DDT-XL/2 achieves a new state-of-the-art FID of 1.28. Additionally, as a beneficial by-product, our decoupled architecture enhances inference speed by enabling the sharing self-condition between adjacent denoising steps. To minimize performance degradation, we propose a novel statistical dynamic programming approach to identify optimal sharing strategies.",
        "timestamp": "2025-04-14T02:10:53.416Z",
        "rating": "novote",
        "publishedDate": "2025/04/08",
        "tags": [
          "cs.CV",
          "cs.AI"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2803,
        "object_id": "paper:arxiv.2504.05741",
        "created_at": "2025-04-14T02:10:54+00:00",
        "updated_at": "2025-04-14T02:12:21+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2410.08164": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.08164",
        "url": "https://arxiv.org/abs/2410.08164",
        "title": "Agent S: An Open Agentic Framework that Uses Computers Like a Human",
        "authors": "Agashe, Saaket, Han, Jiuzhou, Gan, Shuyu, Yang, Jiachen, Li, Ang, Wang, Xin Eric",
        "abstract": "We present Agent S, an open agentic framework that enables autonomous interaction with computers through a Graphical User Interface (GUI), aimed at transforming human-computer interaction by automating complex, multi-step tasks. Agent S aims to address three key challenges in automating computer tasks: acquiring domain-specific knowledge, planning over long task horizons, and handling dynamic, non-uniform interfaces. To this end, Agent S introduces experience-augmented hierarchical planning, which learns from external knowledge search and internal experience retrieval at multiple levels, facilitating efficient task planning and subtask execution. In addition, it employs an Agent-Computer Interface (ACI) to better elicit the reasoning and control capabilities of GUI agents based on Multimodal Large Language Models (MLLMs). Evaluation on the OSWorld benchmark shows that Agent S outperforms the baseline by 9.37% on success rate (an 83.6% relative improvement) and achieves a new state-of-the-art. Comprehensive analysis highlights the effectiveness of individual components and provides insights for future improvements. Furthermore, Agent S demonstrates broad generalizability to different operating systems on a newly-released WindowsAgentArena benchmark. Code available at https://github.com/simular-ai/Agent-S.",
        "timestamp": "2025-04-14T02:16:26.268Z",
        "rating": "novote",
        "publishedDate": "2024/10/10",
        "tags": [
          "cs.AI",
          "cs.CL",
          "cs.CV"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2806,
        "object_id": "paper:arxiv.2410.08164",
        "created_at": "2025-04-14T02:16:26+00:00",
        "updated_at": "2025-04-14T02:17:29+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.00906": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.00906",
        "url": "https://arxiv.org/abs/2504.00906",
        "title": "Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents",
        "authors": "Agashe, Saaket, Wong, Kyle, Tu, Vincent, Yang, Jiachen, Li, Ang, Wang, Xin Eric",
        "abstract": "Computer use agents automate digital tasks by directly interacting with graphical user interfaces (GUIs) on computers and mobile devices, offering significant potential to enhance human productivity by completing an open-ended space of user queries. However, current agents face significant challenges: imprecise grounding of GUI elements, difficulties with long-horizon task planning, and performance bottlenecks from relying on single generalist models for diverse cognitive tasks. To this end, we introduce Agent S2, a novel compositional framework that delegates cognitive responsibilities across various generalist and specialist models. We propose a novel Mixture-of-Grounding technique to achieve precise GUI localization and introduce Proactive Hierarchical Planning, dynamically refining action plans at multiple temporal scales in response to evolving observations. Evaluations demonstrate that Agent S2 establishes new state-of-the-art (SOTA) performance on three prominent computer use benchmarks. Specifically, Agent S2 achieves 18.9% and 32.7% relative improvements over leading baseline agents such as Claude Computer Use and UI-TARS on the OSWorld 15-step and 50-step evaluation. Moreover, Agent S2 generalizes effectively to other operating systems and applications, surpassing previous best methods by 52.8% on WindowsAgentArena and by 16.52% on AndroidWorld relatively. Code available at https://github.com/simular-ai/Agent-S.",
        "timestamp": "2025-04-14T02:16:16.862Z",
        "rating": "novote",
        "publishedDate": "2025/04/01",
        "tags": [
          "cs.AI",
          "cs.CL",
          "cs.CV",
          "cs.LG"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2805,
        "object_id": "paper:arxiv.2504.00906",
        "created_at": "2025-04-14T02:16:17+00:00",
        "updated_at": "2025-04-14T02:17:42+00:00",
        "version": 1
      }
    },
    "interactions:url.7C5763FB": {
      "data": {
        "sourceId": "url",
        "paperId": "7C5763FB",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-14T02:49:49.962Z",
            "data": {
              "session_id": "session_1744598988960_85wm0cz",
              "source_id": "url",
              "paper_id": "7C5763FB",
              "start_time": "2025-04-14T02:49:11.779Z",
              "end_time": "2025-04-14T02:49:48.960Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 2,
              "total_elapsed_seconds": 37
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2810,
        "object_id": "interactions:url.7C5763FB",
        "created_at": "2025-04-14T02:49:51+00:00",
        "updated_at": "2025-04-14T02:50:45+00:00",
        "version": 1
      }
    },
    "interactions:url.41C51E68": {
      "data": {
        "sourceId": "url",
        "paperId": "41C51E68",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-14T02:49:03.963Z",
            "data": {
              "session_id": "session_1744598943171_fg0c9rb",
              "source_id": "url",
              "paper_id": "41C51E68",
              "start_time": "2025-04-14T02:48:51.383Z",
              "end_time": "2025-04-14T02:49:03.171Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2808,
        "object_id": "interactions:url.41C51E68",
        "created_at": "2025-04-14T02:49:04+00:00",
        "updated_at": "2025-04-14T02:50:06+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.08623": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.08623",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-14T04:25:18.850Z",
            "data": {
              "session_id": "session_1744604718098_1fn6dfs",
              "source_id": "arxiv",
              "paper_id": "2504.08623",
              "start_time": "2025-04-14T04:25:03.234Z",
              "end_time": "2025-04-14T04:25:18.098Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 5,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2813,
        "object_id": "interactions:arxiv.2504.08623",
        "created_at": "2025-04-14T04:25:19+00:00",
        "updated_at": "2025-04-14T04:26:24+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.08623": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.08623",
        "url": "https://arxiv.org/abs/2504.08623",
        "title": "Enterprise-Grade Security for the Model Context Protocol (MCP): Frameworks and Mitigation Strategies",
        "authors": "Narajala, Vineeth Sai, Habler, Idan",
        "abstract": "The Model Context Protocol (MCP), introduced by Anthropic, provides a standardized framework for artificial intelligence (AI) systems to interact with external data sources and tools in real-time. While MCP offers significant advantages for AI integration and capability extension, it introduces novel security challenges that demand rigorous analysis and mitigation. This paper builds upon foundational research into MCP architecture and preliminary security assessments to deliver enterprise-grade mitigation frameworks and detailed technical implementation strategies. Through systematic threat modeling and analysis of MCP implementations and analysis of potential attack vectors, including sophisticated threats like tool poisoning, we present actionable security patterns tailored for MCP implementers and adopters. The primary contribution of this research lies in translating theoretical security concerns into a practical, implementable framework with actionable controls, thereby providing essential guidance for the secure enterprise adoption and governance of integrated AI systems.",
        "timestamp": "2025-04-14T04:25:03.962Z",
        "rating": "novote",
        "publishedDate": "2025/04/11",
        "tags": [
          "cs.CR",
          "cs.AI"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2812,
        "object_id": "paper:arxiv.2504.08623",
        "created_at": "2025-04-14T04:25:04+00:00",
        "updated_at": "2025-04-14T04:28:03+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2101.10479": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2101.10479",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-14T04:38:23.591Z",
            "data": {
              "session_id": "session_1744605502702_mqidyso",
              "source_id": "arxiv",
              "paper_id": "2101.10479",
              "start_time": "2025-04-14T04:37:27.879Z",
              "end_time": "2025-04-14T04:38:22.702Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 5,
              "total_elapsed_seconds": 55
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2815,
        "object_id": "interactions:arxiv.2101.10479",
        "created_at": "2025-04-14T04:38:24+00:00",
        "updated_at": "2025-04-14T04:39:24+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2101.10479": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2101.10479",
        "url": "https://arxiv.org/abs/2101.10479",
        "title": "A Monad for Probabilistic Point Processes",
        "authors": "Dash, Swaraj, Staton, Sam",
        "abstract": "A point process on a space is a random bag of elements of that space. In this paper we explore programming with point processes in a monadic style. To this end we identify point processes on a space X with probability measures of bags of elements in X. We describe this view of point processes using the composition of the Giry and bag monads on the category of measurable spaces and functions and prove that this composition also forms a monad using a distributive law for monads. Finally, we define a morphism from a point process to its intensity measure, and show that this is a monad morphism. A special case of this monad morphism gives us Wald's Lemma, an identity used to calculate the expected value of the sum of a random number of random variables. Using our monad we define a range of point processes and point process operations and compositionally compute their corresponding intensity measures using the monad morphism.\n",
        "timestamp": "2025-04-14T04:37:28.438Z",
        "rating": "novote",
        "publishedDate": "2021/01/26",
        "tags": [
          "cs.PL",
          "cs.LO"
        ],
        "doi": "10.4204/EPTCS.333.2",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2814,
        "object_id": "paper:arxiv.2101.10479",
        "created_at": "2025-04-14T04:37:28+00:00",
        "updated_at": "2025-04-14T04:38:42+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.07128": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.07128",
        "url": "https://arxiv.org/abs/2504.07128",
        "title": "DeepSeek-R1 Thoughtology: Let's <think> about LLM Reasoning",
        "authors": "Marjanovi\u0107, Sara Vera, Patel, Arkil, Adlakha, Vaibhav, Aghajohari, Milad, BehnamGhader, Parishad, Bhatia, Mehar, Khandelwal, Aditi, Kraft, Austin, Krojer, Benno, L\u00f9, Xing Han, Meade, Nicholas, Shin, Dongchan, Kazemnejad, Amirhossein, Kamath, Gaurav, Mosbach, Marius, Sta\u0144czak, Karolina, Reddy, Siva",
        "abstract": "Large Reasoning Models like DeepSeek-R1 mark a fundamental shift in how LLMs approach complex problems. Instead of directly producing an answer for a given input, DeepSeek-R1 creates detailed multi-step reasoning chains, seemingly \"thinking\" about a problem before providing an answer. This reasoning process is publicly available to the user, creating endless opportunities for studying the reasoning behaviour of the model and opening up the field of Thoughtology. Starting from a taxonomy of DeepSeek-R1's basic building blocks of reasoning, our analyses on DeepSeek-R1 investigate the impact and controllability of thought length, management of long or confusing contexts, cultural and safety concerns, and the status of DeepSeek-R1 vis-\\`a-vis cognitive phenomena, such as human-like language processing and world modelling. Our findings paint a nuanced picture. Notably, we show DeepSeek-R1 has a 'sweet spot' of reasoning, where extra inference time can impair model performance. Furthermore, we find a tendency for DeepSeek-R1 to persistently ruminate on previously explored problem formulations, obstructing further exploration. We also note strong safety vulnerabilities of DeepSeek-R1 compared to its non-reasoning counterpart, which can also compromise safety-aligned LLMs.",
        "timestamp": "2025-04-13T07:14:12.554Z",
        "rating": "novote",
        "publishedDate": "2025/04/02",
        "tags": [
          "cs.CL"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2794,
        "object_id": "paper:arxiv.2504.07128",
        "created_at": "2025-04-13T07:14:12+00:00",
        "updated_at": "2025-04-14T04:39:05+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.05426": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.05426",
        "url": "https://arxiv.org/abs/2504.05426",
        "title": "Survey on Algorithms for multi-index models",
        "authors": "Bruna, Joan, Hsu, Daniel",
        "abstract": "We review the literature on algorithms for estimating the index space in a multi-index model. The primary focus is on computationally efficient (polynomial-time) algorithms in Gaussian space, the assumptions under which consistency is guaranteed by these methods, and their sample complexity. In many cases, a gap is observed between the sample complexity of the best known computationally efficient methods and the information-theoretical minimum. We also review algorithms based on estimating the span of gradients using nonparametric methods, and algorithms based on fitting neural networks using gradient descent",
        "timestamp": "2025-04-12T23:27:23.392Z",
        "rating": "novote",
        "publishedDate": "2025/04/07",
        "tags": [
          "stat.ML",
          "cs.LG",
          "stat.ME"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2774,
        "object_id": "paper:arxiv.2504.05426",
        "created_at": "2025-04-12T23:27:23+00:00",
        "updated_at": "2025-04-14T04:39:25+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2411.17525": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.17525",
        "url": "https://arxiv.org/abs/2411.17525",
        "title": "Pushing the Limits of Large Language Model Quantization via the Linearity Theorem",
        "authors": "Malinovskii, Vladimir, Panferov, Andrei, Ilin, Ivan, Guo, Han, Richt\u00e1rik, Peter, Alistarh, Dan",
        "abstract": "Quantizing large language models has become a standard way to reduce their memory and computational costs. Typically, existing methods focus on breaking down the problem into individual layer-wise sub-problems, and minimizing per-layer error, measured via various metrics. Yet, this approach currently lacks theoretical justification and the metrics employed may be sub-optimal. In this paper, we present a \"linearity theorem\" establishing a direct relationship between the layer-wise $\\ell_2$ reconstruction error and the model perplexity increase due to quantization. This insight enables two novel applications: (1) a simple data-free LLM quantization method using Hadamard rotations and MSE-optimal grids, dubbed HIGGS, which outperforms all prior data-free approaches such as the extremely popular NF4 quantized format, and (2) an optimal solution to the problem of finding non-uniform per-layer quantization levels which match a given compression constraint in the medium-bitwidth regime, obtained by reduction to dynamic programming. On the practical side, we demonstrate improved accuracy-compression trade-offs on Llama-3.1 and 3.2-family models, as well as on Qwen-family models. Further, we show that our method can be efficiently supported in terms of GPU kernels at various batch sizes, advancing both data-free and non-uniform quantization for LLMs.",
        "timestamp": "2025-04-12T14:17:10.454Z",
        "rating": "novote",
        "publishedDate": "2024/11/26",
        "tags": [
          "cs.LG"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2717,
        "object_id": "paper:arxiv.2411.17525",
        "created_at": "2025-04-12T14:17:10+00:00",
        "updated_at": "2025-04-14T04:40:21+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2412.17808": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2412.17808",
        "url": "https://arxiv.org/pdf/2412.17808",
        "title": "2412.17808",
        "authors": [
          "Rui Chen",
          "Jianfeng Zhang",
          "Yixun Liang",
          "Guan Luo",
          "Weiyu Li",
          "Jiarui Liu",
          "Xiu Li",
          "Xiaoxiao Long",
          "Jiashi Feng",
          "Ping Tan"
        ],
        "abstract": "Recent 3D content generation pipelines commonly employ Variational\nAutoencoders (VAEs) to encode shapes into compact latent representations for\ndiffusion-based generation. However, the widely adopted uniform point sampling\nstrategy in Shape VAE training often leads to a significant loss of geometric\ndetails, limiting the quality of shape reconstruction and downstream generation\ntasks. We present Dora-VAE, a novel approach that enhances VAE reconstruction\nthrough our proposed sharp edge sampling strategy and a dual cross-attention\nmechanism. By identifying and prioritizing regions with high geometric\ncomplexity during training, our method significantly improves the preservation\nof fine-grained shape features. Such sampling strategy and the dual attention\nmechanism enable the VAE to focus on crucial geometric details that are\ntypically missed by uniform sampling approaches. To systematically evaluate VAE\nreconstruction quality, we additionally propose Dora-bench, a benchmark that\nquantifies shape complexity through the density of sharp edges, introducing a\nnew metric focused on reconstruction accuracy at these salient geometric\nfeatures. Extensive experiments on the Dora-bench demonstrate that Dora-VAE\nachieves comparable reconstruction quality to the state-of-the-art dense\nXCube-VAE while requiring a latent space at least 8$\\times$ smaller (1,280 vs.\n> 10,000 codes).",
        "timestamp": "2025-04-12T02:50:45.370Z",
        "rating": "novote",
        "publishedDate": "2024-12-23T18:59:06+00:00",
        "tags": [
          "cs.CV"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2700,
        "object_id": "paper:arxiv.2412.17808",
        "created_at": "2025-04-12T02:50:45+00:00",
        "updated_at": "2025-04-14T04:40:41+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.03090": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.03090",
        "url": "https://arxiv.org/abs/2504.03090",
        "title": "Optimal Erasure Codes and Codes on Graphs",
        "authors": "Chen, Yeyuan, Cheraghchi, Mahdi, Shagrithaya, Nikhil",
        "abstract": "We construct constant-sized ensembles of linear error-correcting codes over any fixed alphabet that can correct a given fraction of adversarial erasures at rates approaching the Singleton bound arbitrarily closely. We provide several applications of our results: 1. Explicit constructions of strong linear seeded symbol-fixing extractors and lossless condensers, over any fixed alphabet, with only a constant seed length and optimal output lengths; 2. A strongly explicit construction of erasure codes on bipartite graphs (more generally, linear codes on matrices of arbitrary dimensions) with optimal rate and erasure-correction trade-offs; 3. A strongly explicit construction of erasure codes on non-bipartite graphs (more generally, linear codes on symmetric square matrices) achieving improved rates; 4. A strongly explicit construction of linear nearly-MDS codes over constant-sized alphabets that can be encoded and decoded in quasi-linear time.",
        "timestamp": "2025-04-11T20:37:15.766Z",
        "rating": "novote",
        "publishedDate": "2025/04/03",
        "tags": [
          "cs.IT",
          "cs.DM",
          "math.CO",
          "math.IT"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2696,
        "object_id": "paper:arxiv.2504.03090",
        "created_at": "2025-04-11T20:37:16+00:00",
        "updated_at": "2025-04-14T04:40:54+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.07543": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.07543",
        "url": "https://arxiv.org/abs/2504.07543",
        "title": "MUFFLER: Secure Tor Traffic Obfuscation with Dynamic Connection Shuffling and Splitting",
        "authors": "Seo, Minjae, You, Myoungsung, Kim, Jaehan, Park, Taejune, Shin, Seungwon, Kim, Jinwoo",
        "abstract": "Tor, a widely utilized privacy network, enables anonymous communication but is vulnerable to flow correlation attacks that deanonymize users by correlating traffic patterns from Tor's ingress and egress segments. Various defenses have been developed to mitigate these attacks; however, they have two critical limitations: (i) significant network overhead during obfuscation and (ii) a lack of dynamic obfuscation for egress segments, exposing traffic patterns to adversaries. In response, we introduce MUFFLER, a novel connection-level traffic obfuscation system designed to secure Tor egress traffic. It dynamically maps real connections to a distinct set of virtual connections between the final Tor nodes and targeted services, either public or hidden. This approach creates egress traffic patterns fundamentally different from those at ingress segments without adding intentional padding bytes or timing delays. The mapping of real and virtual connections is adjusted in real-time based on ongoing network conditions, thwarting adversaries' efforts to detect egress traffic patterns. Extensive evaluations show that MUFFLER mitigates powerful correlation attacks with a TPR of 1% at an FPR of 10^-2 while imposing only a 2.17% bandwidth overhead. Moreover, it achieves up to 27x lower latency overhead than existing solutions and seamlessly integrates with the current Tor architecture.",
        "timestamp": "2025-04-11T05:41:22.461Z",
        "rating": "novote",
        "publishedDate": "2025/04/10",
        "tags": [
          "cs.CR",
          "cs.NI"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2685,
        "object_id": "paper:arxiv.2504.07543",
        "created_at": "2025-04-11T05:41:22+00:00",
        "updated_at": "2025-04-14T04:41:15+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.07341": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.07341",
        "url": "https://arxiv.org/abs/2504.07341",
        "title": "Learning to erase quantum states: thermodynamic implications of quantum learning theory",
        "authors": "Zhao, Haimeng, Zhang, Yuzhen, Preskill, John",
        "abstract": "The energy cost of erasing quantum states depends on our knowledge of the states. We show that learning algorithms can acquire such knowledge to erase many copies of an unknown state at the optimal energy cost. This is proved by showing that learning can be made fully reversible and has no fundamental energy cost itself. With simple counting arguments, we relate the energy cost of erasing quantum states to their complexity, entanglement, and magic. We further show that the constructed erasure protocol is computationally efficient when learning is efficient. Conversely, under standard cryptographic assumptions, we prove that the optimal energy cost cannot be achieved efficiently in general. These results also enable efficient work extraction based on learning. Together, our results establish a concrete connection between quantum learning theory and thermodynamics, highlighting the physical significance of learning processes and enabling efficient learning-based protocols for thermodynamic tasks.",
        "timestamp": "2025-04-11T04:48:30.420Z",
        "rating": "novote",
        "publishedDate": "2025/04/09",
        "tags": [
          "quant-ph",
          "cond-mat.stat-mech",
          "cs.CC",
          "cs.IT",
          "cs.LG",
          "math.IT"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2675,
        "object_id": "paper:arxiv.2504.07341",
        "created_at": "2025-04-11T04:48:30+00:00",
        "updated_at": "2025-04-14T04:41:30+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.07083": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.07083",
        "url": "https://arxiv.org/abs/2504.07083",
        "title": "GenDoP: Auto-regressive Camera Trajectory Generation as a Director of Photography",
        "authors": "Zhang, Mengchen, Wu, Tong, Tan, Jing, Liu, Ziwei, Wetzstein, Gordon, Lin, Dahua",
        "abstract": "Camera trajectory design plays a crucial role in video production, serving as a fundamental tool for conveying directorial intent and enhancing visual storytelling. In cinematography, Directors of Photography meticulously craft camera movements to achieve expressive and intentional framing. However, existing methods for camera trajectory generation remain limited: Traditional approaches rely on geometric optimization or handcrafted procedural systems, while recent learning-based methods often inherit structural biases or lack textual alignment, constraining creative synthesis. In this work, we introduce an auto-regressive model inspired by the expertise of Directors of Photography to generate artistic and expressive camera trajectories. We first introduce DataDoP, a large-scale multi-modal dataset containing 29K real-world shots with free-moving camera trajectories, depth maps, and detailed captions in specific movements, interaction with the scene, and directorial intent. Thanks to the comprehensive and diverse database, we further train an auto-regressive, decoder-only Transformer for high-quality, context-aware camera movement generation based on text guidance and RGBD inputs, named GenDoP. Extensive experiments demonstrate that compared to existing methods, GenDoP offers better controllability, finer-grained trajectory adjustments, and higher motion stability. We believe our approach establishes a new standard for learning-based cinematography, paving the way for future advancements in camera control and filmmaking. Our project website: https://kszpxxzmc.github.io/GenDoP/.",
        "timestamp": "2025-04-11T04:42:40.562Z",
        "rating": "novote",
        "publishedDate": "2025/04/09",
        "tags": [
          "cs.CV"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2671,
        "object_id": "paper:arxiv.2504.07083",
        "created_at": "2025-04-11T04:42:41+00:00",
        "updated_at": "2025-04-14T04:41:43+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.08226": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.08226",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-14T05:36:38.225Z",
            "data": {
              "session_id": "session_1744608997546_qffy6lm",
              "source_id": "arxiv",
              "paper_id": "2504.08226",
              "start_time": "2025-04-14T05:36:20.175Z",
              "end_time": "2025-04-14T05:36:37.546Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 2,
              "total_elapsed_seconds": 17
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2817,
        "object_id": "interactions:arxiv.2504.08226",
        "created_at": "2025-04-14T05:36:39+00:00",
        "updated_at": "2025-04-14T05:37:38+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.08226": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.08226",
        "url": "https://arxiv.org/abs/2504.08226",
        "title": "Uniform estimates for random matrix products and applications",
        "authors": "Hurtado, Omar, Raman, Sidhanth",
        "abstract": "For certain natural families of topologies, we study continuity and stability of statistical properties of random walks on linear groups over local fields. We extend large deviation results known in the Archimedean case to non-Archimedean local fields and also demonstrate certain large deviation estimates for heavy tailed distributions unknown even in the Archimedean case. A key technical result, which may be of independent interest, establishes lower semi-continuity for the gap between the first and second Lyapunov exponents. As applications, we are able to obtain a key technical step towards a localization proof for heavy tailed Anderson models (the full proof appearing in a companion article), and show continuity/stability (taking the geometric data as input) of various statistical data associated to hyperbolic surfaces.",
        "timestamp": "2025-04-14T05:36:20.659Z",
        "rating": "novote",
        "publishedDate": "2025/04/11",
        "tags": [
          "math.PR",
          "math.DS"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2816,
        "object_id": "paper:arxiv.2504.08226",
        "created_at": "2025-04-14T05:36:21+00:00",
        "updated_at": "2025-04-14T05:37:39+00:00",
        "version": 1
      }
    },
    "paper:url.40E47554": {
      "data": {
        "sourceId": "url",
        "paperId": "40E47554",
        "url": "https://www.sciencedirect.com/science/article/abs/pii/S0040580910000997",
        "title": "Properties of Weir and Cockerham\u2019s Fst estimators and associated bootstrap confidence intervals",
        "authors": "",
        "abstract": "Weir and Cockerham introduced single locus and multiloci Fst estimators for the parameter \u03b8. These estimators are commonly used, but little beyond the\u2026",
        "timestamp": "2025-04-14T03:00:18.271Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1016/j.tpb.2010.11.001",
        "journalName": "Theoretical Population Biology",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2811,
        "object_id": "paper:url.40E47554",
        "created_at": "2025-04-14T03:00:18+00:00",
        "updated_at": "2025-04-14T05:36:34+00:00",
        "version": 1
      }
    },
    "paper:url.7C5763FB": {
      "data": {
        "sourceId": "url",
        "paperId": "7C5763FB",
        "url": "https://iopscience.iop.org/article/10.1088/1361-6404/aaeb4d/pdf",
        "title": "7C5763FB",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-14T02:49:12.427Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2809,
        "object_id": "paper:url.7C5763FB",
        "created_at": "2025-04-14T02:49:13+00:00",
        "updated_at": "2025-04-14T05:37:00+00:00",
        "version": 1
      }
    },
    "paper:url.41C51E68": {
      "data": {
        "sourceId": "url",
        "paperId": "41C51E68",
        "url": "https://iopscience.iop.org/article/10.1088/1361-6404/aaeb4d",
        "title": "Stability of democracies: a complex systems perspective",
        "authors": "K Wiesner, A Birdi, T Eliassi-Rad, H Farrell, D Garcia, S Lewandowsky, P Palacios, D Ross, D Sornette, K Th\u00e9bault",
        "abstract": "Stability of democracies: a complex systems perspective, Wiesner, K, Birdi, A, Eliassi-Rad, T, Farrell, H, Garcia, D, Lewandowsky, S, Palacios, P, Ross, D, Sornette, D, Th\u00e9bault, K",
        "timestamp": "2025-04-14T02:48:51.819Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1088/1361-6404/aaeb4d",
        "journalName": "European Journal of Physics",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2807,
        "object_id": "paper:url.41C51E68",
        "created_at": "2025-04-14T02:48:52+00:00",
        "updated_at": "2025-04-14T05:37:26+00:00",
        "version": 1
      }
    },
    "paper:url.4811C99B": {
      "data": {
        "sourceId": "url",
        "paperId": "4811C99B",
        "url": "https://www.nature.com/articles/s41562-020-0889-7",
        "title": "How behavioural sciences can promote truth, autonomy and democratic discourse online",
        "authors": "Lorenz-Spreen, Philipp, Lewandowsky, Stephan, Sunstein, Cass R., Hertwig, Ralph",
        "abstract": "Lorenz-Spreen et al. argue that effective web governance is needed to empower individuals online. They describe two classes of behavioural interventions\u2014nudging and boosting\u2014 that can help redesign online environments for informed and autonomous choice",
        "timestamp": "2025-04-13T19:00:28.547Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1038/s41562-020-0889-7",
        "journalName": "Nature Human Behaviour",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2802,
        "object_id": "paper:url.4811C99B",
        "created_at": "2025-04-13T19:00:29+00:00",
        "updated_at": "2025-04-14T05:37:53+00:00",
        "version": 1
      }
    },
    "paper:url.4EB07019": {
      "data": {
        "sourceId": "url",
        "paperId": "4EB07019",
        "url": "https://scottaaronson.blog/?p=8654",
        "title": "Toward a non-constant cancellation function",
        "authors": "",
        "abstract": "It now seems the switch of Cancel Culture has only two settings: everything is cancellable\u2014including giving intellectual arguments against specific DEI policies, or teaching students about a \u2026",
        "timestamp": "2025-04-13T15:20:35.813Z",
        "rating": "novote",
        "publishedDate": "2025-02-11T20:13:01+00:00",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2797,
        "object_id": "paper:url.4EB07019",
        "created_at": "2025-04-13T15:20:36+00:00",
        "updated_at": "2025-04-14T05:38:59+00:00",
        "version": 1
      }
    },
    "paper:url.280B253C": {
      "data": {
        "sourceId": "url",
        "paperId": "280B253C",
        "url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_FaceChain-ImagineID_Freely_Crafting_High-Fidelity_Diverse_Talking_Faces_from_Disentangled_Audio_CVPR_2024_paper.pdf",
        "title": "280B253C",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-13T06:39:04.621Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 2788,
        "object_id": "paper:url.280B253C",
        "created_at": "2025-04-13T06:39:05+00:00",
        "updated_at": "2025-04-14T05:39:25+00:00",
        "version": 1
      }
    },
    "paper:url.62014F69": {
      "data": {
        "sourceId": "url",
        "paperId": "62014F69",
        "url": "https://proceedings.mlr.press/v235/huh24a.html",
        "title": "Position: The Platonic Representation Hypothesis",
        "authors": "Minyoung Huh, Brian Cheung, Tongzhou Wang, Phillip Isola",
        "abstract": "We argue that representations in AI models, particularly deep networks, are converging. First, we survey many examples of convergence in the literature: over time and across multiple domains, the w...",
        "timestamp": "2025-04-13T05:46:52.906Z",
        "rating": "novote",
        "publishedDate": "2024-07-08T00:00:00+00:00",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2783,
        "object_id": "paper:url.62014F69",
        "created_at": "2025-04-13T05:46:53+00:00",
        "updated_at": "2025-04-14T05:39:51+00:00",
        "version": 1
      }
    },
    "paper:url.7B3BF171": {
      "data": {
        "sourceId": "url",
        "paperId": "7B3BF171",
        "url": "https://web.mit.edu/STS.035/www/PDFs/think.pdf",
        "title": "7B3BF171",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-12T22:24:05.906Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 2771,
        "object_id": "paper:url.7B3BF171",
        "created_at": "2025-04-12T22:24:06+00:00",
        "updated_at": "2025-04-14T05:40:44+00:00",
        "version": 1
      }
    },
    "paper:url.5C398FB3": {
      "data": {
        "sourceId": "url",
        "paperId": "5C398FB3",
        "url": "https://archive.org/details/as-we-may-think",
        "title": "As We May Think : Vannevar Bush : Free Download, Borrow, and Streaming : Internet Archive",
        "authors": "",
        "abstract": "As We May Think is a 1945 essay by Vannevar Bush which has been described as visionary and influential, anticipating many aspects of information society. It...",
        "timestamp": "2025-04-12T22:23:03.175Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2769,
        "object_id": "paper:url.5C398FB3",
        "created_at": "2025-04-12T22:23:03+00:00",
        "updated_at": "2025-04-14T05:41:10+00:00",
        "version": 1
      }
    },
    "paper:url.51CEEA8D": {
      "data": {
        "sourceId": "url",
        "paperId": "51CEEA8D",
        "url": "https://www.cia.gov/resources/csi/static/Rise-Fall-Intelligence-Discipline.pdf",
        "title": "51CEEA8D",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-12T22:19:31.023Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 2767,
        "object_id": "paper:url.51CEEA8D",
        "created_at": "2025-04-12T22:19:31+00:00",
        "updated_at": "2025-04-14T05:41:37+00:00",
        "version": 1
      }
    },
    "paper:url.5F0B09FC": {
      "data": {
        "sourceId": "url",
        "paperId": "5F0B09FC",
        "url": "https://www.disarm.foundation/framework",
        "title": "Framework",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-12T22:08:00.243Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2766,
        "object_id": "paper:url.5F0B09FC",
        "created_at": "2025-04-12T22:08:00+00:00",
        "updated_at": "2025-04-14T05:42:03+00:00",
        "version": 1
      }
    },
    "paper:url.7F1F59AE": {
      "data": {
        "sourceId": "url",
        "paperId": "7F1F59AE",
        "url": "https://www.youtube.com/watch?v=gHqDEMrqTjE&t=2s",
        "title": "DEF CON 32 - Counter Deception: Defending Yourself in a World  Full of Lies - Tom Cross, Greg Conti",
        "authors": "",
        "abstract": "The Internet was supposed to give us access to the world's information, so that people, everywhere, would be able to know the truth. But that\u2019s not how thing...",
        "timestamp": "2025-04-12T22:02:19.363Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "DEF",
          "CON",
          "DEFCON",
          "DEF CON",
          "hacker conference",
          "security conference",
          "information security conference",
          "information security",
          "conference speakers",
          "hackers",
          "hacking",
          "hacking videos",
          "security research"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2764,
        "object_id": "paper:url.7F1F59AE",
        "created_at": "2025-04-12T22:02:19+00:00",
        "updated_at": "2025-04-14T05:42:29+00:00",
        "version": 1
      }
    },
    "paper:url.A4B9D70": {
      "data": {
        "sourceId": "url",
        "paperId": "A4B9D70",
        "url": "https://bsky.app/profile/lewan.bsky.social/post/3lmhdbnpnkc2u",
        "title": "Stephan Lewandowsky: \"Reliance on intuition is associated with decreased legislative productivity and increased polarization, and it is also a leading indicator of societal inequality. The recent trend towards intuition-based language is therefore concerning. 32/n\" \u2014 Bluesky",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-12T20:31:03.708Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2763,
        "object_id": "paper:url.A4B9D70",
        "created_at": "2025-04-12T20:31:04+00:00",
        "updated_at": "2025-04-14T05:42:56+00:00",
        "version": 1
      }
    },
    "paper:url.78095DCB": {
      "data": {
        "sourceId": "url",
        "paperId": "78095DCB",
        "url": "https://forum.safeguar.de/t/nih-repositories-under-threat/564",
        "title": "NIH Repositories Under Threat - Requests \ud83d\udd30 - Safeguarding Research & Culture (SRC) \u2014 Distributing Cultural Memory",
        "authors": "",
        "abstract": "Hi all,  I received news this morning that some NIH repositories now have a message: \u201cThis repository is under review for potential modification in compliance with Administration directives.\u201d Many of these repositories have controlled access data (because of they involve human medical data \u2013 I probably don\u2019t have to say it, but please don\u2019t share restricted human medical data openly\u2026), but some have open access/unrestricted data. Here is a (probably incomplete) list of affected repositories:   D...",
        "timestamp": "2025-04-12T20:11:29.753Z",
        "rating": "novote",
        "publishedDate": "2025-04-01T17:57:08+00:00",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2762,
        "object_id": "paper:url.78095DCB",
        "created_at": "2025-04-12T20:11:30+00:00",
        "updated_at": "2025-04-14T05:43:22+00:00",
        "version": 1
      }
    },
    "paper:url.106AF6C0": {
      "data": {
        "sourceId": "url",
        "paperId": "106AF6C0",
        "url": "https://www.404media.co/nih-archives-repositories-marked-for-review-for-potential-modification/",
        "title": "Massive, Unarchivable Datasets of Cancer, Covid, and Alzheimer's Research Could Be Lost Forever",
        "authors": "",
        "abstract": "Days before Robert F. Kennedy Jr. announced that 10,000 HHS staffers would lose their jobs, a message appeared on NIH research repository sites saying they were \"under review.\"",
        "timestamp": "2025-04-12T20:11:19.942Z",
        "rating": "novote",
        "publishedDate": "2025-04-04T15:15:33.000Z",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2761,
        "object_id": "paper:url.106AF6C0",
        "created_at": "2025-04-12T20:11:20+00:00",
        "updated_at": "2025-04-14T05:43:48+00:00",
        "version": 1
      }
    },
    "paper:url.2386E829": {
      "data": {
        "sourceId": "url",
        "paperId": "2386E829",
        "url": "https://www.sciencedirect.com/science/article/pii/S2352154625000403",
        "title": "A meta-analytic structural equation analysis of the Gateway Belief Model: highlighting scientific consensus increases support for public action on climate change",
        "authors": "",
        "abstract": "The Gateway Belief Model (GBM) posits that correcting influential misperceptions about the scientific consensus on climate change acts as a \u2018gateway\u2019 \u2026",
        "timestamp": "2025-04-12T19:17:10.503Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1016/j.cobeha.2025.101521",
        "journalName": "Current Opinion in Behavioral Sciences",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2760,
        "object_id": "paper:url.2386E829",
        "created_at": "2025-04-12T19:17:10+00:00",
        "updated_at": "2025-04-14T05:44:14+00:00",
        "version": 1
      }
    },
    "paper:url.4497F149": {
      "data": {
        "sourceId": "url",
        "paperId": "4497F149",
        "url": "https://www.iflscience.com/use-of-facts-and-evidence-based-rhetoric-at-all-time-low-in-congressional-speech-78785",
        "title": "\"Gut Feeling\" Is Outweighing Evidence-Based Rhetoric In US Politics\"",
        "authors": "",
        "abstract": "A new study has examined 8 million speeches from 140 years of Congress, showing a massive decline in evidence-based language since the 1970s.",
        "timestamp": "2025-04-12T19:16:54.405Z",
        "rating": "novote",
        "publishedDate": "2025-04-11T13:17:57.000Z",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2759,
        "object_id": "paper:url.4497F149",
        "created_at": "2025-04-12T19:16:54+00:00",
        "updated_at": "2025-04-14T05:44:41+00:00",
        "version": 1
      }
    },
    "paper:url.5BB303CB": {
      "data": {
        "sourceId": "url",
        "paperId": "5BB303CB",
        "url": "https://theconversation.com/how-populist-leaders-like-trump-use-common-sense-as-an-ideological-weapon-to-undermine-facts-248608",
        "title": "How populist leaders like Trump use \u2018common sense\u2019 as an ideological weapon to undermine facts",
        "authors": "Dannagal G. Young",
        "abstract": "When common sense is promoted as a virtue, it\u2019s not just to celebrate how regular people understand the world. It promotes a worldview that rejects empirical facts and paves the way for propaganda.",
        "timestamp": "2025-04-12T19:16:43.041Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2758,
        "object_id": "paper:url.5BB303CB",
        "created_at": "2025-04-12T19:16:43+00:00",
        "updated_at": "2025-04-14T05:45:08+00:00",
        "version": 1
      }
    },
    "paper:url.3BE01DEA": {
      "data": {
        "sourceId": "url",
        "paperId": "3BE01DEA",
        "url": "https://www.washingtonpost.com/politics/2021/01/24/trumps-false-or-misleading-claims-total-30573-over-four-years/",
        "title": "Analysis | Trump\u2019s false or misleading claims total 30,573 over 4 years",
        "authors": "",
        "abstract": "A four-year Fact Checker project comes to an end.",
        "timestamp": "2025-04-12T19:16:34.532Z",
        "rating": "novote",
        "publishedDate": "2021-01-24T08:00:50.086Z",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2757,
        "object_id": "paper:url.3BE01DEA",
        "created_at": "2025-04-12T19:16:35+00:00",
        "updated_at": "2025-04-14T05:45:35+00:00",
        "version": 1
      }
    },
    "paper:url.22AE0A9E": {
      "data": {
        "sourceId": "url",
        "paperId": "22AE0A9E",
        "url": "https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(24)00058-5?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS1364661324000585%3Fshowall%3Dtrue",
        "title": "When liars are considered honest",
        "authors": "Stephan Lewandowsky, David Garcia, Almog Simchon, Fabio Carrella",
        "abstract": "<h2>Abstract</h2><p>This article introduces a theoretical model of truth and honesty from a psychological perspective. We examine its application in political discourse and discuss empirical findings distinguishing between conceptions of honesty and their influence on public perception, misinformation dissemination, and the integrity of democracy.</p>",
        "timestamp": "2025-04-12T19:16:06.616Z",
        "rating": "novote",
        "publishedDate": "2024/05/01",
        "tags": [],
        "doi": "10.1016/j.tics.2024.03.005",
        "journalName": "Trends in Cognitive Sciences",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2756,
        "object_id": "paper:url.22AE0A9E",
        "created_at": "2025-04-12T19:16:07+00:00",
        "updated_at": "2025-04-14T05:46:01+00:00",
        "version": 1
      }
    },
    "paper:url.6AE6A9B2": {
      "data": {
        "sourceId": "url",
        "paperId": "6AE6A9B2",
        "url": "https://www.nature.com/articles/s41562-023-01691-w",
        "title": "From alternative conceptions of honesty to alternative facts in communications by US politicians",
        "authors": "Lasser, Jana, Aroyehun, Segun T., Carrella, Fabio, Simchon, Almog, Garcia, David, Lewandowsky, Stephan",
        "abstract": "By examining patterns in public-facing communications of US politicians, the authors identify two honesty-related concepts: belief speaking and fact speaking. They find that for Republicans, but not Democrats, an increase of belief speaking is associated with a decrease in the quality of the shared content sources.",
        "timestamp": "2025-04-12T19:15:46.515Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1038/s41562-023-01691-w",
        "journalName": "Nature Human Behaviour",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2755,
        "object_id": "paper:url.6AE6A9B2",
        "created_at": "2025-04-12T19:15:47+00:00",
        "updated_at": "2025-04-14T05:46:27+00:00",
        "version": 1
      }
    },
    "paper:url.CC91162": {
      "data": {
        "sourceId": "url",
        "paperId": "CC91162",
        "url": "https://www.nature.com/articles/s41467-025-56753-6",
        "title": "Different honesty conceptions align across US politicians' tweets and public replies",
        "authors": "Carrella, Fabio, Aroyehun, Segun T., Lasser, Jana, Simchon, Almog, Garcia, David, Lewandowsky, Stephan",
        "abstract": "The concept of honesty among US politicians has been split into two forms: evidence-based \u201cfact-speaking\" and intuition-based \u201cbelief-speaking.\" Here, the authors find that replies to US politicians\u2019 tweets mirrored the form used.",
        "timestamp": "2025-04-12T19:15:37.098Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1038/s41467-025-56753-6",
        "journalName": "Nature Communications",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2754,
        "object_id": "paper:url.CC91162",
        "created_at": "2025-04-12T19:15:37+00:00",
        "updated_at": "2025-04-14T05:46:54+00:00",
        "version": 1
      }
    },
    "paper:url.29495ADC": {
      "data": {
        "sourceId": "url",
        "paperId": "29495ADC",
        "url": "https://newrepublic.com/article/193379/yale-professors-canada-higher-education-law-firms-fight-trump",
        "title": "In the Academy and the Legal World, It\u2019s Time to Stay and Fight    ",
        "authors": "Siva Vaidhyanathan",
        "abstract": "Stay or run? Let\u2019s not presume this is a simple question or that everyone\u2019s situation is the same. But I know who my heroes are.",
        "timestamp": "2025-04-12T19:13:20.643Z",
        "rating": "novote",
        "publishedDate": "2025-04-02",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2752,
        "object_id": "paper:url.29495ADC",
        "created_at": "2025-04-12T19:13:21+00:00",
        "updated_at": "2025-04-14T05:47:20+00:00",
        "version": 1
      }
    },
    "paper:url.2E8BBE81": {
      "data": {
        "sourceId": "url",
        "paperId": "2E8BBE81",
        "url": "https://psycnet.apa.org/doiLanding?doi=10.1037%2Fa0015141",
        "title": "Liberals and conservatives rely on different sets of moral foundations.",
        "authors": "",
        "abstract": "APA PsycNet DoiLanding page",
        "timestamp": "2025-04-12T19:10:53.157Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2750,
        "object_id": "paper:url.2E8BBE81",
        "created_at": "2025-04-12T19:10:53+00:00",
        "updated_at": "2025-04-14T05:47:46+00:00",
        "version": 1
      }
    },
    "paper:url.262B1DFE": {
      "data": {
        "sourceId": "url",
        "paperId": "262B1DFE",
        "url": "https://www.pnas.org/doi/10.1073/pnas.2313428121",
        "title": "Do moral values change with the seasons?",
        "authors": "Hohm, Ian, O\u2019Shea, Brian A., Schaller, Mark",
        "abstract": "Moral values guide consequential attitudes and actions. Here, we report evidence of\nseasonal variation in Americans&rsquo; endorsement of some&mdash;but not al...",
        "timestamp": "2025-04-12T19:10:44.346Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "seasonality",
          "moral foundations theory",
          "ecology",
          "emotion",
          "anxiety"
        ],
        "doi": "10.1073/pnas.2313428121",
        "journalName": "Proceedings of the National Academy of Sciences",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2749,
        "object_id": "paper:url.262B1DFE",
        "created_at": "2025-04-12T19:10:44+00:00",
        "updated_at": "2025-04-14T05:48:13+00:00",
        "version": 1
      }
    },
    "paper:url.47E07C7E": {
      "data": {
        "sourceId": "url",
        "paperId": "47E07C7E",
        "url": "https://journals.sagepub.com/doi/10.1177/17456916231178695",
        "title": "Homo temporus: Seasonal Cycles as a Fundamental Source of Variation in Human Psychology - Ian Hohm, Alexandra S. Wormley, Mark Schaller, Michael E. W. Varnum, 2024",
        "authors": "",
        "abstract": "Many animal species exhibit seasonal changes in their physiology and behavior. Yet despite ample evidence that humans are also responsive to seasons, the impact...",
        "timestamp": "2025-04-12T19:10:36.133Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "seasons",
          "ecology",
          "mating"
        ],
        "doi": "",
        "journalName": "Perspectives on Psychological Science",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2748,
        "object_id": "paper:url.47E07C7E",
        "created_at": "2025-04-12T19:10:36+00:00",
        "updated_at": "2025-04-14T05:48:39+00:00",
        "version": 1
      }
    },
    "paper:url.4636A927": {
      "data": {
        "sourceId": "url",
        "paperId": "4636A927",
        "url": "https://spsp.org/news/character-and-context-blog/hohm-schaller-moral-values-seasons-change",
        "title": "Do Moral Values Change with the Seasons? | SPSP",
        "authors": "",
        "abstract": "Endorsement of \u201cconservative\u201d moral principles varies with the seasons.",
        "timestamp": "2025-04-12T19:09:37.009Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2746,
        "object_id": "paper:url.4636A927",
        "created_at": "2025-04-12T19:09:37+00:00",
        "updated_at": "2025-04-14T05:49:05+00:00",
        "version": 1
      }
    },
    "paper:url.45DCC70B": {
      "data": {
        "sourceId": "url",
        "paperId": "45DCC70B",
        "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0050092",
        "title": "The Moral Stereotypes of Liberals and Conservatives: Exaggeration of Differences across the Political Spectrum",
        "authors": "Jesse Graham, Brian A. Nosek, Jonathan Haidt",
        "abstract": "We investigated the moral stereotypes political liberals and conservatives have of themselves and each other. In reality, liberals endorse the individual-focused moral concerns of compassion and fairness more than conservatives do, and conservatives endorse the group-focused moral concerns of ingroup loyalty, respect for authorities and traditions, and physical/spiritual purity more than liberals do. 2,212 U.S. participants filled out the Moral Foundations Questionnaire with their own answers, or as a typical liberal or conservative would answer. Across the political spectrum, moral stereotypes about \u201ctypical\u201d liberals and conservatives correctly reflected the direction of actual differences in foundation endorsement but exaggerated the magnitude of these differences. Contrary to common theories of stereotyping, the moral stereotypes were not simple underestimations of the political outgroup's morality. Both liberals and conservatives exaggerated the ideological extremity of moral concerns for the ingroup as well as the outgroup. Liberals were least accurate about both groups.",
        "timestamp": "2025-04-12T19:08:15.098Z",
        "rating": "novote",
        "publishedDate": "Dec 12, 2012",
        "tags": [
          "Political parties",
          "Political theory",
          "Metaanalysis",
          "Questionnaires",
          "Social theory",
          "Cognition",
          "Irish people",
          "Personality"
        ],
        "doi": "10.1371/journal.pone.0050092",
        "journalName": "PLOS ONE",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2745,
        "object_id": "paper:url.45DCC70B",
        "created_at": "2025-04-12T19:08:15+00:00",
        "updated_at": "2025-04-14T05:49:32+00:00",
        "version": 1
      }
    },
    "paper:url.4938D84F": {
      "data": {
        "sourceId": "url",
        "paperId": "4938D84F",
        "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0241144",
        "title": "Moral \u201cfoundations\u201d as the product of motivated social cognition: Empathy and other psychological underpinnings of ideological divergence in \u201cindividualizing\u201d and \u201cbinding\u201d concerns",
        "authors": "Michael Strupp-Levitsky, Sharareh Noorbaloochi, Andrew Shipley, John T. Jost",
        "abstract": "According to moral foundations theory, there are five distinct sources of moral intuition on which political liberals and conservatives differ. The present research program seeks to contextualize this taxonomy within the broader research literature on political ideology as motivated social cognition, including the observation that conservative judgments often serve system-justifying functions. In two studies, a combination of regression and path modeling techniques were used to explore the motivational underpinnings of ideological differences in moral intuitions. Consistent with our integrative model, the \u201cbinding\u201d foundations (in-group loyalty, respect for authority, and purity) were associated with epistemic and existential needs to reduce uncertainty and threat and system justification tendencies, whereas the so-called \u201cindividualizing\u201d foundations (fairness and avoidance of harm) were generally unrelated to epistemic and existential motives and were instead linked to empathic motivation. Taken as a whole, these results are consistent with the position taken by Hatemi, Crabtree, and Smith that moral \u201cfoundations\u201d are themselves the product of motivated social cognition.",
        "timestamp": "2025-04-12T19:06:32.716Z",
        "rating": "novote",
        "publishedDate": "Nov 10, 2020",
        "tags": [
          "Motivation",
          "Social cognition",
          "Social theory",
          "Political theory",
          "Social psychology",
          "Political parties",
          "Cognitive psychology",
          "Psychological attitudes"
        ],
        "doi": "10.1371/journal.pone.0241144",
        "journalName": "PLOS ONE",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2744,
        "object_id": "paper:url.4938D84F",
        "created_at": "2025-04-12T19:06:33+00:00",
        "updated_at": "2025-04-14T05:49:58+00:00",
        "version": 1
      }
    },
    "paper:url.A37F5": {
      "data": {
        "sourceId": "url",
        "paperId": "A37F5",
        "url": "https://www.nature.com/articles/s41467-021-24786-2",
        "title": "Investigating the role of group-based morality in extreme behavioral expressions of prejudice",
        "authors": "Hoover, Joe, Atari, Mohammad, Mostafazadeh Davani, Aida, Kennedy, Brendan, Portillo-Wightman, Gwenyth, Yeh, Leigh, Dehghani, Morteza",
        "abstract": "Understanding motivations underlying acts of hatred are essential for developing strategies to prevent such acts against marginalized groups. Here the authors show that group-based moral values are associated with tendency to justify extreme behavioural expressions of prejudice.",
        "timestamp": "2025-04-12T19:04:39.317Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1038/s41467-021-24786-2",
        "journalName": "Nature Communications",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2743,
        "object_id": "paper:url.A37F5",
        "created_at": "2025-04-12T19:04:39+00:00",
        "updated_at": "2025-04-14T05:50:24+00:00",
        "version": 1
      }
    },
    "paper:url.3E21573D": {
      "data": {
        "sourceId": "url",
        "paperId": "3E21573D",
        "url": "https://journals.sagepub.com/doi/10.1177/0956797612449177",
        "title": "The Moral Roots of Environmental Attitudes - Matthew Feinberg, Robb Willer, 2013",
        "authors": "",
        "abstract": "Americans\u2019 attitudes about the environment are highly polarized, but it is unclear why this is the case. We conducted five studies to examine this issue. Studie...",
        "timestamp": "2025-04-12T19:03:29.180Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "morality",
          "policymaking",
          "personal values",
          "scientific communication"
        ],
        "doi": "",
        "journalName": "Psychological Science",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2742,
        "object_id": "paper:url.3E21573D",
        "created_at": "2025-04-12T19:03:29+00:00",
        "updated_at": "2025-04-14T05:50:51+00:00",
        "version": 1
      }
    },
    "paper:url.7B6C5796": {
      "data": {
        "sourceId": "url",
        "paperId": "7B6C5796",
        "url": "https://osf.io/preprints/psyarxiv/eq9ma_v1",
        "title": "Wilful Construction of Ignorance: A Tale of Two Ontologies",
        "authors": "Stephan Lewandowsky",
        "abstract": "From Iraq\u2019s mythical weapons of mass destruction (WMD) to Donald Trump\u2019s record of more than 10 daily false or misleading statements, deception and false claims have been an integral part of political discourse for quite some time. Nonetheless, Trump\u2019s blatant disregard for the truth has given rise to much concern about the dawn of a \u201cpost-truth\u201d era. I argue that there are striking differences between the tacit ontologies of truth underlying the WMD deception and Donald Trump\u2019s false claims, respectively. Whereas the WMD campaign contested a single reality, Trump\u2019s false claims often repudiate the very idea of external truths that exist independently of anyone\u2019s opinion. I consider this ontological shift from realism to extreme constructivism to be the most critical aspect of the current \u201cpost-truth\u201d malaise. I note that an extreme constructivist \u201ctruth\u201d has formed an essential aspect of historical fascism and Nazism, as well as of contemporary populist movements, and that those conceptions are incompatible with liberal-democratic norms of truth-seeking. I conclude by pointing towards potential solutions of the \u201cpost-truth\u201d crisis.",
        "timestamp": "2025-04-12T19:02:31.152Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.31234/osf.io/eq9ma",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2741,
        "object_id": "paper:url.7B6C5796",
        "created_at": "2025-04-12T19:02:31+00:00",
        "updated_at": "2025-04-14T05:51:17+00:00",
        "version": 1
      }
    },
    "paper:url.6F28EB05": {
      "data": {
        "sourceId": "url",
        "paperId": "6F28EB05",
        "url": "https://imec-publications.be/bitstream/handle/20.500.12860/39086/Moral_Foundations_and_Political_Orientation__Systematic_Review_an_Meta-Analysis.pdf?sequence=5",
        "title": "6F28EB05",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-12T19:02:00.894Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2739,
        "object_id": "paper:url.6F28EB05",
        "created_at": "2025-04-12T19:02:01+00:00",
        "updated_at": "2025-04-14T05:51:43+00:00",
        "version": 1
      }
    },
    "paper:url.4841805E": {
      "data": {
        "sourceId": "url",
        "paperId": "4841805E",
        "url": "https://spsp.org/news-center/character-context-blog/morality-different-liberals-and-conservatives-its-more-complex",
        "title": "Morality is Different to Liberals and Conservatives, but it\u2019s More Complex than That | SPSP",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-12T19:00:38.973Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2737,
        "object_id": "paper:url.4841805E",
        "created_at": "2025-04-12T19:00:39+00:00",
        "updated_at": "2025-04-14T05:52:10+00:00",
        "version": 1
      }
    },
    "paper:url.6A570F61": {
      "data": {
        "sourceId": "url",
        "paperId": "6A570F61",
        "url": "https://www.bu.edu/articles/2024/do-immigrants-and-immigration-help-the-economy/",
        "title": "Do Immigrants and Immigration Help the Economy?",
        "authors": "",
        "abstract": "With immigration dominating politics and voter concerns, BU economist\u2019s research shows immigration boosts local wages and that having neighbors of foreign descent can reduce prejudice",
        "timestamp": "2025-04-12T17:39:05.562Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2736,
        "object_id": "paper:url.6A570F61",
        "created_at": "2025-04-12T17:39:06+00:00",
        "updated_at": "2025-04-14T05:52:36+00:00",
        "version": 1
      }
    },
    "paper:url.E3598AB": {
      "data": {
        "sourceId": "url",
        "paperId": "E3598AB",
        "url": "https://www.theguardian.com/news/2025/jan/28/the-loudest-megaphone-how-trump-mastered-our-new-attention-age",
        "title": "The loudest megaphone: how Trump mastered our new attention age",
        "authors": "",
        "abstract": "The old model of political debate is over, and spectacle beats argument every time. How did we get here?",
        "timestamp": "2025-04-12T17:38:33.671Z",
        "rating": "novote",
        "publishedDate": "2025-01-28T05:00:07.000Z",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2735,
        "object_id": "paper:url.E3598AB",
        "created_at": "2025-04-12T17:38:34+00:00",
        "updated_at": "2025-04-14T05:53:02+00:00",
        "version": 1
      }
    },
    "paper:url.41708386": {
      "data": {
        "sourceId": "url",
        "paperId": "41708386",
        "url": "https://www.mayoclinichealthsystem.org/hometown-health/speaking-of-health/cognitive-overload",
        "title": "Cognitive overload: Info paralysis",
        "authors": "",
        "abstract": "Cognitive overload happens when your brain tries to process too much information. Learn what it is and how to counter it.",
        "timestamp": "2025-04-12T17:38:19.285Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "information overload"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2734,
        "object_id": "paper:url.41708386",
        "created_at": "2025-04-12T17:38:19+00:00",
        "updated_at": "2025-04-14T05:53:28+00:00",
        "version": 1
      }
    },
    "paper:url.1918CBC5": {
      "data": {
        "sourceId": "url",
        "paperId": "1918CBC5",
        "url": "https://www.humanetech.com/brain-science",
        "title": "Brain Science - Center for Humane Technology",
        "authors": "",
        "abstract": "Technology companies profit from shaping our thoughts and behaviors. Understand how the brain can be manipulated, leading to addiction, fear, and distraction.",
        "timestamp": "2025-04-12T17:38:10.839Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2733,
        "object_id": "paper:url.1918CBC5",
        "created_at": "2025-04-12T17:38:11+00:00",
        "updated_at": "2025-04-14T05:53:55+00:00",
        "version": 1
      }
    },
    "paper:url.3D1D4481": {
      "data": {
        "sourceId": "url",
        "paperId": "3D1D4481",
        "url": "https://www.un.org/sites/un2.un.org/files/attention_economy_feb.pdf",
        "title": "3D1D4481",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-12T17:37:55.781Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 2732,
        "object_id": "paper:url.3D1D4481",
        "created_at": "2025-04-12T17:37:56+00:00",
        "updated_at": "2025-04-14T05:54:21+00:00",
        "version": 1
      }
    },
    "paper:url.25C4F32F": {
      "data": {
        "sourceId": "url",
        "paperId": "25C4F32F",
        "url": "https://www.humanetech.com/youth/the-attention-economy",
        "title": "The Attention Economy",
        "authors": "",
        "abstract": "Learn about the economic factors and larger systems that drive tech companies to sell your attention and behavior to advertisers.",
        "timestamp": "2025-04-12T17:37:49.136Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2731,
        "object_id": "paper:url.25C4F32F",
        "created_at": "2025-04-12T17:37:49+00:00",
        "updated_at": "2025-04-14T05:54:47+00:00",
        "version": 1
      }
    },
    "paper:url.6A0B153E": {
      "data": {
        "sourceId": "url",
        "paperId": "6A0B153E",
        "url": "https://www.climatechangecommunication.org/wp-content/uploads/2023/09/DebunkingHandbook2020.pdf",
        "title": "6A0B153E",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-12T15:40:14.986Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 2729,
        "object_id": "paper:url.6A0B153E",
        "created_at": "2025-04-12T15:40:15+00:00",
        "updated_at": "2025-04-14T05:55:13+00:00",
        "version": 1
      }
    },
    "paper:url.6151E096": {
      "data": {
        "sourceId": "url",
        "paperId": "6151E096",
        "url": "https://www.climatechangecommunication.org/all/the-debunking-handbook-2020/",
        "title": "The Debunking Handbook 2020 - Center for Climate Change Communication",
        "authors": "Mason 4C Team",
        "abstract": "The Debunking Handbook 2020\u00a0summarizes the current state of the science of misinformation and its debunking. It was written by a team of 22 prominent scholars of misinformation and its debunking, and it represents the current consensus on the science of debunking for engaged citizens, policymakers, journalists, and other practitioners. Available in Other Languages:",
        "timestamp": "2025-04-12T15:40:03.147Z",
        "rating": "novote",
        "publishedDate": "2023-09-13T06:58:00+00:00",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2728,
        "object_id": "paper:url.6151E096",
        "created_at": "2025-04-12T15:40:03+00:00",
        "updated_at": "2025-04-14T05:55:39+00:00",
        "version": 1
      }
    },
    "paper:url.61C44604": {
      "data": {
        "sourceId": "url",
        "paperId": "61C44604",
        "url": "https://x.com/georgelakoff/status/1068891959882846208?lang=en",
        "title": "(2) George Lakoff on X: \"Truth Sandwich:\n1. Start with the truth. The first frame gets the advantage.\n2. Indicate the lie. Avoid amplifying the specific language if possible. \n3. Return to the truth. Always repeat truths more than lies.\nHear more in Ep 14 of FrameLab w/@gilduran76\nhttps://t.co/cQNOqgRk0w\" / X",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-12T15:38:57.016Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2727,
        "object_id": "paper:url.61C44604",
        "created_at": "2025-04-12T15:38:57+00:00",
        "updated_at": "2025-04-14T05:56:05+00:00",
        "version": 1
      }
    },
    "paper:url.3CC44432": {
      "data": {
        "sourceId": "url",
        "paperId": "3CC44432",
        "url": "https://link.springer.com/chapter/10.1007/978-3-030-45002-1_20",
        "title": "The Use of Critical Thinking to Identify Fake News: A Systematic Literature Review",
        "authors": "Machete, Paul, Turpin, Marita",
        "abstract": "With the large amount of news currently being published online, the ability to evaluate the credibility of online news has become essential. While there are many studies involving fake news and tools on how to detect it, there is a limited amount of work that focuses...",
        "timestamp": "2025-04-12T15:28:49.455Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1007/978-3-030-45002-1_20",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2726,
        "object_id": "paper:url.3CC44432",
        "created_at": "2025-04-12T15:28:49+00:00",
        "updated_at": "2025-04-14T05:56:32+00:00",
        "version": 1
      }
    },
    "paper:url.572B8B64": {
      "data": {
        "sourceId": "url",
        "paperId": "572B8B64",
        "url": "https://www.cambridge.org/core/journals/episteme/article/social-epistemology-for-individuals-like-us/929BC9DD48AD63C7564DD923E5E091E5",
        "title": "Social epistemology for individuals like us",
        "authors": "Molly O\u2019Rourke-Friel",
        "abstract": "This paper argues that we are not just social epistemic creatures because we operate in social contexts. We are social epistemic creatures because of the nature of our epistemic cognitive capacities. In The Enigma of Reason, Hugo Mercier and Dan Sperber develop and defend the view that reasoning is a social competence that yields epistemic benefits for individuals through social interaction with others. I argue an epistemological consequence of their position is that, when beliefs are formed and sustained by dialogical deliberation, the relevant justification-conferring process doesn\u2019t occur solely within the cognition of the subject whose belief is under evaluation. Rather, it extends to include her interactive engagement with other deliberative participants. I argue this demonstrates that not all justification-conferring is evidential. As such, the analysis not only supports reconceiving the process reliabilist\u2019s notion of justification-conferring processes; it also serves as an argument against evidentialism. A goal of this paper is to demonstrate that social epistemology isn\u2019t merely a siloed offshoot of traditional epistemology. Even when approaching social epistemology using a conservative methodology, our investigation has serious implications for fundamental questions concerning epistemic normativity.",
        "timestamp": "2025-04-12T14:53:05.126Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1017/epi.2024.59",
        "journalName": "Episteme",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2724,
        "object_id": "paper:url.572B8B64",
        "created_at": "2025-04-12T14:53:05+00:00",
        "updated_at": "2025-04-14T05:56:59+00:00",
        "version": 1
      }
    },
    "paper:url.659E2976": {
      "data": {
        "sourceId": "url",
        "paperId": "659E2976",
        "url": "https://openreview.net/forum?id=5B2K4LRgmz#discussion",
        "title": "Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data",
        "authors": "Matthias Gerstgrasser, Rylan Schaeffer, Apratim Dey, Rafael Rafailov, Tomasz Korbak, Henry Sleight, Rajashree Agrawal, John Hughes, Dhruv Bhandarkar Pai, Andrey Gromov, Dan Roberts, Diyi Yang, David L. Donoho, Sanmi Koyejo",
        "abstract": "The proliferation of generative models, combined with pretraining on web-scale data, raises a timely question: what happens when future models are trained on model-generated data? Recent investigations answered that such model-data feedback loops cause performance to progressively degrades with each model-data iteration until fitted models become useless, a phenomenon termed model collapse. However, those studies largely assumed that new data replace old data over time, where a more realistic assumption is that data accumulate over time. In this paper, we ask: what effect does accumulating data have on model collapse? \nWe first empirically study this question by pretraining sequences of language models on text corpora. After confirming that replacing the original real data by each generation's synthetic data does indeed tend towards model collapse, we demonstrate that accumulating synthetic data with real data avoids model collapse; these results hold across a range of sizes, architectures, and hyperparameters. We obtain similar results for other deep generative models: diffusion models for molecule conformation generation and variational autoencoders for image generation. To understand why accumulating data can avoid model collapse, we use an analytically tractable framework introduced by prior work in which a sequence of linear models are fit to previous models' outputs. Previous work used this framework to show that if data are replaced, the test error increases with the number of model-fitting iterations; we extend this argument to prove that if data instead accumulate, the test error has a finite upper bound independent of the number of iterations, meaning model collapse is avoided. \nOur work provides consistent empirical and theoretical evidence that data accumulation avoids model collapse.",
        "timestamp": "2025-04-12T14:32:36.430Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2722,
        "object_id": "paper:url.659E2976",
        "created_at": "2025-04-12T14:32:36+00:00",
        "updated_at": "2025-04-14T05:57:25+00:00",
        "version": 1
      }
    },
    "paper:url.4C8A9A43": {
      "data": {
        "sourceId": "url",
        "paperId": "4C8A9A43",
        "url": "https://openreview.net/forum?id=et5l9qPUhm",
        "title": "Strong Model Collapse",
        "authors": "Elvis Dohmatob, Yunzhen Feng, Arjun Subramonian, Julia Kempe",
        "abstract": "Within the scaling laws paradigm, which underpins the training of large neural networks like ChatGPT and Llama, we consider a supervised regression setting and establish a strong form of the model collapse phenomenon, a critical performance degradation due to synthetic data in the training corpus. Our results show that even the smallest fraction of synthetic data (e.g., as little as 1 per 1000) can still lead to model collapse: larger and larger training sets do not enhance performance.  We further investigate whether increasing model size, an approach aligned with current trends in training large language models, exacerbates or mitigates model collapse. In a simplified regime where neural networks are approximated via random projections of tunable size, we both theoretically and empirically show that larger models can amplify model collapse. Interestingly, our theory also indicates that, beyond the interpolation threshold (which can be extremely high for very large datasets), larger models may mitigate the collapse, although they do not entirely prevent it. Our theoretical findings are empirically verified through experiments on language models and neural networks for images.",
        "timestamp": "2025-04-12T14:20:56.130Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2719,
        "object_id": "paper:url.4C8A9A43",
        "created_at": "2025-04-12T14:20:56+00:00",
        "updated_at": "2025-04-14T05:57:51+00:00",
        "version": 1
      }
    },
    "paper:url.2DF39D4D": {
      "data": {
        "sourceId": "url",
        "paperId": "2DF39D4D",
        "url": "https://zenodo.org/records/8127870",
        "title": "100STYLE Dataset",
        "authors": "Mason, Ian, Starke, Sebastian, Komura, Taku",
        "abstract": "This is the 100STYLE dataset originally presented in the work\u00a0Real-Time Style Modelling of Human Locomotion via Feature-Wise Transformations and Local Motion Phases. This dataset contains over four million frames of stylized motion capture data. Both original bvh motion capture files (100STYLE.zip) and processed data (100Style-Labelled-Data.zip) are provided. The data is labelled using the hybrid approach to local phases described in the paper alongside the extraction of motion features such as joint positions, rotations and velocities. InputLabels.txt and OutputLabels.txt describe each of the feature dimensions extracted.",
        "timestamp": "2025-04-12T02:57:41.965Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1145/3522618",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2707,
        "object_id": "paper:url.2DF39D4D",
        "created_at": "2025-04-12T02:57:42+00:00",
        "updated_at": "2025-04-14T05:58:17+00:00",
        "version": 1
      }
    },
    "paper:url.F230225": {
      "data": {
        "sourceId": "url",
        "paperId": "F230225",
        "url": "https://www.nature.com/articles/s41540-025-00499-w",
        "title": "A mechanism for the emergence of low-dimensional structures in brain dynamics",
        "authors": "Runfola, Claudio, Petkoski, Spase, Sheheitli, Hiba, Bernard, Christophe, McIntosh, Anthony R., Jirsa, Viktor",
        "abstract": "Recent neuroimaging advancements have led to datasets characterized by an overwhelming number of features. Different dimensionality reduction techniques have been employed to uncover low-dimensional manifold representations underlying cognitive functions, while maintaining the fundamental characteristics of the data. These range from linear algorithms to more intricate non-linear methods for manifold extraction. However, the mechanisms responsible for the emergence of these simplified architectures remain a topic of debate. Motivated by concepts from dynamical systems theory, such as averaging and time-scale separation, our study introduces a novel mechanism for the collapse of high dimension brain dynamics onto lower dimensional manifolds. In our framework, fast neuronal activity oscillations average out over time, leading to the resulting dynamics approximating task-related processes occurring at slower time scales. This leads to the emergence of low-dimensional solutions as complex dynamics collapse into slow invariant manifolds. We test this assumption via neural simulations using a simplified model and then enhance the complexity of our simulations by incorporating a large-scale brain network model to mimic realistic neuroimaging signals. We observe in the different cases the convergence of fast oscillatory fluctuations of neuronal activity across time scales that correspond to simulated behavioral configurations. Specifically, by employing various dimensionality reduction techniques and manifold extraction schemes, we observe the reduction of high-dimensional dynamics onto lower-dimensional spaces, revealing emergent low-dimensional solutions. Our findings shed light on the role of frequency and time-scale separation in neuronal activity, proposing and testing a novel theoretical framework for understanding the inner mechanisms governing low-dimensional pattern formation in brain dynamics.",
        "timestamp": "2025-04-11T20:31:25.492Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1038/s41540-025-00499-w",
        "journalName": "npj Systems Biology and Applications",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2693,
        "object_id": "paper:url.F230225",
        "created_at": "2025-04-11T20:31:25+00:00",
        "updated_at": "2025-04-14T05:59:09+00:00",
        "version": 1
      }
    },
    "paper:url.4E8FECCF": {
      "data": {
        "sourceId": "url",
        "paperId": "4E8FECCF",
        "url": "https://citeseerx.ist.psu.edu/document?doi=f4dbe3101378625e2c6ef5a0e88fc1e1aa62315f&repid=rep1&type=pdf",
        "title": "4E8FECCF",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-11T14:54:08.314Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2690,
        "object_id": "paper:url.4E8FECCF",
        "created_at": "2025-04-11T14:54:08+00:00",
        "updated_at": "2025-04-14T05:59:36+00:00",
        "version": 1
      }
    },
    "paper:url.2EE72C2": {
      "data": {
        "sourceId": "url",
        "paperId": "2EE72C2",
        "url": "https://www.pnas.org/doi/10.1073/pnas.2317608121",
        "title": "Unraveling the mesoscale organization induced by network-driven processes",
        "authors": "Barzon, Giacomo, Artime, Oriol, Suweis, Samir, Domenico, Manlio De",
        "abstract": "Complex systems are characterized by emergent patterns created by the nontrivial interplay\nbetween dynamical processes and the networks of interact...",
        "timestamp": "2025-04-11T06:00:12.840Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "Jacobian distance",
          "latent geometry",
          "network-driven processes"
        ],
        "doi": "10.1073/pnas.2317608121",
        "journalName": "Proceedings of the National Academy of Sciences",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2687,
        "object_id": "paper:url.2EE72C2",
        "created_at": "2025-04-11T06:00:13+00:00",
        "updated_at": "2025-04-14T06:00:02+00:00",
        "version": 1
      }
    },
    "paper:url.2969854F": {
      "data": {
        "sourceId": "url",
        "paperId": "2969854F",
        "url": "https://journals.aps.org/prxquantum/abstract/10.1103/PRXQuantum.3.020333",
        "title": "Many-Body Quantum Magic",
        "authors": "Zi-Wen Liu, Andreas Winter",
        "abstract": "As with many good things, of the quantum computational resource of magic one can have either too little or too much, and to be useful it has to be consumed in moderation.",
        "timestamp": "2025-04-11T04:50:53.402Z",
        "rating": "novote",
        "publishedDate": "2022/05/12",
        "tags": [],
        "doi": "10.1103/PRXQuantum.3.020333",
        "journalName": "PRX Quantum",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2678,
        "object_id": "paper:url.2969854F",
        "created_at": "2025-04-11T04:50:54+00:00",
        "updated_at": "2025-04-14T06:00:29+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.08385": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.08385",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-14T12:10:53.055Z",
            "data": {
              "session_id": "session_1744632651816_0m6jwam",
              "source_id": "arxiv",
              "paper_id": "2504.08385",
              "start_time": "2025-04-14T12:10:26.002Z",
              "end_time": "2025-04-14T12:10:51.816Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 1,
              "total_elapsed_seconds": 26
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2820,
        "object_id": "interactions:arxiv.2504.08385",
        "created_at": "2025-04-14T12:10:53+00:00",
        "updated_at": "2025-04-14T12:11:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.08385": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.08385",
        "url": "https://arxiv.org/abs/2504.08385",
        "title": "Scholar Inbox: Personalized Paper Recommendations for Scientists",
        "authors": "Flicke, Markus, Angrabeit, Glenn, Iyengar, Madhav, Protsenko, Vitalii, Shakun, Illia, Cicvaric, Jovan, Kargi, Bora, He, Haoyu, Schuler, Lukas, Scholz, Lewin, Agnihotri, Kavyanjali, Cao, Yong, Geiger, Andreas",
        "abstract": "Scholar Inbox is a new open-access platform designed to address the challenges researchers face in staying current with the rapidly expanding volume of scientific literature. We provide personalized recommendations, continuous updates from open-access archives (arXiv, bioRxiv, etc.), visual paper summaries, semantic search, and a range of tools to streamline research workflows and promote open research access. The platform's personalized recommendation system is trained on user ratings, ensuring that recommendations are tailored to individual researchers' interests. To further enhance the user experience, Scholar Inbox also offers a map of science that provides an overview of research across domains, enabling users to easily explore specific topics. We use this map to address the cold start problem common in recommender systems, as well as an active learning strategy that iteratively prompts users to rate a selection of papers, allowing the system to learn user preferences quickly. We evaluate the quality of our recommendation system on a novel dataset of 800k user ratings, which we make publicly available, as well as via an extensive user study. https://www.scholar-inbox.com/",
        "timestamp": "2025-04-14T12:10:26.480Z",
        "rating": "novote",
        "publishedDate": "2025/04/11",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.IR"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2819,
        "object_id": "paper:arxiv.2504.08385",
        "created_at": "2025-04-14T12:10:27+00:00",
        "updated_at": "2025-04-14T12:12:23+00:00",
        "version": 1
      }
    },
    "paper:url.709EF5F7": {
      "data": {
        "sourceId": "url",
        "paperId": "709EF5F7",
        "url": "https://www.nature.com/articles/s41564-025-01972-2",
        "title": "Moving from genome-scale to community-scale metabolic models for the human gut microbiome",
        "authors": "Quinn-Bohmann, Nick, Carr, Alex V., Diener, Christian, Gibbons, Sean M.",
        "abstract": "In this Perspective, Deiner, Gibbons and colleagues describe the current strengths and limitations of microbial community-scale metabolic models in microbiome research.",
        "timestamp": "2025-04-14T15:37:28.393Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1038/s41564-025-01972-2",
        "journalName": "Nature Microbiology",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2821,
        "object_id": "paper:url.709EF5F7",
        "created_at": "2025-04-14T15:37:28+00:00",
        "updated_at": "2025-04-14T15:39:33+00:00",
        "version": 1
      }
    },
    "paper:url.E31DE86": {
      "data": {
        "sourceId": "url",
        "paperId": "E31DE86",
        "url": "https://horace.io/brrr_intro.html",
        "title": "Making Deep Learning go Brrrr From First Principles",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-14T20:47:57.977Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2822,
        "object_id": "paper:url.E31DE86",
        "created_at": "2025-04-14T20:47:58+00:00",
        "updated_at": "2025-04-14T20:49:41+00:00",
        "version": 1
      }
    },
    "interactions:url.E31DE86": {
      "data": {
        "sourceId": "url",
        "paperId": "E31DE86",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-14T20:51:10.206Z",
            "data": {
              "session_id": "session_1744663869246_4lazh5f",
              "source_id": "url",
              "paper_id": "E31DE86",
              "start_time": "2025-04-14T20:47:58.171Z",
              "end_time": "2025-04-14T20:51:09.246Z",
              "heartbeat_count": 38,
              "duration_seconds": 190,
              "idle_seconds": 1,
              "total_elapsed_seconds": 191
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2823,
        "object_id": "interactions:url.E31DE86",
        "created_at": "2025-04-14T20:51:11+00:00",
        "updated_at": "2025-04-14T20:52:12+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2310.16834": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2310.16834",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-14T23:55:21.637Z",
            "data": {
              "session_id": "session_1744674921625_hzb6phi",
              "source_id": "arxiv",
              "paper_id": "2310.16834",
              "start_time": "2025-04-14T23:54:39.417Z",
              "end_time": "2025-04-14T23:55:21.625Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 2,
              "total_elapsed_seconds": 42
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T00:01:42.389Z",
            "data": {
              "session_id": "session_1744675302188_cgbva4t",
              "source_id": "arxiv",
              "paper_id": "2310.16834",
              "start_time": "2025-04-15T00:01:33.383Z",
              "end_time": "2025-04-15T00:01:42.187Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T01:12:44.555Z",
            "data": {
              "session_id": "session_1744679564546_gudrxh1",
              "source_id": "arxiv",
              "paper_id": "2310.16834",
              "start_time": "2025-04-15T01:12:24.532Z",
              "end_time": "2025-04-15T01:12:44.546Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 15,
              "total_elapsed_seconds": 20
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2826,
        "object_id": "interactions:arxiv.2310.16834",
        "created_at": "2025-04-14T23:55:22+00:00",
        "updated_at": "2025-04-15T01:36:27+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.01384": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.01384",
        "url": "https://arxiv.org/abs/2502.01384",
        "title": "Fine-Tuning Discrete Diffusion Models with Policy Gradient Methods",
        "authors": "Zekri, Oussama, Boull\u00e9, Nicolas",
        "abstract": "Discrete diffusion models have recently gained significant attention due to their ability to process complex discrete structures for language modeling. However, fine-tuning these models with policy gradient methods, as is commonly done in Reinforcement Learning from Human Feedback (RLHF), remains a challenging task. We propose an efficient, broadly applicable, and theoretically justified policy gradient algorithm, called Score Entropy Policy Optimization (SEPO), for fine-tuning discrete diffusion models over non-differentiable rewards. Our numerical experiments across several discrete generative tasks demonstrate the scalability and efficiency of our method. Our code is available at https://github.com/ozekri/SEPO",
        "timestamp": "2025-04-14T23:54:35.284Z",
        "rating": "novote",
        "publishedDate": "2025/02/03",
        "tags": [
          "stat.ML",
          "cs.AI",
          "cs.CL",
          "cs.LG"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2825,
        "object_id": "paper:arxiv.2502.01384",
        "created_at": "2025-04-14T23:54:36+00:00",
        "updated_at": "2025-04-14T23:56:08+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2310.16834": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2310.16834",
        "url": "https://arxiv.org/abs/2310.16834",
        "title": "Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution",
        "authors": "Lou, Aaron, Meng, Chenlin, Ermon, Stefano",
        "abstract": "Despite their groundbreaking performance for many generative modeling tasks, diffusion models have fallen short on discrete data domains such as natural language. Crucially, standard diffusion models rely on the well-established theory of score matching, but efforts to generalize this to discrete structures have not yielded the same empirical gains. In this work, we bridge this gap by proposing score entropy, a novel loss that naturally extends score matching to discrete spaces, integrates seamlessly to build discrete diffusion models, and significantly boosts performance. Experimentally, we test our Score Entropy Discrete Diffusion models (SEDD) on standard language modeling tasks. For comparable model sizes, SEDD beats existing language diffusion paradigms (reducing perplexity by $25$-$75$\\%) and is competitive with autoregressive models, in particular outperforming GPT-2. Furthermore, compared to autoregressive mdoels, SEDD generates faithful text without requiring distribution annealing techniques like temperature scaling (around $6$-$8\\times$ better generative perplexity than un-annealed GPT-2), can trade compute and quality (similar quality with $32\\times$ fewer network evaluations), and enables controllable infilling (matching nucleus sampling quality while enabling other strategies besides left to right prompting).",
        "timestamp": "2025-04-14T23:54:03.775Z",
        "rating": "novote",
        "publishedDate": "2023/10/25",
        "tags": [
          "stat.ML",
          "cs.CL",
          "cs.LG"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2824,
        "object_id": "paper:arxiv.2310.16834",
        "created_at": "2025-04-14T23:54:04+00:00",
        "updated_at": "2025-04-14T23:56:48+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2502.01384": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.01384",
        "interactions": []
      },
      "meta": {
        "issue_number": 2829,
        "object_id": "interactions:arxiv.2502.01384",
        "created_at": "2025-04-15T00:29:02+00:00",
        "updated_at": "2025-04-15T00:29:05+00:00",
        "version": 1
      }
    },
    "interactions:url.2A987BF0": {
      "data": {
        "sourceId": "url",
        "paperId": "2A987BF0",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T00:28:20.864Z",
            "data": {
              "session_id": "session_1744676899819_4ukz1jn",
              "source_id": "url",
              "paper_id": "2A987BF0",
              "start_time": "2025-04-15T00:25:34.782Z",
              "end_time": "2025-04-15T00:28:19.819Z",
              "heartbeat_count": 33,
              "duration_seconds": 165,
              "idle_seconds": 0,
              "total_elapsed_seconds": 165
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2828,
        "object_id": "interactions:url.2A987BF0",
        "created_at": "2025-04-15T00:28:21+00:00",
        "updated_at": "2025-04-15T00:29:22+00:00",
        "version": 1
      }
    },
    "paper:url.2A987BF0": {
      "data": {
        "sourceId": "url",
        "paperId": "2A987BF0",
        "url": "https://aaronlou.com/blog/2024/discrete-diffusion/",
        "title": "Language Modeling by Estimating the Ratios of the Data Distribution | Aaron Lou",
        "authors": "Aaron Lou",
        "abstract": "Modern large language models (like ChatGPT) learn to generate new samples by modeling the data distribution of natural text. However, the underlying methodology has largely remained stagnant over the last century: although different architectures have been developed, models are all based on autoregressive modeling (i.e. next token prediction). In this blog post, I will talk about our work on Score Entropy Discrete Diffusion models, an alternative probablistic modeling technique that achieves highly competitive performance (at the scale of GPT-2) while introducing distinct algorithmic benefits. Our empirical results challenge the longstanding dominance of autoregressive modeling and can potentially pave the way for an alternative class of language models built from radically different principles.",
        "timestamp": "2025-04-15T00:25:33.885Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "machine learning",
          "geometric deep learning",
          "stanford",
          "aaron lou"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2827,
        "object_id": "paper:url.2A987BF0",
        "created_at": "2025-04-15T00:25:34+00:00",
        "updated_at": "2025-04-15T00:27:41+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.08716": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.08716",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T01:06:37.744Z",
            "data": {
              "session_id": "session_1744679196901_71p7pd4",
              "source_id": "arxiv",
              "paper_id": "2504.08716",
              "start_time": "2025-04-15T01:06:31.417Z",
              "end_time": "2025-04-15T01:06:36.901Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2831,
        "object_id": "interactions:arxiv.2504.08716",
        "created_at": "2025-04-15T01:06:39+00:00",
        "updated_at": "2025-04-15T01:07:43+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.08716": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.08716",
        "url": "https://arxiv.org/abs/2504.08716",
        "title": "ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on Transformer Encoder Models Performance",
        "authors": "Antoun, Wissam, Sagot, Beno\u00eet, Seddah, Djam\u00e9",
        "abstract": "Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce architectural advancements aimed at improving efficiency and performance. Although the authors of ModernBERT report improved performance over DeBERTaV3 on several benchmarks, the lack of disclosed training data and the absence of comparisons using a shared dataset make it difficult to determine whether these gains are due to architectural improvements or differences in training data. In this work, we conduct a controlled study by pretraining ModernBERT on the same dataset as CamemBERTaV2, a DeBERTaV3 French model, isolating the effect of model design. Our results show that the previous model generation remains superior in sample efficiency and overall benchmark performance, with ModernBERT's primary advantage being faster training and inference speed. However, the new proposed model still provides meaningful architectural improvements compared to earlier models such as BERT and RoBERTa. Additionally, we observe that high-quality pre-training data accelerates convergence but does not significantly improve final performance, suggesting potential benchmark saturation. These findings show the importance of disentangling pretraining data from architectural innovations when evaluating transformer models.",
        "timestamp": "2025-04-15T01:06:31.958Z",
        "rating": "novote",
        "publishedDate": "2025/04/11",
        "tags": [
          "cs.CL"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2830,
        "object_id": "paper:arxiv.2504.08716",
        "created_at": "2025-04-15T01:06:32+00:00",
        "updated_at": "2025-04-15T01:08:58+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.07965": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.07965",
        "url": "https://arxiv.org/abs/2504.07965",
        "title": "Cat, Rat, Meow: On the Alignment of Language Model and Human Term-Similarity Judgments",
        "authors": "Linhardt, Lorenz, Neuh\u00e4user, Tom, T\u011btkov\u00e1, Lenka, Eberle, Oliver",
        "abstract": "Small and mid-sized generative language models have gained increasing attention. Their size and availability make them amenable to being analyzed at a behavioral as well as a representational level, allowing investigations of how these levels interact. We evaluate 32 publicly available language models for their representational and behavioral alignment with human similarity judgments on a word triplet task. This provides a novel evaluation setting to probe semantic associations in language beyond common pairwise comparisons. We find that (1) even the representations of small language models can achieve human-level alignment, (2) instruction-tuned model variants can exhibit substantially increased agreement, (3) the pattern of alignment across layers is highly model dependent, and (4) alignment based on models' behavioral responses is highly dependent on model size, matching their representational alignment only for the largest evaluated models.",
        "timestamp": "2025-04-13T05:45:47.871Z",
        "rating": "novote",
        "publishedDate": "2025/04/10",
        "tags": [
          "cs.LG",
          "cs.CL"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2781,
        "object_id": "paper:arxiv.2504.07965",
        "created_at": "2025-04-13T05:45:48+00:00",
        "updated_at": "2025-04-15T01:14:52+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2406.07524": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.07524",
        "url": "https://arxiv.org/abs/2406.07524",
        "title": "Simple and Effective Masked Diffusion Language Models",
        "authors": "Sahoo, Subham Sekhar, Arriola, Marianne, Schiff, Yair, Gokaslan, Aaron, Marroquin, Edgar, Chiu, Justin T, Rush, Alexander, Kuleshov, Volodymyr",
        "abstract": "While diffusion models excel at generating high-quality images, prior work reports a significant performance gap between diffusion and autoregressive (AR) methods in language modeling. In this work, we show that simple masked discrete diffusion is more performant than previously thought. We apply an effective training recipe that improves the performance of masked diffusion models and derive a simplified, Rao-Blackwellized objective that results in additional improvements. Our objective has a simple form -- it is a mixture of classical masked language modeling losses -- and can be used to train encoder-only language models that admit efficient samplers, including ones that can generate arbitrary lengths of text semi-autoregressively like a traditional language model. On language modeling benchmarks, a range of masked diffusion models trained with modern engineering practices achieves a new state-of-the-art among diffusion models, and approaches AR perplexity. We provide the code, along with a blog post and video tutorial on the project page: https://s-sahoo.com/mdlm",
        "timestamp": "2025-04-15T03:55:08.921Z",
        "rating": "novote",
        "publishedDate": "2024/06/11",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2832,
        "object_id": "paper:arxiv.2406.07524",
        "created_at": "2025-04-15T03:55:09+00:00",
        "updated_at": "2025-04-15T03:57:34+00:00",
        "version": 1
      }
    },
    "interactions:url.31903D26": {
      "data": {
        "sourceId": "url",
        "paperId": "31903D26",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T04:25:12.344Z",
            "data": {
              "session_id": "session_1744691111379_1cbdyde",
              "source_id": "url",
              "paper_id": "31903D26",
              "start_time": "2025-04-15T04:23:25.921Z",
              "end_time": "2025-04-15T04:25:11.379Z",
              "heartbeat_count": 21,
              "duration_seconds": 105,
              "idle_seconds": 0,
              "total_elapsed_seconds": 105
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T04:28:22.989Z",
            "data": {
              "session_id": "session_1744691302387_2e5qt0t",
              "source_id": "url",
              "paper_id": "31903D26",
              "start_time": "2025-04-15T04:25:11.380Z",
              "end_time": "2025-04-15T04:28:22.387Z",
              "heartbeat_count": 38,
              "duration_seconds": 190,
              "idle_seconds": 1,
              "total_elapsed_seconds": 191
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2834,
        "object_id": "interactions:url.31903D26",
        "created_at": "2025-04-15T04:25:13+00:00",
        "updated_at": "2025-04-15T04:29:23+00:00",
        "version": 1
      }
    },
    "paper:url.31903D26": {
      "data": {
        "sourceId": "url",
        "paperId": "31903D26",
        "url": "https://www.youtube.com/watch?v=OZbCyBMfy2c",
        "title": "Jon Stewart on Kilmar Abrego Garcia\u2019s Deportation and How Trump Fails to Deliver | The Daily Show - YouTube",
        "authors": "",
        "abstract": "Enjoy the videos and music you love, upload original content, and share it all with friends, family, and the world on YouTube.",
        "timestamp": "2025-04-15T04:23:26.491Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "video",
          "sharing",
          "camera phone",
          "video phone",
          "free",
          "upload"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2833,
        "object_id": "paper:url.31903D26",
        "created_at": "2025-04-15T04:23:27+00:00",
        "updated_at": "2025-04-15T04:25:18+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.09161": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.09161",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T04:59:54.699Z",
            "data": {
              "session_id": "session_1744693193812_nkjfnmu",
              "source_id": "arxiv",
              "paper_id": "2504.09161",
              "start_time": "2025-04-15T04:57:26.748Z",
              "end_time": "2025-04-15T04:59:53.812Z",
              "heartbeat_count": 29,
              "duration_seconds": 145,
              "idle_seconds": 2,
              "total_elapsed_seconds": 147
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2837,
        "object_id": "interactions:arxiv.2504.09161",
        "created_at": "2025-04-15T04:59:55+00:00",
        "updated_at": "2025-04-15T05:01:05+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.09161": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.09161",
        "url": "https://arxiv.org/abs/2504.09161",
        "title": "An index for unitarizable $\\mathfrak{sl}(m\\vert n)$-supermodules",
        "authors": "Schmidt, Steffen, Walcher, Johannes",
        "abstract": "The \"superconformal index\" is a character-valued invariant attached by theoretical physics to unitary representations of Lie superalgebras, such as $\\mathfrak{su}(2,2\\vert n)$, that govern certain quantum field theories. The index can be calculated as a supertrace over Hilbert space, and is constant in families induced by variation of physical parameters. This is because the index receives contributions only from \"short\" irreducible representations such that it is invariant under recombination at the boundary of the region of unitarity. The purpose of this paper is to develop these notions for unitarizable supermodules over the special linear Lie superalgebras $\\mathfrak{sl}(m\\vert n)$ with $m\\ge 2$, $n\\ge 1$. To keep it self-contained, we include a fair amount of background material on structure theory, unitarizable supermodules, the Duflo-Serganova functor, and elements of Harish-Chandra theory. Along the way, we provide a precise dictionary between various notions from theoretical physics and mathematical terminology. Our final result is a kind of \"index theorem\" that relates the counting of atypical constituents in a general unitarizable $\\mathfrak{sl}(m\\vert n)$-supermodule to the character-valued $Q$-Witten index, expressed as a supertrace over the full supermodule. The formal superdimension of holomorphic discrete series $\\mathfrak{sl}(m\\vert n)$-supermodules can also be formulated in this framework.",
        "timestamp": "2025-04-15T04:57:25.932Z",
        "rating": "novote",
        "publishedDate": "2025/04/12",
        "tags": [
          "math.RT",
          "hep-th",
          "math-ph",
          "math.MP"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2835,
        "object_id": "paper:arxiv.2504.09161",
        "created_at": "2025-04-15T04:57:26+00:00",
        "updated_at": "2025-04-15T05:00:13+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.09811": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.09811",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T06:28:11.535Z",
            "data": {
              "session_id": "session_1744698490679_006skci",
              "source_id": "arxiv",
              "paper_id": "2504.09811",
              "start_time": "2025-04-15T06:27:35.664Z",
              "end_time": "2025-04-15T06:28:10.679Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 5,
              "total_elapsed_seconds": 35
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2839,
        "object_id": "interactions:arxiv.2504.09811",
        "created_at": "2025-04-15T06:28:12+00:00",
        "updated_at": "2025-04-15T06:29:10+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.09811": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.09811",
        "url": "https://arxiv.org/html/2504.09811v1",
        "title": "Volume estimates for the singular sets of mean curvature flows",
        "authors": [
          "Hanbing Fang",
          "Yu Li"
        ],
        "abstract": "In this paper, we establish uniform and sharp volume estimates for the\nsingular set and the quantitative singular strata of mean curvature flows\nstarting from a smooth, closed, mean-convex hypersurface in $\\mathbb R^{n+1}$.",
        "timestamp": "2025-04-15T06:27:36.270Z",
        "rating": "novote",
        "publishedDate": "2025-04-14T02:21:30+00:00",
        "tags": [
          "math.DG"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2838,
        "object_id": "paper:arxiv.2504.09811",
        "created_at": "2025-04-15T06:27:36+00:00",
        "updated_at": "2025-04-15T06:29:55+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.08144": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.08144",
        "url": "https://arxiv.org/pdf/2504.08144",
        "title": "2504.08144",
        "authors": [
          "Roger Casals",
          "Yoon Jae Nho"
        ],
        "abstract": "We introduce and develop the theory of spectral networks in real contact and\nsymplectic topology. First, we establish the existence and pseudoholomorphic\ncharacterization of spectral networks for Lagrangian fillings in the cotangent\nbundle of a smooth surface. These are proven via analytic results on the\nadiabatic degeneration of Floer trajectories and the explicit computation of\ncontinuation strips. Second, we construct a Family Floer functor for Lagrangian\nfillings endowed with a spectral network and prove its equivalence to the\nnon-abelianization functor. In particular, this implies that both the framed\n2d-4d BPS states and the Gaiotto-Moore-Neitzke non-abelianized parallel\ntransport are realized as part of the $A_\\infty$-operations of the associated\n4d partially wrapped Fukaya categories. To conclude, we present a new\nconstruction relating spectral networks and Lagrangian fillings using Demazure\nweaves, and show the precise relation between spectral networks and\naugmentations of the Legendrian contact dg-algebra.",
        "timestamp": "2025-04-15T06:33:18.566Z",
        "rating": "novote",
        "publishedDate": "2025-04-10T22:12:14+00:00",
        "tags": [
          "math.SG",
          "math-ph",
          "math.DG",
          "math.MP",
          "57K43, 53D12"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2840,
        "object_id": "paper:arxiv.2504.08144",
        "created_at": "2025-04-15T06:33:19+00:00",
        "updated_at": "2025-04-15T06:35:18+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.08144": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.08144",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T06:49:07.481Z",
            "data": {
              "session_id": "session_1744699745961_y0uotlj",
              "source_id": "arxiv",
              "paper_id": "2504.08144",
              "start_time": "2025-04-15T06:33:17.798Z",
              "end_time": "2025-04-15T06:49:05.961Z",
              "heartbeat_count": 189,
              "duration_seconds": 945,
              "idle_seconds": 3,
              "total_elapsed_seconds": 948
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2841,
        "object_id": "interactions:arxiv.2504.08144",
        "created_at": "2025-04-15T06:49:08+00:00",
        "updated_at": "2025-04-15T06:50:07+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.07290": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.07290",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T06:50:47.374Z",
            "data": {
              "session_id": "session_1744699846571_too65sx",
              "source_id": "arxiv",
              "paper_id": "2504.07290",
              "start_time": "2025-04-15T06:50:21.560Z",
              "end_time": "2025-04-15T06:50:46.571Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 15,
              "total_elapsed_seconds": 25
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2843,
        "object_id": "interactions:arxiv.2504.07290",
        "created_at": "2025-04-15T06:50:48+00:00",
        "updated_at": "2025-04-15T06:51:49+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.07290": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.07290",
        "url": "https://arxiv.org/pdf/2504.07290",
        "title": "2504.07290",
        "authors": [
          "Karen Butt",
          "Alena Erchenko",
          "Tristan Humbert",
          "Daniel Mitsutani"
        ],
        "abstract": "Using geometric and microlocal methods, we show that the Liouville entropy of\nthe geodesic flow of a closed surface of non-constant negative curvature is\nstrictly increasing along the normalized Ricci flow. This affirmatively answers\na question of Manning from 2004. More generally, we obtain an explicit formula\nfor the derivative of the Liouville entropy along arbitrary area-preserving\nconformal perturbations in this setting. In addition, we show the mean root\ncurvature, a purely geometric quantity which is a lower bound for the Liouville\nentropy, is also strictly increasing along the normalized Ricci flow.",
        "timestamp": "2025-04-15T06:50:22.116Z",
        "rating": "novote",
        "publishedDate": "2025-04-09T21:29:53+00:00",
        "tags": [
          "math.DS",
          "math.DG"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2842,
        "object_id": "paper:arxiv.2504.07290",
        "created_at": "2025-04-15T06:50:22+00:00",
        "updated_at": "2025-04-15T06:52:38+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.06954": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.06954",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T06:53:11.466Z",
            "data": {
              "session_id": "session_1744699990496_9yru7xv",
              "source_id": "arxiv",
              "paper_id": "2504.06954",
              "start_time": "2025-04-15T06:52:19.833Z",
              "end_time": "2025-04-15T06:53:10.496Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 1,
              "total_elapsed_seconds": 51
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2845,
        "object_id": "interactions:arxiv.2504.06954",
        "created_at": "2025-04-15T06:53:12+00:00",
        "updated_at": "2025-04-15T06:54:14+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.06954": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.06954",
        "url": "https://arxiv.org/pdf/2504.06954",
        "title": "2504.06954",
        "authors": [
          "Yirmeyahu Kaminski",
          "Pierre Lochak"
        ],
        "abstract": "For a family of dynamical systems with $k > 0$ independent first integrals\nevolving in a compact region of an Euclidean space, we study the equilibrium\nlocus. We show that under mild and generic conditions, it is a smooth manifold\nthat can be viewed as the total space of a certain fiber bundle and that this\nbundle comes equipped with a natural connection. We then proceed to show\nparallel transport for this connection does exist and explore some of its\nproperties. In particular, we elucidate how one can to some extent measure the\nvariation of the system eigenvalues restricted to a given fiber.",
        "timestamp": "2025-04-15T06:52:20.261Z",
        "rating": "novote",
        "publishedDate": "2025-04-09T15:02:10+00:00",
        "tags": [
          "math.DS",
          "math.DG",
          "53B15, 53C05, 53Z05, 53Z10"
        ],
        "doi": "10.3390/math12030457",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2844,
        "object_id": "paper:arxiv.2504.06954",
        "created_at": "2025-04-15T06:52:20+00:00",
        "updated_at": "2025-04-15T06:54:32+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.09696": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.09696",
        "url": "https://arxiv.org/abs/2504.09696",
        "title": "GRPO-LEAD: A Difficulty-Aware Reinforcement Learning Approach for Concise Mathematical Reasoning in Language Models",
        "authors": "Zhang, Jixiao, Zuo, Chunsheng",
        "abstract": "Recent advances in R1-like reasoning models leveraging Group Relative Policy Optimization (GRPO) have significantly improved the performance of language models on mathematical reasoning tasks. However, current GRPO implementations encounter critical challenges, including reward sparsity due to binary accuracy metrics, limited incentives for conciseness, and insufficient focus on complex reasoning tasks. To address these issues, we propose GRPO-LEAD, a suite of novel enhancements tailored for mathematical reasoning. Specifically, GRPO-LEAD introduces (1) a length-dependent accuracy reward to encourage concise and precise solutions, (2) an explicit penalty mechanism for incorrect answers to sharpen decision boundaries, and (3) a difficulty-aware advantage reweighting strategy that amplifies learning signals for challenging problems. Furthermore, we systematically examine the impact of model scale and supervised fine-tuning (SFT) strategies, demonstrating that larger-scale base models and carefully curated datasets significantly enhance reinforcement learning effectiveness. Extensive empirical evaluations and ablation studies confirm that GRPO-LEAD substantially mitigates previous shortcomings, resulting in language models that produce more concise, accurate, and robust reasoning across diverse mathematical tasks.",
        "timestamp": "2025-04-15T06:55:53.637Z",
        "rating": "novote",
        "publishedDate": "2025/04/13",
        "tags": [
          "cs.CL"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2846,
        "object_id": "paper:arxiv.2504.09696",
        "created_at": "2025-04-15T06:55:54+00:00",
        "updated_at": "2025-04-15T06:57:55+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.09522": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.09522",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T07:27:52.224Z",
            "data": {
              "session_id": "session_1744702070511_5gv69a3",
              "source_id": "arxiv",
              "paper_id": "2504.09522",
              "start_time": "2025-04-15T07:27:04.134Z",
              "end_time": "2025-04-15T07:27:50.511Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 1,
              "total_elapsed_seconds": 46
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T07:30:00.436Z",
            "data": {
              "session_id": "session_1744702199827_7g9n64u",
              "source_id": "arxiv",
              "paper_id": "2504.09522",
              "start_time": "2025-04-15T07:27:56.705Z",
              "end_time": "2025-04-15T07:29:59.827Z",
              "heartbeat_count": 24,
              "duration_seconds": 120,
              "idle_seconds": 3,
              "total_elapsed_seconds": 123
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T07:32:43.724Z",
            "data": {
              "session_id": "session_1744702363528_cc2hrj7",
              "source_id": "arxiv",
              "paper_id": "2504.09522",
              "start_time": "2025-04-15T07:32:25.547Z",
              "end_time": "2025-04-15T07:32:43.528Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 3,
              "total_elapsed_seconds": 18
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T07:34:34.570Z",
            "data": {
              "session_id": "session_1744702474546_0wrc748",
              "source_id": "arxiv",
              "paper_id": "2504.09522",
              "start_time": "2025-04-15T07:34:23.963Z",
              "end_time": "2025-04-15T07:34:34.546Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2848,
        "object_id": "interactions:arxiv.2504.09522",
        "created_at": "2025-04-15T07:27:53+00:00",
        "updated_at": "2025-04-15T07:35:29+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.09522": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.09522",
        "url": "https://arxiv.org/abs/2504.09522",
        "title": "How new data permeates LLM knowledge and how to dilute it",
        "authors": "Sun, Chen, Aksitov, Renat, Zhmoginov, Andrey, Miller, Nolan Andrew, Vladymyrov, Max, Rueckert, Ulrich, Kim, Been, Sandler, Mark",
        "abstract": "Large language models learn and continually learn through the accumulation of gradient-based updates, but how individual pieces of new information affect existing knowledge, leading to both beneficial generalization and problematic hallucination, remains poorly understood. We demonstrate that when learning new information, LLMs exhibit a \"priming\" effect: learning a new fact can cause the model to inappropriately apply that knowledge in unrelated contexts. To systematically study this phenomenon, we introduce \"Outlandish,\" a carefully curated dataset of 1320 diverse text samples designed to probe how new knowledge permeates through an LLM's existing knowledge base. Using this dataset, we show that the degree of priming after learning new information can be predicted by measuring the token probability of key words before learning. This relationship holds robustly across different model architectures (PALM-2, Gemma, Llama), sizes, and training stages. Finally, we develop two novel techniques to modulate how new knowledge affects existing model behavior: (1) a ``stepping-stone'' text augmentation strategy and (2) an ``ignore-k'' update pruning method. These approaches reduce undesirable priming effects by 50-95\\% while preserving the model's ability to learn new information. Our findings provide both empirical insights into how LLMs learn and practical tools for improving the specificity of knowledge insertion in language models. Further materials: https://sunchipsster1.github.io/projects/outlandish/",
        "timestamp": "2025-04-15T07:27:02.812Z",
        "rating": "novote",
        "publishedDate": "2025/04/13",
        "tags": [
          "cs.CL",
          "cs.AI"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2847,
        "object_id": "paper:arxiv.2504.09522",
        "created_at": "2025-04-15T07:27:03+00:00",
        "updated_at": "2025-04-15T07:29:21+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.08066": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.08066",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T07:39:26.211Z",
            "data": {
              "session_id": "session_1744702765284_7q5bm1l",
              "source_id": "arxiv",
              "paper_id": "2504.08066",
              "start_time": "2025-04-15T07:39:02.535Z",
              "end_time": "2025-04-15T07:39:25.284Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 3,
              "total_elapsed_seconds": 23
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2850,
        "object_id": "interactions:arxiv.2504.08066",
        "created_at": "2025-04-15T07:39:27+00:00",
        "updated_at": "2025-04-15T07:40:27+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.08066": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.08066",
        "url": "https://arxiv.org/abs/2504.08066",
        "title": "The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search",
        "authors": "Yamada, Yutaro, Lange, Robert Tjarko, Lu, Cong, Hu, Shengran, Lu, Chris, Foerster, Jakob, Clune, Jeff, Ha, David",
        "abstract": "AI is increasingly playing a pivotal role in transforming how scientific discoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic system capable of producing the first entirely AI generated peer-review-accepted workshop paper. This system iteratively formulates scientific hypotheses, designs and executes experiments, analyzes and visualizes data, and autonomously authors scientific manuscripts. Compared to its predecessor (v1, Lu et al., 2024 arXiv:2408.06292), The AI Scientist-v2 eliminates the reliance on human-authored code templates, generalizes effectively across diverse machine learning domains, and leverages a novel progressive agentic tree-search methodology managed by a dedicated experiment manager agent. Additionally, we enhance the AI reviewer component by integrating a Vision-Language Model (VLM) feedback loop for iterative refinement of content and aesthetics of the figures. We evaluated The AI Scientist-v2 by submitting three fully autonomous manuscripts to a peer-reviewed ICLR workshop. Notably, one manuscript achieved high enough scores to exceed the average human acceptance threshold, marking the first instance of a fully AI-generated paper successfully navigating a peer review. This accomplishment highlights the growing capability of AI in conducting all aspects of scientific research. We anticipate that further advancements in autonomous scientific discovery technologies will profoundly impact human knowledge generation, enabling unprecedented scalability in research productivity and significantly accelerating scientific breakthroughs, greatly benefiting society at large. We have open-sourced the code at https://github.com/SakanaAI/AI-Scientist-v2 to foster the future development of this transformative technology. We also discuss the role of AI in science, including AI safety.",
        "timestamp": "2025-04-15T07:39:03.087Z",
        "rating": "novote",
        "publishedDate": "2025/04/10",
        "tags": [
          "cs.AI",
          "cs.CL",
          "cs.LG"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2849,
        "object_id": "paper:arxiv.2504.08066",
        "created_at": "2025-04-15T07:39:03+00:00",
        "updated_at": "2025-04-15T07:41:01+00:00",
        "version": 1
      }
    },
    "interactions:url.464FA689": {
      "data": {
        "sourceId": "url",
        "paperId": "464FA689",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T14:49:02.226Z",
            "data": {
              "session_id": "session_1744728541245_jmfvbok",
              "source_id": "url",
              "paper_id": "464FA689",
              "start_time": "2025-04-15T14:47:26.186Z",
              "end_time": "2025-04-15T14:49:01.245Z",
              "heartbeat_count": 19,
              "duration_seconds": 95,
              "idle_seconds": 0,
              "total_elapsed_seconds": 95
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2852,
        "object_id": "interactions:url.464FA689",
        "created_at": "2025-04-15T14:49:03+00:00",
        "updated_at": "2025-04-15T14:50:09+00:00",
        "version": 1
      }
    },
    "paper:url.464FA689": {
      "data": {
        "sourceId": "url",
        "paperId": "464FA689",
        "url": "https://allenai.org/blog/datadecide",
        "title": "DataDecide: How to predict best pretraining data with small experiments  | Ai2",
        "authors": "",
        "abstract": "Explore the secrets of how language model developers make decisions with DataDecide.",
        "timestamp": "2025-04-15T14:47:24.250Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2851,
        "object_id": "paper:url.464FA689",
        "created_at": "2025-04-15T14:47:24+00:00",
        "updated_at": "2025-04-15T14:49:27+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2406.11634": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.11634",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T15:35:28.662Z",
            "data": {
              "session_id": "session_1744731327828_qunynfn",
              "source_id": "arxiv",
              "paper_id": "2406.11634",
              "start_time": "2025-04-15T15:35:19.705Z",
              "end_time": "2025-04-15T15:35:27.828Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2854,
        "object_id": "interactions:arxiv.2406.11634",
        "created_at": "2025-04-15T15:35:29+00:00",
        "updated_at": "2025-04-15T15:37:06+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2406.11634": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.11634",
        "url": "https://arxiv.org/abs/2406.11634v1",
        "title": "The Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance",
        "authors": "Moore, Kyle, Roberts, Jesse, Pham, Thao, Ewaleifoh, Oseremhen, Fisher, Doug",
        "abstract": "Cloze testing is a common method for measuring the behavior of large language models on a number of benchmark tasks. Using the MMLU dataset, we show that the base-rate probability (BRP) differences across answer tokens are significant and affect task performance ie. guess A if uncertain. We find that counterfactual prompting does sufficiently mitigate the BRP effect. The BRP effect is found to have a similar effect to test taking strategies employed by humans leading to the conflation of task performance and test-taking ability. We propose the Nvr-X-MMLU task, a variation of MMLU, which helps to disambiguate test-taking ability from task performance and reports the latter.",
        "timestamp": "2025-04-15T15:35:19.752Z",
        "rating": "novote",
        "publishedDate": "2024/06/17",
        "tags": [
          "cs.CL",
          "cs.AI"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2853,
        "object_id": "paper:arxiv.2406.11634",
        "created_at": "2025-04-15T15:35:20+00:00",
        "updated_at": "2025-04-15T15:37:20+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2410.15661": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.15661",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T19:40:10.972Z",
            "data": {
              "session_id": "session_1744746010141_9ebvtgo",
              "source_id": "arxiv",
              "paper_id": "2410.15661",
              "start_time": "2025-04-15T19:39:38.126Z",
              "end_time": "2025-04-15T19:40:10.141Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 2,
              "total_elapsed_seconds": 32
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2857,
        "object_id": "interactions:arxiv.2410.15661",
        "created_at": "2025-04-15T19:40:11+00:00",
        "updated_at": "2025-04-15T19:41:18+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2410.15661": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.15661",
        "url": "https://arxiv.org/abs/2410.15661",
        "title": "Scalable Data Ablation Approximations for Language Models through Modular Training and Merging",
        "authors": "Na, Clara, Magnusson, Ian, Jha, Ananya Harsh, Sherborne, Tom, Strubell, Emma, Dodge, Jesse, Dasigi, Pradeep",
        "abstract": "Training data compositions for Large Language Models (LLMs) can significantly affect their downstream performance. However, a thorough data ablation study exploring large sets of candidate data mixtures is typically prohibitively expensive since the full effect is seen only after training the models; this can lead practitioners to settle for sub-optimal data mixtures. We propose an efficient method for approximating data ablations which trains individual models on subsets of a training corpus and reuses them across evaluations of combinations of subsets. In continued pre-training experiments, we find that, given an arbitrary evaluation set, the perplexity score of a single model trained on a candidate set of data is strongly correlated with perplexity scores of parameter averages of models trained on distinct partitions of that data. From this finding, we posit that researchers and practitioners can conduct inexpensive simulations of data ablations by maintaining a pool of models that were each trained on partitions of a large training corpus, and assessing candidate data mixtures by evaluating parameter averages of combinations of these models. This approach allows for substantial improvements in amortized training efficiency -- scaling only linearly with respect to new data -- by enabling reuse of previous training computation, opening new avenues for improving model performance through rigorous, incremental data assessment and mixing.",
        "timestamp": "2025-04-15T19:37:26.339Z",
        "rating": "novote",
        "publishedDate": "2024/10/21",
        "tags": [
          "cs.CL",
          "cs.LG"
        ],
        "doi": "10.18653/v1/2024.emnlp-main.1176",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2855,
        "object_id": "paper:arxiv.2410.15661",
        "created_at": "2025-04-15T19:37:26+00:00",
        "updated_at": "2025-04-15T19:40:41+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2307.10928": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2307.10928",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T19:46:43.649Z",
            "data": {
              "session_id": "session_1744746402802_ug1xvyp",
              "source_id": "arxiv",
              "paper_id": "2307.10928",
              "start_time": "2025-04-15T19:46:13.616Z",
              "end_time": "2025-04-15T19:46:42.802Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 4,
              "total_elapsed_seconds": 29
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2863,
        "object_id": "interactions:arxiv.2307.10928",
        "created_at": "2025-04-15T19:46:44+00:00",
        "updated_at": "2025-04-15T19:47:47+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2302.07027": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2302.07027",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T19:46:01.755Z",
            "data": {
              "session_id": "session_1744746361038_zzqhyaa",
              "source_id": "arxiv",
              "paper_id": "2302.07027",
              "start_time": "2025-04-15T19:45:42.811Z",
              "end_time": "2025-04-15T19:46:01.038Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 3,
              "total_elapsed_seconds": 18
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2860,
        "object_id": "interactions:arxiv.2302.07027",
        "created_at": "2025-04-15T19:46:02+00:00",
        "updated_at": "2025-04-15T19:47:04+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2401.00625": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2401.00625",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T19:50:29.054Z",
            "data": {
              "session_id": "session_1744746628280_cwda3jx",
              "source_id": "arxiv",
              "paper_id": "2401.00625",
              "start_time": "2025-04-15T19:50:02.507Z",
              "end_time": "2025-04-15T19:50:28.280Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 1,
              "total_elapsed_seconds": 26
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T20:00:11.749Z",
            "data": {
              "session_id": "session_1744747211308_dauftzc",
              "source_id": "arxiv",
              "paper_id": "2401.00625",
              "start_time": "2025-04-15T20:00:00.368Z",
              "end_time": "2025-04-15T20:00:11.308Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-16T04:42:26.418Z",
            "data": {
              "session_id": "session_1744778546031_ff01rim",
              "source_id": "arxiv",
              "paper_id": "2401.00625",
              "start_time": "2025-04-16T04:42:14.901Z",
              "end_time": "2025-04-16T04:42:26.031Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2868,
        "object_id": "interactions:arxiv.2401.00625",
        "created_at": "2025-04-15T19:50:29+00:00",
        "updated_at": "2025-04-16T04:44:01+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2404.02948": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.02948",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T19:49:36.636Z",
            "data": {
              "session_id": "session_1744746575694_cm9co8o",
              "source_id": "arxiv",
              "paper_id": "2404.02948",
              "start_time": "2025-04-15T19:47:34.081Z",
              "end_time": "2025-04-15T19:49:35.694Z",
              "heartbeat_count": 24,
              "duration_seconds": 120,
              "idle_seconds": 2,
              "total_elapsed_seconds": 122
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2865,
        "object_id": "interactions:arxiv.2404.02948",
        "created_at": "2025-04-15T19:49:37+00:00",
        "updated_at": "2025-04-15T19:50:45+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2404.02948": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.02948",
        "url": "https://arxiv.org/abs/2404.02948",
        "title": "PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models",
        "authors": "Meng, Fanxu, Wang, Zhaohui, Zhang, Muhan",
        "abstract": "To parameter-efficiently fine-tune (PEFT) large language models (LLMs), the low-rank adaptation (LoRA) method approximates the model changes $\\Delta W \\in \\mathbb{R}^{m \\times n}$ through the product of two matrices $A \\in \\mathbb{R}^{m \\times r}$ and $B \\in \\mathbb{R}^{r \\times n}$, where $r \\ll \\min(m, n)$, $A$ is initialized with Gaussian noise, and $B$ with zeros. LoRA freezes the original model $W$ and updates the \"Noise & Zero\" adapter, which may lead to slow convergence. To overcome this limitation, we introduce Principal Singular values and Singular vectors Adaptation (PiSSA). PiSSA shares the same architecture as LoRA, but initializes the adaptor matrices $A$ and $B$ with the principal components of the original matrix $W$, and put the remaining components into a residual matrix $W^{res} \\in \\mathbb{R}^{m \\times n}$ which is frozen during fine-tuning. Compared to LoRA, PiSSA updates the principal components while freezing the \"residual\" parts, allowing faster convergence and enhanced performance. Comparative experiments of PiSSA and LoRA across 12 different models, ranging from 184M to 70B, encompassing 5 NLG and 8 NLU tasks, reveal that PiSSA consistently outperforms LoRA under identical experimental setups. On the GSM8K benchmark, Mistral-7B fine-tuned with PiSSA achieves an accuracy of 72.86%, surpassing LoRA's 67.7% by 5.16%. Due to the same architecture, PiSSA is also compatible with quantization to further reduce the memory requirement of fine-tuning. Compared to QLoRA, QPiSSA exhibits smaller quantization errors in the initial stages. Fine-tuning LLaMA-3-70B on GSM8K, QPiSSA attains an accuracy of 86.05%, exceeding the performances of QLoRA at 81.73%. Leveraging a fast SVD technique, PiSSA can be initialized in only a few seconds, presenting a negligible cost for transitioning from LoRA to PiSSA. Code is available at https://github.com/GraphPKU/PiSSA.",
        "timestamp": "2025-04-15T19:47:34.664Z",
        "rating": "novote",
        "publishedDate": "2024/04/03",
        "tags": [
          "cs.LG",
          "cs.AI"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2864,
        "object_id": "paper:arxiv.2404.02948",
        "created_at": "2025-04-15T19:47:35+00:00",
        "updated_at": "2025-04-15T19:49:31+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2302.07027": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2302.07027",
        "url": "https://arxiv.org/abs/2302.07027",
        "title": "AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models",
        "authors": "Chronopoulou, Alexandra, Peters, Matthew E., Fraser, Alexander, Dodge, Jesse",
        "abstract": "Pretrained language models (PLMs) are trained on massive corpora, but often need to specialize to specific domains. A parameter-efficient adaptation method suggests training an adapter for each domain on the task of language modeling. This leads to good in-domain scores but can be impractical for domain- or resource-restricted settings. A solution is to use a related-domain adapter for the novel domain at test time. In this paper, we introduce AdapterSoup, an approach that performs weight-space averaging of adapters trained on different domains. Our approach is embarrassingly parallel: first, we train a set of domain-specific adapters; then, for each novel domain, we determine which adapters should be averaged at test time. We present extensive experiments showing that AdapterSoup consistently improves performance to new domains without extra training. We also explore weight averaging of adapters trained on the same domain with different hyper-parameters, and show that it preserves the performance of a PLM on new domains while obtaining strong in-domain results. We explore various approaches for choosing which adapters to combine, such as text clustering and semantic similarity. We find that using clustering leads to the most competitive results on novel domains.",
        "timestamp": "2025-04-15T19:45:42.822Z",
        "rating": "novote",
        "publishedDate": "2023/02/14",
        "tags": [
          "cs.CL"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2859,
        "object_id": "paper:arxiv.2302.07027",
        "created_at": "2025-04-15T19:45:43+00:00",
        "updated_at": "2025-04-15T19:50:04+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2309.02033": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2309.02033",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T19:57:39.680Z",
            "data": {
              "session_id": "session_1744747058901_vwtdqvh",
              "source_id": "arxiv",
              "paper_id": "2309.02033",
              "start_time": "2025-04-15T19:57:30.976Z",
              "end_time": "2025-04-15T19:57:38.901Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2870,
        "object_id": "interactions:arxiv.2309.02033",
        "created_at": "2025-04-15T19:57:40+00:00",
        "updated_at": "2025-04-15T19:58:42+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2309.02033": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2309.02033",
        "url": "https://arxiv.org/abs/2309.02033",
        "title": "Data-Juicer: A One-Stop Data Processing System for Large Language Models",
        "authors": "Chen, Daoyuan, Huang, Yilun, Ma, Zhijian, Chen, Hesen, Pan, Xuchen, Ge, Ce, Gao, Dawei, Xie, Yuexiang, Liu, Zhaoyang, Gao, Jinyang, Li, Yaliang, Ding, Bolin, Zhou, Jingren",
        "abstract": "The immense evolution in Large Language Models (LLMs) has underscored the importance of massive, heterogeneous, and high-quality data. A data recipe is a mixture of data from different sources for training LLMs, which plays a vital role in LLMs' performance. Existing open-source tools for LLM data processing are mostly tailored for specific data recipes. To continuously uncover the potential of LLMs, incorporate data from new sources, and improve LLMs' performance, we build a new system named Data-Juicer, with which we can efficiently generate diverse data recipes, explore different possibilities in forming data mixtures, and evaluate their effects on model performance. Different from traditional data-analytics pipelines, Data-Juicer faces some unique challenges. Firstly, the possible data sources for forming data recipes are truly heterogeneous and massive with various qualities. Secondly, it is extremely expensive to precisely evaluate data recipes' impact on LLMs' performance. Thirdly, the end users of Data-Juicer, model developers, need sufficient flexibility to configure and evaluate different data recipes. Data-Juicer features a fine-grained abstraction of pipelines for constructing data recipes, with over 50 built-in operators for easy composition and extension. By incorporating visualization and auto-evaluation capabilities, Data-Juicer enables a timely feedback loop for both LLM pre-training and fine-tuning. Further, Data-Juicer is optimized and integrated with ecosystems for LLM training, evaluation, and distributed computing. The data recipes derived with Data-Juicer gain notable improvements on state-of-the-art LLMs, by up to 7.45% increase in averaged score across 16 LLM benchmarks and 17.5% higher win rate in pair-wise GPT-4 evaluations. Our system, data recipes, and tutorials are released, calling for broader data-centric research on training and understanding LLMs.",
        "timestamp": "2025-04-15T19:57:31.524Z",
        "rating": "novote",
        "publishedDate": "2023/09/05",
        "tags": [
          "cs.LG",
          "cs.DB",
          "cs.DC"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2869,
        "object_id": "paper:arxiv.2309.02033",
        "created_at": "2025-04-15T19:57:32+00:00",
        "updated_at": "2025-04-15T19:59:32+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2401.00625": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2401.00625",
        "url": "https://arxiv.org/abs/2401.00625",
        "title": "Beyond Efficiency: A Systematic Survey of Resource-Efficient Large Language Models",
        "authors": "Bai, Guangji, Chai, Zheng, Ling, Chen, Wang, Shiyu, Lu, Jiaying, Zhang, Nan, Shi, Tingwei, Yu, Ziyang, Zhu, Mengdan, Zhang, Yifei, Song, Xinyuan, Yang, Carl, Cheng, Yue, Zhao, Liang",
        "abstract": "The burgeoning field of Large Language Models (LLMs), exemplified by sophisticated models like OpenAI's ChatGPT, represents a significant advancement in artificial intelligence. These models, however, bring forth substantial challenges in the high consumption of computational, memory, energy, and financial resources, especially in environments with limited resource capabilities. This survey aims to systematically address these challenges by reviewing a broad spectrum of techniques designed to enhance the resource efficiency of LLMs. We categorize methods based on their optimization focus: computational, memory, energy, financial, and network resources and their applicability across various stages of an LLM's lifecycle, including architecture design, pretraining, finetuning, and system design. Additionally, the survey introduces a nuanced categorization of resource efficiency techniques by their specific resource types, which uncovers the intricate relationships and mappings between various resources and corresponding optimization techniques. A standardized set of evaluation metrics and datasets is also presented to facilitate consistent and fair comparisons across different models and techniques. By offering a comprehensive overview of the current sota and identifying open research avenues, this survey serves as a foundational reference for researchers and practitioners, aiding them in developing more sustainable and efficient LLMs in a rapidly evolving landscape.",
        "timestamp": "2025-04-15T19:50:02.053Z",
        "rating": "novote",
        "publishedDate": "2024/01/01",
        "tags": [
          "cs.LG"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2866,
        "object_id": "paper:arxiv.2401.00625",
        "created_at": "2025-04-15T19:50:02+00:00",
        "updated_at": "2025-04-15T19:52:52+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2305.06677": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2305.06677",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T20:00:35.556Z",
            "data": {
              "session_id": "session_1744747234714_0g8ctk6",
              "source_id": "arxiv",
              "paper_id": "2305.06677",
              "start_time": "2025-04-15T20:00:27.617Z",
              "end_time": "2025-04-15T20:00:34.714Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2872,
        "object_id": "interactions:arxiv.2305.06677",
        "created_at": "2025-04-15T20:00:36+00:00",
        "updated_at": "2025-04-15T20:01:43+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2305.06677": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2305.06677",
        "url": "https://arxiv.org/abs/2305.06677",
        "title": "INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Language Models",
        "authors": "Renduchintala, H S V N S Kowndinya, Killamsetty, Krishnateja, Bhatia, Sumit, Aggarwal, Milan, Ramakrishnan, Ganesh, Iyer, Rishabh, Krishnamurthy, Balaji",
        "abstract": "A salient characteristic of pre-trained language models (PTLMs) is a remarkable improvement in their generalization capability and emergence of new capabilities with increasing model capacity and pre-training dataset size. Consequently, we are witnessing the development of enormous models pushing the state-of-the-art. It is, however, imperative to realize that this inevitably leads to prohibitively long training times, extortionate computing costs, and a detrimental environmental impact. Significant efforts are underway to make PTLM training more efficient through innovations in model architectures, training pipelines, and loss function design, with scant attention being paid to optimizing the utility of training data. The key question that we ask is whether it is possible to train PTLMs by employing only highly informative subsets of the training data while maintaining downstream performance? Building upon the recent progress in informative data subset selection, we show how we can employ submodular optimization to select highly representative subsets of the training corpora and demonstrate that the proposed framework can be applied to efficiently train multiple PTLMs (BERT, BioBERT, GPT-2) using only a fraction of data. Further, we perform a rigorous empirical evaluation to show that the resulting models achieve up to $\\sim99\\%$ of the performance of the fully-trained models. We made our framework publicly available at https://github.com/Efficient-AI/ingenious.",
        "timestamp": "2025-04-15T20:00:28.024Z",
        "rating": "novote",
        "publishedDate": "2023/05/11",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2871,
        "object_id": "paper:arxiv.2305.06677",
        "created_at": "2025-04-15T20:00:28+00:00",
        "updated_at": "2025-04-15T20:02:56+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2206.14486": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2206.14486",
        "url": "https://arxiv.org/abs/2206.14486",
        "title": "Beyond neural scaling laws: beating power law scaling via data pruning",
        "authors": "Sorscher, Ben, Geirhos, Robert, Shekhar, Shashank, Ganguli, Surya, Morcos, Ari S.",
        "abstract": "Widely observed neural scaling laws, in which error falls off as a power of the training set size, model size, or both, have driven substantial performance improvements in deep learning. However, these improvements through scaling alone require considerable costs in compute and energy. Here we focus on the scaling of error with dataset size and show how in theory we can break beyond power law scaling and potentially even reduce it to exponential scaling instead if we have access to a high-quality data pruning metric that ranks the order in which training examples should be discarded to achieve any pruned dataset size. We then test this improved scaling prediction with pruned dataset size empirically, and indeed observe better than power law scaling in practice on ResNets trained on CIFAR-10, SVHN, and ImageNet. Next, given the importance of finding high-quality pruning metrics, we perform the first large-scale benchmarking study of ten different data pruning metrics on ImageNet. We find most existing high performing metrics scale poorly to ImageNet, while the best are computationally intensive and require labels for every image. We therefore developed a new simple, cheap and scalable self-supervised pruning metric that demonstrates comparable performance to the best supervised metrics. Overall, our work suggests that the discovery of good data-pruning metrics may provide a viable path forward to substantially improved neural scaling laws, thereby reducing the resource costs of modern deep learning.",
        "timestamp": "2025-04-15T20:03:16.131Z",
        "rating": "novote",
        "publishedDate": "2022/06/29",
        "tags": [
          "cs.LG",
          "cs.AI",
          "cs.CV",
          "stat.ML"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2874,
        "object_id": "paper:arxiv.2206.14486",
        "created_at": "2025-04-15T20:03:16+00:00",
        "updated_at": "2025-04-15T20:05:07+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2306.03109": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2306.03109",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T20:05:43.038Z",
            "data": {
              "session_id": "session_1744747542370_ixlcgyq",
              "source_id": "arxiv",
              "paper_id": "2306.03109",
              "start_time": "2025-04-15T20:03:20.700Z",
              "end_time": "2025-04-15T20:05:42.370Z",
              "heartbeat_count": 28,
              "duration_seconds": 140,
              "idle_seconds": 2,
              "total_elapsed_seconds": 142
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2875,
        "object_id": "interactions:arxiv.2306.03109",
        "created_at": "2025-04-15T20:05:43+00:00",
        "updated_at": "2025-04-15T20:06:42+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2306.03109": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2306.03109",
        "url": "https://arxiv.org/abs/2306.03109",
        "title": "Machine Learning Force Fields with Data Cost Aware Training",
        "authors": "Bukharin, Alexander, Liu, Tianyi, Wang, Shengjie, Zuo, Simiao, Gao, Weihao, Yan, Wen, Zhao, Tuo",
        "abstract": "Machine learning force fields (MLFF) have been proposed to accelerate molecular dynamics (MD) simulation, which finds widespread applications in chemistry and biomedical research. Even for the most data-efficient MLFFs, reaching chemical accuracy can require hundreds of frames of force and energy labels generated by expensive quantum mechanical algorithms, which may scale as $O(n^3)$ to $O(n^7)$, with $n$ proportional to the number of basis functions. To address this issue, we propose a multi-stage computational framework -- ASTEROID, which lowers the data cost of MLFFs by leveraging a combination of cheap inaccurate data and expensive accurate data. The motivation behind ASTEROID is that inaccurate data, though incurring large bias, can help capture the sophisticated structures of the underlying force field. Therefore, we first train a MLFF model on a large amount of inaccurate training data, employing a bias-aware loss function to prevent the model from overfitting tahe potential bias of this data. We then fine-tune the obtained model using a small amount of accurate training data, which preserves the knowledge learned from the inaccurate training data while significantly improving the model's accuracy. Moreover, we propose a variant of ASTEROID based on score matching for the setting where the inaccurate training data are unlabeled. Extensive experiments on MD datasets and downstream tasks validate the efficacy of ASTEROID. Our code and data are available at https://github.com/abukharin3/asteroid.",
        "timestamp": "2025-04-15T20:03:09.822Z",
        "rating": "novote",
        "publishedDate": "2023/06/05",
        "tags": [
          "q-bio.QM",
          "cs.LG",
          "physics.chem-ph"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2873,
        "object_id": "paper:arxiv.2306.03109",
        "created_at": "2025-04-15T20:03:10+00:00",
        "updated_at": "2025-04-15T20:05:43+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2106.08903": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2106.08903",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T20:08:12.680Z",
            "data": {
              "session_id": "session_1744747691835_o9oowce",
              "source_id": "arxiv",
              "paper_id": "2106.08903",
              "start_time": "2025-04-15T20:07:10.730Z",
              "end_time": "2025-04-15T20:08:11.835Z",
              "heartbeat_count": 12,
              "duration_seconds": 60,
              "idle_seconds": 1,
              "total_elapsed_seconds": 61
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2877,
        "object_id": "interactions:arxiv.2106.08903",
        "created_at": "2025-04-15T20:08:13+00:00",
        "updated_at": "2025-04-15T20:09:16+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2106.08903": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2106.08903",
        "url": "https://arxiv.org/abs/2106.08903",
        "title": "GemNet: Universal Directional Graph Neural Networks for Molecules",
        "authors": "Gasteiger, Johannes, Becker, Florian, G\u00fcnnemann, Stephan",
        "abstract": "Effectively predicting molecular interactions has the potential to accelerate molecular dynamics by multiple orders of magnitude and thus revolutionize chemical simulations. Graph neural networks (GNNs) have recently shown great successes for this task, overtaking classical methods based on fixed molecular kernels. However, they still appear very limited from a theoretical perspective, since regular GNNs cannot distinguish certain types of graphs. In this work we close this gap between theory and practice. We show that GNNs with spherical representations are indeed universal approximators for predictions that are invariant to translation, and equivariant to permutation and rotation. We then discretize such GNNs via directed edge embeddings and two-hop message passing, and incorporate multiple structural improvements to arrive at the geometric message passing neural network (GemNet). We demonstrate the benefits of the proposed changes in multiple ablation studies. GemNet outperforms previous models on the COLL, MD17, and OC20 datasets by 34%, 41%, and 20%, respectively, and performs especially well on the most challenging molecules. Our implementation is available online.",
        "timestamp": "2025-04-15T20:07:11.275Z",
        "rating": "novote",
        "publishedDate": "2021/06/02",
        "tags": [
          "physics.comp-ph",
          "cs.LG",
          "physics.chem-ph",
          "stat.ML"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2876,
        "object_id": "paper:arxiv.2106.08903",
        "created_at": "2025-04-15T20:07:11+00:00",
        "updated_at": "2025-04-15T20:09:09+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2206.14486": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2206.14486",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T20:28:30.242Z",
            "data": {
              "session_id": "session_1744748910225_c5n1kkh",
              "source_id": "arxiv",
              "paper_id": "2206.14486",
              "start_time": "2025-04-15T20:28:23.523Z",
              "end_time": "2025-04-15T20:28:30.225Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2878,
        "object_id": "interactions:arxiv.2206.14486",
        "created_at": "2025-04-15T20:28:31+00:00",
        "updated_at": "2025-04-15T20:29:36+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2404.04125": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.04125",
        "url": "https://arxiv.org/abs/2404.04125",
        "title": "No \"Zero-Shot\" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance",
        "authors": "Udandarao, Vishaal, Prabhu, Ameya, Ghosh, Adhiraj, Sharma, Yash, Torr, Philip H. S., Bibi, Adel, Albanie, Samuel, Bethge, Matthias",
        "abstract": "Web-crawled pretraining datasets underlie the impressive \"zero-shot\" evaluation performance of multimodal models, such as CLIP for classification/retrieval and Stable-Diffusion for image generation. However, it is unclear how meaningful the notion of \"zero-shot\" generalization is for such multimodal models, as it is not known to what extent their pretraining datasets encompass the downstream concepts targeted for during \"zero-shot\" evaluation. In this work, we ask: How is the performance of multimodal models on downstream concepts influenced by the frequency of these concepts in their pretraining datasets? We comprehensively investigate this question across 34 models and five standard pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M, LAION-Aesthetics), generating over 300GB of data artifacts. We consistently find that, far from exhibiting \"zero-shot\" generalization, multimodal models require exponentially more data to achieve linear improvements in downstream \"zero-shot\" performance, following a sample inefficient log-linear scaling trend. This trend persists even when controlling for sample-level similarity between pretraining and downstream datasets, and testing on purely synthetic data distributions. Furthermore, upon benchmarking models on long-tailed data sampled based on our analysis, we demonstrate that multimodal models across the board perform poorly. We contribute this long-tail test set as the \"Let it Wag!\" benchmark to further research in this direction. Taken together, our study reveals an exponential need for training data which implies that the key to \"zero-shot\" generalization capabilities under large-scale training paradigms remains to be found.",
        "timestamp": "2025-04-15T20:31:21.472Z",
        "rating": "novote",
        "publishedDate": "2024/04/04",
        "tags": [
          "cs.CV",
          "cs.CL",
          "cs.LG"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2880,
        "object_id": "paper:arxiv.2404.04125",
        "created_at": "2025-04-15T20:31:22+00:00",
        "updated_at": "2025-04-15T20:32:27+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2310.05773": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2310.05773",
        "url": "https://arxiv.org/abs/2310.05773",
        "title": "Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory Matching",
        "authors": "Guo, Ziyao, Wang, Kai, Cazenavette, George, Li, Hui, Zhang, Kaipeng, You, Yang",
        "abstract": "The ultimate goal of Dataset Distillation is to synthesize a small synthetic dataset such that a model trained on this synthetic set will perform equally well as a model trained on the full, real dataset. Until now, no method of Dataset Distillation has reached this completely lossless goal, in part due to the fact that previous methods only remain effective when the total number of synthetic samples is extremely small. Since only so much information can be contained in such a small number of samples, it seems that to achieve truly loss dataset distillation, we must develop a distillation method that remains effective as the size of the synthetic dataset grows. In this work, we present such an algorithm and elucidate why existing methods fail to generate larger, high-quality synthetic sets. Current state-of-the-art methods rely on trajectory-matching, or optimizing the synthetic data to induce similar long-term training dynamics as the real data. We empirically find that the training stage of the trajectories we choose to match (i.e., early or late) greatly affects the effectiveness of the distilled dataset. Specifically, early trajectories (where the teacher network learns easy patterns) work well for a low-cardinality synthetic set since there are fewer examples wherein to distribute the necessary information. Conversely, late trajectories (where the teacher network learns hard patterns) provide better signals for larger synthetic sets since there are now enough samples to represent the necessary complex patterns. Based on our findings, we propose to align the difficulty of the generated patterns with the size of the synthetic dataset. In doing so, we successfully scale trajectory matching-based methods to larger synthetic datasets, achieving lossless dataset distillation for the very first time. Code and distilled datasets are available at https://gzyaftermath.github.io/DATM.",
        "timestamp": "2025-04-15T20:30:31.030Z",
        "rating": "novote",
        "publishedDate": "2023/10/09",
        "tags": [
          "cs.CV"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2879,
        "object_id": "paper:arxiv.2310.05773",
        "created_at": "2025-04-15T20:30:31+00:00",
        "updated_at": "2025-04-15T20:33:02+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2407.06483": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.06483",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T20:36:38.943Z",
            "data": {
              "session_id": "session_1744749398057_rgy7qsv",
              "source_id": "arxiv",
              "paper_id": "2407.06483",
              "start_time": "2025-04-15T20:35:47.095Z",
              "end_time": "2025-04-15T20:36:38.057Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 1,
              "total_elapsed_seconds": 51
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2882,
        "object_id": "interactions:arxiv.2407.06483",
        "created_at": "2025-04-15T20:36:39+00:00",
        "updated_at": "2025-04-15T20:37:38+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2407.06483": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.06483",
        "url": "https://arxiv.org/abs/2407.06483",
        "title": "Composable Interventions for Language Models",
        "authors": "Kolbeinsson, Arinbjorn, O'Brien, Kyle, Huang, Tianjin, Gao, Shanghua, Liu, Shiwei, Schwarz, Jonathan Richard, Vaidya, Anurag, Mahmood, Faisal, Zitnik, Marinka, Chen, Tianlong, Hartvigsen, Thomas",
        "abstract": "Test-time interventions for language models can enhance factual accuracy, mitigate harmful outputs, and improve model efficiency without costly retraining. But despite a flood of new methods, different types of interventions are largely developing independently. In practice, multiple interventions must be applied sequentially to the same model, yet we lack standardized ways to study how interventions interact. We fill this gap by introducing composable interventions, a framework to study the effects of using multiple interventions on the same language models, featuring new metrics and a unified codebase. Using our framework, we conduct extensive experiments and compose popular methods from three emerging intervention categories -- Knowledge Editing, Model Compression, and Machine Unlearning. Our results from 310 different compositions uncover meaningful interactions: compression hinders editing and unlearning, composing interventions hinges on their order of application, and popular general-purpose metrics are inadequate for assessing composability. Taken together, our findings showcase clear gaps in composability, suggesting a need for new multi-objective interventions. All of our code is public: https://github.com/hartvigsen-group/composable-interventions.",
        "timestamp": "2025-04-15T20:35:47.687Z",
        "rating": "novote",
        "publishedDate": "2024/07/09",
        "tags": [
          "cs.LG",
          "cs.CL"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2881,
        "object_id": "paper:arxiv.2407.06483",
        "created_at": "2025-04-15T20:35:48+00:00",
        "updated_at": "2025-04-15T20:37:45+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2305.16264": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2305.16264",
        "url": "https://arxiv.org/abs/2305.16264",
        "title": "Scaling Data-Constrained Language Models",
        "authors": "Muennighoff, Niklas, Rush, Alexander M., Barak, Boaz, Scao, Teven Le, Piktus, Aleksandra, Tazi, Nouamane, Pyysalo, Sampo, Wolf, Thomas, Raffel, Colin",
        "abstract": "The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at https://github.com/huggingface/datablations.",
        "timestamp": "2025-04-15T20:38:54.898Z",
        "rating": "novote",
        "publishedDate": "2023/05/25",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2884,
        "object_id": "paper:arxiv.2305.16264",
        "created_at": "2025-04-15T20:38:55+00:00",
        "updated_at": "2025-04-15T20:40:36+00:00",
        "version": 1
      }
    },
    "paper:url.465B2971": {
      "data": {
        "sourceId": "url",
        "paperId": "465B2971",
        "url": "https://openreview.net/forum?id=9VbGjXLzig",
        "title": "No \"Zero-Shot\" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance",
        "authors": "Vishaal Udandarao, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip Torr, Adel Bibi, Samuel Albanie, Matthias Bethge",
        "abstract": "Web-crawled pretraining datasets underlie the impressive \"zero-shot\" evaluation performance of multimodal models, such as CLIP for classification and Stable-Diffusion for image generation. However, it is unclear how meaningful the notion of \"zero-shot\" generalization is for such multimodal models, as it is not known to what extent their pretraining datasets encompass the downstream concepts targeted for during \"zero-shot\" evaluation. In this work, we ask: How is the performance of multimodal models on downstream concepts influenced by the frequency of these concepts in their pretraining datasets?\n\nWe comprehensively investigate this question across 34 models and 5 standard pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M, LAION-Aesthetics), generating over 300GB of data artifacts. We consistently find that, far from exhibiting \"zero-shot\" generalization, multimodal models require exponentially more data to achieve linear improvements in downstream \"zero-shot\" performance, following a sample inefficient log-linear scaling trend. This trend persists even when controlling for sample-level similarity between pretraining and downstream datasets, and testing on purely synthetic data distributions. Furthermore, upon benchmarking models on long-tailed data sampled based on our analysis, we demonstrate that multimodal models across the board perform poorly. We contribute this long-tail test set as the Let it Wag! benchmark to further research in this direction. Taken together, our study reveals an exponential need for training data which implies that the key to \"zero-shot\" generalization capabilities under large-scale training data and compute paradigms remains to be found.",
        "timestamp": "2025-04-15T20:38:26.677Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2883,
        "object_id": "paper:url.465B2971",
        "created_at": "2025-04-15T20:38:27+00:00",
        "updated_at": "2025-04-15T20:41:05+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2305.16264": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2305.16264",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-15T20:44:48.226Z",
            "data": {
              "session_id": "session_1744749887680_0hw682s",
              "source_id": "arxiv",
              "paper_id": "2305.16264",
              "start_time": "2025-04-15T20:43:48.819Z",
              "end_time": "2025-04-15T20:44:47.680Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 4,
              "total_elapsed_seconds": 59
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-16T04:42:13.524Z",
            "data": {
              "session_id": "session_1744778533298_jwkfthw",
              "source_id": "arxiv",
              "paper_id": "2305.16264",
              "start_time": "2025-04-16T04:42:03.992Z",
              "end_time": "2025-04-16T04:42:13.298Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2885,
        "object_id": "interactions:arxiv.2305.16264",
        "created_at": "2025-04-15T20:44:49+00:00",
        "updated_at": "2025-04-16T04:43:26+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.09381": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.09381",
        "url": "https://arxiv.org/abs/2504.09381",
        "title": "DiTSE: High-Fidelity Generative Speech Enhancement via Latent Diffusion Transformers",
        "authors": "Guimar\u00e3es, Heitor R., Su, Jiaqi, Kumar, Rithesh, Falk, Tiago H., Jin, Zeyu",
        "abstract": "Real-world speech recordings suffer from degradations such as background noise and reverberation. Speech enhancement aims to mitigate these issues by generating clean high-fidelity signals. While recent generative approaches for speech enhancement have shown promising results, they still face two major challenges: (1) content hallucination, where plausible phonemes generated differ from the original utterance; and (2) inconsistency, failing to preserve speaker's identity and paralinguistic features from the input speech. In this work, we introduce DiTSE (Diffusion Transformer for Speech Enhancement), which addresses quality issues of degraded speech in full bandwidth. Our approach employs a latent diffusion transformer model together with robust conditioning features, effectively addressing these challenges while remaining computationally efficient. Experimental results from both subjective and objective evaluations demonstrate that DiTSE achieves state-of-the-art audio quality that, for the first time, matches real studio-quality audio from the DAPS dataset. Furthermore, DiTSE significantly improves the preservation of speaker identity and content fidelity, reducing hallucinations across datasets compared to state-of-the-art enhancers. Audio samples are available at: http://hguimaraes.me/DiTSE",
        "timestamp": "2025-04-16T01:03:04.843Z",
        "rating": "novote",
        "publishedDate": "2025/04/13",
        "tags": [
          "eess.AS",
          "cs.SD"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2890,
        "object_id": "paper:arxiv.2504.09381",
        "created_at": "2025-04-16T01:03:05+00:00",
        "updated_at": "2025-04-16T01:05:04+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2409.14623": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2409.14623",
        "url": "https://arxiv.org/abs/2409.14623",
        "title": "From Lazy to Rich: Exact Learning Dynamics in Deep Linear Networks",
        "authors": "Domin\u00e9, Cl\u00e9mentine C. J., Anguita, Nicolas, Proca, Alexandra M., Braun, Lukas, Kunin, Daniel, Mediano, Pedro A. M., Saxe, Andrew M.",
        "abstract": "Biological and artificial neural networks develop internal representations that enable them to perform complex tasks. In artificial networks, the effectiveness of these models relies on their ability to build task specific representation, a process influenced by interactions among datasets, architectures, initialization strategies, and optimization algorithms. Prior studies highlight that different initializations can place networks in either a lazy regime, where representations remain static, or a rich/feature learning regime, where representations evolve dynamically. Here, we examine how initialization influences learning dynamics in deep linear neural networks, deriving exact solutions for lambda-balanced initializations-defined by the relative scale of weights across layers. These solutions capture the evolution of representations and the Neural Tangent Kernel across the spectrum from the rich to the lazy regimes. Our findings deepen the theoretical understanding of the impact of weight initialization on learning regimes, with implications for continual learning, reversal learning, and transfer learning, relevant to both neuroscience and practical applications.",
        "timestamp": "2025-04-15T23:17:28.384Z",
        "rating": "novote",
        "publishedDate": "2024/09/22",
        "tags": [
          "cs.LG"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2887,
        "object_id": "paper:arxiv.2409.14623",
        "created_at": "2025-04-15T23:17:28+00:00",
        "updated_at": "2025-04-15T23:22:23+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2406.06158": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.06158",
        "url": "https://arxiv.org/abs/2406.06158",
        "title": "Get rich quick: exact solutions reveal how unbalanced initializations promote rapid feature learning",
        "authors": "Kunin, Daniel, Ravent\u00f3s, Allan, Domin\u00e9, Cl\u00e9mentine, Chen, Feng, Klindt, David, Saxe, Andrew, Ganguli, Surya",
        "abstract": "While the impressive performance of modern neural networks is often attributed to their capacity to efficiently extract task-relevant features from data, the mechanisms underlying this rich feature learning regime remain elusive, with much of our theoretical understanding stemming from the opposing lazy regime. In this work, we derive exact solutions to a minimal model that transitions between lazy and rich learning, precisely elucidating how unbalanced layer-specific initialization variances and learning rates determine the degree of feature learning. Our analysis reveals that they conspire to influence the learning regime through a set of conserved quantities that constrain and modify the geometry of learning trajectories in parameter and function space. We extend our analysis to more complex linear models with multiple neurons, outputs, and layers and to shallow nonlinear networks with piecewise linear activation functions. In linear networks, rapid feature learning only occurs from balanced initializations, where all layers learn at similar speeds. While in nonlinear networks, unbalanced initializations that promote faster learning in earlier layers can accelerate rich learning. Through a series of experiments, we provide evidence that this unbalanced rich regime drives feature learning in deep finite-width networks, promotes interpretability of early layers in CNNs, reduces the sample complexity of learning hierarchical data, and decreases the time to grokking in modular arithmetic. Our theory motivates further exploration of unbalanced initializations to enhance efficient feature learning.",
        "timestamp": "2025-04-15T23:13:31.380Z",
        "rating": "novote",
        "publishedDate": "2024/06/10",
        "tags": [
          "cs.LG",
          "cs.AI",
          "stat.ML"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2886,
        "object_id": "paper:arxiv.2406.06158",
        "created_at": "2025-04-15T23:13:31+00:00",
        "updated_at": "2025-04-15T23:22:02+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.09381": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.09381",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-16T02:11:39.008Z",
            "data": {
              "session_id": "session_1744769498413_imdk1pu",
              "source_id": "arxiv",
              "paper_id": "2504.09381",
              "start_time": "2025-04-16T02:11:30.494Z",
              "end_time": "2025-04-16T02:11:38.413Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-16T02:13:45.530Z",
            "data": {
              "session_id": "session_1744769625482_wvx38ff",
              "source_id": "arxiv",
              "paper_id": "2504.09381",
              "start_time": "2025-04-16T02:13:38.069Z",
              "end_time": "2025-04-16T02:13:45.482Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2891,
        "object_id": "interactions:arxiv.2504.09381",
        "created_at": "2025-04-16T02:11:40+00:00",
        "updated_at": "2025-04-16T02:14:31+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.10281": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.10281",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-16T02:15:12.732Z",
            "data": {
              "session_id": "session_1744769711780_phpbhh2",
              "source_id": "arxiv",
              "paper_id": "2504.10281",
              "start_time": "2025-04-16T02:13:45.513Z",
              "end_time": "2025-04-16T02:15:11.780Z",
              "heartbeat_count": 17,
              "duration_seconds": 85,
              "idle_seconds": 1,
              "total_elapsed_seconds": 86
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2893,
        "object_id": "interactions:arxiv.2504.10281",
        "created_at": "2025-04-16T02:15:13+00:00",
        "updated_at": "2025-04-16T02:16:26+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.10281": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.10281",
        "url": "https://arxiv.org/abs/2504.10281",
        "title": "Zero-shot Autonomous Microscopy for Scalable and Intelligent Characterization of 2D Materials",
        "authors": "Yang, Jingyun, Yin, Ruoyan Avery, Jiang, Chi, Hu, Yuepeng, Zhu, Xiaokai, Hu, Xingjian, Kumar, Sutharsika, Wang, Xiao, Zhai, Xiaohua, Rong, Keran, Zhu, Yunyue, Zhang, Tianyi, Yin, Zongyou, Kong, Jing, Gong, Neil Zhenqiang, Ren, Zhichu, Wang, Haozhe",
        "abstract": "Characterization of atomic-scale materials traditionally requires human experts with months to years of specialized training. Even for trained human operators, accurate and reliable characterization remains challenging when examining newly discovered materials such as two-dimensional (2D) structures. This bottleneck drives demand for fully autonomous experimentation systems capable of comprehending research objectives without requiring large training datasets. In this work, we present ATOMIC (Autonomous Technology for Optical Microscopy & Intelligent Characterization), an end-to-end framework that integrates foundation models to enable fully autonomous, zero-shot characterization of 2D materials. Our system integrates the vision foundation model (i.e., Segment Anything Model), large language models (i.e., ChatGPT), unsupervised clustering, and topological analysis to automate microscope control, sample scanning, image segmentation, and intelligent analysis through prompt engineering, eliminating the need for additional training. When analyzing typical MoS2 samples, our approach achieves 99.7% segmentation accuracy for single layer identification, which is equivalent to that of human experts. In addition, the integrated model is able to detect grain boundary slits that are challenging to identify with human eyes. Furthermore, the system retains robust accuracy despite variable conditions including defocus, color temperature fluctuations, and exposure variations. It is applicable to a broad spectrum of common 2D materials-including graphene, MoS2, WSe2, SnSe-regardless of whether they were fabricated via chemical vapor deposition or mechanical exfoliation. This work represents the implementation of foundation models to achieve autonomous analysis, establishing a scalable and data-efficient characterization paradigm that fundamentally transforms the approach to nanoscale materials research.",
        "timestamp": "2025-04-16T02:13:45.887Z",
        "rating": "novote",
        "publishedDate": "2025/04/14",
        "tags": [
          "cond-mat.mtrl-sci",
          "cond-mat.mes-hall",
          "cs.AI",
          "cs.CV",
          "cs.LG"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2892,
        "object_id": "paper:arxiv.2504.10281",
        "created_at": "2025-04-16T02:13:46+00:00",
        "updated_at": "2025-04-16T02:16:19+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.08637": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.08637",
        "url": "https://arxiv.org/abs/2504.08637",
        "title": "Simple low-dimensional computations explain variability in neuronal activity",
        "authors": "Lynn, Christopher W.",
        "abstract": "Our understanding of neural computation is founded on the assumption that neurons fire in response to a linear summation of inputs. Yet experiments demonstrate that some neurons are capable of complex computations that require interactions between inputs. Here we show, across multiple brain regions and species, that simple computations (without interactions between inputs) explain most of the variability in neuronal activity. Neurons are quantitatively described by models that capture the measured dependence on each input individually, but assume nothing about combinations of inputs. These minimal models, which are equivalent to binary artificial neurons, predict complex higher-order dependencies and recover known features of synaptic connectivity. The inferred computations are low-dimensional, indicating a highly redundant neural code that is necessary for error correction. These results suggest that, despite intricate biophysical details, most neurons perform simple computations typically reserved for artificial models.",
        "timestamp": "2025-04-16T02:18:28.026Z",
        "rating": "novote",
        "publishedDate": "2025/04/11",
        "tags": [
          "physics.bio-ph",
          "q-bio.NC"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2894,
        "object_id": "paper:arxiv.2504.08637",
        "created_at": "2025-04-16T02:18:28+00:00",
        "updated_at": "2025-04-16T02:20:37+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.08637": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.08637",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-16T02:43:28.472Z",
            "data": {
              "session_id": "session_1744771408084_5jj31i6",
              "source_id": "arxiv",
              "paper_id": "2504.08637",
              "start_time": "2025-04-16T02:42:48.920Z",
              "end_time": "2025-04-16T02:43:28.084Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 4,
              "total_elapsed_seconds": 39
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2895,
        "object_id": "interactions:arxiv.2504.08637",
        "created_at": "2025-04-16T02:43:29+00:00",
        "updated_at": "2025-04-16T02:44:29+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.1909.04101": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1909.04101",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-16T02:45:03.435Z",
            "data": {
              "session_id": "session_1744771502619_2o2h8rv",
              "source_id": "arxiv",
              "paper_id": "1909.04101",
              "start_time": "2025-04-16T02:44:55.200Z",
              "end_time": "2025-04-16T02:45:02.619Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2897,
        "object_id": "interactions:arxiv.1909.04101",
        "created_at": "2025-04-16T02:45:04+00:00",
        "updated_at": "2025-04-16T02:46:01+00:00",
        "version": 1
      }
    },
    "paper:arxiv.1909.04101": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1909.04101",
        "url": "https://arxiv.org/abs/1909.04101",
        "title": "Neural Naturalist: Generating Fine-Grained Image Comparisons",
        "authors": "Forbes, Maxwell, Kaeser-Chen, Christine, Sharma, Piyush, Belongie, Serge",
        "abstract": "We introduce the new Birds-to-Words dataset of 41k sentences describing fine-grained differences between photographs of birds. The language collected is highly detailed, while remaining understandable to the everyday observer (e.g., \"heart-shaped face,\" \"squat body\"). Paragraph-length descriptions naturally adapt to varying levels of taxonomic and visual distance---drawn from a novel stratified sampling approach---with the appropriate level of detail. We propose a new model called Neural Naturalist that uses a joint image encoding and comparative module to generate comparative language, and evaluate the results with humans who must use the descriptions to distinguish real images. Our results indicate promising potential for neural models to explain differences in visual embedding space using natural language, as well as a concrete path for machine learning to aid citizen scientists in their effort to preserve biodiversity.",
        "timestamp": "2025-04-16T02:44:55.741Z",
        "rating": "novote",
        "publishedDate": "2019/09/09",
        "tags": [
          "cs.CL",
          "cs.CV"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2896,
        "object_id": "paper:arxiv.1909.04101",
        "created_at": "2025-04-16T02:44:56+00:00",
        "updated_at": "2025-04-16T02:47:35+00:00",
        "version": 1
      }
    },
    "interactions:url.63382234": {
      "data": {
        "sourceId": "url",
        "paperId": "63382234",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-16T03:16:44.815Z",
            "data": {
              "session_id": "session_1744773403995_dws3wa2",
              "source_id": "url",
              "paper_id": "63382234",
              "start_time": "2025-04-16T03:15:01.101Z",
              "end_time": "2025-04-16T03:16:43.995Z",
              "heartbeat_count": 20,
              "duration_seconds": 100,
              "idle_seconds": 3,
              "total_elapsed_seconds": 103
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2899,
        "object_id": "interactions:url.63382234",
        "created_at": "2025-04-16T03:16:45+00:00",
        "updated_at": "2025-04-16T03:17:45+00:00",
        "version": 1
      }
    },
    "paper:url.63382234": {
      "data": {
        "sourceId": "url",
        "paperId": "63382234",
        "url": "https://openreview.net/forum?id=EoiuRII7MQ",
        "title": "Lower Ricci Curvature for Efficient Community Detection",
        "authors": "Yun Jin Park, Didong Li",
        "abstract": "This study introduces the Lower Ricci Curvature (LRC), a novel, scalable, and scale-free discrete curvature designed to enhance community detection in networks. Addressing the computational challenges posed by existing curvature-based methods, LRC offers a streamlined approach with linear computational complexity, which makes it well suited for large-scale network analysis. We further develop an LRC-based preprocessing method that effectively augments popular community detection algorithms. Through applications on multiple real-world datasets, including the NCAA football league network, the DBLP collaboration network, the Amazon product co-purchasing network, and the YouTube social network, we demonstrate the efficacy of our method in significantly improving the performance of various community detection algorithms.",
        "timestamp": "2025-04-16T03:14:57.622Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "Transactions on Machine Learning Research",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2898,
        "object_id": "paper:url.63382234",
        "created_at": "2025-04-16T03:14:58+00:00",
        "updated_at": "2025-04-16T03:16:41+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.1603.00386": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1603.00386",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-16T03:18:54.161Z",
            "data": {
              "session_id": "session_1744773533333_wvrfp0s",
              "source_id": "arxiv",
              "paper_id": "1603.00386",
              "start_time": "2025-04-16T03:18:42.110Z",
              "end_time": "2025-04-16T03:18:53.333Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2902,
        "object_id": "interactions:arxiv.1603.00386",
        "created_at": "2025-04-16T03:18:55+00:00",
        "updated_at": "2025-04-16T03:19:56+00:00",
        "version": 1
      }
    },
    "paper:url.51DBBD4F": {
      "data": {
        "sourceId": "url",
        "paperId": "51DBBD4F",
        "url": "https://openreview.net/forum?id=7UmjRGzp-A",
        "title": "Understanding over-squashing and bottlenecks on graphs via curvature",
        "authors": "Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, Michael M. Bronstein",
        "abstract": "Most graph neural networks (GNNs) use the message passing paradigm, in which node features are propagated on the input graph. Recent works pointed to the distortion of information flowing from distant nodes as a factor limiting the efficiency of message passing for tasks relying on long-distance interactions. This phenomenon, referred to as 'over-squashing', has been heuristically attributed to graph bottlenecks where the number of $k$-hop neighbors grows rapidly with $k$. We provide a precise description of the over-squashing phenomenon in GNNs and analyze how it arises from bottlenecks in the graph. For this purpose, we introduce a new edge-based combinatorial curvature and prove that negatively curved edges are responsible for the over-squashing issue. We also propose and experimentally test a  curvature-based graph rewiring method to alleviate the over-squashing.",
        "timestamp": "2025-04-16T03:24:27.635Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2903,
        "object_id": "paper:url.51DBBD4F",
        "created_at": "2025-04-16T03:24:28+00:00",
        "updated_at": "2025-04-16T03:26:17+00:00",
        "version": 1
      }
    },
    "paper:arxiv.1603.00386": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1603.00386",
        "url": "https://arxiv.org/abs/1603.00386",
        "title": "Forman curvature for complex networks",
        "authors": "Sreejith, R. P., Mohanraj, Karthikeyan, Jost, J\u00fcrgen, Saucan, Emil, Samal, Areejit",
        "abstract": "We adapt Forman's discretization of Ricci curvature to the case of undirected networks, both weighted and unweighted, and investigate the measure in a variety of model and real-world networks. We find that most nodes and edges in model and real networks have a negative curvature. Furthermore, the distribution of Forman curvature of nodes and edges is narrow in random and small-world networks, while the distribution is broad in scale-free and real-world networks. In most networks, Forman curvature is found to display significant negative correlation with degree and centrality measures. However, Forman curvature is uncorrelated with clustering coefficient in most networks. Importantly, we find that both model and real networks are vulnerable to targeted deletion of nodes with highly negative Forman curvature. Our results suggest that Forman curvature can be employed to gain novel insights on the organization of complex networks.",
        "timestamp": "2025-04-16T03:18:41.434Z",
        "rating": "novote",
        "publishedDate": "2016/03/01",
        "tags": [
          "q-bio.MN",
          "cond-mat.dis-nn",
          "physics.soc-ph"
        ],
        "doi": "10.1088/1742-5468/2016/06/063206",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2900,
        "object_id": "paper:arxiv.1603.00386",
        "created_at": "2025-04-16T03:18:41+00:00",
        "updated_at": "2025-04-16T03:22:12+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2203.16837": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2203.16837",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-16T03:31:24.449Z",
            "data": {
              "session_id": "session_1744774284435_dmh5rrd",
              "source_id": "arxiv",
              "paper_id": "2203.16837",
              "start_time": "2025-04-16T03:31:17.554Z",
              "end_time": "2025-04-16T03:31:24.435Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2907,
        "object_id": "interactions:arxiv.2203.16837",
        "created_at": "2025-04-16T03:31:25+00:00",
        "updated_at": "2025-04-16T03:32:27+00:00",
        "version": 1
      }
    },
    "interactions:url.443ECA2F": {
      "data": {
        "sourceId": "url",
        "paperId": "443ECA2F",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-16T03:33:49.316Z",
            "data": {
              "session_id": "session_1744774428592_xcaorya",
              "source_id": "url",
              "paper_id": "443ECA2F",
              "start_time": "2025-04-16T03:33:41.821Z",
              "end_time": "2025-04-16T03:33:48.592Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2910,
        "object_id": "interactions:url.443ECA2F",
        "created_at": "2025-04-16T03:33:50+00:00",
        "updated_at": "2025-04-16T03:34:58+00:00",
        "version": 1
      }
    },
    "interactions:url.22D8AB8A": {
      "data": {
        "sourceId": "url",
        "paperId": "22D8AB8A",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-16T03:34:15.011Z",
            "data": {
              "session_id": "session_1744774454096_8e5uiq2",
              "source_id": "url",
              "paper_id": "22D8AB8A",
              "start_time": "2025-04-16T03:34:03.660Z",
              "end_time": "2025-04-16T03:34:14.096Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 0,
              "total_elapsed_seconds": 10
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2913,
        "object_id": "interactions:url.22D8AB8A",
        "created_at": "2025-04-16T03:34:15+00:00",
        "updated_at": "2025-04-16T03:37:24+00:00",
        "version": 1
      }
    },
    "paper:url.22D8AB8A": {
      "data": {
        "sourceId": "url",
        "paperId": "22D8AB8A",
        "url": "https://www.sciencedirect.com/science/article/pii/S000187081930369X",
        "title": "Ollivier Ricci curvature for general graph Laplacians: Heat equation, Laplacian comparison, non-explosion and diameter bounds",
        "authors": "",
        "abstract": "Discrete time random walks on a finite set naturally translate via a one-to-one correspondence to discrete Laplace operators. Typically, Ollivier curv\u2026",
        "timestamp": "2025-04-16T03:34:03.124Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1016/j.aim.2019.106759",
        "journalName": "Advances in Mathematics",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2911,
        "object_id": "paper:url.22D8AB8A",
        "created_at": "2025-04-16T03:34:03+00:00",
        "updated_at": "2025-04-16T03:36:44+00:00",
        "version": 1
      }
    },
    "paper:url.443ECA2F": {
      "data": {
        "sourceId": "url",
        "paperId": "443ECA2F",
        "url": "https://www.math.uchicago.edu/~shmuel/QuantCourse%20/Metric%20Space/Ollivier,%20Ricci%20curvature%20of%20Metric%20Spaces.pdf",
        "title": "443ECA2F",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-16T03:33:42.183Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 2909,
        "object_id": "paper:url.443ECA2F",
        "created_at": "2025-04-16T03:33:42+00:00",
        "updated_at": "2025-04-16T03:37:24+00:00",
        "version": 1
      }
    },
    "paper:url.8AF26E0": {
      "data": {
        "sourceId": "url",
        "paperId": "8AF26E0",
        "url": "https://www.nature.com/articles/s41598-019-46079-x",
        "title": "Ollivier-Ricci Curvature-Based Method to Community Detection in Complex Networks",
        "authors": "Sia, Jayson, Jonckheere, Edmond, Bogdan, Paul",
        "abstract": "Identification of community structures in complex network is of crucial importance for understanding the system\u2019s function, organization, robustness and security. Here, we present a novel Ollivier-Ricci curvature (ORC) inspired approach to community identification in complex networks. We demonstrate that the intrinsic geometric underpinning of the ORC offers a natural approach to discover inherent community structures within a network based on interaction among entities. We develop an ORC-based community identification algorithm based on the idea of sequential removal of negatively curved edges symptomatic of high interactions (e.g., traffic, attraction). To illustrate and compare the performance with other community identification methods, we examine the ORC-based algorithm with stochastic block model artificial networks and real-world examples ranging from social to drug-drug interaction networks. The ORC-based algorithm is able to identify communities with either better or comparable performance accuracy and to discover finer hierarchical structures of the network. This opens new geometric avenues for analysis of complex networks dynamics.",
        "timestamp": "2025-04-16T03:33:30.372Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "10.1038/s41598-019-46079-x",
        "journalName": "Scientific Reports",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2908,
        "object_id": "paper:url.8AF26E0",
        "created_at": "2025-04-16T03:33:30+00:00",
        "updated_at": "2025-04-16T03:38:03+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.1103.4037": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1103.4037",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-16T03:38:38.997Z",
            "data": {
              "session_id": "session_1744774718155_66sd6ec",
              "source_id": "arxiv",
              "paper_id": "1103.4037",
              "start_time": "2025-04-16T03:38:33.058Z",
              "end_time": "2025-04-16T03:38:38.155Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2917,
        "object_id": "interactions:arxiv.1103.4037",
        "created_at": "2025-04-16T03:38:39+00:00",
        "updated_at": "2025-04-16T03:39:39+00:00",
        "version": 1
      }
    },
    "paper:url.18613DD1": {
      "data": {
        "sourceId": "url",
        "paperId": "18613DD1",
        "url": "https://journals.aps.org/prresearch/pdf/10.1103/PhysRevResearch.3.013211",
        "title": "18613DD1",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-04-16T03:37:11.668Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2914,
        "object_id": "paper:url.18613DD1",
        "created_at": "2025-04-16T03:37:12+00:00",
        "updated_at": "2025-04-16T03:40:34+00:00",
        "version": 1
      }
    },
    "paper:arxiv.1103.4037": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1103.4037",
        "url": "https://arxiv.org/abs/1103.4037",
        "title": "Ollivier's Ricci curvature, local clustering and curvature dimension inequalities on graphs",
        "authors": "Jost, J\u00fcrgen, Liu, Shiping",
        "abstract": "In this paper, we explore the relationship between one of the most elementary and important properties of graphs, the presence and relative frequency of triangles, and a combinatorial notion of Ricci curvature. We employ a definition of generalized Ricci curvature proposed by Ollivier in a general framework of Markov processes and metric spaces and applied in graph theory by Lin-Yau. In analogy with curvature notions in Riemannian geometry, we interpret this Ricci curvature as a control on the amount of overlap between neighborhoods of two neighboring vertices. It is therefore naturally related to the presence of triangles containing those vertices, or more precisely, the local clustering coefficient, that is, the relative proportion of connected neighbors among all the neighbors of a vertex. This suggests to derive lower Ricci curvature bounds on graphs in terms of such local clustering coefficients. We also study curvature dimension inequalities on graphs, building upon previous work of several authors.",
        "timestamp": "2025-04-16T03:31:17.383Z",
        "rating": "novote",
        "publishedDate": "2011/03/21",
        "tags": [
          "math.CO",
          "math.DG",
          "math.MG",
          "math.PR"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2906,
        "object_id": "paper:arxiv.1103.4037",
        "created_at": "2025-04-16T03:31:17+00:00",
        "updated_at": "2025-04-16T03:40:07+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2404.07411": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.07411",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-16T03:43:24.943Z",
            "data": {
              "session_id": "session_1744775004120_mi089mt",
              "source_id": "arxiv",
              "paper_id": "2404.07411",
              "start_time": "2025-04-16T03:42:56.298Z",
              "end_time": "2025-04-16T03:43:24.120Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 3,
              "total_elapsed_seconds": 28
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2920,
        "object_id": "interactions:arxiv.2404.07411",
        "created_at": "2025-04-16T03:43:25+00:00",
        "updated_at": "2025-04-16T03:44:34+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2404.07411": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.07411",
        "url": "https://arxiv.org/abs/2404.07411",
        "title": "Joint mixed-effects models for causal inference in clustered network-based observational studies",
        "authors": "McNealis, Vanessa, Moodie, Erica E. M., Dean, Nema",
        "abstract": "Causal inference on populations embedded in social networks poses technical challenges, since the typical no interference assumption frequently does not hold. Existing methods developed in the context of network interference rely upon the assumption of no unmeasured confounding. However, when faced with multilevel network data, there may be a latent factor influencing both the exposure and the outcome at the cluster level. We propose a Bayesian inference approach that combines a joint mixed-effects model for the outcome and the exposure with direct standardization to identify and estimate causal effects in the presence of network interference and unmeasured cluster confounding. In simulations, we compare our proposed method with linear mixed and fixed effects models and show that unbiased estimation is achieved using the joint model. Having derived valid tools for estimation, we examine the effect of maternal college education on adolescent school performance using data from the National Longitudinal Study of Adolescent Health.",
        "timestamp": "2025-04-16T03:42:56.686Z",
        "rating": "novote",
        "publishedDate": "2024/04/11",
        "tags": [
          "stat.ME"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2919,
        "object_id": "paper:arxiv.2404.07411",
        "created_at": "2025-04-16T03:42:57+00:00",
        "updated_at": "2025-04-16T03:45:11+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2210.12048": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2210.12048",
        "url": "https://arxiv.org/abs/2210.12048",
        "title": "Ollivier-Ricci Curvature for Hypergraphs: A Unified Framework",
        "authors": "Coupette, Corinna, Dalleiger, Sebastian, Rieck, Bastian",
        "abstract": "Bridging geometry and topology, curvature is a powerful and expressive invariant. While the utility of curvature has been theoretically and empirically confirmed in the context of manifolds and graphs, its generalization to the emerging domain of hypergraphs has remained largely unexplored. On graphs, the Ollivier-Ricci curvature measures differences between random walks via Wasserstein distances, thus grounding a geometric concept in ideas from probability theory and optimal transport. We develop ORCHID, a flexible framework generalizing Ollivier-Ricci curvature to hypergraphs, and prove that the resulting curvatures have favorable theoretical properties. Through extensive experiments on synthetic and real-world hypergraphs from different domains, we demonstrate that ORCHID curvatures are both scalable and useful to perform a variety of hypergraph tasks in practice.",
        "timestamp": "2025-04-16T03:30:58.959Z",
        "rating": "novote",
        "publishedDate": "2022/10/21",
        "tags": [
          "cs.LG",
          "cs.SI",
          "stat.ML",
          "68R10"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2905,
        "object_id": "paper:arxiv.2210.12048",
        "created_at": "2025-04-16T03:30:59+00:00",
        "updated_at": "2025-04-16T03:40:41+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2203.16837": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2203.16837",
        "url": "https://arxiv.org/abs/2203.16837",
        "title": "Discrete Ollivier-Ricci curvature",
        "authors": "Fathi, Zohreh, Lakzian, Sajjad",
        "abstract": "We analyze both continuous and discrete-time Ollivier-Ricci curvatures of locally-finite weighted graphs $\\G$ equipped with a given distance \"$\\dist$\" (w.r.t. which $\\G$ is metrically complete) and for general random walks. We show the continuous-time Ollivier-Ricci curvature is well-defined for a large class of Markovian and non-Markovian random walks and provide a criterion for existence of continuous-time Ollivier-Ricci curvature; the said results generalize the previous rather limited constructions in the literature.\nIn addition, important properties of both discrete-time and continuous-time Ollivier-Ricci curvatures are obtained including -- to name a few -- Lipschitz continuity, concavity properties, piece-wise regularity (piece-wise linearity in the case of linear walks) for the discrete-time Ollivier-Ricci as well as Lipschitz continuity and limit-free formulation for the continuous-time Ollivier-Ricci. these properties were previously known only for very specific distances and very specific random walks. As an application of Lipschitz continuity, we obtain existence and uniqueness of generalized continuous-time Ollivier-Ricci curvature flows.\nAlong the way, we obtain -- by optimizing McMullen's upper bounds -- a sharp upper bound estimate on the number of vertices of a convex polytope in terms of number of its facets and the ambient dimension, which might be of independent interest in convex geometry. The said upper bound allows us to bound the number of polynomial pieces of the discrete-time Ollivier-Ricci curvature as a function of time in the time-polynomial random walk. The limit-free formulation we establish allows us to define an operator theoretic Ollivier-Ricci curvature which is a non-linear concave functional on suitable operator spaces.",
        "timestamp": "2025-04-16T03:30:36.941Z",
        "rating": "novote",
        "publishedDate": "2022/03/31",
        "tags": [
          "math.MG",
          "math.CA",
          "math.DG",
          "math.FA",
          "math.PR",
          "Primary: 52xx, 47Dxx, Secondary: 05Cxx, 51Fxx"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2904,
        "object_id": "paper:arxiv.2203.16837",
        "created_at": "2025-04-16T03:30:37+00:00",
        "updated_at": "2025-04-16T03:41:38+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.08842": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.08842",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-16T03:44:59.652Z",
            "data": {
              "session_id": "session_1744775099228_7rht7e6",
              "source_id": "arxiv",
              "paper_id": "2504.08842",
              "start_time": "2025-04-16T03:43:30.908Z",
              "end_time": "2025-04-16T03:44:59.228Z",
              "heartbeat_count": 17,
              "duration_seconds": 85,
              "idle_seconds": 3,
              "total_elapsed_seconds": 88
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-04-16T03:51:48.638Z",
            "data": {
              "session_id": "session_1744775508122_xeomgsq",
              "source_id": "arxiv",
              "paper_id": "2504.08842",
              "start_time": "2025-04-16T03:44:59.230Z",
              "end_time": "2025-04-16T03:51:48.122Z",
              "heartbeat_count": 81,
              "duration_seconds": 405,
              "idle_seconds": 4,
              "total_elapsed_seconds": 409
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2921,
        "object_id": "interactions:arxiv.2504.08842",
        "created_at": "2025-04-16T03:45:00+00:00",
        "updated_at": "2025-04-16T03:52:49+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.08842": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.08842",
        "url": "https://arxiv.org/abs/2504.08842",
        "title": "Towards Combinatorial Interpretability of Neural Computation",
        "authors": "Adler, Micah, Alistarh, Dan, Shavit, Nir",
        "abstract": "We introduce combinatorial interpretability, a methodology for understanding neural computation by analyzing the combinatorial structures in the sign-based categorization of a network's weights and biases. We demonstrate its power through feature channel coding, a theory that explains how neural networks compute Boolean expressions and potentially underlies other categories of neural network computation. According to this theory, features are computed via feature channels: unique cross-neuron encodings shared among the inputs the feature operates on. Because different feature channels share neurons, the neurons are polysemantic and the channels interfere with one another, making the computation appear inscrutable. We show how to decipher these computations by analyzing a network's feature channel coding, offering complete mechanistic interpretations of several small neural networks that were trained with gradient descent. Crucially, this is achieved via static combinatorial analysis of the weight matrices, without examining activations or training new autoencoding networks. Feature channel coding reframes the superposition hypothesis, shifting the focus from neuron activation directionality in high-dimensional space to the combinatorial structure of codes. It also allows us for the first time to exactly quantify and explain the relationship between a network's parameter size and its computational capacity (i.e. the set of features it can compute with low error), a relationship that is implicitly at the core of many modern scaling laws. Though our initial studies of feature channel coding are restricted to Boolean functions, we believe they provide a rich, controlled, and informative research space, and that the path we propose for combinatorial interpretation of neural computation can provide a basis for understanding both artificial and biological neural circuits.",
        "timestamp": "2025-04-16T03:42:36.282Z",
        "rating": "novote",
        "publishedDate": "2025/04/10",
        "tags": [
          "cs.LG",
          "cs.NE",
          "I.2.0"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2918,
        "object_id": "paper:arxiv.2504.08842",
        "created_at": "2025-04-16T03:42:36+00:00",
        "updated_at": "2025-04-16T03:45:58+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2409.15318": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2409.15318",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-16T03:52:27.017Z",
            "data": {
              "session_id": "session_1744775547004_53myxl6",
              "source_id": "arxiv",
              "paper_id": "2409.15318",
              "start_time": "2025-04-16T03:52:12.367Z",
              "end_time": "2025-04-16T03:52:27.004Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 5,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2923,
        "object_id": "interactions:arxiv.2409.15318",
        "created_at": "2025-04-16T03:52:28+00:00",
        "updated_at": "2025-04-16T03:53:27+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2412.02159": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2412.02159",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-16T03:55:33.520Z",
            "data": {
              "session_id": "session_1744775732772_jtseqdl",
              "source_id": "arxiv",
              "paper_id": "2412.02159",
              "start_time": "2025-04-16T03:55:13.298Z",
              "end_time": "2025-04-16T03:55:32.772Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 4,
              "total_elapsed_seconds": 19
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2925,
        "object_id": "interactions:arxiv.2412.02159",
        "created_at": "2025-04-16T03:55:34+00:00",
        "updated_at": "2025-04-16T03:56:35+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2409.15318": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2409.15318",
        "url": "https://arxiv.org/abs/2409.15318",
        "title": "On the Complexity of Neural Computation in Superposition",
        "authors": "Adler, Micah, Shavit, Nir",
        "abstract": "Recent advances in the understanding of neural networks suggest that superposition, the ability of a single neuron to represent multiple features simultaneously, is a key mechanism underlying the computational efficiency of large-scale networks. This paper explores the theoretical foundations of computing in superposition, focusing on explicit, provably correct algorithms and their efficiency. We present the first lower bounds showing that for a broad class of problems, including permutations and pairwise logical operations, a neural network computing in superposition requires at least $\\Omega(m' \\log m')$ parameters and $\\Omega(\\sqrt{m' \\log m'})$ neurons, where $m'$ is the number of output features being computed. This implies that any ``lottery ticket'' sparse sub-network must have at least $\\Omega(m' \\log m')$ parameters no matter what the initial dense network size. Conversely, we show a nearly tight upper bound: logical operations like pairwise AND can be computed using $O(\\sqrt{m'} \\log m')$ neurons and $O(m' \\log^2 m')$ parameters. There is thus an exponential gap between computing in superposition, the subject of this work, and representing features in superposition, which can require as little as $O(\\log m'$) neurons based on the Johnson-Lindenstrauss Lemma. Our hope is that our results open a path for using complexity theoretic techniques in neural network interpretability research.",
        "timestamp": "2025-04-16T03:52:07.393Z",
        "rating": "novote",
        "publishedDate": "2024/09/05",
        "tags": [
          "cs.CC",
          "cs.AI",
          "cs.DS",
          "cs.NE",
          "F.1.1; F.2.2; I.2.m; E.3"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2922,
        "object_id": "paper:arxiv.2409.15318",
        "created_at": "2025-04-16T03:52:07+00:00",
        "updated_at": "2025-04-16T03:55:59+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.10664": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.10664",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-16T04:08:48.773Z",
            "data": {
              "session_id": "session_1744776527903_59emz80",
              "source_id": "arxiv",
              "paper_id": "2504.10664",
              "start_time": "2025-04-16T04:08:04.391Z",
              "end_time": "2025-04-16T04:08:47.903Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 4,
              "total_elapsed_seconds": 44
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2940,
        "object_id": "interactions:arxiv.2504.10664",
        "created_at": "2025-04-16T04:08:49+00:00",
        "updated_at": "2025-04-16T04:15:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.10664": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.10664",
        "url": "https://arxiv.org/abs/2504.10664",
        "title": "A cute proof that makes $e$ natural",
        "authors": "Loh, Po-Shen",
        "abstract": "The number $e$ has rich connections throughout mathematics, and has the honor of being the base of the natural logarithm. However, most students finish secondary school (and even university) without suitably memorable intuition for why $e$'s various mathematical properties are related. This article presents a solution. Various proofs for all of the mathematical facts in this article have been well-known for years. This exposition contributes a short, conceptual, intuitive, and visual proof (comprehensible to Pre-Calculus students) of the equivalence of two of the most commonly-known properties of $e$, connecting the continuously-compounded-interest limit $\\big(1 + \\frac{1}{n}\\big)^n$ to the fact that $e^x$ is its own derivative. The exposition further deduces a host of commonly-taught properties of $e$, while minimizing pre-requisite knowledge, so that this article can be practically used for developing secondary school curricula. Since $e$ is such a well-trodden concept, it is hard to imagine that our visual proof is new, but it certainly is not widely known. The author checked 100 books across 7 countries, as well as YouTube videos totaling over 25 million views, and still has not found this method taught anywhere. This article seeks to popularize the 3-page explanation of $e$, while providing a unified, practical, and open-access reference for teaching about $e$.",
        "timestamp": "2025-04-16T04:08:04.921Z",
        "rating": "novote",
        "publishedDate": "2025/04/14",
        "tags": [
          "math.HO",
          "97D80 (Primary), 97I40 (Secondary)"
        ],
        "doi": null,
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2939,
        "object_id": "paper:arxiv.2504.10664",
        "created_at": "2025-04-16T04:08:05+00:00",
        "updated_at": "2025-04-16T04:17:00+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2405.15756": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2405.15756",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-16T03:57:58.024Z",
            "data": {
              "session_id": "session_1744775878011_j34km45",
              "source_id": "arxiv",
              "paper_id": "2405.15756",
              "start_time": "2025-04-16T03:56:22.738Z",
              "end_time": "2025-04-16T03:57:58.011Z",
              "heartbeat_count": 19,
              "duration_seconds": 95,
              "idle_seconds": 0,
              "total_elapsed_seconds": 95
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2927,
        "object_id": "interactions:arxiv.2405.15756",
        "created_at": "2025-04-16T03:57:58+00:00",
        "updated_at": "2025-04-16T04:15:58+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2302.07348": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2302.07348",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-04-16T03:58:45.092Z",
            "data": {
              "session_id": "session_1744775924368_y1ss5n0",
              "source_id": "arxiv",
              "paper_id": "2302.07348",
              "start_time": "2025-04-16T03:58:19.412Z",
              "end_time": "2025-04-16T03:58:44.368Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 5,
              "total_elapsed_seconds": 25
            }
          }
        ]
      },
      "meta": {
        "issue_number": 2930,
        "object_id": "interactions:arxiv.2302.07348",
        "created_at": "2025-04-16T03:58:46+00:00",
        "updated_at": "2025-04-16T04:15:57+00:00",
        "version": 1
      }
    },
    "paper:url.26FF84B7": {
      "data": {
        "sourceId": "url",
        "paperId": "26FF84B7",
        "url": "https://www.annualreviews.org/content/journals/10.1146/annurev-virology-093022-013037",
        "title": "The Emergence and Evolution of SARS-CoV-2",
        "authors": "Edward C. Holmes",
        "abstract": "The origin of SARS-CoV-2 has evoked heated debate and strong accusations, yet seemingly little resolution. I review the scientific evidence on the origin of SARS-CoV-2 and its subsequent spread through the human population. The available data clearly point to a natural zoonotic emergence within, or closely linked to, the Huanan Seafood Wholesale Market in Wuhan. There is no direct evidence linking the emergence of SARS-CoV-2 to laboratory work conducted at the Wuhan Institute of Virology. The subsequent global spread of SARS-CoV-2 was characterized by a gradual adaptation to humans, with dual increases in transmissibility and virulence until the emergence of the Omicron variant. Of note has been the frequent transmission of SARS-CoV-2 from humans to other animals, marking it as a strongly host generalist virus. Unless lessons from the origin of SARS-CoV-2 are learned, it is inevitable that more zoonotic events leading to more epidemics and pandemics will plague human populations.",
        "timestamp": "2025-04-16T04:35:29.032Z",
        "rating": "novote",
        "publishedDate": "2024/09/26",
        "tags": [],
        "doi": "10.1146/annurev-virology-093022-013037",
        "journalName": "Annual Review of Virology",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2944,
        "object_id": "paper:url.26FF84B7",
        "created_at": "2025-04-16T04:35:29+00:00",
        "updated_at": "2025-04-16T04:36:31+00:00",
        "version": 1
      }
    },
    "paper:url.6B526954": {
      "data": {
        "sourceId": "url",
        "paperId": "6B526954",
        "url": "https://www.science.org/doi/full/10.1126/science.abp8337",
        "title": "The molecular epidemiology of multiple zoonotic origins of SARS-CoV-2",
        "authors": "",
        "abstract": "SARS-CoV-2 genomic diversity early in the COVID-19 pandemic points to emergence involving at least two zoonotic events.",
        "timestamp": "2025-04-16T04:35:14.164Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "Science",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2943,
        "object_id": "paper:url.6B526954",
        "created_at": "2025-04-16T04:35:14+00:00",
        "updated_at": "2025-04-16T04:37:02+00:00",
        "version": 1
      }
    },
    "paper:url.720A87C5": {
      "data": {
        "sourceId": "url",
        "paperId": "720A87C5",
        "url": "https://journals.asm.org/doi/full/10.1128/jvi.01240-24",
        "title": "The harms of promoting the lab leak hypothesis for SARS-CoV-2 origins without evidence | Journal of Virology",
        "authors": "",
        "abstract": "On Monday, June 3, 2024, Dr. Anthony Fauci, former Director of the National Institutes\nof Allergy and Infectious Diseases and a dedicated public health professional for\nover 40 years, testified voluntarily before the House subcommittee investigating the COVID-19 pandemic. Among\nother topics, he was asked about the origin of SARS-CoV-2, the coronavirus that causes\nCOVID-19. The hearing was often disrupted and marked by contentious, disrespectful,\nand unfounded calls for Dr. Fauci to be \u201cprosecuted\u201d and imprisoned for \u201ccrimes against\nhumanity.\u201d",
        "timestamp": "2025-04-16T04:34:39.646Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "SARS-CoV-2",
          "COVID-19",
          "pandemic",
          "origin",
          "lab leak",
          "zoonosis",
          "science policy",
          "science advocacy",
          "anti-science",
          "spillover"
        ],
        "doi": "",
        "journalName": "Journal of Virology",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2942,
        "object_id": "paper:url.720A87C5",
        "created_at": "2025-04-16T04:34:40+00:00",
        "updated_at": "2025-04-16T04:37:32+00:00",
        "version": 1
      }
    },
    "paper:url.6B9F612": {
      "data": {
        "sourceId": "url",
        "paperId": "6B9F612",
        "url": "https://www.cell.com/cell/fulltext/S0092-8674(24)00901-2?_hsmi=324423428",
        "title": "Genetic tracing of market wildlife and viruses at the epicenter of the COVID-19 pandemic",
        "authors": "Alexander Crits-Christoph, Joshua I. Levy, Jonathan E. Pekar, Stephen A. Goldstein, Reema Singh, Zach Hensel, Karthik Gangavarapu, Matthew B. Rogers, Niema Moshiri, Robert F. Garry, Edward C. Holmes, Marion P.G. Koopmans, Philippe Lemey, Thomas P. Peacock, Saskia Popescu, Andrew Rambaut, David L. Robertson, Marc A. Suchard, Joel O. Wertheim, Angela L. Rasmussen, Kristian G. Andersen, Michael Worobey, Florence D\u00e9barre",
        "abstract": "<h2>Summary</h2><p>Zoonotic spillovers of viruses have occurred through the animal trade worldwide. The start of the COVID-19 pandemic was traced epidemiologically to the Huanan Seafood Wholesale Market. Here, we analyze environmental qPCR and sequencing data collected in the Huanan market in early 2020. We demonstrate that market-linked severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) genetic diversity is consistent with market emergence and find increased SARS-CoV-2 positivity near and within a wildlife stall. We identify wildlife DNA in all SARS-CoV-2-positive samples from this stall, including species such as civets, bamboo rats, and raccoon dogs, previously identified as possible intermediate hosts. We also detect animal viruses that infect raccoon dogs, civets, and bamboo rats. Combining metagenomic and phylogenetic approaches, we recover genotypes of market animals and compare them with those from farms and other markets. This analysis provides the genetic basis for a shortlist of potential intermediate hosts of SARS-CoV-2 to prioritize for serological and viral sampling.</p>",
        "timestamp": "2025-04-16T04:34:26.621Z",
        "rating": "novote",
        "publishedDate": "2024/09/19",
        "tags": [],
        "doi": "10.1016/j.cell.2024.08.010",
        "journalName": "Cell",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2941,
        "object_id": "paper:url.6B9F612",
        "created_at": "2025-04-16T04:34:27+00:00",
        "updated_at": "2025-04-16T04:38:02+00:00",
        "version": 1
      }
    }
  }
}