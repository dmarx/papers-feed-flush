---
File: scripts/hydrate_metadata.py
---
# this could probably be rolled into enrichment processing
#!/usr/bin/env python
# fetch_arxiv_metadata.py
"""
Fetches metadata for arXiv papers identified by issue labels and stores it using gh-store.
"""

import json
import sys
import os
import re
from typing import Dict, List, Optional, Any
import fire
from loguru import logger
import arxiv
import requests

from gh_store.core.store import GitHubStore
from gh_store.tools.canonicalize import CanonicalStore
#from gh_store.tools.canonicalize import CanonicalStore as GitHubStore
from gh_store.core.constants import LabelNames
from gh_store.core.types import get_object_id_from_labels, StoredObject
from gh_store.core.exceptions import DuplicateUIDError, ConcurrentUpdateError

def is_metadata_satisfied(data: dict) -> bool:
    return data and data.get('title') and not (data.get('id') in data.get('title'))

def is_valid_arxiv_id(arxiv_id: str) -> bool:
    """Validate arXiv ID format."""
    return bool(re.match(r'\d{4}\.\d{4,5}(v\d+)?|\w+\/\d{7}(v\d+)?', arxiv_id))

def extract_arxiv_id_from_object_id(object_id: str) -> str:
    """Extract the arXiv ID from a paper ID with various prefixing schemes."""
    prefix = 'arxiv'
    
    # Case 1: Format is "prefix:id"
    if object_id.startswith(f"{prefix}:"):
        return object_id[len(prefix)+1:]
    
    # Case 2: Format is "prefix.id"
    if object_id.startswith(f"{prefix}."):
        return object_id[len(prefix)+1:]
    
    # Case 3: Format is "prefix:prefix:id"
    if object_id.startswith(f"{prefix}:{prefix}:"):
        return object_id[len(prefix)*2+2:]
    
    # Case 4: Format is "prefix.prefix.id"
    if object_id.startswith(f"{prefix}.{prefix}."):
        return object_id[len(prefix)*2+2:]
    
    # Case 5: If none of the above, return the original ID
    return object_id

def fetch_arxiv_metadata(arxiv_id: str) -> Dict[str, Any]:
    """Fetch metadata from arXiv API for a given ID using the arxiv client."""
    logger.info(f"Fetching metadata for arXiv ID: {arxiv_id}")
    
    client = arxiv.Client()
    search = arxiv.Search(id_list=[arxiv_id])
    paper = next(client.results(search))
    if not paper:
        raise ValueError(f"No paper found with arXiv ID: {arxiv_id}")
    
    metadata = {
        #'id': paper.entry_id,
        'title': paper.title,
        'authors': [author.name for author in paper.authors],
        'publishedDate': paper.published.isoformat() if paper.published else None,
        #'updated': paper.updated.isoformat() if paper.updated else None,
        'doi': paper.doi,
        'tags': paper.categories,
        'abstract': paper.summary,
        #'links': [{'href': link.href, 'type': link.type} for link in paper.links],
        #'comment': paper.comment,
        #'journal_ref': paper.journal_ref,
        #'primary_category': paper.primary_category,
        #'pdf_url': paper.pdf_url,
    }
    
    logger.info(f"Successfully fetched metadata for arXiv ID: {arxiv_id}")
    logger.info(metadata)
    return metadata
    

def hydrate_issue_metadata(issue: int, token:str, repo:str):
    #store = GitHubStore(token=token, repo=repo, config_path=None)
    store = CanonicalStore(token=token, repo=repo, config_path=None)
    
    obj = store.issue_handler.get_object_by_number(issue)
    object_id = obj.meta.object_id
    #object_id = get_object_id_from_labels(issue)
    if not object_id.startswith("paper:"):
        logger.info("Not a paper object, exiting.")
        sys.exit(0)
    if 'url' in object_id:
        logger.info("Metadata hydration is currently only supported for the arxiv source type.")
        store.process_updates(issue) # ...why is this a separate second step? sheesh, I reaaly did rube goldberg the shit out of this thing
        return
        
    
    paper_id = object_id[len('paper:'):]
    if paper_id.startswith('arxiv'):
        arxiv_id = extract_arxiv_id_from_object_id(paper_id)
    elif is_valid_arxiv_id(paper_id):
        arxiv_id = paper_id
    else:
        raise TypeError(f"Unable to identify arxiv_id from object_id: {object_id}")

    updates = {}
    arxiv_meta = fetch_arxiv_metadata(arxiv_id)
    for k, v_new in arxiv_meta.items():
        #v_old = getattr(obj.data, k)
        v_old = obj.data.get(k)
        if not v_old:
            updates[k] = v_new

    metadata_satisfied = False
    if updates:
        # Issue is open because we are processing it right now, which acts as an implicit lock on updates.
        # so we close it before pushing the new update
        #store.repo.get_issue(issue).edit(state='closed') # ...this is awkward af. in fact, I think I should just eliminate that whole ConcurrentUpdateError
        # finally: what we came here for
        store.update(object_id=object_id, changes=updates)
        store.process_updates(issue) # ...why is this a separate second step? sheesh, I reaaly did rube goldberg the shit out of this thing
        metadata_satisfied = True
    else:
        metadata_satisfied = is_metadata_satisfied(obj.data)

    if metadata_satisfied:
        store.repo.get_issue(issue).remove_from_labels("TODO:hydrate-metadata")    

# TODO: upstream this to gh-store utilities
def get_open_issues(token:str, repo:str, extra_labels: list|None = None):
    store = GitHubStore(token=token, repo=repo, config_path=None)
    #store = CanonicalStore(token=token, repo=repo, config_path=None)
    
    query_labels = [LabelNames.GH_STORE, LabelNames.STORED_OBJECT]
    if extra_labels: # 
        query_labels += extra_labels
    return store.repo.get_issues(
            labels=query_labels,
            state="open"
        )

def hydrate_all_open_issues(token:str, repo:str):
    store = CanonicalStore(token=token, repo=repo, config_path=None)
    for issue in get_open_issues(token=token, repo=repo, extra_labels=["TODO:hydrate-metadata"]):
        try:
            hydrate_issue_metadata(issue=issue.number, token=token, repo=repo)
        except TypeError:
            logger.info("unsupported source for issue %s", issue.number)
        except DuplicateUIDError:
            #logger.info("Issue %s has dupes, skipping for now. Run deduplification." % issue.number)
            logger.info("Issue %s has dupes. Running deduplification." % issue.number)
            #object_id = StoredObject.from_issue(issue).object_id
            object_id = get_object_id_from_labels(issue)
            dedupe_status = store.deduplicate_object(object_id)
            hydrate_issue_metadata(issue=dedupe_status.get('canonical_issue'), token=token, repo=repo)
        except ConcurrentUpdateError:
            logger.info("Issue %s has too many unprocessed concurrent updates. Either adjust this threshold, or reconcile the updates manually.", issue.number)

# class Main:
#     def hydrate_issue_metadata(self, issue: int, token:str, repo:str):
#         hydrate_issue_metadata(issue=issue, token=token, repo=repo)

#     def hydrate_all_open_issues(self, token:str, repo:str):
#         hydrate_all_open_issues(token=token, repo=repo)


if __name__ == "__main__":
    #fire.Fire(Main)
    fire.Fire(
        { "hydrate_issue_metadata":hydrate_issue_metadata, "hydrate_all_open_issues":hydrate_all_open_issues }
    )



---
File: scripts/process_pdf.py
---
# .github/scripts/process_pdf.py

import os
from pathlib import Path
from typing import Literal

import fire
import requests
from loguru import logger
from lxml import etree
from llamero.utils import commit_and_push

OutputFormat = Literal['markdown', 'tei']

def remove_extra_whitespace(text: str)->str:
    while '\n\n\n' in text:
        text = text.replace('\n\n\n', '\n\n')
    return text

def remove_gibberish(
    text: str,
    cutoff=2000
)->str:
    good_lines = []
    for line in text.split('\n'):
        _line = line[:]
        if _line.startswith("$"):
            _line = _line[1:-1]
        n_tok = len(_line)
        n_space = _line.count(" ")
        # I think this might remove some formulas if we use cutoff=0
        token_sparsity=1
        if n_tok:
            token_sparsity = n_space/n_tok
        
        _line = line[:]
        _line = _line.replace(" ","")

        skip=False
        if (abs(token_sparsity - .5) < .01) and (len(line) > cutoff):
            skip=True
        if "texitsha1_base64" in _line:
            skip=True
        if "texit>" in _line:
            skip=True
        if skip:
            logger.info(f"removing gibberish")
            logger.info(line)
            continue
        good_lines.append(line)
    return '\n'.join(good_lines)

def sanitize_markdown(text: str)->str:
    text=remove_extra_whitespace(text)
    text=remove_gibberish(text)
    return text

def get_feature_path(base_path: Path, feature_type: str, paper_id: str, ext: str) -> Path:
    """Create feature directory if it doesn't exist and return the full path."""
    feature_dir = base_path / 'features' / feature_type
    feature_dir.mkdir(parents=True, exist_ok=True)
    return feature_dir / f"{paper_id}{ext}"

def process_pdf_grobid(
    pdf_path: str, 
    format: OutputFormat = 'markdown', 
    tag: str = "grobid",
    output_path: str | None = None,
    regenerate_tei: bool = True,
) -> None:
    """
    Process a PDF file using Grobid and convert to the specified format.
    
    Output files will be saved in feature-specific directories:
    - TEI XML files go to features/tei-xml-grobid/
    - Markdown files go to features/markdown-grobid/
    
    Args:
        pdf_path: Path to the PDF file relative to the repository root.
        format: Output format, either 'markdown' or 'tei'.
        tag: Optional tag to append to the output filename (default: "grobid").
        output_path: Optional path where the output file should be saved. If provided,
            this overrides the default feature directory behavior.
        regenerate_tei: Whether to regenerate TEI XML even if it exists.
    """
    pdf_path = Path(pdf_path)
    if not pdf_path.exists():
        raise FileNotFoundError(f"PDF file not found: {pdf_path}")

    # Get paper directory
    paper_dir = pdf_path.parent

    # Generate paper ID from PDF filename
    paper_id = pdf_path.stem

    # Determine output paths
    if output_path:
        output_path = Path(output_path)
        tei_path = output_path.with_suffix('.tei.xml')
        md_path = output_path.with_suffix('.md')
    else:
        # Use feature directory structure
        tei_path = get_feature_path(paper_dir, f'tei-xml-{tag}', paper_id, '.xml')
        md_path = get_feature_path(paper_dir, f'markdown-{tag}', paper_id, '.md')
    
    logger.info(f"Processing {pdf_path}")
    logger.info(f"TEI output will go to {tei_path}")
    logger.info(f"Markdown output will go to {md_path}")

    if regenerate_tei or (not tei_path.exists()):
        grobid_host = os.environ.get('GROBID_HOST', 'localhost')
        base_url = f"http://{grobid_host}:8070"
        
        # Call Grobid to process the PDF into TEI XML
        with open(pdf_path, 'rb') as f:
            files = {'input': (pdf_path.name, f, 'application/pdf')}
            resp = requests.post(
                f"{base_url}/api/processFulltextDocument",
                files=files,
                headers={'Accept': 'application/xml'},
                timeout=300  # 5 minute timeout
            )
        
        if resp.status_code != 200:
            raise RuntimeError(f"Grobid processing failed: {resp.status_code}")
        
        # Ensure the feature directory exists and save the TEI output
        tei_path.parent.mkdir(parents=True, exist_ok=True)
        tei_path.write_text(resp.text)
        logger.info(f"Saved TEI XML to {tei_path}")
    
    if format == 'markdown':
        # Convert TEI to Markdown using XSLT
        xslt_path = Path(__file__).parent / 'tei2md.xslt'
        if not xslt_path.exists():
            raise FileNotFoundError(f"XSLT stylesheet not found: {xslt_path}")
        
        xslt = etree.parse(str(xslt_path))
        transform = etree.XSLT(xslt)
        
        tei_doc = etree.parse(str(tei_path))
        markdown = str(transform(tei_doc))
        markdown = sanitize_markdown(markdown)
        
        # Ensure the feature directory exists and save Markdown output
        md_path.parent.mkdir(parents=True, exist_ok=True)
        md_path.write_text(markdown)
        logger.info(f"Saved Markdown to {md_path}")
    else:
        logger.info(f"Output TEI XML saved at {tei_path}")

process_pdf = process_pdf_grobid

# Files to ignore during operations
ignore_files = [
    "gh-store-snapshot.json",
    "papers-archive.json",
    "papers.json",
    "papers.yaml"
]

# def flush_old_conversions(data_path: str = "data/papers", tag: str = "grobid"):
#     """
#     Remove all previous conversions with the specified tag from feature directories.
#     """
#     base_path = Path(data_path).parent
#     tei_dir = base_path / 'features' / f'tei-xml-{tag}'
#     md_dir = base_path / 'features' / f'markdown-{tag}'
    
#     if tei_dir.exists():
#         for fpath in tei_dir.glob("*.xml"):
#             fpath.unlink()
#         tei_dir.rmdir()
    
#     if md_dir.exists():
#         for fpath in md_dir.glob("*.md"):
#             fpath.unlink()
#         md_dir.rmdir()

def generate_missing_conversions(
    data_path: str = "data/papers",
    tag: str = "grobid",
    checkpoint_cadence=5,
    regenerate_tei: bool = True,
):
    """
    Generate missing conversions for PDFs, saving outputs to feature directories.
    """
    data_path = Path(data_path)
    modified_files = []
    
    for i, pdf_fpath in enumerate(data_path.rglob("*.pdf")):
        # Skip PDFs in source directories
        if "source" in str(pdf_fpath):
            continue
            
        # Determine feature paths
        #base_dir = pdf_fpath.parent.parent
        paper_dir = pdf_fpath.parent
        paper_id = pdf_fpath.stem
        md_path = get_feature_path(paper_dir, f'markdown-{tag}', paper_id, '.md')
        
        if not md_path.exists():
            process_pdf_grobid(pdf_fpath, regenerate_tei=regenerate_tei)
            # Add both markdown and TEI paths
            tei_path = get_feature_path(paper_dir, f'tei-xml-{tag}', paper_id, '.xml')
            modified_files.extend([md_path, tei_path])
            logger.info(f"Generated conversions for {pdf_fpath.name}")
            
        if (i % checkpoint_cadence) == 0 and modified_files:
            msg = "Persisting feature conversions"
            commit_and_push(files_to_commit=modified_files, message=msg)
            modified_files = []
            
    if modified_files:
        commit_and_push(files_to_commit=modified_files, message="Persisting remaining feature conversions")

if __name__ == '__main__':
    fire.Fire({
        "process_pdf": process_pdf,
        "generate_missing_conversions": generate_missing_conversions,
        #"flush_old_conversions": flush_old_conversions,
    })


